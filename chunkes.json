{"d76189f2-3c59-488a-bf2a-d8f2105cd65b": {"text": "1\nModular RAG: Transforming RAG Systems into\nLEGO-like Reconfigurable Frameworks\nYunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\nAbstract\u2014Retrieval-augmented Generation (RAG) has\nmarkedly enhanced the capabilities of Large Language Models\n(LLMs) in tackling knowledge-intensive tasks. The increasing\ndemands of application scenarios have driven the evolution\nof RAG, leading to the integration of advanced retrievers,\nLLMs and other complementary technologies, which in turn\nhas amplified the intricacy of RAG systems. However, the rapid\nadvancements are outpacing the foundational RAG paradigm,\nwith many methods struggling to be unified under the process\nof \u201cretrieve-then-generate\u201d. In this context, this paper examines\nthe limitations of the existing RAG paradigm and introduces\nthe modular RAG framework. By decomposing complex RAG\nsystems into independent modules and specialized operators, it\nfacilitates a highly reconfigurable framework. Modular RAG\ntranscends the traditional linear architecture, embracing a\nmore advanced design that integrates routing, scheduling, and\nfusion mechanisms. Drawing on extensive research, this paper\nfurther identifies prevalent RAG patterns\u2014linear, conditional,\nbranching, and looping\u2014and offers a comprehensive analysis\nof their respective implementation nuances. Modular RAG\npresents innovative opportunities for the conceptualization\nand deployment of RAG systems. Finally, the paper explores\nthe potential emergence of new operators and paradigms,\nestablishing a solid theoretical foundation and a practical\nroadmap for the continued evolution and practical deployment\nof RAG technologies.\nIndex Terms\u2014Retrieval-augmented generation, large language\nmodel, modular system, information retrieval\nI. I NTRODUCTION\nL\nARGE Language Models (LLMs) have demonstrated\nremarkable capabilities, yet they still face numerous\nchallenges, such as hallucination and the lag in information up-\ndates [1]. Retrieval-augmented Generation (RAG), by access-\ning external knowledge bases, provides LLMs with important\ncontextual information, significantly enhancing their perfor-\nmance on knowledge-intensive tasks [2]. Currently, RAG, as\nan enhancement method, has been widely applied in various\npractical application scenarios, including knowledge question\nanswering, recommendation systems, customer service, and\npersonal assistants. [3]\u2013[6]\nDuring the nascent stages of RAG , its core framework is\nconstituted by indexing, retrieval, and generation, a paradigm\nreferred to as Naive RAG [7]. However, as the complexity", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "ed42e3cd-32ae-4bcc-ad2f-cc77d239fad7": {"text": "personal assistants. [3]\u2013[6]\nDuring the nascent stages of RAG , its core framework is\nconstituted by indexing, retrieval, and generation, a paradigm\nreferred to as Naive RAG [7]. However, as the complexity\nof tasks and the demands of applications have escalated, the\nYunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\nSystems, Tongji University, Shanghai, 201210, China.\nYun Xiong is with Shanghai Key Laboratory of Data Science, School of\nComputer Science, Fudan University, Shanghai, 200438, China.\nMeng Wang and Haofen Wang are with College of Design and Innovation,\nTongji University, Shanghai, 20092, China. (Corresponding author: Haofen\nWang. E-mail: carter.whfcarter@gmail.com)\nlimitations of Naive RAG have become increasingly apparent.\nAs depicted in Figure 1, it predominantly hinges on the\nstraightforward similarity of chunks, result in poor perfor-\nmance when confronted with complex queries and chunks with\nsubstantial variability. The primary challenges of Naive RAG\ninclude: 1) Shallow Understanding of Queries. The semantic\nsimilarity between a query and document chunk is not always\nhighly consistent. Relying solely on similarity calculations\nfor retrieval lacks an in-depth exploration of the relationship\nbetween the query and the document [8]. 2) Retrieval Re-\ndundancy and Noise. Feeding all retrieved chunks directly\ninto LLMs is not always beneficial. Research indicates that\nan excess of redundant and noisy information may interfere\nwith the LLM\u2019s identification of key information, thereby\nincreasing the risk of generating erroneous and hallucinated\nresponses. [9]\nTo overcome the aforementioned limitations, Advanced\nRAG paradigm focuses on optimizing the retrieval phase,\naiming to enhance retrieval efficiency and strengthen the\nutilization of retrieved chunks. As shown in Figure 1 ,typical\nstrategies involve pre-retrieval processing and post-retrieval\nprocessing. For instance, query rewriting is used to make\nthe queries more clear and specific, thereby increasing the\naccuracy of retrieval [10], and the reranking of retrieval results\nis employed to enhance the LLM\u2019s ability to identify and\nutilize key information [11].\nDespite the improvements in the practicality of Advanced\nRAG, there remains a gap between its capabilities and real-\nworld application requirements. On one hand, as RAG tech-\nnology advances, user expectations rise, demands continue to\nevolve, and application settings become more complex. For\ninstance, the integration of heterogeneous data and the new", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "1d64a99e-70f1-4b5e-88ca-c2fa3633a9de": {"text": "world application requirements. On one hand, as RAG tech-\nnology advances, user expectations rise, demands continue to\nevolve, and application settings become more complex. For\ninstance, the integration of heterogeneous data and the new\ndemands for system transparency, control, and maintainability.\nOn the other hand, the growth in application demands has\nfurther propelled the evolution of RAG technology.\nAs shown in Figure 2, to achieve more accurate and efficient\ntask execution, modern RAG systems are progressively inte-\ngrating more sophisticated function, such as organizing more\nrefined index base in the form of knowledge graphs, integrat-\ning structured data through query construction methods, and\nemploying fine-tuning techniques to enable encoders to better\nadapt to domain-specific documents.\nIn terms of process design, the current RAG system has\nsurpassed the traditional linear retrieval-generation paradigm.\nResearchers use iterative retrieval [12] to obtain richer con-\ntext, recursive retrieval [13] to handle complex queries, and\nadaptive retrieval [14] to provide overall autonomy and flex-\nibility. This flexibility in the process significantly enhances\narXiv:2407.21059v1  [cs.CL]  26 Jul 2024", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "a7187dbe-94ef-4942-b211-2d41101e8d60": {"text": "2\nFig. 1. Cases of Naive RAG and Advanced RAG.When faced with complex\nquestions, both encounter limitations and struggle to provide satisfactory\nanswers. Despite the fact that Advanced RAG improves retrieval accuracy\nthrough hierarchical indexing, pre-retrieval, and post-retrieval processes, these\nrelevant documents have not been used correctly.\nthe expressive power and adaptability of RAG systems, en-\nabling them to better adapt to various application scenarios.\nHowever, this also makes the orchestration and scheduling of\nworkflows more complex, posing greater challenges to system\ndesign. Specifically, RAG currently faces the following new\nchallenges:\nComplex data sources integration. RAG are no longer\nconfined to a single type of unstructured text data source but\nhave expanded to include various data types, such as semi-\nstructured data like tables and structured data like knowledge\ngraphs [15]. Access to heterogeneous data from multiple\nsources can provide the system with a richer knowledge\nbackground, and more reliable knowledge verification capa-\nbilities [16].\nNew demands for system interpretability, controllability,\nFig. 2. Case of current Modular RAG.The system integrates diverse data\nand more functional components. The process is no longer confined to linear\nbut is controlled by multiple control components for retrieval and generation,\nmaking the entire system more flexible and complex.", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "887246af-a21a-48cd-a1cf-2fd07e624dae": {"text": "3\nand maintainability. With the increasing complexity of sys-\ntems, system maintenance and debugging have become more\nchallenging. Additionally, when issues arise, it is essential to\nquickly pinpoint the specific components that require opti-\nmization.\nComponent selection and optimization. More neural net-\nworks are involved in the RAG system, necessitating the\nselection of appropriate components to meet the needs of spe-\ncific tasks and resource configurations. Moreover, additional\ncomponents enhance the effectiveness of RAG but also bring\nnew collaborative work requirements [17]. Ensuring that these\nmodels perform as intended and work efficiently together to\nenhance the overall system performance is crucial.\nWorkflow orchestration and scheduling. Components\nmay need to be executed in a specific order, processed in paral-\nlel under certain conditions, or even judged by the LLM based\non different outputs. Reasonable planning of the workflow is\nessential for improving system efficiency and achieving the\ndesired outcomes [18].\nTo address the design, management, and maintenance chal-\nlenges posed by the increasing complexity of RAG systems,\nand to meet the ever-growing and diverse demands and ex-\npectations, this paper proposes Modular RAG architecture.\nIn modern computing systems, modularization is becoming\na trend. It can enhance the system\u2019s scalability and maintain-\nability and achieve efficient task execution through process\ncontrol.\nThe Modular RAG system consists of multiple independent\nyet tightly coordinated modules, each responsible for handling\nspecific functions or tasks. This architecture is divided into\nthree levels: the top level focuses on the critical stages of\nRAG, where each stage is treated as an independent module.\nThis level not only inherits the main processes from the\nAdvanced RAG paradigm but also introduces an orchestration\nmodule to control the coordination of RAG processes. The\nmiddle level is composed of sub-modules within each module,\nfurther refining and optimizing the functions. The bottom level\nconsists of basic units of operation\u2014operators. Within the\nModular RAG framework, RAG systems can be represented\nin the form of computational graphs, where nodes represent\nspecific operators. The comparison of the three paradigms is\nshown in the Figure 3. Modular RAG evolves based on the\nprevious development of RAG. The relationships among these\nthree paradigms are ones of inheritance and development.\nAdvanced RAG is a special case of Modular RAG, while Naive", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "845625db-e277-4071-b466-e1c1f0e46e34": {"text": "shown in the Figure 3. Modular RAG evolves based on the\nprevious development of RAG. The relationships among these\nthree paradigms are ones of inheritance and development.\nAdvanced RAG is a special case of Modular RAG, while Naive\nRAG is a special case of Advanced RAG.\nThe advantages of Modular RAG are significant, as it\nenhances the flexibility and scalability of RAG systems. Users\ncan flexibly combine different modules and operators accord-\ning to the requirements of data sources and task scenarios. In\nsummary, the contributions of this paper are as follows:\n\u2022 This paper proposes a new paradigm called modular\nRAG, which employs a three-tier architectural design\ncomprising modules, sub-modules, and operators to de-\nfine the RAG system in a unified and structured manner.\nThis design not only enhances the system\u2019s flexibility and\nscalability but also, through the independent design of\noperators, strengthens the system\u2019s maintainability and\ncomprehensibility.\n\u2022 Under the framework of Modular RAG, the orchestration\nof modules and operators forms the RAG Flow, which\ncan flexibly express current RAG methods. This paper has\nfurther summarized six typical flow patterns and specific\nmethods have been analyzed to reveal the universality of\nmodular RAG in practical scenarios.\n\u2022 The Modular RAG framework offers exceptional flexi-\nbility and extensibility. This paper delves into the new\nopportunities brought by Modular RAG and provides a\nthorough discussion on the adaptation and expansion of\nnew methods in different application scenarios, offering\nguidance for future research directions and practical ex-\nploration.\nII. R ELATED WORK\nThe development of RAG technology can be summarized\nin three stages. Initially, retrieval-augmented techniques were\nintroduced to improve the performance of pre-trained lan-\nguage models on knowledge-intensive tasks [19], [20]. In\nspecific implementations, Retro [21] optimized pre-trained\nautoregressive models through retrieval augmentation, while\nAtlas [22] utilized a retrieval-augmented few-shot fine-tuning\nmethod, enabling language models to adapt to diverse tasks.\nIRCOT [23] further enriched the reasoning process during\nthe inference phase by combining chain-of-thought and multi-\nstep retrieval processes. Entering the second stage, as the\nlanguage processing capabilities of LLMs significantly im-\nproved, retrieval-augmented techniques began to serve as a\nmeans of supplementing additional knowledge and providing\nreferences, aiming to reduce the hallucination. For instance,", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "2f4f59a7-e12e-498c-83a0-b3899d47e2aa": {"text": "language processing capabilities of LLMs significantly im-\nproved, retrieval-augmented techniques began to serve as a\nmeans of supplementing additional knowledge and providing\nreferences, aiming to reduce the hallucination. For instance,\nRRR [24] improved the rewriting phase, and LLMlingua [25]\nremoved redundant tokens in retrieved document chunks.\nWith the continuous progress of RAG technology, research\nhas become more refined and focused, while also achieving\ninnovative integration with other technologies such as graph\nneural networks [26] and fine-tuning techniques [27]. The\noverall pipeline has also become more flexible, such as using\nLLMs to proactively determine the timing of retrieval and\ngeneration [14], [28].\nThe development of RAG technology has been acceler-\nated by LLM technology and practical application needs.\nResearchers are examining and organizing the RAG frame-\nwork and development pathways from different perspectives.\nBuilding upon the enhanced stages of RAG, Gao et al., [2] sub-\ndivided RAG into enhancement during pre-training, inference,\nand fine-tuning stages. Based on the main processes of RAG,\nrelevant works on RAG were organized from the perspectives\nof retrieval, generation, and augmentation methods. Huang\net al., [29] categorize RAG methods into four main classes:\npre-retrieval, retrieval, post-retrieval, generation, and provide\na detailed discussion of the methods and techniques within\neach class. Hu et al., [30] discuss Retrieval-Augmented Lan-\nguage Models (RALMs) form three key components, including\nretrievers, language models, augmentations, and how their\ninteractions lead to different model structures and applications.", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "0cc9b436-f2a3-4811-9da9-64442942e5e3": {"text": "4\nFig. 3. Comparison between three RAG paradigms. Modular RAG has evolved from previous paradigms and aligns with the current practical needs of RAG\nsystems.\nThey emphasize the importance of considering robustness,\naccuracy, and relevance when evaluating RALMs and pro-\npose several evaluation methods. Ding et al., [31] provide a\ncomprehensive review from the perspectives of architecture,\ntraining strategies, and applications. They specifically discuss\nfour training methods of RALMs: training-free methods, in-\ndependent training methods, sequence training methods, and\njoint training methods, and compare their advantages and\ndisadvantages. Zhao et al., [32]analyze the applications of\nRAG technology in various fields such as text generation,\ncode generation, image generation, and video generation from\nthe perspective of augmented intelligence with generative\ncapabilities.\nThe current collation of RAG systems primarily focuses\non methods with a fixed process, mainly concerned with\noptimizing the retrieval and generation stages. However, it has\nnot turned its attention to the new characteristics that RAG\nresearch is continuously evolving, namely the characteristics\nof process scheduling and functional componentization. There\nis currently a lack of comprehensive analysis of the overall\nRAG system, which has led to research on paradigms lagging\nbehind the development of RAG technology.\nIII. F RAMEWORK AND NOTATION\nFor query Q = {qi}, a typical RAG system mainly consists\nof three key components. 1) Indexing. Given documents D =\n{d1, d2, . . . , dn} , where di represents the document chunk.\nIndexing is the process of converting di into vectors through\nan embedding model fe(\u00b7) , and then store vectors in vector\ndatabase.\nI = {e1, e2, . . . , en} and e i = fe(di) \u2208 Rd (1)\nNotation Description\nq The original query\ny The output of LLM\nD A document retrieval repository composed of chunks di.\nR(q, D) Retriever,find similar chunks from D based on q.\nF RAG Flow\nP RAG Flow pattern\nfqe Query expansion function\nfqc Query transform function\nfcomp Chunk compression function\nfsel Chunk selection function\nfr Routing function\nM Module in modular RAG\nop The specific operators within the Module.\nTABLE I\nIMPORTANT NOTATION\n2) Retrieval . Transform the query into a vector using the\nsame encoding model, and then filter out the top k document\nchunks that are most similar based on vector similarity.\nR : topk\ndi\u2208D\nSim(q, di) \u2192 Dq (2)\nDq = {d1, d2, . . . , dk} represents the relevant documents for", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "00d00dce-8a37-4978-a172-0d5e1d4ad782": {"text": "same encoding model, and then filter out the top k document\nchunks that are most similar based on vector similarity.\nR : topk\ndi\u2208D\nSim(q, di) \u2192 Dq (2)\nDq = {d1, d2, . . . , dk} represents the relevant documents for\nquestion q. The similarity function Sim(\u00b7) commonly used are\ndot product or cosine similarity.\nSim(q, di) = eq \u00b7 edi or eq \u00b7 edi\n\u2225eq\u2225 \u00b7 \u2225edi\u2225 (3)\n3) Generation . After getting the relevant documents. The\nquery q and the retrieved document Dq chunks are inputted\ntogether to the LLM to generate the final answer, where [\u00b7, \u00b7]\nstands for concatenation.\ny = LLM([Dq, q]) (4)", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "679e5264-bfbe-4df2-ab24-ddb3059f3b78": {"text": "5\nWith the evolution of RAG technology, more and more func-\ntional components are being integrated into systems. Modular\nRAG paradigm includes three levels, ranging from large to\nsmall:\nL1 Module (M = {Ms}). The core process in RAG\nsystem.\nL2 Sub-module (Ms = {Op}).The functional modules in\nmodule.\nL3 Operator (Op = {f\u03b8i}). The the specific functional\nimplementation in a module or sub-module. As a result, a\nModular RAG system can be represented as:\nG = {q, D,M, {Ms}, {Op}} (5)\nThe arrangement between modules and operators constitutes\nthe RAG Flow F = ( M\u03d51 , . . . , M\u03d5n) where \u03d5 stands for\nthe set of module parameters. A modular rag flow can be\ndecomposed into a graph of sub-functions. In the simplest\ncase,the graph is a linear chain.\nNaiveRAG : q\nR(q,D)\n\u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2192\nText\u2212Embedding\nDq LLM([q,Dq])\n\u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2192\nOpenAI/GPT \u22124\ny\n(6)\nIV. M ODULE AND OPERATOR\nThis chapter will specifically introduce modules and op-\nerators under the Modular RAG framework. Based on the\ncurrent stage of RAG development, we have established\nsix main modules: Indexing, Pre-retrieval, Retrieval, Post-\nretrieval, Generation, and Orchestration.\nA. Indexing\nIndexing is the process of split document into manageable\nchunks and it is a key step in organizing a system. Indexing\nfaces three main challenges. 1) Incomplete content represen-\ntation.The semantic information of chunks is influenced by the\nsegmentation method, resulting in the loss or submergence of\nimportant information within longer contexts. 2) Inaccurate\nchunk similarity search . As data volume increases, noise in\nretrieval grows, leading to frequent matching with erroneous\ndata, making the retrieval system fragile and unreliable. 3)\nUnclear reference trajectory. The retrieved chunks may orig-\ninate from any document, devoid of citation trails, potentially\nresulting in the presence of chunks from multiple different\ndocuments that, despite being semantically similar, contain\ncontent on entirely different topics.\n1) Chunk Optimization: The size of the chunks and the\noverlap between the chunks play a crucial role in the overall\neffectiveness of the RAG system. Given a chunk di, its chunk\nsize is denoted as Li = |di|, and the overlap is denoted as\nLo\ni = |di \u2229 di+1|. Larger chunks can capture more context,\nbut they also generate more noise, requiring longer processing\ntime and higher costs. While smaller chunks may not fully\nconvey the necessary context, they do have less noise [17].\nSliding Window using overlapping chunks in a sliding win-", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "aeda5635-9af2-4548-b4c8-8eaa19986a34": {"text": "but they also generate more noise, requiring longer processing\ntime and higher costs. While smaller chunks may not fully\nconvey the necessary context, they do have less noise [17].\nSliding Window using overlapping chunks in a sliding win-\ndow enhances semantic transitions. However, it has limitations\nsuch as imprecise context size control, potential truncation of\nwords or sentences, and lacking semantic considerations.\nMetadata Attachment. Chunks can be enriched with meta-\ndata like page number, file name, author, timestamp, sum-\nmary, or relevant questions. This metadata allows for filtered\nretrieval, narrowing the search scope.\nSmall-to-Big [33] separate the chunks used for retrieval\nfrom those used for synthesis. Smaller chunks enhance re-\ntrieval accuracy, while larger chunks provide more context.\nOne approach is to retrieve smaller summarized chunks and\nreference their parent larger chunks. Alternatively, individual\nsentences could be retrieved along with their surrounding text.\n2) Structure Organization: One effective method for en-\nhancing information retrieval is to establish a hierarchical\nstructure for the documents. By constructing chunks structure,\nRAG system can expedite the retrieval and processing of\npertinent data.\nHierarchical Index . In the hierarchical structure of docu-\nments, nodes are arranged in parent-child relationships, with\nchunks linked to them. Data summaries are stored at each\nnode, aiding in the swift traversal of data and assisting the\nRAG system in determining which chunks to extract. This\napproach can also mitigate the illusion caused by chunk\nextraction issues. The methods for constructing a structured\nindex primarily include: 1) Structural awareness based on\nparagraph and sentence segmentation in docs. 2) Content\nawareness based on inherent structure in PDF, HTML, and\nLatex. 3) Semantic awareness based on semantic recognition\nand segmentation of text.\nKG Index [34]. Using Knowledge Graphs (KGs) to struc-\nture documents helps maintain consistency by clarifying con-\nnections between concepts and entities, reducing the risk of\nmismatch errors. KGs also transform information retrieval\ninto instructions intelligible to language models, improving re-\ntrieval accuracy and enabling contextually coherent responses.\nThis enhances the overall efficiency of the RAG system.\nFor example, organizing a corpus in the format of graph\nG = {V, E, X}, where node V = {vi}n\ni=1 represent document\nstructures (e.g.passage, pages, table) , edge E \u2282 V \u00d7 Vrep-", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "347eaaae-56b1-40e1-b2b0-18d30b2ef64c": {"text": "This enhances the overall efficiency of the RAG system.\nFor example, organizing a corpus in the format of graph\nG = {V, E, X}, where node V = {vi}n\ni=1 represent document\nstructures (e.g.passage, pages, table) , edge E \u2282 V \u00d7 Vrep-\nresent semantic or lexical similarity and belonging relations,\nand node features X = {Xi}n\ni=1 represent text or markdown\ncontent for passage.\nB. Pre-retrieval\nOne of the primary challenges with Naive RAG is its\ndirect reliance on the user\u2019s original query as the basis for\nretrieval. Formulating a precise and clear question is difficult,\nand imprudent queries result in subpar retrieval effectiveness.\nThe primary challenges in this module include: 1) Poorly\nworded queries . The question itself is complex, and the\nlanguage is not well-organized. 2) Language complexity and\nambiguity. Language models often struggle when dealing\nwith specialized vocabulary or ambiguous abbreviations with\nmultiple meanings. For instance, they may not discern whether\nLLM refers to Large Language Model or a Master of Laws in\na legal context.\n1) Query Expansion : Expanding a single query into mul-\ntiple queries enriches the content of the query, providing", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "d37ae40b-4086-4cbe-9fa1-bf5342c56b45": {"text": "6\nfurther context to address any lack of specific nuances, thereby\nensuring the optimal relevance of the generated answers.\nfqe(q) = {q1, q2, . . . , qn} \u2200qi \u2208 {q1, q2, . . . , qn}, qi /\u2208 Q\n(7)\nMulti-Query uses prompt engineering to expand queries\nvia LLMs, allowing for parallel execution. These expansions\nare meticulously designed to ensure diversity and coverage.\nHowever, this approach can dilute the user\u2019s original intent.\nTo mitigate this, the model can be instructed to assign greater\nweight to the original query.\nSub-Query. By decomposing and planning for complex\nproblems, multiple sub-problems are generated. Specifically,\nleast-to-most prompting [35] can be employed to decom-\npose the complex problem into a series of simpler sub-\nproblems. Depending on the structure of the original problem,\nthe generated sub-problems can be executed in parallel or\nsequentially. Another approach involves the use of the Chain-\nof-Verification (CoVe) [36]. The expanded queries undergo\nvalidation by LLM to achieve the effect of reducing hallu-\ncinations.\n2) Query Transformation: Retrieve and generate based on\na transformed query instead of the user\u2019s original query.\nfqt(q) = q\u2032 (8)\nRewrite. Original queries often fall short for retrieval in\nreal-world scenarios. To address this, LLMs can be prompted\nto rewrite. Specialized smaller models can also be employed\nfor this purpose [24]. The implementation of the query rewrite\nmethod in Taobao has significantly improved recall effective-\nness for long-tail queries, leading to an increase in GMV [10].\nHyDE [37]. In order to bridge the semantic gap between\nquestions and answers, it constructs hypothetical documents\n(assumed answers) when responding to queries instead of\ndirectly searching the query. It focuses on embedding simi-\nlarity from answer to answer rather than seeking embedding\nsimilarity for the problem or query. In addition, it also in-\ncludes reverse HyDE, which generate hypothetical query for\neach chunks and focuses on retrieval from query to query.\nStep-back Prompting [38]. The original query is abstracted\ninto a high-level concept question (step-back question). In the\nRAG system, both the step-back question and the original\nquery are used for retrieval, and their results are combined\nto generate the language model\u2019s answer.\n3) Query Construction: In addition to text data, an in-\ncreasing amount of structured data, such as tables and graph\ndata, is being integrated into RAG systems. To accommodate\nvarious data types, it is necessary to restructure the user\u2019s", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "19b6fad8-0e68-432b-a8b6-a9de23bbbc59": {"text": "3) Query Construction: In addition to text data, an in-\ncreasing amount of structured data, such as tables and graph\ndata, is being integrated into RAG systems. To accommodate\nvarious data types, it is necessary to restructure the user\u2019s\nquery. This involve converting the query into another query\nlanguage to access alternative data sources, with common\nmethods including Text-to-SQL or Text-to-Cypher . In many\nscenarios, structured query languages (e.g., SQL, Cypher)\nare often used in conjunction with semantic information and\nmetadata to construct more complex queries.\nfqc(q) = q\u2217, q\u2217 \u2208 Q\u2217 = {SQL, Cypher, . . .} (9)\nC. Retrieval\nThe retrieval process is pivotal in RAG systems. By lever-\naging powerful embedding models, queries and text can be\nefficiently represented in latent spaces, which facilitates the\nestablishment of semantic similarity between questions and\ndocuments, thereby enhancing retrieval. Three main consider-\nations that need to be addressed include retrieval efficiency,\nquality, and the alignment of tasks, data and models.\n1) Retriever Selection: With the widespread adoption of\nRAG technology, the development of embedding models has\nbeen in full swing. In addition to traditional models based\non statistics and pre-trained models based on the encoder\nstructure, embedding models fine-tuned on LLMs have also\ndemonstrated powerful capabilities [39]. However, they often\ncome with more parameters, leading to weaker inference\nand retrieval efficiency. Therefore, it is crucial to select the\nappropriate retriever based on different task scenarios.\nSparse Retriever uses statistical methods to convert queries\nand documents into sparse vectors. Its advantage lies in its\nefficiency in handling large datasets, focusing only on non-zero\nelements. However, it may be less effective than dense vectors\nin capturing complex semantics. Common methods include\nTF-IDF and BM25.\nDense Retriever employs pre-trained language models\n(PLMs) to provide dense representations of queries and doc-\numents. Despite higher computational and storage costs, it\noffers more complex semantic representations. Typical models\ninclude BERT structure PLMs, like ColBERT, and multi-task\nfine-tuned models like BGE [40] and GTE [41].\nHybrid Retriever is to use both sparse and dense retrievers\nsimultaneously. Two embedding techniques complement each\nother to enhance retrieval effectiveness. Sparse retriever can\nprovide initial screening results. Additionally, sparse models\nenhance the zero-shot retrieval capabilities of dense models,", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "bc847de4-5539-4153-80ca-2e04499a01b0": {"text": "simultaneously. Two embedding techniques complement each\nother to enhance retrieval effectiveness. Sparse retriever can\nprovide initial screening results. Additionally, sparse models\nenhance the zero-shot retrieval capabilities of dense models,\nparticularly in handling queries with rare entities, thereby\nincreasing system robustness.\n2) Retriever Fine-tuning: In cases where the context may\ndiverge from pre-trained corpus, particularly in highly special-\nized fields like healthcare, law, and other domains abundant in\nproprietary terminology. While this adjustment demands addi-\ntional effort, it can substantially enhance retrieval efficiency\nand domain alignment.\nSupervised Fine-Tuning (SFT). Fine-tuning a retrieval\nmodel based on labeled domain data is typically done using\ncontrastive learning. This involves reducing the distance be-\ntween positive samples while increasing the distance between\nnegative samples. The commonly used loss calculation is\nshown in the following:\nL(DR) = \u2212 1\nT\nTX\ni=1\nlog e(sim(qi,d+\ni ))\ne(sim(qi,d+\ni )) + PN\nj=1 e(sim(qi,d\u2212\ni ))\n(10)\nwhere d+\ni is the positive sample document corresponding to\nthe i-th query, d\u2212\ni is several negative sample, T is the total\nnumber of queries, N is the number of negative samples, and\nDR is the fine-tuning dataset.\nLM-supervised Retriever (LSR) . In contrast to directly\nconstructing a fine-tuning dataset from the dataset, LSR uti-", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "388e52da-7068-4874-80cd-530ce0bec7c8": {"text": "7\nlizes the LM-generated results as supervisory signals to fine-\ntune the embedding model during the RAG process.\nPLSR(d|q, y) = ePLM(y|d,q)/\u03b2\nP\nd\u2032\u2208D ePLM(y|d,q)/\u03b2) (11)\nPLM (y|d, q) is LM probability of the ground truth output y\ngiven the input context d and query q, and \u03b2 is a hyper-\nparamter.\nAdapter. At times, fine-tuning a large retriever can be\ncostly, especially when dealing with retrievers based on LLMs\nlike gte-Qwen. In such cases, it can mitigate this by incorpo-\nrating an adapter module and conducting fine-tuning. Another\nbenefit of adding an adapter is the ability to achieve better\nalignment with specific downstream tasks [42].\nD. Post-retrieval\nFeeding all retrieved chunks directly into the LLM is not an\noptimal choice. Post-processing the chunks can aid in better\nleveraging the contextual information. The primary challenges\ninclude: 1) Lost in the middle . Like humans, LLM tends\nto remember only the beginning or the end of long texts,\nwhile forgetting the middle portion [43]. 2) Noise/anti-fact\nchunks. Retrieved noisy or factually contradictory documents\ncan impact the final retrieval generation [44]. 3) Context\nWindow. Despite retrieving a substantial amount of relevant\ncontent, the limitation on the length of contextual information\nin large models prevents the inclusion of all this content.\n1) Rerank: Rerank the retrieved chunks without altering\ntheir content or length, to enhance the visibility of the more\ncrucial document chunks. Given the retrieved set Dq and a\nre-ranking method frerank to obtain the re-ranked set:\nDq\nr = frerank(q, Dq) = {d\u2032\n1, d\u2032\n2, . . . , d\u2032\nk}\nwheref(d\u2032\n1) \u2265 f(d\u2032\n2) \u2265 . . .\u2265 f(d\u2032\nk). (12)\nRule-base rerank. Metrics are calculated to rerank chunks\naccording to certain rules. Common metrics include: diversity,\nrelevance and MRR (Maximal Marginal Relevance) [45]. The\nidea is to reduce redundancy and increase result diversity.\nMMR selects phrases for the final key phrase list based on a\ncombined criterion of query relevance and information novelty.\nModel-base rerank. Utilize a language model to reorder the\ndocument chunks, commonly based on the relevance between\nthe chunks and the query. Rerank models have become an\nimportant component of RAG systems, and relevant model\ntechnologies are also being iteratively upgraded. The scope\nreordering has also been extended to multimodal data such as\ntables and images [46].\n2) Compression: A common misconception in the RAG\nprocess is the belief that retrieving as many relevant docu-\nments as possible and concatenating them to form a lengthy", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "eac75492-f643-4a62-b00d-9e54ceee9374": {"text": "tables and images [46].\n2) Compression: A common misconception in the RAG\nprocess is the belief that retrieving as many relevant docu-\nments as possible and concatenating them to form a lengthy\nretrieval prompt is beneficial. However, excessive context can\nintroduce more noise, diminishing the LLM\u2019s perception of\nkey information. A common approach to address this is to\ncompress and select the retrieved content.\nDq\nc = fcomp(q, Dq), where|dqc\ni | < |dq\ni | \u2200dq\ni \u2208 Dq (13)\n(Long)LLMLingua [47]. By utilizing aligned and trained\nsmall language models, such as GPT-2 Small or LLaMA-\n7B, the detection and removal of unimportant tokens from\nthe prompt is achieved, transforming it into a form that is\nchallenging for humans to comprehend but well understood by\nLLMs. This approach presents a direct and practical method\nfor prompt compression, eliminating the need for additional\ntraining of LLMs while balancing language integrity and\ncompression ratio.\n3) Selection: Unlike compressing the content of document\nchunks, Selection directly removes irrelevant chunks.\nDq\ns = fsel(Dq) = {di \u2208 D | \u00acP(di)} (14)\nWhere fsel is the function for deletion operation and P(di) is\na conditional predicate indicating that document ( di) satisfies\na certain condition. If document ( di) satisfies ( P(di)), it will\nbe deleted. Conversely, documents for which ( \u00acP(di)) is true\nwill be retained.\nSelective Context. By identifying and removing redundant\ncontent in the input context, the input is refined, thus improv-\ning the language model\u2019s reasoning efficiency. In practice, se-\nlective context assesses the information content of lexical units\nbased on the self-information computed by the base language\nmodel. By retaining content with higher self-information, this\nmethod offers a more concise and efficient textual representa-\ntion, without compromising their performance across diverse\napplications. However, it overlooks the interdependence be-\ntween compressed content and the alignment between the\ntargeted language model and the small language model utilized\nfor prompting compression [48].\nLLM-Critique. Another straightforward and effective ap-\nproach involves having the LLM evaluate the retrieved content\nbefore generating the final answer. This allows the LLM\nto filter out documents with poor relevance through LLM\ncritique. For instance, in Chatlaw [49], the LLM is prompted\nto self-suggestion on the referenced legal provisions to assess\ntheir relevance.\nE. Generation\nUtilize the LLM to generate answers based on the user\u2019s", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "694c06a0-f608-4e6c-8d1f-a40412608693": {"text": "critique. For instance, in Chatlaw [49], the LLM is prompted\nto self-suggestion on the referenced legal provisions to assess\ntheir relevance.\nE. Generation\nUtilize the LLM to generate answers based on the user\u2019s\nquery and the retrieved contextual information. Select an\nappropriate model based on the task requirements, considering\nfactors such as the need for fine-tuning, inference efficiency,\nand privacy protection.\n1) Generator Fine-tuning: In addition to direct LLM usage,\ntargeted fine-tuning based on the scenario and data character-\nistics can yield better results. This is also one of the greatest\nadvantages of using an on-premise setup LLMs.\nInstruct-Tuning. When LLMs lack data in a specific do-\nmain, additional knowledge can be provided to the LLM\nthrough fine-tuning. General fine-tuning dataset can also be\nused as an initial step. Another benefit of fine-tuning is the\nability to adjust the model\u2019s input and output. For example, it\ncan enable LLM to adapt to specific data formats and generate\nresponses in a particular style as instructed [50].\nReinforcement learning. Aligning LLM outputs with hu-\nman or retriever preferences through reinforcement learning is", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "b6edfd64-3793-47ce-9933-55e69f42e259": {"text": "8\na potential approach [51]. For instance, manually annotating\nthe final generated answers and then providing feedback\nthrough reinforcement learning. In addition to aligning with\nhuman preferences, it is also possible to align with the\npreferences of fine-tuned models and retrievers.\nDual Fine-tuing Fine-tuning both generator and retriever\nsimultaneously to align their preferences. A typical approach,\nsuch as RA-DIT [27], aligns the scoring functions between\nretriever and generator using KL divergence. Retrieval likeli-\nhood of each retrieved document d is calculated as :\nPR(d|q) = e(sim(d,q))/\u03b3P\nd\u2208Dq e(sim(d,q)/\u03b3 (15)\nPLM (y|d, q) is the LM probability of the ground truth output y\ngiven the input context d, question q, and \u03b3 is a hyperparamter.\nThe overall loss is calculated as:\nL = 1\n|T|\nTX\ni=1\nKL(PR(d|q)||PLSR(d|q, y|)) (16)\n2) Verification : Although RAG enhances the reliability\nof LLM-generated answers, in many scenarios, it requires to\nminimize the probability of hallucinations. Therefore, it can\nfilter out responses that do not meet the required standards\nthrough additional verification module. Common verification\nmethods include knowledge-base and model-base .\nyk = fverify (q, Dq, y) (17)\nKnowledge-base verification refers to directly validating the\nresponses generated by LLMs through external knowledge.\nGenerally, it extracts specific statements or triplets from re-\nsponse first. Then, relevant evidence is retrieved from verified\nknowledge base such as Wikipedia or specific knowledge\ngraphs. Finally, each statement is incrementally compared with\nthe evidence to determine whether the statement is supported,\nrefuted, or if there is insufficient information [52].\nModel-based verification refers to using a small language\nmodel to verify the responses generated by LLMs [53].\nGiven the input question, the retrieved knowledge, and the\ngenerated answer, a small language model is trained to de-\ntermine whether the generated answer correctly reflects the\nretrieved knowledge. This process is framed as a multiple-\nchoice question, where the verifier needs to judge whether the\nanswer reflects correct answer . If the generated answer does\nnot correctly reflect the retrieved knowledge, the answer can\nbe iteratively regenerated until the verifier confirms that the\nanswer is correct.\nF . Orchestration\nOrchestration pertains to the control modules that govern the\nRAG process. Unlike the traditional, rigid approach of a fixed\nprocess, RAG now incorporates decision-making at pivotal", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "01c057dd-19a7-4791-8c3d-6b282c25572f": {"text": "answer is correct.\nF . Orchestration\nOrchestration pertains to the control modules that govern the\nRAG process. Unlike the traditional, rigid approach of a fixed\nprocess, RAG now incorporates decision-making at pivotal\njunctures and dynamically selects subsequent steps contingent\nupon the previous outcomes. This adaptive and modular ca-\npability is a hallmark of modular RAG, distinguishing it from\nthe more simplistic Naive and Advance RAG paradigm.\n1) Routing: In response to diverse queries, the RAG system\nroutes to specific pipelines tailored for different scenario, a\nfeature essential for a versatile RAG architecture designed\nto handle a wide array of situations. A decision-making\nmechanism is necessary to ascertain which modules will be\nengaged, based on the input from the model or supplementary\nmetadata. Different routes are employed for distinct prompts\nor components. This routing mechanism is executed through\na function, denoted as fr(\u00b7), which assigns a score \u03b1i to\neach module. These scores dictate the selection of the active\nsubset of modules. Mathematically, the routing function is\nrepresented as:\nfr : Q \u2192 F (18)\nwhere fr(\u00b7) maps the identified query to its corresponding\nRAG flow.\nMetadata routing involves extracting key terms, or entities,\nfrom the query, applying a filtration process that uses these\nkeywords and associated metadata within the chunks to refine\nthe routing parameters. For a specific RAG flow, denoted as\nFi, the pre-defined routing keywords are represented as the\nset Ki = {ki1, ki2, . . . , kin}. The keyword identified within\nthe query qi is designated as K\u2032\ni. The matching process for\nthe query q is quantified by the key score equation:\nscorekey(qi, Fj) = 1\n|K\u2032\nj||Ki \u2229 K\u2032\nj| (19)\nThis equation calculates the overlap between the pre-defined\nkeywords and those identified in the query, normalized by the\ncount of keywords in K\u2032\nj. The final step is to determine the\nmost relevant flow for the query q:\nFi(q) = argmaxFj\u2208Fscore(q, Fj) (20)\nSemantic routing routes to different modules based on the\nsemantic information of the query. Given a pre-defined intent\n\u0398 = {\u03b81, \u03b82, . . . , \u03b8n}, the possibility of intent for query q is\nP\u0398(\u03b8|q) = ePLM(\u03b8|q)\nP\n\u03b8\u2208\u0398 ePLM(\u03b8|q)) . Routing to specific RAG flow is\ndetermined by the semantic score:\nsocresemantic(q, Fj) = argmax\u03b8j\u2208\u0398P(\u0398) (21)\nThe function \u03b4(\u00b7) serves as a mapping function that assigns\nan intent to a distinct RAG flow Fi = \u03b4(\u03b8i)\nHybrid Routing can be implemented to improve query\nrouting by integrating both semantic analysis and metadata-", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "d2c7d7d2-1ee7-4afd-9359-d2bd78b86917": {"text": "The function \u03b4(\u00b7) serves as a mapping function that assigns\nan intent to a distinct RAG flow Fi = \u03b4(\u03b8i)\nHybrid Routing can be implemented to improve query\nrouting by integrating both semantic analysis and metadata-\nbased approaches, which can be defined as follows:\n\u03b1i = a\u00b7scorekey(q, Fj)+(1\u2212\u03b1)\u00b7max\u03b8j\u2208\u0398socresemantic(q, Fj)\n(22)\na is a weighting factor that balances the contribution of the\nkey-based score and the semantic score.\n2) Scheduling: The RAG system evolves in complexity\nand adaptability, with the ability to manage processes through\na sophisticated scheduling module. The scheduling module\nplays a crucial role in the modular RAG , identifying critical\njunctures that require external data retrieval, assessing the\nadequacy of the responses, and deciding on the necessity for\nfurther investigation. It is commonly utilized in scenarios that\ninvolve recursive, iterative, and adaptive retrieval, ensuring", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "2c26d1fb-7c70-4a84-9d64-04329d52dddf": {"text": "9\nthat the system makes informed decisions on when to cease\ngeneration or initiate a new retrieval loop.\nRule judge. The subsequent steps are dictated by a set of\nestablished rules. Typically, the system evaluates the quality of\ngenerated answers through scoring mechanisms. The decision\nto proceed or halt the process is contingent upon whether these\nscores surpass certain predetermined thresholds, often related\nto the confidence levels of individual tokens, which can be\ndefined as follow:\nyt =\n(\n\u02c6st if all tokens of \u02c6st have probs \u2265 \u03c4\nst = LM([Dqt, x, y<t]) otherwise\nHere, \u02c6st represents the tentative answer, and st is the output\nfrom the language model. The condition for accepting \u02c6st is that\nall tokens within it must have associated probabilities greater\nthan or equal to the threshold \u03c4. If this condition is not met,\nthe system reverts to generating a new answer.\nLLM judge. The LLM independently determines the sub-\nsequent course of action. Two primary approaches facilitate\nthis capability. The first method leverages LLM \u2019s in-context\nlearning capability, and make judgments through prompt\nengineering. A significant advantage of this method is the\nelimination of model fine-tuning. Nonetheless, the format of\nthe judgment output is contingent upon the LLM\u2019s adherence\nto the provided instructions.\nThe second approach involves the LLM generating specific\ntokens that initiate targeted actions through fine-tuning. This\ntechnique, with roots in the Toolformer [50], has been inte-\ngrated into frameworks like Self-RAG [28]. This allows for a\nmore direct control mechanism over the LLM\u2019s actions, en-\nhancing the system\u2019s responsiveness to specific triggers within\nthe conversational context. However, it requires generating a\nlarge number of compliant instruction sets to fine-tune LLM.\nKnowledge-guide scheduling. Beyond the confines of rule-\nbased methods and the complete reliance on LLMs for process\ncontrol, a more adaptable intermediate approach emerges with\nknowledge-guided scheduling [26]. These methods harness\nthe power of knowledge graphs, to steer the retrieval and\ngeneration processes. Specifically, it involves extracting infor-\nmation relevant to the question from a knowledge graph and\nconstructing a reasoning chain. This reasoning chain consists\nof a series of logically interconnected nodes, each containing\ncritical information for the problem-solving process. Based\non the information from the nodes in this reasoning chain,\ninformation retrieval and content generation can be performed", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "6f8b62ab-fc43-4a8c-b0e9-f72e147854ce": {"text": "of a series of logically interconnected nodes, each containing\ncritical information for the problem-solving process. Based\non the information from the nodes in this reasoning chain,\ninformation retrieval and content generation can be performed\nseparately. By integrating this approach, it enhance not only\nthe efficacy and precision of problem-solving but also the\nclarity of the explanations provided.\n3) Fusion: As RAG process has evolved beyond a linear\npipeline, it frequently necessitates broadening the retrieval\nscope or enhancing diversity by exploring multiple pipelines.\nConsequently, after the expansion into various branches, the\nfusion module effectively integrates the information, ensuring\na comprehensive and coherent response. The fusion module\u2019s\nreliance is not just for merging answers but also for ensuring\nthat the final output is both rich in content and reflective of\nthe multifaceted nature of the inquiry.\nLLM fusion .One of the most straightforward methods for\nmulti-branch aggregation is to leverage the powerful capa-\nbilities of LLMs to analyze and integrate information from\ndifferent branches. However, this approach also faces some\nchallenges, particularly when dealing with long answers that\nexceeds the LLM\u2019s context window limitation. To mitigate this\nissue, it is common practice to first summarize each branch\u2019s\nanswer, extracting the key information before inputting it into\nthe LLM, thus ensuring that the most important content is\nretained even within length constraints.\nWeighted ensemble is based on the weighted values of\ndifferent tokens generated from multiple branches, leading to\nthe comprehensive selection of the final output. This approach\ncan be calculated as :\np(y|q, Dq) =\nX\nd\u2208Dq\np(y|d, q) \u00b7 \u03bb(d, q) (23)\nThe weight \u03bb(d, q) is determined by the similarity score\nbetween the document d and the input query q. This weight is\ncalculated using the softmax function, which ensures that the\nweights are normalized and sum up to one.\n\u03bb(d, q) = es(d,q)\nP\nd\u2208Dq es(d,q) (24)\nRRF (Reciprocal Rank Fusion) is an ensemble technique\nthat synthesizes multiple retrieval result rankings into a co-\nhesive, unified list [54]. It employs a tailored weighted aver-\naging approach to enhance collective predictive performance\nand ranking precision. The method\u2019s strength is its dynamic\nweight assignment, which is informed by the interplay among\nbranches. RRF is especially potent in scenarios characterized\nby model or source heterogeneity, where it can markedly\namplify the accuracy of predictions.", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "ff7530bb-2d46-4d2e-b669-92075d8b328c": {"text": "weight assignment, which is informed by the interplay among\nbranches. RRF is especially potent in scenarios characterized\nby model or source heterogeneity, where it can markedly\namplify the accuracy of predictions.\nV. RAG F LOW AND FLOW PATTERN\nThe collaboration between operators forms the workflow\nof the module, which we refer to as RAG flow F =\n(M\u03d51 , . . . , M\u03d5n), where \u03d5 stands for the set of module param-\neters. A modular rag flow can be decomposed into a graph of\nsub-functions. Through control logic, the operators can execute\nin a predetermined pipeline, while also performing conditional,\nbranching or looping when necessary. In the simplest case. the\ngraph is a linear chain.\nAfter conducting an in-depth analysis of current RAG meth-\nods, we have identified a set of common RAG flow patterns,\ndenoted as P. These patterns transcend various application\ndomains and demonstrate a high level of consistency and\nreusability, revealing the prevalent structures and behaviors in\nprocess design. A RAG flow pattern can be defined as P =\n{M\u03d51 : {Op1} \u2192M\u03d52 : {Op2} \u2192. . .\u2192 M\u03d5n : {Opn}}\nA. Linear Pattern\nThe modules in the modular RAG system are organized in\na linear way, and can be described as Algorithm 1.\nPlinear = {M1 \u2192 M2 \u2192 . . .\u2192 Mn} (25)", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "72b198f4-966c-4d22-99b9-41cf51b1cfa4": {"text": "10\nFig. 4. Linear RAG flow pattern. Each module is processed in a fixed\nsequential order.\nFig. 5. RRR [24] is a typical linear flow that introduces a learnable query\nrewrite module before retrieval. This module employs reinforcement based on\nthe output results of the LLM.\nThe linear flow pattern is the simplest and most com-\nmonly used pattern. As shown in Figure 4, the full linear\nRAG flow pattern mainly includes pre-retrieval processing,\nretrieval, post-retrieval processing, and generation modules.\nPlinearfull = {Mindexing \u2192 Mpre-retrieval \u2192 Mretrieval \u2192\nMpost-retrieval \u2192 Mgenerate}. If there are no pre-retrieval and\npost-retrieval modules, it follows the Naive RAG paradigm.\nAlgorithm 1 Linear RAG Flow Pattern\nRequire: original query q, documents D, retriever R, lan-\nguage model LLM, pre-processing function fpre, post-\nprocessing function fpost\nEnsure: final output \u02c6y\n1: Initialize:\n2: q\u2032 \u2190 fpre(q) // Pre-process the original query\n3: Dq\u2032\n\u2190 R(q\u2032, D) // Retrieve documents related to the pre-\nprocessed query\n4: \u02c6Dq\u2032\n\u2190 fpost(q\u2032, Dq\u2032\n) // Post-process the retrieved docu-\nments\n5: \u02c6y \u2190 LLM([q, \u02c6Dq\u2032\n]) // Generate output using the lan-\nguage model with the original query and post-processed\ndocuments\n6: return \u02c6y // Return the final output\nCommon linear RAG flow involves a query transform\nmodule (such as rewrite or HyDE operators) at the pre-retrieval\nstage and utilize rerank at the post-retrieval stage. Rewrite-\nRetrieve-Read (RRR) [24] is a typical linear structure. As\nillustrated in Figure 5, the query rewrite module frewrite is a\nsmaller trainable language model fine-tuned on T5-large, and\nin the context of reinforcement learning, the optimization of\nthe rewriter is formalized as a Markov decision process, with\nthe final output of the LLM serving as the reward. The retriever\nutilizes a sparse encoding model, BM25.\nB. Conditional Pattern\nThe RAG flow with conditional structure involves select-\ning different RAG pipeline based on different conditions,\nas illustrated in Figure 6. A detailed definition is shown in\nAlgorithm 2. Typically, pipleline selection is accomplished\nFig. 6. The conditional flow pattern. There is a routing module that controls\nwhich RAG flow the query is directed to. Typically, different flows are used for\nvarious configurations to meet the general requirements of the RAG system.\nFig. 7. Pre-retrieval branching flow pattern.Each branch performs retrieval\nand generation separately, and then they are aggregated at the end.\nthrough a routing module that determines the next module\nin the flow.", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "5d413234-47cc-44f1-8949-b51f8b15cfca": {"text": "Fig. 7. Pre-retrieval branching flow pattern.Each branch performs retrieval\nand generation separately, and then they are aggregated at the end.\nthrough a routing module that determines the next module\nin the flow.\nPconditional = {Mi\nfr\n\u2212 \u2192Mj \u2228 Mk} (26)\nWhere\nfr\n\u2212 \u2192represents that based on routing function fr(\u00b7), the\nflow can go to module Mj or Mk.\nAlgorithm 2 Conditional RAG Flow Pattern\nRequire: original query q, documents D, language model\nLM, retriever R, routing function fr\nEnsure: final output \u02c6y\n1: Initialize:\n2: q\u2032 \u2190 QueryTransform(q) // Pre-process the initial query\nif needed\n3: D\u2032 \u2190 R(q\u2032, D) // Retrieve or update documents related\nto the query\n4: Mnext \u2190 fr(q\u2032, D\u2032) // Determine the next module using\nthe routing function\n5: if Mnext = Mj then\n6: \u02c6y \u2190 Mj(q\u2032, D\u2032) // Execute module Mj\n7: else if Mnext = Mk then\n8: \u02c6y \u2190 Mk(q\u2032, D\u2032) Mk\n9: end if\n10: return \u02c6y\nPipeline selection is determined by the nature of the ques-\ntion, directing different flows tailored to specific scenarios. For\nexample, the tolerance for responses generated by LLMs varies\nacross questions related to serious issues, political matters,\nor entertainment topics. These routing flow often diverge in\nterms of retrieval sources, retrieval processes, configurations,\nmodels, and prompts.", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "ef49aff6-6e98-4382-87f0-3e927d5c89a4": {"text": "11\nFig. 8. Post-retrieval branching flow pattern.Only one retrieval performed, and\nthen generation is carried out separately for each retrieved document chunks,\nfollowed by aggregation.\nC. Branching\nIn many cases, the RAG flow system may have multiple\nparallel running branches , usually to increase the diver-\nsity of generated results. Assuming multiple branches bi are\ngenerated in module B = Msplit(\u00b7) = {b1, b2, . . . , bm}.\nFor each branch bi \u2208 B, the same or different RAG pro-\ncesses can be executed, passing through multiple processing\nmodules {M1, M2, . . . , Mk} to obtain branch output result\npi = Mik(. . . Mi2(Mi1(bi)) . . .). The results of multiple\nbranches are aggregated using an aggregation function to\nobtain intermediate output results. \u02c6O = Mmerge({pi | bi \u2208\nB}). However, aggregation is not necessarily the end of the\nRAG flow, as it can continue to connect to other modules,\nMjn(. . . Mj2(Mj1( \u02c6O)) . . .). For example, after aggregating\nmultiple model responses, they can continue through a val-\nidation module. Therefore, the entire branch flow pattern can\nbe represented as:\nPbranch =Mjn(. . . Mj1(Mmerge({Mik\n(. . . Mi1(bi) . . .) | bi \u2208 Msplit(q)})) . . .) (27)\nAlgorithm 3 Pre-retrieval Branching Flow Pattern\nRequire: original query q, documents D, query expand mod-\nule Mexpand, retriever Mretrieve, language model LLM,\nmerge module Mmerge\nEnsure: final output \u02c6y\n1: Initialize:\n2: Q\u2032 \u2190 Mexpand(q) // Expand the original query to multiple\nsub-queries\n3: for all q\u2032\ni \u2208 Q\u2032 do\n4: D\u2032\ni \u2190 Mretrieve(q\u2032\ni, D) // Retrieve documents for each\nsub-query\n5: Gi \u2190 \u2205// Initialize an empty set for generated results\nof the sub-query\n6: for all d\u2032\nij \u2208 D\u2032\ni do\n7: yij \u2190 LLM([q\u2032\ni, d\u2032\nij]) // Generate results for each\ndocument of the sub-query\n8: Oi \u2190 Oi \u222a {yij} // Add generated results to the set\n9: end for\n10: \u02c6y \u2190 Mmerge(Oi) // Merge generated results of the sub-\nquery into the final result\n11: end for\n12: return \u02c6y\nThe RAG flow with a branching structure differs from\nthe conditional approach in that it involves multiple parallel\nbranches, as opposed to selecting one branch from multiple\noptions in the conditional approach. Structurally, it can be\ncategorized into two types, which are depicted in Figure 7\nand Figure 8.\nPre-Retrieval Branching (Multi-Query, Parallel Retrieval).\nAs shown in Algorithm 3, the process involves initially taking\na query q and expanding it through a module Mexpand to gen-\nerate multiple sub-queries Q\u2032. Each sub-query q\u2032\ni is then used\nto retrieve relevant documents via Mretrieve, forming document", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "4ed70df3-3cc2-43a6-88e3-a0dac88a4712": {"text": "As shown in Algorithm 3, the process involves initially taking\na query q and expanding it through a module Mexpand to gen-\nerate multiple sub-queries Q\u2032. Each sub-query q\u2032\ni is then used\nto retrieve relevant documents via Mretrieve, forming document\nsets D\u2032\ni. These document sets, along with the corresponding\nsub-queries, are fed into a generation module Mgenerate to\nproduce a set of answers Gi. Ultimately, all these generated\nanswers are combined using a merging module Mmerge to\nform the final result y. This entire flow can be mathematically\nrepresented as:\nPbranchpre =Mmerge(q\u2032\ni\u2208Mexpand(q){Mgenerate(q\u2032\ni, d\u2032\nij) |\nd\u2032\nij \u2208 Mretrieve(q\u2032\ni)}) (28)\nPost-Retrieval Branching (Single Query, Parallel Genera-\ntion). As shown in Algorithm 4, in the post-retrieval branching\npattern, the process starts with a single query q which is\nused to retrieve multiple document chunks through a retrieval\nmodule Mretrieve, resulting in a set of documents Dq. Each\ndocument dq\ni from this set is then independently processed by\na generation module Mgenerate to produce a set of generated\nresults G. These results are subsequently merged using a\nmerge module Mmerge to form the final result y. The process\ncan be succinctly represented as y = Mmerge(Oi), where Oi is\nthe collection of all generated results from each document dq\ni\nin Dq. Therefore, the entire process can be represented as:\nPbranchpost = Mmerge({Mgenerate(dq\ni ) | dq\ni \u2208 Mretrieve(q)})\n(29)\nAlgorithm 4 Post-retrieval Branching Flow Pattern\nRequire: original query q, documents D, retriever R, lan-\nguage model LLM, merge module Mmerge\nEnsure: final output \u02c6y\n1: Initialize:\n2: q\u2032 \u2190 fpre(q) // Pre-process the original query\n3: Dq\u2032\n\u2190 R(q\u2032, D) // Retrieve a set of documents based on\nthe pre-processed query\n4: G \u2190 \u2205// Initialize an empty set to store generated results\n5: for all di \u2208 Dq\u2032\ndo\n6: yi \u2190 LLM([q, di]) // Generate results independently\nfor each document chunk using the language model\n7: Oi \u2190 Oi \u222a {yi} // Add the generated result to the set\nof results\n8: end for\n9: \u02c6y \u2190 Mmerge(Oi) // Merge all generated results using the\nmerge function\n10: return \u02c6y\nREPLUG [55] embodies a classic post-retrieval branching\nstructure, wherein the probability of each token is predicted\nfor each branch. Through weighted possibility ensemble, the\ndifferent branches are aggregated, and the final generation", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "84da14d6-b883-46da-aa4e-a063cb1bd93d": {"text": "12\nFig. 9. The RAG flow in REPLUG [55], which follows a typical post-retrieval\nbranching pattern. Each retrieved chunks undergoes parallel generation, and\nthen they are aggregated using a weighted probability ensemble.\nresult is used to fine-tune the retriever, known as Contriever,\nthrough feedback.\nD. Loop Pattern\nThe RAG flow with a loop structure, as an important char-\nacteristic of Modular RAG, involves interdependent retrieval\nand generation steps. It typically includes a scheduling module\nfor flow control. The modular RAG system can be abstracted\nas a directed graph G = (V, E), where V is the set of vertices\nrepresenting the various modules Mi in the system, and E is\nthe set of edges representing the control flow or data flow be-\ntween modules. If there is a vertex sequence Mi1 , Mi2 , ..., Min\nsuch that Min can reach Mi1 (i.e., Min \u2192 Mi1 ), then this\nRAG system forms a loop. If Mj is the successor module of\nMi and Mi decides whether to return to Mj or a previous\nmodule Mk through a Judge module, it can be represented\nas: Mi\nJudge\n\u2212 \u2212 \u2212 \u2192Mj or Mi\nJudge\n\u2212 \u2212 \u2212 \u2192Mk where Mk is the\npredecessor module of Mj. If Mi return to Mj, it can be\nrepresented as: \u2203Judge(Mi, Mj) s.t. (Mi, Mj) \u2208 E and\nJudge(Mi, Mj) = true. If the Judge module not to return\nto any previous module, it can be represented as: \u2200Mi \u2208\nV, Judge(Mi, Mj) = false for all Mj that are predecessors\nof Mi. Loop pattern can be further categorized into iterative,\nrecursive, and adaptive (active) retrieval approaches.\nIterative retrieval At times, a single retrieval and genera-\ntion may not effectively address complex questions requiring\nextensive knowledge. Therefore, an iterative approach can be\nused in RAG (see Algorithm 5), typically involving a fixed\nnumber of iterations for retrieval. At step t, given the query\nqt and the previous output sequence y<t = [ y0, . . . , yt\u22121] ,\niterations proceed under the condition that t is less than the\nmaximum allowed iterations T. In each loop, it retrieves a\ndocument chunks Dt\u22121 using the last output yt\u22121 and the\ncurrent query qt. Subsequently, a new output yt is generated.\nThe continuation of the iteration is determined by a Judge\nmodule, which makes its decision based on the yt, y<t, qt,\nand the Dt\u22121.\nAn exemplary case of iterative retrieval is ITER-\nRETGEN [56] (Figure 11), which iterates retrieval-augmented\ngeneration and generation-augmented retrieval. Retrieval-\naugmented generation outputs a response to a task input based\non all retrieved knowledge. In each iteration, ITER-RETGEN", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "ea82ba34-3b21-417c-be6a-e526df072a5f": {"text": "RETGEN [56] (Figure 11), which iterates retrieval-augmented\ngeneration and generation-augmented retrieval. Retrieval-\naugmented generation outputs a response to a task input based\non all retrieved knowledge. In each iteration, ITER-RETGEN\nleverages the model output from the previous iteration as a\nspecific context to help retrieve more relevant knowledge.\nFig. 10. Loop flow pattern. Typically, a RAG system performs multiple rounds\nof retrieval and generation. It can be categorized into three forms: iterative,\nrecursive, and adaptive.\nAlgorithm 5 Iterative RAG Flow Pattern\nRequire: original query q, documents D, maximum iterative\ntimes T, language model LLM, retriever R, initial output\ny<1 = \u2205\nEnsure: final output \u02c6y\n1: Initialize:\n2: qt \u2190 q // Initialize query for the first iteration\n3: y<1 \u2190 \u2205// Initialize previous outputs as empty\n4: t \u2190 1 // Initialize iteration step\n5: while t \u2264 T do\n6: qt \u2190 QueryTransform(y<t\u22121, qt\u22121) // Generate query\nbased on previous output and original query\n7: Dt \u2190 R(yt\u22121||qt, D) // Retrieve or update documents\nrelated to the current query\n8: yt \u2190 LLM([y<t\u22121, qt, Dt]) // Generate output using\nthe language model\n9: y<t \u2190 [y<t\u22121, yt] // Update the list of previous outputs\n10: if Judge(yt, q) = false then\n11: break\n12: end if\n13: t \u2190 t + 1 // Increment iteration step\n14: end while\n15: yfinal = synthesizeOutput(y\u2264t) // Synthesize final output\nfrom the list of outputs\n16: return \u02c6y", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "1ab61aea-3b7a-49bb-ac56-b21db3812968": {"text": "13\nFig. 11. ITER-RETGEN [56] is a typical iterative structure. Multiple rounds\nof retrieval and generation are performed within the limit of the maximum\nnumber of iterations.\nTermination of the loop is determined by a predefined number\nof iterations.\nRecursive retrieval The characteristic feature of recursive\nretrieval (see Algorithm 6), as opposed to iterative retrieval, is\nits clear dependency on the previous step and its continuous\ndeepening of retrieval. Typically, it follows a tree-like structure\nand there is a clear termination mechanism as an exit condition\nfor recursive retrieval. In RAG systems, recursive retrieval usu-\nally involves query transform, relying on the newly rewritten\nquery for each retrieval.\nAlgorithm 6 Recursive RAG Flow Pattern\nRequire: initial query q, document D, retriever R, language\nmodel LM, maximum recursive depth Kmax\nEnsure: final output \u02c6y\n1: Initialize:\n2: Q \u2190 {q}\n3: k \u2190 0 // Initialize recursion depth\n4: while Q \u0338= \u2205 and k < Kmax do\n5: Q\u2032 \u2190 \u2205// To store queries for the next recursion level\n6: for all q \u2208 Q do\n7: Dq \u2190 R(q, D) // Retrieve or update documents\nrelated to the current query\n8: Y \u2190 LM([q, Dq]) // Generate outputs using the\nlanguage model\n9: Q\u2032\u2032 \u2190 deriveNewQueries(q, Dq, Y) // Derive new\nqueries from generated outputs\n10: for all q\u2032 \u2208 Q\u2032\u2032 do\n11: if q\u2032 /\u2208 Q\u2032 and q\u2032 /\u2208 Q then\n12: Q\u2032 \u2190 Q\u2032 \u222a {q\u2032}\n13: end if\n14: end for\n15: end for\n16: Q \u2190 Q\u2032 // Update the set of queries for the next\nrecursion\n17: k \u2190 k + 1 // Increment recursion depth\n18: end while\n19: \u02c6y = synthesizeOutput(Y ) // Synthesize final output from\ngenerated outputs\n20: return \u02c6y\nA typical implementation of recursive retrieval, such as\nToC [13] (see Figure 12 ), involves recursively executing RAC\n(Recursive Augmented Clarification) to gradually insert sub-\nnodes into the clarification tree from the initial ambiguous\nquestion (AQ). At each expansion step, paragraph re-ranking\nis performed based on the current query to generate a disam-\nFig. 12. RAG flow of ToC [13]. A typical characteristic of this process is\nthat each recursive retrieval uses the new query generated from the previous\nstep, thereby progressively deepening analysis of the original complex query.\nbiguous Question (DQ). The exploration of the tree concludes\nupon reaching the maximum number of valid nodes or the\nmaximum depth. Once the clarification tree is constructed,\nToC gathers all valid nodes and generates a comprehensive\nlong-text answer to address AQ.\nAdaptive (Active) retrieval With the advancement of RAG,", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "c94b1d06-f4d8-4ae6-a479-3a5765acdf99": {"text": "maximum depth. Once the clarification tree is constructed,\nToC gathers all valid nodes and generates a comprehensive\nlong-text answer to address AQ.\nAdaptive (Active) retrieval With the advancement of RAG,\nthere has been a gradual shift beyond passive retrieval to the\nemergence of adaptive retrieval (see Algorithm 7) , also known\nas active retrieval, which is partly attributed to the powerful\ncapabilities of LLM. This shares a core concept with LLM\nAgent [57]. RAG systems can actively determine the timing\nof retrieval and decide when to conclude the entire process and\nproduce the final result. Based on the criteria for judgment,\nthis can be further categorized into Prompt-base and Tuning-\nbase approaches.\nAlgorithm 7 Active RAG Flow Pattern\nRequire: original query Q, documents D, maximum iterative\ntimes T, language model LLM, retriever R\nEnsure: final output \u02c6y\n1: Initialize:\n2: t \u2190 1 // Initialize loop step\n3: qt \u2190 q // Initialize query for the first iteration\n4: y<1 \u2190 \u2205// Initialize previous outputs as empty\n5: while t \u2264 T do\n6: Qt \u2190 QueryTransform(y<t\u22121, qt\u22121) // Derive new\nquery from previous output and query\n7: if Evaluate(Qt, y<t\u22121) then\n8: Dt \u2190 R(qt, D) // Retrieve documents based on the\nnew query\n9: yt \u2190 LLM([qt, Dt]) // Generate output using the\nlanguage model\n10: else\n11: yt \u2190 \u2205// Set output as empty if query evaluation is\nfalse\n12: end if\n13: y<t \u2190 [y<t\u22121, yt] // Update the list of previous outputs\n14: if isOutputAcceptable(yt, y<t, qt) = false then\n15: break // Break if the output is not acceptable\n16: end if\n17: t \u2190 t + 1 // Increment iteration step\n18: end while\n19: \u02c6y = synthesizeOutput(y\u2264t) // Synthesize final output from\nthe list of outputs\n20: return \u02c6y\nPrompt-base. The prompt-base approach involves control-\nling the flow using Prompt Engineering to direct LLM. A", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "1ac13e92-99ec-422c-b1ac-f3d695177f5b": {"text": "14\nFig. 13. RAG flow of FLARE [14]. The generated provisional answer will\nundergo confidence assessment. If it does not meet the required confidence\nlevel, the process will return to the retrieval stage and generate anew. The\nassessment criteria are implemented through prompt\nFig. 14. RAG flow of SELF-RAG [28]. First, it prompt GPT-4 to obtain\na suitable instruct fine-tuning dataset to fine-tune the deployed open-source\nLLM. This allows the model to output four specific tokens during generation,\nwhich are used to control the RAG process.\ntypical implementation example is FLARE [14]. Its core\nconcept is that LLMs should only retrieve when essential\nknowledge is lacking, to avoid unnecessary or inappropriate\nretrieval in an enhanced LM. FLARE iteratively generates the\nnext provisional sentence and checks for the presence of low-\nprobability tokens. If found, the system retrieves relevant docu-\nments and regenerates the sentence. Tuning-base. The tuning-\nbased approach involves fine-tuning LLM to generate special\ntokens, thereby triggering retrieval or generation. This concept\ncan be traced back to Toolformer [50], where the generation of\nspecific content assists in invoking tools. In RAG systems, this\napproach is used to control both retrieval and generation steps.\nA typical case is Self-RAG [28](see Figure 14). Given an\ninput prompt and the preceding generation result, first predict\nwhether the special token Retrieve is helpful for enhancing\nthe continued generation through retrieval. Then, if retrieval\nis needed, the model generates a critique token to evaluate the\nretrieved passage\u2019s relevance. and a critique token to evaluate\nif the information in the response is supported by the retrieved\npassage. Finally, a critique token evaluates the overall utility of\nthe response and selects the optimal result as the final output.\nE. Tuning Pattern\nRAG is continuously integrating with more LLM-related\ntechnologies. In Modular RAG, many components are com-\nposed of trainable language models. Through fine-tuning, the\nperformance of the components and the compatibility with\nthe overall flow can be further optimized. This section will\nintroduce three main patterns of fine-tuning stages, namely\nretriever fine-tuning, generator fine-tuning, and dual fine-\ntuning.\nFig. 15. Retriever fine-tuning pattern, mainly includes direct SFT, adding\ntrainable adapter, LM-supervised retrieval and LLM Reward RL.\n1) Retriever FT: In the RAG flow, common methods for\nfine-tuning the retriever is shown in Figure 15 ,which include:", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "276d51f0-bda2-4710-aed6-78434b701d00": {"text": "trainable adapter, LM-supervised retrieval and LLM Reward RL.\n1) Retriever FT: In the RAG flow, common methods for\nfine-tuning the retriever is shown in Figure 15 ,which include:\n\u2022 Direct supervised fine-tuning of the retriever. Construct-\ning a specialized dataset for retrieval and fine-tuning the\ndense retriever. For example, using open-source retrieval\ndatasets or constructing one based on domain-specific\ndata.\n\u2022 Adding trainable adapter modules. Sometimes, direct\nfine-tuning of the API-base embedding model (e.g., Ope-\nnAI Ada-002 and Cohere) is not feasible. Incorporating\nan adapter module can enhance the representation of\nyour data. Additionally, the adapter module facilitates\nbetter alignment with downstream tasks, whether for task-\nspecific (e.g., PRCA [42]) or general purposes (e.g.,\nAAR [58]).\n\u2022 LM-supervised Retrieval (LSR). Fine-tuning the retriever\nbased on the results generated by LLM.\n\u2022 LLM Reward RL. Still using the LLM output results as\nthe supervisory signal. Employing reinforcement learning\nto align the retriever with the generator. The whole re-\ntrieval process is disassembled in the form of a generative\nMarkov chain.\n2) Generator FT: The primary methods for fine-tuning a\ngenerator in RAG flow is shown in Figure 16, which include:\n\u2022 Direct supervised fine-tuning . Fine-tuning through an\nexternal dataset can supplement the generator with ad-\nditional knowledge. Another benefit is the ability to\ncustomize input and output formats. By setting the Q&A\nformat, LLM can understand specific data formats and\noutput according to instructions.\n\u2022 Distillation. When using on-premise deployment of open-\nsource models, a simple and effective Optimization\nmethod is to use GPT-4 to batch construct fine-tuning\ndata to enhance the capabilities of the open-source model.\n\u2022 RL from LLM/human feedback. Reinforcement learning\nbased on feedback from the final generated answers. In\naddition to using human evaluations, powerful LLMs can\nalso serve as an evaluative judge.\n3) Dual FT: In the RAG system, fine-tuning both the\nretriever and the generator simultaneously is a unique feature\nof the RAG system. It is important to note that the emphasis\nof system fine-tuning is on the coordination between the\nretriever and the generator. An exemplary implementation is\nRA-DIT [27], which fine-tunes both the LLM and the retriever.\nThe LM-ft component updates the LLM to maximize the", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "713068a1-920e-41eb-adad-d6cd954fdf69": {"text": "15\nFig. 16. Generator fine-tuning pattern, The main methods include SFT,\ndistillation and RL from LLM/human feedback.\nFig. 17. Dual fine-tuning pattern. In this mode, both the retriever and\ngenerator participate in fine-tuning, and their preferences will be aligned.\nlikelihood of the correct answer given the retrieval-augmented\ninstructions while the R-ft component updates the retriever\nto minimize the KL-Divergence between the retriever score\ndistribution and the LLM preference.\nVI. D ISCUSSION\nIn this chapter, we explore the innovative horizons opened\nby the modular RAG paradigm. We examine its compatibility\nwith cutting-edge methodologies in the progression of RAG\ntechnology, emphasizing its scalability. It not only fosters a\nfertile ground for model innovation but also paves the way for\nseamless adaptation to the dynamic requirements of various\napplications.\nA. Opportunities in Modular RAG\nThe benefits of Modular RAG are evident, providing a\nfresh and comprehensive perspective on existing RAG-related\nwork. Through modular organization, relevant technologies\nand methods are clearly summarized.\nFrom a research perspective. Modular RAG is highly\nscalable, it empowers researchers to introduce innovative mod-\nules and operators, leveraging a deep understanding of RAG\u2019s\nevolving landscape. This flexibility enables the exploration of\nnew theoretical and practical dimensions in the field.\nFrom an application perspective . The modularity of RAG\nsystems simplifies their design and implementation. Users can\ntailor RAG flows to fit their specific data, use cases, and\ndownstream tasks, enhancing the adaptability of the system\nto diverse requirements. Developers can draw from existing\nflow architectures and innovate by defining new flows and\npatterns that are tailored to various application contexts and\ndomains. This approach not only streamlines the development\nprocess but also enriches the functionality and versatility of\nRAG applications.\nB. Compatibility with new methods\nModular RAG paradigm demonstrates exceptional compati-\nbility with new developments. To gain a deeper understanding\nof this, we list three typical scalability cases, which clearly\nshows that Modular RAG paradigm provides robust support\nand flexibility for the innovation and development of RAG\ntechnology.\n1) Recombination of the current modules: In this scenario,\nno new modules or operators are proposed; rather, specific\nproblems are addressed through the combination of existing\nmodules.DR-RAG [59] employs a two-stage retrieval strategy", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "9d1e8ccb-57a8-4309-b324-adfd4be130ac": {"text": "technology.\n1) Recombination of the current modules: In this scenario,\nno new modules or operators are proposed; rather, specific\nproblems are addressed through the combination of existing\nmodules.DR-RAG [59] employs a two-stage retrieval strategy\nand classifier selection mechanism, incorporating a branching\nretrieval structure. In the first stage, retrieving chunks relevant\nto the query. In the second stage, the query is combined\nindividually with each chunk retrieved in the first stage, and a\nparallel secondary retrieval is conducted. The retrieved content\nis then input into a classifier to filter out the most relevant\ndynamic documents. This ensures that the retrieved documents\nare highly relevant to the query while reducing redundant\ninformation. DR-RAG improved retrieval method significantly\nenhances the accuracy and efficiency of answers, bolstering\nRAG\u2019s performance in multi-hop question-answering scenar-\nios.\n2) New flow without adding new operators.: This refers\nto redesigning the processes for retrieval and generation to\naddress more complex scenarios without proposing new mod-\nules. The core idea of PlanRAG [18] lies in its introduction of\na preliminary planning stage, a crucial step that occurs before\nretrieval and generation. Initially, the system employs a judge\nmodule to assess whether the current context necessitates the\nformulation of a new plan or adjustments to an existing one.\nWhen encountering a problem for the first time, the system\ninitiates the planning process, while in subsequent interactions,\nit decides whether to execute re-planning based on previous\nplans and retrieved data.\nNext, the system devises an execution plan tailored to the\nquery, treating this process as a logical decomposition of\ncomplex queries. Specifically, PlanRAG uses a query expan-\nsion module to extend and refine the query. For each derived\nsub-query, the system conducts targeted retrieval. Following\nretrieval, another judge module evaluates the current results to\ndecide whether further retrieval is required or if it should return\nto the planning stage for re-planning. Through this strategy,\nPlanRAG is able to handle complex decision-making problems\nthat require multi-step data analysis more efficiently.\n3) New flow derived from new operators.: New operators\noften introduce novel flow design, exemplified by Multi-Head\nRAG [60]. Existing RAG solutions do not focus on queries that\nmay require retrieving multiple documents with significantly\ndifferent content. Such queries are common but difficult to", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "6173b65a-34c0-490b-b24d-95b979fd7238": {"text": "often introduce novel flow design, exemplified by Multi-Head\nRAG [60]. Existing RAG solutions do not focus on queries that\nmay require retrieving multiple documents with significantly\ndifferent content. Such queries are common but difficult to\nhandle because embeddings of these documents may be far\napart in the embedding space. Multi-Head RAG addresses this\nby designing a new retriever that uses the activations of the\nmulti-head attention layers of the Transformer, rather than the\ndecoder layers, as keys for retrieving multifaceted documents.\nDifferent attention heads can learn to capture different aspects\nof the data. By using the corresponding activation results,\nembeddings that represent different aspects of the data items\nand the query can be generated, thereby enhancing the retrieval\naccuracy for complex queries.", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "8047fd18-6b39-45f5-8aa3-3922c92b4851": {"text": "16\nVII. C ONCLUSION\nRAG is emerging as a pivotal technology for LLM applica-\ntions. As technological landscapes evolve and the intricacies of\napplication requirements escalate, RAG systems are being en-\nhanced by integrating a diverse suite of technologies, thereby\nachieving a higher level of complexity and functionality. This\npaper introduces the innovative paradigm of Modular RAG.\nThis approach systematically disassembles the complex archi-\ntecture of RAG systems into well-defined, discrete functional\nmodules. Each module is meticulously characterized by its\nspecific operational functions, ensuring clarity and precision.\nTherefore, the entire system is composed of those modules\nand operators, akin to Lego bricks. By conducting an in-\ndepth analysis of numerous studies, the paper also distills\ncommon RAG design patterns and scrutinizes key case studies\nto illustrate these patterns in practice.\nModular RAG not only offers a structured framework for\nthe design and application of RAG systems but also en-\nables a scenario-based customization of these systems. The\nmodularity inherent in this design facilitates ease of tracking\nand debugging, significantly enhancing the maintainability and\nscalability of RAG systems. Furthermore, Modular RAG opens\nup new avenues for the future progression of RAG technology.\nIt encourages the innovation of novel functional modules and\nthe crafting of innovative workflows, thereby driving forward\nthe frontiers of RAG systems.\nREFERENCES\n[1] Y . Zhang, Y . Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\nY . Zhang, Y . Chenet al., \u201cSiren\u2019s song in the ai ocean: A survey on hal-\nlucination in large language models,\u201d arXiv preprint arXiv:2309.01219,\n2023.\n[2] Y . Gao, Y . Xiong, X. Gao, K. Jia, J. Pan, Y . Bi, Y . Dai, J. Sun, and\nH. Wang, \u201cRetrieval-augmented generation for large language models:\nA survey,\u201d arXiv preprint arXiv:2312.10997 , 2023.\n[3] Z. Xu, M. J. Cruz, M. Guevara, T. Wang, M. Deshpande, X. Wang,\nand Z. Li, \u201cRetrieval-augmented generation with knowledge graphs\nfor customer service question answering,\u201d in Proceedings of the 47th\nInternational ACM SIGIR Conference on Research and Development in\nInformation Retrieval, 2024, pp. 2905\u20132909.\n[4] C. Zhang, S. Wu, H. Zhang, T. Xu, Y . Gao, Y . Hu, and E. Chen,\n\u201cNotellm: A retrievable large language model for note recommendation,\u201d\nin Companion Proceedings of the ACM on Web Conference 2024 , 2024,\npp. 170\u2013179.\n[5] R. Anantha, T. Bethi, D. V odianik, and S. Chappidi, \u201cContext tuning", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "6744a07f-5577-409d-acb3-25e5106270d7": {"text": "\u201cNotellm: A retrievable large language model for note recommendation,\u201d\nin Companion Proceedings of the ACM on Web Conference 2024 , 2024,\npp. 170\u2013179.\n[5] R. Anantha, T. Bethi, D. V odianik, and S. Chappidi, \u201cContext tuning\nfor retrieval augmented generation,\u201d arXiv preprint arXiv:2312.05708 ,\n2023.\n[6] Y . Gao, T. Sheng, Y . Xiang, Y . Xiong, H. Wang, and J. Zhang, \u201cChat-\nrec: Towards interactive and explainable llms-augmented recommender\nsystem,\u201d arXiv preprint arXiv:2303.14524 , 2023.\n[7] J. Liu, \u201cBuilding production-ready rag applications,\u201d https://www.ai.\nengineer/summit/schedule/building-production-ready-rag-applications,\n2023.\n[8] D. S. Asudani, N. K. Nagwani, and P. Singh, \u201cImpact of word embedding\nmodels on text analytics in deep learning environment: a review,\u201d\nArtificial intelligence review, vol. 56, no. 9, pp. 10 345\u201310 425, 2023.\n[9] F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano,\nY . Maarek, N. Tonellotto, and F. Silvestri, \u201cThe power of noise:\nRedefining retrieval for rag systems,\u201d arXiv preprint arXiv:2401.14887,\n2024.\n[10] W. Peng, G. Li, Y . Jiang, Z. Wang, D. Ou, X. Zeng, E. Chen et al. ,\n\u201cLarge language model based long-tail query rewriting in taobao search,\u201d\narXiv preprint arXiv:2311.03758 , 2023.\n[11] Y . Xi, J. Lin, W. Liu, X. Dai, W. Zhang, R. Zhang, R. Tang, and\nY . Yu, \u201cA bird\u2019s-eye view of reranking: from list level to page level,\u201d\nin Proceedings of the Sixteenth ACM International Conference on Web\nSearch and Data Mining , 2023, pp. 1075\u20131083.\n[12] Z. Feng, X. Feng, D. Zhao, M. Yang, and B. Qin, \u201cRetrieval-\ngeneration synergy augmented large language models,\u201d arXiv preprint\narXiv:2310.05149, 2023.\n[13] G. Kim, S. Kim, B. Jeon, J. Park, and J. Kang, \u201cTree of clarifica-\ntions: Answering ambiguous questions with retrieval-augmented large\nlanguage models,\u201d arXiv preprint arXiv:2310.14696 , 2023.\n[14] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y . Yang,\nJ. Callan, and G. Neubig, \u201cActive retrieval augmented generation,\u201d arXiv\npreprint arXiv:2305.06983, 2023.\n[15] D. Edge, H. Trinh, N. Cheng, J. Bradley, A. Chao, A. Mody, S. Truitt,\nand J. Larson, \u201cFrom local to global: A graph rag approach to query-\nfocused summarization,\u201d arXiv preprint arXiv:2404.16130 , 2024.\n[16] Q. Leng, K. Uhlenhuth, and A. Polyzotis, \u201cBest practices for\nllm evaluation of rag applications,\u201d https://www.databricks.com/blog/\nLLM-auto-eval-best-practices-RAG, 2023.\n[17] X. Wang, Z. Wang, X. Gao, F. Zhang, Y . Wu, Z. Xu, T. Shi, Z. Wang,", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "d248f1b3-bb7a-4edc-bf16-870ac7163dd0": {"text": "[16] Q. Leng, K. Uhlenhuth, and A. Polyzotis, \u201cBest practices for\nllm evaluation of rag applications,\u201d https://www.databricks.com/blog/\nLLM-auto-eval-best-practices-RAG, 2023.\n[17] X. Wang, Z. Wang, X. Gao, F. Zhang, Y . Wu, Z. Xu, T. Shi, Z. Wang,\nS. Li, Q. Qian et al., \u201cSearching for best practices in retrieval-augmented\ngeneration,\u201d arXiv preprint arXiv:2407.01219 , 2024.\n[18] M. Lee, S. An, and M.-S. Kim, \u201cPlanrag: A plan-then-retrieval aug-\nmented generation for generative large language models as decision\nmakers,\u201d arXiv preprint arXiv:2406.12430 , 2024.\n[19] D. Arora, A. Kini, S. R. Chowdhury, N. Natarajan, G. Sinha, and\nA. Sharma, \u201cGar-meets-rag paradigm for zero-shot information re-\ntrieval,\u201d arXiv preprint arXiv:2310.20158 , 2023.\n[20] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal,\nH. K \u00a8uttler, M. Lewis, W.-t. Yih, T. Rockt \u00a8aschel et al. , \u201cRetrieval-\naugmented generation for knowledge-intensive nlp tasks,\u201d Advances in\nNeural Information Processing Systems , vol. 33, pp. 9459\u20139474, 2020.\n[21] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli-\ncan, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark et al.,\n\u201cImproving language models by retrieving from trillions of tokens,\u201d in\nInternational conference on machine learning. PMLR, 2022, pp. 2206\u2013\n2240.\n[22] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,\nJ. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, \u201cFew-shot\nlearning with retrieval augmented language models,\u201d arXiv preprint\narXiv:2208.03299, 2022.\n[23] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, \u201cInterleav-\ning retrieval with chain-of-thought reasoning for knowledge-intensive\nmulti-step questions,\u201d arXiv preprint arXiv:2212.10509 , 2022.\n[24] X. Ma, Y . Gong, P. He, H. Zhao, and N. Duan, \u201cQuery rewrit-\ning for retrieval-augmented large language models,\u201d arXiv preprint\narXiv:2305.14283, 2023.\n[25] N. Anderson, C. Wilson, and S. D. Richardson, \u201cLingua: Addressing\nscenarios for live interpretation and automatic dubbing,\u201d in Proceedings\nof the 15th Biennial Conference of the Association for Machine\nTranslation in the Americas (Volume 2: Users and Providers Track and\nGovernment Track), J. Campbell, S. Larocca, J. Marciano, K. Savenkov,\nand A. Yanishevsky, Eds. Orlando, USA: Association for Machine\nTranslation in the Americas, Sep. 2022, pp. 202\u2013209. [Online].\nAvailable: https://aclanthology.org/2022.amta-upg.14\n[26] L. Luo, Y .-F. Li, G. Haffari, and S. Pan, \u201cReasoning on graphs: Faith-", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "d5e5f4da-fed7-4d84-8058-eaeb1e61e635": {"text": "Translation in the Americas, Sep. 2022, pp. 202\u2013209. [Online].\nAvailable: https://aclanthology.org/2022.amta-upg.14\n[26] L. Luo, Y .-F. Li, G. Haffari, and S. Pan, \u201cReasoning on graphs: Faith-\nful and interpretable large language model reasoning,\u201d arXiv preprint\narXiv:2310.01061, 2023.\n[27] X. V . Lin, X. Chen, M. Chen, W. Shi, M. Lomeli, R. James, P. Rodriguez,\nJ. Kahn, G. Szilvasy, M. Lewis et al., \u201cRa-dit: Retrieval-augmented dual\ninstruction tuning,\u201d arXiv preprint arXiv:2310.01352 , 2023.\n[28] A. Asai, Z. Wu, Y . Wang, A. Sil, and H. Hajishirzi, \u201cSelf-rag: Learning\nto retrieve, generate, and critique through self-reflection,\u201d arXiv preprint\narXiv:2310.11511, 2023.\n[29] Y . Huang and J. Huang, \u201cA survey on retrieval-augmented text gen-\neration for large language models,\u201d arXiv preprint arXiv:2404.10981 ,\n2024.\n[30] Y . Hu and Y . Lu, \u201cRag and rau: A survey on retrieval-augmented\nlanguage model in natural language processing,\u201d arXiv preprint\narXiv:2404.19543, 2024.\n[31] Y . Ding, W. Fan, L. Ning, S. Wang, H. Li, D. Yin, T.-S. Chua, and\nQ. Li, \u201cA survey on rag meets llms: Towards retrieval-augmented large\nlanguage models,\u201d arXiv preprint arXiv:2405.06211 , 2024.\n[32] P. Zhao, H. Zhang, Q. Yu, Z. Wang, Y . Geng, F. Fu, L. Yang, W. Zhang,\nand B. Cui, \u201cRetrieval-augmented generation for ai-generated content:\nA survey,\u201d arXiv preprint arXiv:2402.19473 , 2024.\n[33] S. Yang, \u201cAdvanced rag 01: Small-to-\nbig retrieval,\u201d https://towardsdatascience.com/\nadvanced-rag-01-small-to-big-retrieval-172181b396d4, 2023.", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "6dbe68f0-083e-4fea-baa5-a0d0588a4d53": {"text": "17\n[34] Y . Wang, N. Lipka, R. A. Rossi, A. Siu, R. Zhang, and T. Derr,\n\u201cKnowledge graph prompting for multi-document question answering,\u201d\narXiv preprint arXiv:2308.11730 , 2023.\n[35] D. Zhou, N. Sch \u00a8arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schu-\nurmans, C. Cui, O. Bousquet, Q. Le et al. , \u201cLeast-to-most prompting\nenables complex reasoning in large language models,\u201d arXiv preprint\narXiv:2205.10625, 2022.\n[36] S. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz,\nand J. Weston, \u201cChain-of-verification reduces hallucination in large\nlanguage models,\u201d arXiv preprint arXiv:2309.11495 , 2023.\n[37] L. Gao, X. Ma, J. Lin, and J. Callan, \u201cPrecise zero-shot dense retrieval\nwithout relevance labels,\u201d arXiv preprint arXiv:2212.10496 , 2022.\n[38] H. S. Zheng, S. Mishra, X. Chen, H.-T. Cheng, E. H. Chi, Q. V . Le,\nand D. Zhou, \u201cTake a step back: Evoking reasoning via abstraction in\nlarge language models,\u201d arXiv preprint arXiv:2310.06117 , 2023.\n[39] H. Cao, \u201cRecent advances in text embedding: A comprehensive review\nof top-performing methods on the mteb benchmark,\u201d arXiv preprint\narXiv:2406.01607, 2024.\n[40] BAAI, \u201cFlagembedding,\u201d https://github.com/FlagOpen/FlagEmbedding,\n2023.\n[41] Z. Li, X. Zhang, Y . Zhang, D. Long, P. Xie, and M. Zhang, \u201cTowards\ngeneral text embeddings with multi-stage contrastive learning,\u201d arXiv\npreprint arXiv:2308.03281, 2023.\n[42] H. Yang, Z. Li, Y . Zhang, J. Wang, N. Cheng, M. Li, and J. Xiao,\n\u201cPrca: Fitting black-box large language models for retrieval question an-\nswering via pluggable reward-driven contextual adapter,\u201d arXiv preprint\narXiv:2310.18347, 2023.\n[43] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and\nP. Liang, \u201cLost in the middle: How language models use long contexts,\u201d\narXiv preprint arXiv:2307.03172 , 2023.\n[44] Y . Lyu, Z. Li, S. Niu, F. Xiong, B. Tang, W. Wang, H. Wu, H. Liu,\nT. Xu, and E. Chen, \u201cCrud-rag: A comprehensive chinese benchmark\nfor retrieval-augmented generation of large language models,\u201d arXiv\npreprint arXiv:2401.17043, 2024.\n[45] L. Xia, J. Xu, Y . Lan, J. Guo, and X. Cheng, \u201cLearning maximal\nmarginal relevance model via directly optimizing diversity evaluation\nmeasures,\u201d in Proceedings of the 38th international ACM SIGIR con-\nference on research and development in information retrieval , 2015, pp.\n113\u2013122.\n[46] Cohere, \u201cSay goodbye to irrelevant search results: Cohere rerank is\nhere,\u201d https://txt.cohere.com/rerank/, 2023.\n[47] H. Jiang, Q. Wu, X. Luo, D. Li, C.-Y . Lin, Y . Yang, and L. Qiu,", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "ab38c414-38e7-4475-a73c-969d8c30fbf3": {"text": "113\u2013122.\n[46] Cohere, \u201cSay goodbye to irrelevant search results: Cohere rerank is\nhere,\u201d https://txt.cohere.com/rerank/, 2023.\n[47] H. Jiang, Q. Wu, X. Luo, D. Li, C.-Y . Lin, Y . Yang, and L. Qiu,\n\u201cLongllmlingua: Accelerating and enhancing llms in long context sce-\nnarios via prompt compression,\u201d arXiv preprint arXiv:2310.06839, 2023.\n[48] R. Litman, O. Anschel, S. Tsiper, R. Litman, S. Mazor, and R. Man-\nmatha, \u201cScatter: selective context attentional scene text recognizer,\u201d in\nproceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, 2020, pp. 11 962\u201311 972.\n[49] J. Cui, Z. Li, Y . Yan, B. Chen, and L. Yuan, \u201cChatlaw: Open-source\nlegal large language model with integrated external knowledge bases,\u201d\narXiv preprint arXiv:2306.16092 , 2023.\n[50] T. Schick, J. Dwivedi-Yu, R. Dess `\u0131, R. Raileanu, M. Lomeli, L. Zettle-\nmoyer, N. Cancedda, and T. Scialom, \u201cToolformer: Language models\ncan teach themselves to use tools,\u201d arXiv preprint arXiv:2302.04761 ,\n2023.\n[51] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al. , \u201cTraining language\nmodels to follow instructions with human feedback,\u201d Advances in neural\ninformation processing systems , vol. 35, pp. 27 730\u201327 744, 2022.\n[52] S. J. Semnani, V . Z. Yao, H. C. Zhang, and M. S. Lam, \u201cWikichat:\nStopping the hallucination of large language model chatbots by few-\nshot grounding on wikipedia,\u201d arXiv preprint arXiv:2305.14292 , 2023.\n[53] J. Baek, S. Jeong, M. Kang, J. C. Park, and S. J. Hwang,\n\u201cKnowledge-augmented language model verification,\u201d arXiv preprint\narXiv:2310.12836, 2023.\n[54] G. V . Cormack, C. L. Clarke, and S. Buettcher, \u201cReciprocal rank\nfusion outperforms condorcet and individual rank learning methods,\u201d\nin Proceedings of the 32nd international ACM SIGIR conference on\nResearch and development in information retrieval , 2009, pp. 758\u2013759.\n[55] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-\nmoyer, and W.-t. Yih, \u201cReplug: Retrieval-augmented black-box language\nmodels,\u201d arXiv preprint arXiv:2301.12652 , 2023.\n[56] Z. Shao, Y . Gong, Y . Shen, M. Huang, N. Duan, and W. Chen,\n\u201cEnhancing retrieval-augmented large language models with iterative\nretrieval-generation synergy,\u201d arXiv preprint arXiv:2305.15294 , 2023.\n[57] S. Hong, X. Zheng, J. Chen, Y . Cheng, J. Wang, C. Zhang, Z. Wang,\nS. K. S. Yau, Z. Lin, L. Zhou et al., \u201cMetagpt: Meta programming for\nmulti-agent collaborative framework,\u201d arXiv preprint arXiv:2308.00352,\n2023.", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "18a25bbc-a6a1-4b5b-9086-9d5178e2e4ee": {"text": "[57] S. Hong, X. Zheng, J. Chen, Y . Cheng, J. Wang, C. Zhang, Z. Wang,\nS. K. S. Yau, Z. Lin, L. Zhou et al., \u201cMetagpt: Meta programming for\nmulti-agent collaborative framework,\u201d arXiv preprint arXiv:2308.00352,\n2023.\n[58] Z. Yu, C. Xiong, S. Yu, and Z. Liu, \u201cAugmentation-adapted retriever\nimproves generalization of language models as generic plug-in,\u201d arXiv\npreprint arXiv:2305.17331, 2023.\n[59] Z. Hei, W. Wei, W. Ou, J. Qiao, J. Jiao, Z. Zhu, and G. Song,\n\u201cDr-rag: Applying dynamic document relevance to retrieval-augmented\ngeneration for question-answering,\u201d arXiv preprint arXiv:2406.07348 ,\n2024.\n[60] M. Besta, A. Kubicek, R. Niggli, R. Gerstenberger, L. Weitzen-\ndorf, M. Chi, P. Iff, J. Gajda, P. Nyczyk, J. M \u00a8uller et al. , \u201cMulti-\nhead rag: Solving multi-aspect problems with llms,\u201d arXiv preprint\narXiv:2406.05085, 2024.", "metadata": {"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "pdf_url": "http://arxiv.org/pdf/2407.21059v1"}}, "6260ba66-47fd-44af-98b4-82722a1e6a73": {"text": "Seven Failure Points When Engineering a Retrieval Augmented\nGeneration System\nScott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mohamed Abdelrazek\n{scott.barnett,stefanus.kurniawan,srikanth.thudumu,zach.brannelly,mohamed.abdelrazek}@deakin.edu.au\nApplied Artificial Intelligence Institute\nGeelong, Australia\nABSTRACT\nSoftware engineers are increasingly adding semantic search capabil-\nities to applications using a strategy known as Retrieval Augmented\nGeneration (RAG). A RAG system involves finding documents that\nsemantically match a query and then passing the documents to a\nlarge language model (LLM) such as ChatGPT to extract the right\nanswer using an LLM. RAG systems aim to: a) reduce the problem\nof hallucinated responses from LLMs, b) link sources/references\nto generated responses, and c) remove the need for annotating\ndocuments with meta-data. However, RAG systems suffer from lim-\nitations inherent to information retrieval systems and from reliance\non LLMs. In this paper, we present an experience report on the\nfailure points of RAG systems from three case studies from separate\ndomains: research, education, and biomedical. We share the lessons\nlearned and present 7 failure points to consider when designing a\nRAG system. The two key takeaways arising from our work are: 1)\nvalidation of a RAG system is only feasible during operation, and\n2) the robustness of a RAG system evolves rather than designed in\nat the start. We conclude with a list of potential research directions\non RAG systems for the software engineering community.\nCCS CONCEPTS\n\u2022 Software and its engineering \u2192Empirical software valida-\ntion.\nKEYWORDS\nRetrieval Augmented Generation, RAG, SE4AI, Case Study\nACM Reference Format:\nScott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mo-\nhamed Abdelrazek . 2024. Seven Failure Points When Engineering a Retrieval\nAugmented Generation System. In Proceedings of 3rd International Confer-\nence on AI Engineering \u2014 Software Engineering for AI (CAIN 2024). ACM,\nNew York, NY, USA, 6 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\n1 INTRODUCTION\nThe new advancements of Large Language Models (LLMs), includ-\ning ChatGPT, have given software engineers new capabilities to\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation", "metadata": {"title": "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented\n  Instructions", "pdf_url": "http://arxiv.org/pdf/2501.00353v1"}}, "da762418-8d3f-4e29-b574-05ab7df795cb": {"text": "classroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nCAIN 2024, April 2024, Lisbon, Portugal\n\u00a9 2024 Association for Computing Machinery.\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\nbuild new HCI solutions, complete complex tasks, summarise docu-\nments, answer questions in a given artefact(s), and generate new\ncontent. However, LLMs suffer from limitations when it comes\nto up-to-date knowledge or domain-specific knowledge currently\ncaptured in company\u2019s repositories. Two options to address this\nproblem are: a) Finetuning LLMs (continue training an LLM using\ndomain specific artifacts) which requires managing or serving a\nfine-tuned LLM; or b) use Retrieval-Augmented Generation (RAG)\nSystems that rely on LLMs for generation of answers using existing\n(extensible) knowledge artifacts. Both options have pros and cons\nrelated to privacy/security of data, scalability, cost, skills required,\netc. In this paper, we focus on the RAG option.\nRetrieval-Augmented Generation (RAG) systems offer a com-\npelling solution to this challenge. By integrating retrieval mecha-\nnisms with the generative capabilities of LLMs, RAG systems can\nsynthesise contextually relevant, accurate, and up-to-date informa-\ntion. A Retrieval-Augmented Generation (RAG) system combines\ninformation retrieval capabilities, and generative prowess of LLMs.\nThe retrieval component focuses on retrieving relevant information\nfor a user query from a data store. The generation component fo-\ncuses on using the retrieved information as a context to generate an\nanswer for the user query. RAG systems are an important use case\nas all unstructured information can now be indexed and available\nto query reducing development time no knowledge graph creation\nand limited data curation and cleaning.\nSoftware engineers building RAG systems are expected to pre-\nprocess domain knowledge captured as artifacts in different formats,\nstore processed information in appropriate data store (vector data-\nbase), implement or integrate the right query-artifact matching", "metadata": {"title": "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented\n  Instructions", "pdf_url": "http://arxiv.org/pdf/2501.00353v1"}}, "5b59df86-9ca8-40cf-8d6a-c560447d1f71": {"text": "process domain knowledge captured as artifacts in different formats,\nstore processed information in appropriate data store (vector data-\nbase), implement or integrate the right query-artifact matching\nstrategy, rank matched artifacts, and call the LLMs API passing in\nuser queries and context documents. New advances for building\nRAG systems are constantly emerging [8, 12] but how they relate\nand perform for a specific application context has to be discovered.\nIn this work we present the lessons learned and 7 failure points\narising from 3 case studies. The purpose of this paper is to provide\n1) a reference to practitioners and 2) to present a research road\nmap for RAG systems. To the best of our knowledge, we present\nthe first empirical insight into the challenges with creating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n\u2022What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using\nthe BioASQ data set to report on potential failure points. The\nexperiment involved 15,000 documents and 1000 question\narXiv:2401.05856v1  [cs.SE]  11 Jan 2024", "metadata": {"title": "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented\n  Instructions", "pdf_url": "http://arxiv.org/pdf/2501.00353v1"}}, "89e26c92-3080-4c67-96b5-9f5a1efe4048": {"text": "CAIN 2024, April 2024, Lisbon, Portugal Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mohamed Abdelrazek\nand answer pairs. We indexed all documents then ran the\nqueries and stored the generated responses using GPT-4. All\nquestion and answer pairs were then validated with OpenAI\nevals 1. Manual inspection (all discrepancies, all flagged as\nincorrect, and a sample of correct labels) was analysed to\nidentify the patterns.\n\u2022What are the key considerations when engineering a RAG\nsystem? (section 6) We present the lessons learned from three\ncase studies involving the implementation of a RAG system.\nThis presents the challenges faced and insights gained.\nContributions arising from this work include:\n\u2022A catalogue of failure points (FP) that occur in RAG systems.\n\u2022An experience report from 3 case studies of implementing a\nRAG system. Two currently running at Deakin University.\n\u2022A research direction for RAG systems based on the lessons\nlearned from the 3 case studies.\n2 RELATED WORK\nRetrieval augmented generation encompasses using documents\nto augment large language models through pre-training and at\ninference time [7, 9, 12]. Due to the compute cost, data preparation\ntime and required resources using RAG without training or fine-\ntuning is an attractive proposition. However, challenges arise when\nusing large language models for information extraction such as\nperformance with long text [8].\nA recent survey [ 19] showed that large language models are\nused across the RAG pipeline including retriever, data generation,\nrewriter, and reader. Our work complements this survey by taking\na software engineering perspective to shine a light on what issues\nengineers will face and what software engineering research is nec-\nessary to realise solutions with the current state-of-the-art RAG\nsystems.\nEmerging work has looked at benchmarking RAG systems [3]\nbut not at the failures occurring during implementation. Software\nengineering research has investigated the use of RAG systems for\ncode-related tasks [15]. However, the application of RAG systems\nis broader than software engineering tasks. This paper comple-\nments existing work by presenting challenges faced during the\nimplementation of a RAG system with a focus on practitioners.\nErrors and failures that arise from RAG systems overlap with\nother information retrieval systems including 1) no metrics for\nquery rewriting, 2) document re-ranking, and 3) effective content\nsummarisation [19]. Our results confirm this The unique aspects", "metadata": {"title": "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented\n  Instructions", "pdf_url": "http://arxiv.org/pdf/2501.00353v1"}}, "f651ad4d-d280-4f80-8750-75fe1a470454": {"text": "other information retrieval systems including 1) no metrics for\nquery rewriting, 2) document re-ranking, and 3) effective content\nsummarisation [19]. Our results confirm this The unique aspects\nare related to the semantic and generative nature of the use of large\nlanguage models including evaluating factual accuracy [16].\n3 RETRIEVAL AUGMENTED GENERATION\nWith the explosion in popularity of large language model services\nsuch as ChatGPT2, Claude3, and Bard 4, people have explored their\nuse as a question and answering systems. While the performance\nis impressive [16] there are two fundamental challenges: 1) hallu-\ncinations - where the LLM produces a response that looks right\n1https://github.com/openai/evals\n2https://chat.openai.com/\n3https://claude.ai/\n4https://bard.google.com/\nbut is incorrect, and 2) unbounded - no way to direct or update\nthe content of the output (other than through prompt engineering).\nA RAG system is an information retrieval approach designed to\novercome the limitations of using a LLM directly.\nRAG works by taking a natural language query is converted into\nan embedding which is used to semantically search a set of docu-\nments. Retrieved documents are then passed to a large language\nmodel to generate an answer. An overview of a RAG system is\nshown in Figure 1 as two separate processes, Index and Query. See\nthis survey for more details [19]\n3.1 Index Process\nIn a RAG system, the retrieval system works using embeddings\nthat provide a compressed semantic representation of the docu-\nment. An embedding is expressed as a vector of numbers. During\nthe Index process each document is split into smaller chunks that\nare converted into an embedding using an embedding model. The\noriginal chunk and the embedding are then indexed in a database.\nSoftware engineers face design decisions around how best to chunk\nthe document and how large a chunk should be. If chunks are too\nsmall certain questions cannot be answered, if the chunks are too\nlong then the answers include generated noise.\nDifferent types of documents require different chunking and pro-\ncessing stages. For example, video content requires a transcription\npipeline to extract the audio and convert to text prior to encoding\n(see subsection 4.2. The choice of which embedding to use also\nmatters as changing the embedding strategy requires re-indexing\nall chunks. An embedding should be chosen based on the ability to\nsemantically retrieve correct responses. This process depends on\nthe size of the chunks, the types of questions expected, the structure", "metadata": {"title": "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented\n  Instructions", "pdf_url": "http://arxiv.org/pdf/2501.00353v1"}}, "c82d686c-12ef-47d6-8da1-3369473e3b9e": {"text": "all chunks. An embedding should be chosen based on the ability to\nsemantically retrieve correct responses. This process depends on\nthe size of the chunks, the types of questions expected, the structure\nof the content and the application domain.\n3.2 Query Process\nThe Query process takes place at run time. A question expressed\nas natural language is first converted into a general query. To gen-\neralise the query a large language model is used which enables\nadditional context such as previous chat history to be included\nin the new query. An embedding is then calculated from the new\nquery to use for locating relevant documents from the database.\nTop-k similar documents are retrieved using a similarity method\nsuch as cosine similarity (vector databases have techniques such as\ninverted indexes to speed up retrieval time). The intuition is that\nchunks that are semantically close to the query are likely to contain\nthe answer.\nRetrieved documents are then re-ranked to maximise the likeli-\nhood that the chunk with the answer is located near the top. The\nnext stage is the Consolidator which is responsible for processing\nthe chunks. This stage is needed to overcome the limitations of\nlarge language models 1) token limit and 2) rate limit. Services such\nas OpenAI have hard limits on the amount of text to include in a\nprompt. This restricts the number of chunks to include in a prompt\nto extract out an answer and a reduction strategy is needed to chain\nprompts to obtain an answer. These online services also restrict the\nnumber of tokens to use within a time frame restricting the latency\nof a system. Software engineers need to consider these tradeoffs\nwhen designing a RAG system.", "metadata": {"title": "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented\n  Instructions", "pdf_url": "http://arxiv.org/pdf/2501.00353v1"}}, "9fc2e929-773f-4f6d-a24f-fc5ffff91c53": {"text": "Seven Failure Points When Engineering a Retrieval Augmented Generation System CAIN 2024, April 2024, Lisbon, Portugal\nFigure 1: Indexing and Query processes required for creating a Retrieval Augmented Generation (RAG) system. The indexing\nprocess is typically done at development time and queries at runtime. Failure points identified in this study are shown in red\nboxes. All required stages are underlined. Figure expanded from [19].\nThe final stage of a RAG pipeline is when the answer is extracted\nfrom the generated text. Readers are responsible for filtering the\nnoise from the prompt, adhering to formatting instructions (i.e. an-\nswer the question as a list of options), and producing the output to\nreturn for the query. Implementation of a RAG system requires cus-\ntomising multiple prompts to process questions and answers. This\nprocess ensures that questions relevant for the domain are returned.\nThe use of large language models to answer real time questions\nfrom documents opens up new application domains where question\nand answering is new capability. Thus, RAG systems are difficult\nto test as no data exists and needs to be experimentally discov-\nered through either a) synthetic data generation, or b) piloting the\nsystem with minimal testing.\n4 CASE STUDIES\nThis study conducted three case studies to discover the challenges\nthat arise when implementing RAG systems. A summary of each of\nthe case studies is shown in Table 1. All scripts, data, and examples\nof each of the failure points for the BioASQ case study are available\nonline 5. The other two case studies have been excluded due to\nconfidentiality concerns.\n4.1 Cognitive Reviewer\nCognitive Reviewer is a RAG system designed to support researchers\nin analysing scientific documents. Researchers specify a research\nquestion or objective and then upload a collection of related re-\nsearch papers. All of the documents are then ranked in accordance\nwith the stated objective for the researcher to manually review.\nThe researcher can also ask questions directly against all of the\ndocuments. Cognitive Reviewer is currently used by PhD students\nfrom Deakin University to support their literature reviews. The\nCognitive Reviewer does the Index process at run time and relies\n5https://figshare.com/s/fbf7805b5f20d7f7e356\non a robust data processing pipeline to handle uploaded documents\ni.e. no quality control possible at development time. This system\nalso uses a ranking algorithm to sort the uploaded documents.\n4.2 AI Tutor", "metadata": {"title": "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented\n  Instructions", "pdf_url": "http://arxiv.org/pdf/2501.00353v1"}}, "e05c3406-8518-4bbc-99c9-b2e763e59b0e": {"text": "on a robust data processing pipeline to handle uploaded documents\ni.e. no quality control possible at development time. This system\nalso uses a ranking algorithm to sort the uploaded documents.\n4.2 AI Tutor\nThe AI Tutor is a RAG system where students ask questions about\nthe unit and answers are sourced from the learning content. Stu-\ndents are able to verify the answers by accessing a sources list from\nwhere the answer came from. The AI Tutor works by integrating\ninto Deakin\u2019s learning management system, indexing all of the\ncontent including PDF documents, videos, and text documents. As\npart of the Index process, videos are transcribed using the deep\nlearning model Whisper [17] before being chunked. The AI Tutor\nwas developed between August 2023 to November 2023 for a pilot\nin a unit with 200 students that commenced the 30th of October\n2023. Our intention is to present the lessons learned during imple-\nmentation and present a followup findings at the conclusion of the\npilot. This RAG pipeline includes a rewriter to generalise queries.\nWe implemented a chat interface where previous dialogue between\nthe user and the AI Tutor was used as part of the context for each\nquestion. The rewriter considers this context and rewrites the query\nto resolve ambiguous requests such as \u2018Explain this concept further. \u2019\n4.3 Biomedical Question and Answer\nThe previous case studies focused on documents with smaller con-\ntent sizes. To explore the issues at a larger scale we created a RAG\nsystem using the BioASQ [10] dataset comprised of questions, links\nto document, and answers. The answers to questions were one of\nyes/no, text summarisation, factoid, or list. This dataset was pre-\npared by biomedical experts and contains domain specific question\nand answer pairs. We downloaded 4017 open access documents\nfrom the BioASQ dataset and had a total of 1000 questions. All\ndocuments were indexed and the questions asked against the RAG\nsystem. The generated questions were then evaluated using the", "metadata": {"title": "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented\n  Instructions", "pdf_url": "http://arxiv.org/pdf/2501.00353v1"}}, "9e0fecd8-0c7c-4824-97ca-c0ebef448e73": {"text": "CAIN 2024, April 2024, Lisbon, Portugal Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mohamed Abdelrazek\nCase Study Domain Doc Types Dataset Size RAG Stages Sample Questions\nCognitive\nReviewer*\nResearch PDFs (Any size) Chunker, Rewriter, Re-\ntriever, Reader\nWhat are the key points covered in\nthis paper?\nAI Tutor* Education Videos, HTML,\nPDF\n38 Chunker, Rewriter,\nRetriever, Reader\nWhat were the topics covered in\nweek 6?\nBioASQ Biomedical Scientific PDFs 4017 Chunker, Retriever,\nReader\nDefine pseudotumor cerebri. How\nis it treated?\nTable 1: A summary of the RAG case studies presented in this paper. Case studies marked with a * are running systems currently\nin use.\nOpenEvals technique implemented by OpenAI6. From the gener-\nated questions we manually inspected 40 issues and all issues that\nthe OpenEvals flagged as inaccurate. We found that the automated\nevaluation was more pessimistic than a human rater for this domain.\nHowever, one threat to validity with this finding is that BioASQ is\na domain specific dataset and the reviewers were not experts i.e.\nthe large language model may know more than a non-expert.\n5 FAILURE POINTS OF RAG SYSTEMS\nFrom the case studies we identified a set of failure points presented\nbelow. The following section addresses the research question What\nare the failure points that occur when engineering a RAG system?\nFP1 Missing Content The first fail case is when asking a ques-\ntion that cannot be answered from the available documents.\nIn the happy case the RAG system will respond with some-\nthing like \u201cSorry, I don\u2019t know\". However, for questions that\nare related to the content but don\u2019t have answers the system\ncould be fooled into giving a response.\nFP2 Missed the Top Ranked Documents The answer to the\nquestion is in the document but did not rank highly enough\nto be returned to the user. In theory, all documents are ranked\nand used in the next steps. However, in practice the top K\ndocuments are returned where K is a value selected based\non performance.\nFP3 Not in Context - Consolidation strategy Limitations\nDocuments with the answer were retrieved from the data-\nbase but did not make it into the context for generating an\nanswer. This occurs when many documents are returned\nfrom the database and a consolidation process takes place to\nretrieve the answer.\nFP4 Not Extracted Here the answer is present in the context,\nbut the large language model failed to extract out the correct\nanswer. Typically, this occurs when there is too much noise", "metadata": {"title": "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented\n  Instructions", "pdf_url": "http://arxiv.org/pdf/2501.00353v1"}}, "2de6b514-8607-4d64-84eb-f6796bca57fa": {"text": "retrieve the answer.\nFP4 Not Extracted Here the answer is present in the context,\nbut the large language model failed to extract out the correct\nanswer. Typically, this occurs when there is too much noise\nor contradicting information in the context.\nFP5 Wrong Format The question involved extracting informa-\ntion in a certain format such as a table or list and the large\nlanguage model ignored the instruction.\nFP6 Incorrect Specificity The answer is returned in the re-\nsponse but is not specific enough or is too specific to address\nthe user\u2019s need. This occurs when the RAG system designers\nhave a desired outcome for a given question such as teach-\ners for students. In this case, specific educational content\nshould be provided with answers not just the answer. Incor-\nrect specificity also occurs when users are not sure how to\nask a question and are too general.\n6https://github.com/openai/evals\nFP7 Incomplete Incomplete answers are not incorrect but miss\nsome of the information even though that information was in\nthe context and available for extraction. An example question\nsuch as \u201cWhat are the key points covered in documents\nA, B and C?\u201d A better approach is to ask these questions\nseparately.\n6 LESSONS AND FUTURE RESEARCH\nDIRECTIONS\nThe lessons learned from the three case studies are shown in Table 2.\nWe present our findings for the research question: What are the\nkey considerations when engineering a RAG system? Based on our\ntakeaways we identified multiple potential research areas linked to\nRAG as follows:\n6.1 Chunking and Embeddings\nChunking documents sounds trivial. However, the quality of chunk-\ning affects the retrieval process in many ways and in particular\non the embeddings of the chunk then affects the similarity and\nmatching of chunks to user queries. There are two ways of chunk-\ning: heuristics based (using punctuation, end of paragraph, etc.),\nand semantic chunking (using the semantics in the text to inform\nstart-end of a chunk). Further research should explore the tradeoffs\nbetween these methods and their effects on critical downstream\nprocesses like embedding and similarity matching. A systematic\nevaluation framework comparing chunking techniques on metrics\nlike query relevance and retrieval accuracy would benefit the field.\nEmbeddings represent another active research area, including\ngenerating embeddings for multimedia and multimodal chunks\nsuch as tables, figures, formulas, etc. Chunk embeddings are typ-\nically created once during system development or when a new", "metadata": {"title": "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented\n  Instructions", "pdf_url": "http://arxiv.org/pdf/2501.00353v1"}}, "7a880614-0175-464d-bd42-ffce54f2887c": {"text": "Embeddings represent another active research area, including\ngenerating embeddings for multimedia and multimodal chunks\nsuch as tables, figures, formulas, etc. Chunk embeddings are typ-\nically created once during system development or when a new\ndocument is indexed. Query preprocessing significantly impacts\na RAG system\u2019s performance, particularly handling negative or\nambiguous queries. Further research is needed on architectural pat-\nterns and approaches [5] to address the inherent limitations with\nembeddings (quality of a match is domain specific).\n6.2 RAG vs Finetuning\nLLMs are great world models due to the amount of training data, and\nfinetuning tasks applied on the model before it\u2019s released. However,\nthese models are general-purpose models (may not know the very\nspecifics of your domain) and also not up to date (there is a cutoff\ndate on their knowledge). Fine-tuning and RAG offer two potential\ncustomisation pathways, each with distinct tradeoffs. Finetuning\nrequires curating internal datasets to adapt and train the LLM on.\nHowever, all your data are baked into the model and you need to", "metadata": {"title": "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented\n  Instructions", "pdf_url": "http://arxiv.org/pdf/2501.00353v1"}}, "6f4ff19e-fde4-406a-a6fb-48711fb96203": {"text": "Seven Failure Points When Engineering a Retrieval Augmented Generation System CAIN 2024, April 2024, Lisbon, Portugal\nFP Lesson Description Case Studies\nFP4 Larger context get better results (Context refers to a\nparticular setting or situation in which the content\noccurs)\nA larger context enabled more accurate responses\n(8K vs 4K). Contrary to prior work with GPT-3.5 [13]\nAI Tutor\nFP1 Semantic caching drives cost and latency down RAG systems struggle with concurrent users due to\nrate limits and the cost of LLMs. Prepopulate the\nsemantic cache with frequently asked questions [1].\nAI Tutor\nFP5-7 Jailbreaks bypass the RAG system and hit the safety\ntraining.\nResearch suggests fine-tuning LLMs reverses safety\ntraining [11], test all fine-tuned LLMs for RAG sys-\ntem.\nAI Tutor\nFP2, FP4 Adding meta-data improves retrieval. Adding the file name and chunk number into the\nretrieved context helped the reader extract the re-\nquired information. Useful for chat dialogue.\nAI Tutor\nFP2, FP4-7 Open source embedding models perform better for\nsmall text.\nOpensource sentence embedding models performed\nas well as closed source alternatives on small text.\nBioASQ, AI Tutor\nFP2-7 RAG systems require continuous calibration. RAG systems receive unknown input at runtime\nrequiring constant monitoring.\nAI Tutor, BioASQ\nFP1, FP2 Implement a RAG pipeline for configuration. A RAG system requires calibrating chunk size,\nembedding strategy, chunking strategy, retrieval\nstrategy, consolidation strategy, context size, and\nprompts.\nCognitive Reviewer,\nAI Tutor, BioASQ\nFP2, FP4 RAG pipelines created by assembling bespoke solu-\ntions are suboptima.\nEnd-to-end training enhances domain adaptation\nin RAG systems [18].\nBioASQ, AI Tutor\nFP2-7 Testing performance characteristics are only possi-\nble at runtime.\nOffline evaluation techniques such as G-Evals [14]\nlook promising but are premised on having access\nto labelled question and answer pairs.\nCognitive Reviewer,\nAI Tutor\nTable 2: The lessons learned from the three case studies with key takeaways for future RAG implementations\nsort out the security/privacy (who can access what). Furthermore,\nas the foundation model itself evolves or you get new data to add to\nthe model, you will need to run finetuning again. On the other side,\nRAG systems seem to offer a pragmatic solution allowing you to\nchunk your data as needed and only use relevant chunks into the\ncontext to ask the LLM to generate an answer from the included\ncontext. This facilitates continuously updating the knowledge with", "metadata": {"title": "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented\n  Instructions", "pdf_url": "http://arxiv.org/pdf/2501.00353v1"}}, "73454bc9-4f2f-4546-8cb5-a8c692739b7a": {"text": "chunk your data as needed and only use relevant chunks into the\ncontext to ask the LLM to generate an answer from the included\ncontext. This facilitates continuously updating the knowledge with\nnew documents and also gives the control over what chunks the user\nis able to access. However, optimal strategies for chunk embedding,\nretrieval, and contextual fusion remain active research. Further\nwork should systematically compare finetuning and RAG paradigms\nacross factors including accuracy, latency, operating costs, and\nrobustness.\n6.3 Testing and Monitoring RAG systems\nSoftware engineering best practices are still emerging for RAG sys-\ntems. Software testing and test case generation are one of the areas\nfor refinement. RAG systems require questions and answers that are\napplication specific often unavailable when indexing unstructured\ndocuments. Emerging work has considered using LLMs for gen-\nerating questions from multiple documents [4]. How to generate\nrealistic domain relevant questions and answers remains an open\nproblem.\nOnce suitable test data is available quality metrics are also re-\nquired to assist engineers in making quality tradeoffs. Using large\nlanguage models is expensive, introduces latency concerns, and has\nperformance characteristics that all change with each new release.\nThis characteristic has previously been studied for machine learn-\ning systems [5, 6] but the required adaptations (if any) have yet to\nbe applied to LLM based systems such as RAGs. Another idea is to\nincorporate ideas from self-adaptive systems to support monitoring\nand adapting RAG systems, preliminary work has started for other\nmachine learning applications [2].\n7 CONCLUSION\nRAG systems are a new information retrieval that leverages LLMs.\nSoftware engineers increasingly interact with RAG systems a)\nthrough implementing semantic search, or b) through new code-\ndependent tasks. This paper presented the lessons learned from 3\ncase studies including an empirical investigation involving 15,000\ndocuments and 1000 questions. Our findings provide a guide to\npractitioners by presenting the challenges faced when implement-\ning RAG systems. We also included future research directions for\nRAG systems related to 1) chunking and embeddings, 2) RAG vs\nFinetuning, and 3) Testing and Monitoring. Large language models\nare going to continue to obtain new capabilities of interest to engi-\nneers and researchers. This paper presents the first investigation\ninto RAG systems from a software engineering perspective.\nACKNOWLEDGMENTS", "metadata": {"title": "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented\n  Instructions", "pdf_url": "http://arxiv.org/pdf/2501.00353v1"}}, "6f81910f-4063-4f36-a85d-557efb6a6975": {"text": "are going to continue to obtain new capabilities of interest to engi-\nneers and researchers. This paper presents the first investigation\ninto RAG systems from a software engineering perspective.\nACKNOWLEDGMENTS\nTo Amanda Edgar, Rajesh Vasa, Kon Mouzakis, Matteo Vergani,\nTrish McCluskey, Kathryn Perus, Tara Draper, Joan Sutherland and\nRuary Ross for their support and involvement in making the AI\nTutor project possible.", "metadata": {"title": "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented\n  Instructions", "pdf_url": "http://arxiv.org/pdf/2501.00353v1"}}, "74c1536b-8720-492b-bbf9-5a16a060a2ec": {"text": "CAIN 2024, April 2024, Lisbon, Portugal Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mohamed Abdelrazek\nREFERENCES\n[1] Fu Bang. 2023. GPTCache: An Open-Source Semantic Cache for LLM Applications\nEnabling Faster Answers and Cost Savings. In3rd Workshop for Natural Language\nProcessing Open Source Software .\n[2] Maria Casimiro, Paolo Romano, David Garlan, Gabriel Moreno, Eunsuk Kang, and\nMark Klein. 2022. Self-adaptive Machine Learning Systems: Research Challenges\nand Opportunities . 133\u2013155. https://doi.org/10.1007/978-3-031-15116-3_7\n[3] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2023. Benchmarking\nLarge Language Models in Retrieval-Augmented Generation. arXiv preprint\narXiv:2309.01431 (2023).\n[4] Mingda Chen, Xilun Chen, and Wen-tau Yih. 2023. Efficient Open Domain\nMulti-Hop Question Answering with Few-Shot Data Synthesis. arXiv preprint\narXiv:2305.13691 (2023).\n[5] Alex Cummaudo, Scott Barnett, Rajesh Vasa, and John Grundy. 2020. Threshy:\nSupporting safe usage of intelligent web services. In Proceedings of the 28th ACM\nJoint Meeting on European Software Engineering Conference and Symposium on\nthe Foundations of Software Engineering . 1645\u20131649.\n[6] Alex Cummaudo, Scott Barnett, Rajesh Vasa, John Grundy, and Mohamed Ab-\ndelrazek. 2020. Beware the evolving \u2018intelligent\u2019web service! An integration\narchitecture tactic to guard AI-first components. In Proceedings of the 28th ACM\nJoint Meeting on European Software Engineering Conference and Symposium on\nthe Foundations of Software Engineering . 269\u2013280.\n[7] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020.\nRetrieval augmented language model pre-training. In International conference on\nmachine learning . PMLR, 3929\u20133938.\n[8] Sebastian Hofst\u00e4tter, Jiecao Chen, Karthik Raman, and Hamed Zamani. 2023. Fid-\nlight: Efficient and effective retrieval-augmented text generation. In Proceedings\nof the 46th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval . 1437\u20131447.\n[9] Gautier Izacard and Edouard Grave. 2020. Leveraging passage retrieval with\ngenerative models for open domain question answering. arXiv preprint\narXiv:2007.01282 (2020).\n[10] Anastasia Krithara, Anastasios Nentidis, Konstantinos Bougiatiotis, and Georgios\nPaliouras. 2023. BioASQ-QA: A manually curated corpus for biomedical question\nanswering. Scientific Data 10 (2023), 170. Citation Key: 422.\n[11] Simon Lermen, Charlie Rogers-Smith, and Jeffrey Ladish. 2023. LoRA Fine-tuning", "metadata": {"title": "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented\n  Instructions", "pdf_url": "http://arxiv.org/pdf/2501.00353v1"}}, "ae42aee1-77c9-4a75-9c68-8f8e7609d8a3": {"text": "Paliouras. 2023. BioASQ-QA: A manually curated corpus for biomedical question\nanswering. Scientific Data 10 (2023), 170. Citation Key: 422.\n[11] Simon Lermen, Charlie Rogers-Smith, and Jeffrey Ladish. 2023. LoRA Fine-tuning\nEfficiently Undoes Safety Training in Llama 2-Chat 70B. arXiv:2310.20624 [cs.LG]\n[12] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nNaman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel,\net al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks.\nAdvances in Neural Information Processing Systems 33 (2020), 9459\u20139474.\n[13] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,\nFabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models\nuse long contexts. arXiv preprint arXiv:2307.03172 (2023).\n[14] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang\nZhu. 2023. G-eval: Nlg evaluation using gpt-4 with better human alignment, may\n2023. arXiv preprint arXiv:2303.16634 (2023).\n[15] Noor Nashid, Mifta Sintaha, and Ali Mesbah. 2023. Retrieval-based prompt selec-\ntion for code-related few-shot learning. In Proceedings of the 45th International\nConference on Software Engineering (ICSE\u201923) .\n[16] OpenAI. 2023. GPT-4 Technical Report. https://doi.org/10.48550/ARXIV.2303.\n08774\n[17] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and\nIlya Sutskever. 2023. Robust speech recognition via large-scale weak supervision.\nIn International Conference on Machine Learning . PMLR, 28492\u201328518.\n[18] Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kalu-\narachchi, Rajib Rana, and Suranga Nanayakkara. 2023. Improving the domain\nadaptation of retrieval augmented generation (RAG) models for open domain\nquestion answering. Transactions of the Association for Computational Linguistics\n11 (2023), 1\u201317.\n[19] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chen-\nlong Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large language models for\ninformation retrieval: A survey. arXiv preprint arXiv:2308.07107 (2023).", "metadata": {"title": "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented\n  Instructions", "pdf_url": "http://arxiv.org/pdf/2501.00353v1"}}, "44cc90ba-14d1-4467-a551-25baf1a410c0": {"text": "Astute RAG : Overcoming Imperfect\nRetrieval Augmentation and Knowledge\nConflicts for Large Language Models\nFei Wang1 2 *, Xingchen Wan1, Ruoxi Sun1, Jiefeng Chen1 and Sercan \u00d6. Ar\u0131k1\n1Google Cloud AI Research,2University of Southern California\nRetrieval-Augmented Generation (RAG), while effective in integrating external knowledge to address\nthe limitations of large language models (LLMs), can be undermined byimperfect retrieval, which may\nintroduce irrelevant, misleading, or even malicious information. Despite its importance, previous studies\nhave rarely explored the behavior of RAG through joint analysis on how errors from imperfect retrieval\nattribute and propagate, and how potential conflicts arise between the LLMs\u2019 internal knowledge and\nexternal sources. We find that imperfect retrieval augmentation might be inevitable and quite harmful,\nthrough controlled analysis under realistic conditions. We identify theknowledge conflictsbetween LLM-\ninternal and external knowledge from retrieval as a bottleneck to overcome in the post-retrieval stage of\nRAG. To render LLMs resilient to imperfect retrieval, we proposeAstute RAG , a novel RAG approach\nthat adaptively elicits essential information from LLMs\u2019 internal knowledge,iteratively consolidates\ninternalandexternalknowledgewith source-awareness,andfinalizestheansweraccordingtoinformation\nreliability. Our experiments using Gemini and Claude demonstrate thatAstute RAG significantly\noutperforms previous robustness-enhanced RAG methods. Notably,Astute RAG is the only approach\nthat matches or exceeds the performance of LLMs without RAG under worst-case scenarios. Further\nanalysis reveals thatAstute RAG effectively resolves knowledge conflicts, improving the reliability\nand trustworthiness of RAG systems.\n1. Introduction\nRetrieval augmented generation (RAG) has become the standard approach for large language models\n(LLMs) to tackle knowledge-intensive tasks (Guu et al., 2020; Lewis et al., 2020). Prior works mainly\nleverage RAG to address the inherent knowledge limitations of LLMs, effectively integrating missing\ninformation and grounding to reliable sources. However, recent research has highlighted a significant\ndrawback that RAG might rely onimperfect retrievalresults, including irrelevant, misleading, or even\nmalicious information, which eventually leads to inaccurate LLM responses (Chen et al., 2024a; Xiang\net al., 2024; Zou et al., 2024). For example, when asked about the practice of eating rocks, LLMs", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "1894123d-771d-4b5c-acf8-6d0055f4bc19": {"text": "malicious information, which eventually leads to inaccurate LLM responses (Chen et al., 2024a; Xiang\net al., 2024; Zou et al., 2024). For example, when asked about the practice of eating rocks, LLMs\nmight cite misleading information, such as a satirical news source claiming that one should consume\nat least one rock per day.1 The occurrence of imperfect retrieval augmentation is inevitable, driven by\nfactors such as corpus quality limitations (Shao et al., 2024), the reliability of retrievers (Dai et al.,\n2024), and the complexity of the queries (Su et al., 2024). This poses a significant challenge to the\ntrustworthiness of RAG.\nWhile there have been independent analyses of information retrieval and RAG in the context of\nLLMs (Mallen et al., 2023; Su et al., 2024), previous studies have rarely connected the behaviors of\nretrieval and subsequent generation, particularly regarding the propagation of information retrieval\nerrors,whichmayleadto knowledgeconflicts (Longpreetal.,2021;Wangetal.,2023a;Xuetal.,2024b)\nbetween LLMs and context. To this end, we conduct comprehensive analyses on the occurrence of\n1https://www.bbc.com/news/articles/cd11gzejgz4o.\nCorresponding author(s): fwang598@usc.edu, soarik@google.com\n* This work was done while Fei was a research intern at Google Cloud AI Research.\narXiv:2410.07176v1  [cs.CL]  9 Oct 2024", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "af8a79c3-6c87-4563-9652-95a08bcce7f9": {"text": "Astute RAG : Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models\nFigure 1|Knowledge conflicts between the LLMs\u2019 internal knowledge and retrieved knowledge from\nexternal sources. We report the overall results with Claude under the setting in Section 4.1.\nimperfectretrievalaugmentationanditsimpactonLLMbehaviorunderrealisticconditions(Section2).\nWe conduct controlled experiments on a diverse range of general, domain-specific, and long-tail\nquestions from NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), BioASQ (Tsatsaronis\net al., 2015), and PopQA (Mallen et al., 2023). We observe that imperfect retrieval augmentation is\nwidespread even with adept real-world search engine (such as Google Search with Web as corpus)\n\u2013 roughly 70% retrieved passages do not directly contain true answers, leading to the impeded\nperformance of LLM with RAG augmentation.2\nThese findings underscore the potential severity of the imperfect retrieval issue in real-world\nRAG and highlight the widespread existence of knowledge conflicts as the bottleneck to overcome\nit (Figure 1). Recent studies demonstrate that LLM-internal and external knowledge offer distinct\nadvantages, but LLMs often struggle to consolidate conflicting information reliably, failing to respond\nbased on collective knowledge (Jin et al., 2024; Mallen et al., 2023; Tan et al., 2024; Xie et al., 2024).\nThis raises the following research question:Is there an effective method to combine internal (from LLMs\u2019\npretrained weights) and external (from specific corpora or knowledge bases) knowledge for more reliable\nRAG? Previous work has widely explored using external knowledge to enhance LLMs through RAG.\nWe seek to further leverage LLMs\u2019 internal knowledge to recover from RAG failures\nMotivated by these important real-world challenges, we proposeAstute RAG (Section 3), a\nnovel RAG approach designed to be resilient to imperfect retrieval augmentation, while preserving\nRAG grounding effect when RAG is reliable. To this end,Astute RAG needs effectively differentiate\nthe reliability of the LLM\u2019s intrinsic knowledge and the external information retrieved in RAG, utilizing\neach only when trustworthy and ensuring proper integration. Specifically,Astute RAG initially\nelicits information from LLMs\u2019 internal knowledge to explicitly complement the passages retrieved\nfrom external sources. Then,Astute RAG conducts source-aware knowledge consolidation of", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "ebe8b836-8a76-4fbc-8d75-4df0cd2f8702": {"text": "elicits information from LLMs\u2019 internal knowledge to explicitly complement the passages retrieved\nfrom external sources. Then,Astute RAG conducts source-aware knowledge consolidation of\ninformation from various internal and external sources. The desiderata is combining consistent\ninformation, identifying conflicting information, and filtering out irrelevant information. Finally,\nAstute RAG proposes answers based on each group of consistent passages and compares the an-\nswers from different passage groups to determine the final answer. Our experiments involving Gemini\nand Claude3 on various datasets (Section 4) demonstrate the superior performance ofAstute RAG\ncompared to previous RAG approaches designed to be robust against retrieval corruptions. Moreover,\nAstute RAG consistently outperforms baselines across different retrieval quality levels. Notably,\n2Note that some passages may contain information indirectly relevant to the answer, but may unintentionally mislead or\ndistract LLMs.\n3https://www.anthropic.com/claude\n2", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "975618d0-0a28-46f6-bc23-ce3cb3135f62": {"text": "Astute RAG : Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models\nFigure 2|Imperfect retrieval (samples with low retrieval precision) is prevalent in real-world RAG.\nAstute RAG is the only RAG method that achieves performance comparable to or even surpassing\nconventional use of LLMs under the worst-case scenario where all retrieved passages are unhelpful.\nFurther analysis reveals the effectiveness ofAstute RAG in resolving knowledge conflicts between\ninternal and external knowledge.\nTo conclude, our core contributions are threefold. First, we analyze RAG under realistic condi-\ntions, identifying imperfect retrieval augmentation as a significant contributor to RAG failures and\npinpointing knowledge conflicts as the primary bottleneck in overcoming it. Second, we propose\nAstute RAG , which explicitly addresses conflicts between LLM-internal and external knowledge,\ntherebyrecoveringfromRAGfailures. Third, experimentswithvariousLLMsanddatasetsdemonstrate\nthe effectiveness ofAstute RAG , even in the most challenging scenarios.\n2. Imperfect Retrieval: The Pitfall of RAG\nTo better showcase the common real-world challenges and to make better motivate for improved\nmethodological designs, we evaluate retrieval quality, end-to-end RAG performance, and knowledge\nconflicts on a controlled set of data. The selected data encompass a diverse range of general, domain-\nspecific, and long-tail questions from NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017),\nBioASQ (Tsatsaronis et al., 2015), and PopQA (Mallen et al., 2023). Our analysis is based onrealistic\nretrieval results with Google Search4 as the retriever and the Web as the corpus. This setting allows\nus to analyze the severity of imperfect retrieval in real-world RAG. Overall, we sample 1K short-form\nQA instances from these datasets, and pair each instance with 10 retrieved passages.\nImperfect retrieval is common.We examine the occurrence of correct answers in retrieved passages\nas an approximation of retrieval quality. Since we mainly focus on short-form QA which provides\nmost variants of the correct answer for each question, the approximation through string matching\ncan give us a rouge intuition of how precise the retrieval result is. Specifically, we define the retrieval\nprecision as the ratio of passages containing the correct answer for each instance:\nRetrieval Precision= {number of retrieved passages containing correct answer}\n{number of total retrieved passages}", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "2aca8fdc-8544-42da-b2d7-aa55bfb42e21": {"text": "precision as the ratio of passages containing the correct answer for each instance:\nRetrieval Precision= {number of retrieved passages containing correct answer}\n{number of total retrieved passages}\nAs shown in Figure 2, although instances from different datasets exhibit different data distributions,\nimperfect retrieval is prevalent. Specifically,\u223c20% of the overall data have no mentions of the correct\nanswer within any retrieved passage, including 34% on NQ, 18% on TriviaQA, 24% on BioASQ, and\n50% on PopQA. This finding also aligns with previous observation on information retrieval (Thakur\net al., 2024), that highlights that the number of positive passages can be very limited.\n4https://developers.google.com/custom-search/v1/overview\n3", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "60716359-477f-434d-bb83-d0b84e323b9c": {"text": "Astute RAG : Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models\nFigure 3|Overview of the proposedAstute RAG framework. Astute RAG is designed to better\ncombine the information from the external sources (e.g. web, domain-specific corpora, knowledge\nbases) and internal knowledge of the LLMs by employing a consolidation mechanism to address the\ninformation conflicts, which eventually leads to better quality generated outputs.\nImperfect retrieval leads to RAG failures.We further analyze the relation between retrieval quality\nand RAG performance. We compare the performance of Claude 3.5 Sonnet, with and without RAG\nand report the results by retrieval precision in Figure 4. In general, RAG is helpful when the retrieval\nprecision is not lower than 20%. When the retrieval precision is close to 0, the model with RAG\nperforms much worse than without RAG, indicating that imperfect retrieval augmentation can be the\ncause of RAG failures. This finding aligns with the previous observation from Yu et al. (2024) that\nadding more retrieved passages does not necessarily lead to better performance, as the additional\npassages might reduce the retrieval precision.\nKnowledge conflicts widely exist in RAG failures.We provide an in-depth analyses of knowledge\nconflicts between LLMs\u2019 internal knowledge and retrieved passages from external sources. With\nClaude 3.5 Sonnet as the LLM, Figure 1 shows that 19.2% of the overall data exhibit knowledge\nconflicts, where either the answer with or without RAG is correct. Among the conflicting cases, the\ninternal knowledge is correct on 47.4% of them, while the external knowledge is correct on the\nremaining 52.6%. These results emphasize the importance ofeffectively combining the internal and\nexternal knowledge to overcome the inherent limitation of relying solely on either source. However,\nprevious work (Jin et al., 2024; Tan et al., 2024; Xie et al., 2024) show that LLMs might respond based\non misleading information rather than comprehensive understanding of the conflicting knowledge in\nthis context.\n3. Astute RAG : Overcoming the Pitfall\nWe begin with formulating the problem of imperfect retrieval in RAG (Section 3.1). We then provide\nan overview ofAstute RAG , designed to overcome this problem (Section 3.2). Subsequently,\nwe delve into the three major steps ofAstute RAG , including adaptive generation of internal\nknowledge(Section3.3),source-awareknowledgeconsolidation(Section3.4), andanswerfinalization\n(Section 3.5).\n4", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "1fa223a6-5245-4c41-a1d3-fd82530948d2": {"text": "Astute RAG : Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models\nAlgorithm 1Astute RAG\nRequire: Query\ud835\udc5e, Retrieved Passages\ud835\udc38 = [\ud835\udc521,...,\ud835\udc52 \ud835\udc5b], Large Language ModelM, Number of Iteration\n\ud835\udc61, Max Number of Generated Passages\u02c6\ud835\udc5a, Prompt Templates\ud835\udc5d\ud835\udc54\ud835\udc52\ud835\udc5b,\ud835\udc5d\ud835\udc50\ud835\udc5c\ud835\udc5b,\ud835\udc5d\ud835\udc4e\ud835\udc5b\ud835\udc60\n1: Adaptively generate passages:\ud835\udc3c \u2190M(\ud835\udc5d\ud835\udc54\ud835\udc52\ud835\udc5b,\ud835\udc5e, \u02c6\ud835\udc5a) \u22b2Section 3.3\n2: Combine internal and external passages:\ud835\udc370 \u2190\ud835\udc38\u2295\ud835\udc3c\n3: Assign passage sources:\ud835\udc460 \u2190[1 {\ud835\udc51\u2208\ud835\udc38}for \ud835\udc51 in \ud835\udc370]\n4: if \ud835\udc61 >1 then\n5: for \ud835\udc57= 1,...,\ud835\udc61 \u22121 do \u22b2Section 3.4\n6: Consolidate knowledge:\u27e8\ud835\udc37\ud835\udc57+1,\ud835\udc46\ud835\udc57+1\u27e9\u2190M( \ud835\udc5d\ud835\udc50\ud835\udc5c\ud835\udc5b,\ud835\udc5e, \u27e8\ud835\udc370,\ud835\udc460\u27e9,\u27e8\ud835\udc37\ud835\udc57,\ud835\udc46\ud835\udc57\u27e9)\n7: end for\n8: Finally consolidate and answer:\ud835\udc4e\u2190M(\ud835\udc5d\ud835\udc4e\ud835\udc5b\ud835\udc60,\ud835\udc5e, \u27e8\ud835\udc370,\ud835\udc460\u27e9,\u27e8\ud835\udc37\ud835\udc61\u22121,\ud835\udc46\ud835\udc61\u22121\u27e9) \u22b2Section 3.5\n9: else\n10: Consolidate knowledge and finalize the answer:\ud835\udc4e\u2190M(\ud835\udc5d\ud835\udc4e\ud835\udc5b\ud835\udc60,\ud835\udc5e, \u27e8\ud835\udc370,\ud835\udc460\u27e9)\n11: end if\n12: return \ud835\udc4e\n3.1. Problem Formulation\nOur objective is to mitigate the effects of imperfect retrieval augmentation, resolve knowledge conflicts\nbetween the LLM\u2019s internal knowledge and external sources (such as custom/public corpora and\nknowledge bases), and ultimately produce more accurate and reliable responses from LLMs.\nGiven a set of retrieved passages from external sources\ud835\udc38 = [\ud835\udc521,...,\ud835\udc52 \ud835\udc5b], a pre-trained LLMM\n(accessible through prediction-only APIs, encompassing commercial black-box ones), and a query\ud835\udc5e,\nthe task is to generate the corresponding correct answer\ud835\udc4e\u2217. Notably, this setting is orthogonal to\nprior work on improving the retriever, training LLMs, or conducting adaptive retrieval, which are\nmainly preliminary steps.\n3.2. Overview of the Framework\nAstute RAG is designed to better leverage collective knowledge from both internal knowledge\nof LLMs and external corpus, for more reliable responses. As shown in Figure 3 and Algorithm 1,\nAstute RAG starts from acquiring the most accurate, relevant, and thorough passage set from the\nLLMs\u2019 internal knowledge. Then, internal and external knowledge are consolidated in an iterative way,\nby comparing the generated and retrieved passages. Finally, the reliability of conflicting information\nis compared and the final output is generated according to the most reliable knowledge.\n3.3. Adaptive Generation of Internal Knowledge\nIn the first step, we elicit internal knowledge from LLMs. This LLM-internal knowledge, reflecting\nthe consensus from extensive pre-training and instruction-tuning data, can supplement any missing\ninformation from the limited set of retrieved passages and enable mutual confirmation between\nLLM-internal and external knowledge. This is especially valuable when the majority of retrieved", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "cc3a1915-1057-4c23-b893-e6935838737a": {"text": "information from the limited set of retrieved passages and enable mutual confirmation between\nLLM-internal and external knowledge. This is especially valuable when the majority of retrieved\npassages might be irrelevant or misleading. Specifically, we prompt LLMs to generate passages based\non the given question\ud835\udc5e, following Yu et al. (2023a). While Yu et al. (2023a) primarily focused on\ngenerating diverse internal passages, we emphasize the importance of reliability and trustworthiness\nof generated passages. To achieve this goal, we enhance the original method withconstitutional\nprinciples and adaptive generation.\nInspired by Constitutional AI (Bai et al., 2022), we provideconstitutional principlesindicating\n5", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "cae8b54a-b0ce-405f-a225-11605322ac9f": {"text": "Astute RAG : Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models\nthedesiredpropertiesofinternalpassagesintheprompt \ud835\udc5d\ud835\udc54\ud835\udc52\ud835\udc5b (seeAppendixAfordetails)toguidetheir\ngeneration, emphasizing that the generated passages should be accurate, relevant, and hallucination-\nfree. Moreover,weallowtheLLMtoperform adaptivegeneration ofpassagesinitsinternalknowledge.\nThe LLM can decide how many passages to generate by itself. Rather generating a fix number of\npassages, we request the LLM to generate at most\u02c6\ud835\udc5a passages, each covering distinct information,\nand to directly indicate if no more reliable information is available. This adaptive approach allows\nthe LLM to generate fewer passages (or even no passages at all) when the useful information within\ninternal knowledge is limited and more passages when there are multiple feasible answers in the\ninternal knowledge. In this step, the LLM generates\ud835\udc5a \u2264\u02c6\ud835\udc5a passages based on its internal knowledge:\n\ud835\udc3c = [\ud835\udc561,...\ud835\udc56 \ud835\udc5a]= M(\ud835\udc5d\ud835\udc54\ud835\udc52\ud835\udc5b,\ud835\udc5e, \u02c6\ud835\udc5a).\n3.4. Iterative Source-aware Knowledge Consolidation\nIn the second step, we employ the LLM to explicitly consolidate information from both passages\ngenerated from its internal knowledge and passages retrieved from external sources. Initially, we\ncombine passages from both internal and external knowledge sources\ud835\udc370 = \ud835\udc38\u2295\ud835\udc3c.\nWe additionally ensuresource-awarenessby providing the source of each passage to LLMs when\nconsolidating knowledge. The source information (internal or external, such as a website) is helpful in\nassessing the reliability of passages. Here, we provide the passage source as\ud835\udc460 = [1 {\ud835\udc51\u2208\ud835\udc38}for \ud835\udc51 in \ud835\udc370].\nTo consolidate knowledge, we prompt the LLM (with\ud835\udc5d\ud835\udc50\ud835\udc5c\ud835\udc5b in Appendix A) to identify consistent\ninformation across passages, detect conflicting information between each group of consistent passages,\nand filter out irrelevant information. This step would regroup the unreliable knowledge in input\npassages into fewer refined passages. The regrouped passages will also attribute their source to the\ncorresponding one or more input passages\n\u27e8\ud835\udc37\ud835\udc57+1,\ud835\udc46\ud835\udc57+1\u27e9= M(\ud835\udc5d\ud835\udc50\ud835\udc5c\ud835\udc5b,\ud835\udc5e, \u27e8\ud835\udc370,\ud835\udc460\u27e9,\u27e8\ud835\udc37\ud835\udc57,\ud835\udc46\ud835\udc57\u27e9).\nWe find that this is especially helpful in comparing the reliability of conflicting knowledge and\naddressing knowledge conflicts. Moreover, this knowledge consolidation process can runiteratively\nfor \ud835\udc61 times to improve the context to be more and more useful. Users can assign a larger number of\niterations when the context is lengthy.\n3.5. Answer Finalization", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "247251f5-c337-437f-a3e7-4cfaf7128827": {"text": "for \ud835\udc61 times to improve the context to be more and more useful. Users can assign a larger number of\niterations when the context is lengthy.\n3.5. Answer Finalization\nIn the last step, we prompt the LLM (with\ud835\udc5d\ud835\udc4e\ud835\udc5b\ud835\udc60 in Appendix A) to generate one answer based on each\ngroup of passages (\u27e8\ud835\udc37\ud835\udc61,\ud835\udc46\ud835\udc61\u27e9), and then compare their reliability and select the most reliable one as\nthe final answer. This comparison allows the LLM to comprehensively consider knowledge source,\ncross-source confirmation, frequency, and information thoroughness when making the final decision.\nNotably, this step can be merged into the last knowledge consolidation step to reduce the inference\ncomplexity (the amount of prediction API calls) using a combined prompt:\n\ud835\udc4e= M(\ud835\udc5d\ud835\udc4e\ud835\udc5b\ud835\udc60,\ud835\udc5e, \u27e8\ud835\udc370,\ud835\udc460\u27e9,\u27e8\ud835\udc37\ud835\udc61,\ud835\udc46\ud835\udc61\u27e9).\nWhen \ud835\udc61 = 1, the initial passages will be fed into the model directly for knowledge consolidation and\nsubsequent answering:\ud835\udc4e= M(\ud835\udc5d\ud835\udc4e\ud835\udc5b\ud835\udc60,\ud835\udc5e, \u27e8\ud835\udc370,\ud835\udc460\u27e9).\n4. Experiments\nWe evaluate the effectiveness ofAstute RAG on overcoming imperfect retrieval augmentation and\naddressing knowledge conflicts. In this section, we first introduce the experiment setting in detail\n6", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "88cb8b96-ffaf-4822-bcd4-4f24f386e516": {"text": "Astute RAG : Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models\nMethod #API Calls NQ TriviaQA BioASQ PopQA Overall\nClaude 3.5 Sonnet (20240620)\nNo RAG 1 47.12 81.98 50.35 29.78 54.51\nRAG 1 44.41 76.68 58.04 35.96 55.47\nUSC (Chen et al., 2024b) 4 48.14 80.21 61.54 37.64 58.73\nGenRead (Yu et al., 2023a) 2 42.03 74.20 56.99 34.27 53.55\nRobustRAG (Xiang et al., 2024) 11 47.80 78.09 56.29 37.08 56.53\nInstructRAG (Wei et al., 2024) 1 47.12 83.04 58.04 41.01 58.83\nSelf-Route (Xu et al., 2024a) 1-2 47.46 78.80 59.09 41.01 58.06\nAstute RAG (t=1) 2 52.20 84.10 60.14 44.38 61.71\nAstute RAG (t=2) 3 53.22 84.45 61.89 44.94 62.67\nAstute RAG (t=3) 4 53.56 84.45 62.24 44.94 62.86\nTable 1|Main results on Claude under zero-shot setting, showing the accuracy of different benchmark\nmethods vs.Astute RAG , along with their prediction complexity, in number of prediction API calls.\nBest scores are in bold.\n(Section 4.1). Then, we compare the performance ofAstute RAG with various baselines on diverse\ndatasets (Section 4.2). Finally, we provide in-depth analyses (Section 4.3).\n4.1. Experimental Settings\nDatasets and metrics.We conduct experiments on the data collected in Section 2 consisting of\ndata from NQ, TriviaQA, BioASQ, and PopQA. For each instance from these datasets, we provide 10\npassages collected under a realistic retrieval setting: for each question in our benchmark, we query\nGoogle Search to retrieve the top 30 results and select the first 10 accessible websites. From each\nretrieved website, we extract the paragraph corresponding to the snippet provided in Google Search\nresults as the retrieved passage.. Most of the retrieval results contains natural noise with irrelevant\nor misleading information. We do not consider enhancements to the retrieval side, such as query\nrewriting, as such enhancements are typically already incorporated into commercial information\nretrieval systems. Notably, we do not select questions or annotate answers based on the retrieval\nresults. This setting allows us to analyze the severity of imperfect retrieval in real-world RAG. It\ndistinguishes our benchmark from previous ones that employ synthetic retrieval corruptions or that\nunintentionally reduce the frequency of imperfect retrieval with biased construction protocols (Chen\net al., 2024a; Yang et al., 2024). We also evaluate our method on RGB (Chen et al., 2024a), a RAG\ndiagnostic benchmark evaluating several crucial RAG abilities. Specifically, we choose the English", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "60d984b7-beac-4859-be3c-4b867b12b8e9": {"text": "et al., 2024a; Yang et al., 2024). We also evaluate our method on RGB (Chen et al., 2024a), a RAG\ndiagnostic benchmark evaluating several crucial RAG abilities. Specifically, we choose the English\nsubset of RGB focusing on noise robustness. The benchmark have positive and negative passage sets\nfor each question. We select five negative documents per question as the context to form a worst-case\nscenario. All the data in these datasets are short-form QA. Following previous work (Mallen et al.,\n2023; Wei et al., 2024; Xiang et al., 2024), a model response is considered correct if it contains the\nground-truth answer. To enhance evaluation reliability, we prompt LLMs to enclose the exact answer\nwithin special tokens, extracting them as the final responses.\nGeneral Settings of LLMs and RAG.We conduct experiments on two advanced LLMs, including Gem-\nini 1.5 Pro5 (gemini-1.5-pro-002) and Claude 3.5 Sonnet6 (claude-3-5-sonnet@20240620).\nThe generation temperature is set to 0 and the maximum output tokens is set to 1,024, if not\nspecified otherwise. By default, the passages are presented in the prompt by reversed order. All\n5https://deepmind.google/technologies/gemini/pro/\n6https://www.anthropic.com/news/claude-3-5-sonnet\n7", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "2719a11a-ee73-4d8f-9cd9-b4cfc2e3c271": {"text": "Astute RAG : Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models\nMethod #API Calls NQ TriviaQA BioASQ PopQA Overall\nGemini 1.5 Pro (002)\nNo RAG 1 44.75 80.21 45.80 25.28 51.34\nRAG 1 42.71 75.97 55.24 33.71 53.65\nUSC (Chen et al., 2024b) 4 46.44 76.68 58.39 37.64 56.43\nGenRead (Yu et al., 2023a) 2 45.08 77.39 54.90 34.27 54.70\nRobustRAG (Xiang et al., 2024)8 11 34.24 67.49 44.06 32.02 45.59\nInstructRAG (Wei et al., 2024) 1 46.78 80.57 54.90 34.83 56.14\nSelf-Route (Xu et al., 2024a) 1-2 47.46 79.86 58.04 38.20 57.58\nAstute RAG (t=1) 2 50.17 81.63 58.04 40.45 59.21\nAstute RAG (t=2) 3 51.53 81.27 58.74 40.45 59.69\nAstute RAG (t=3) 4 48.47 80.21 60.14 42.13 59.21\nTable 2|Main results on Gemini under zero-shot setting, showing the accuracy of different benchmark\nmethods vs.Astute RAG , along with their prediction complexity, in number of prediction API calls.\nBest scores are in bold.\nexperiments are under the zero-shot setting for controlled evaluation, where no demonstrations for\nQA or method-specific steps are provided.\nBaselines. We compareAstute RAG with various RAG methods designed for enhanced robustness\nand representative inference strategies designed to improve response trustworthiness.USC (Chen\net al., 2024b) is the universal self-consistency method that samples multiple LLM responses given\nthe same context and aggregates the answers. It provides a reference of naive improvements using\nadditional API calls. The temperature for sampling responses in this baseline is set to 0.7.Genread\n(Yu et al., 2023a) augments retrieved passages with LLM-generated passages. It provide a reference\nof presenting passages from both internal and external knowledge in the prompt without effectively\ncombining them.RobustRAG(Xiang et al., 2024) aggregates answers from each independent passage\nto provide certifiable robustness. We use the keyword aggregation variant as it is shown to be the\nbest-performing variant on advanced LLMs.InstructRAG (Wei et al., 2024) instructs the LLM to\nprovide a rationale connecting the answer with information in passages. For a fair comparison, we\nuse the instructions without training or in-context learning.Self-Route(Xu et al., 2024a) adaptively\nswitches between LLMs with and without RAG.7 This baseline provides a reference of switching\nbetween LLMs\u2019 internal and external knowledge.\nImplementation Details ofAstute RAG . The prompt templates forAstute RAG can be found\nin Appendix A. By default, we use 2 API calls per query, setting\ud835\udc61 = 1 to merge the prompt for", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "c4c79ac3-8e79-43d4-a1f3-55cb8538c485": {"text": "between LLMs\u2019 internal and external knowledge.\nImplementation Details ofAstute RAG . The prompt templates forAstute RAG can be found\nin Appendix A. By default, we use 2 API calls per query, setting\ud835\udc61 = 1 to merge the prompt for\nknowledge consolidation and answer finalization. For adaptive generation of internal knowledge, we\nprompt the LLM to generate no more than one passage.\n4.2. Main Results\nTable 1 and Table 2 presents the results on data with realistic retrieval augmentation for each dataset.\nBy comparing RAG and No RAG, we find that retrieved passages might not always bring benefits to\ndownstream performance \u2013 on NQ and TriviaQA, RAG performance lags behind No RAG. We attribute\nthis to that the questions being covered by the LLM\u2019s internal knowledge and the noise in retrieval\n7The original Self-Route switches between RAG and long-context LLMs, while our implementation switches between\nLLMs with and without RAG to better align with the problem formulation in this paper.\n8We observe a high refusal rate in responses of RobustRAG.\n8", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "192fe976-abc6-4ea1-8511-d57a07470e0e": {"text": "Astute RAG : Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models\nresults misleading the LLM. In contrast, on BioASQ and PopQA, which focus on domain-specific and\nlong-tail questions, RAG significantly improves LLM performance. However, due to imperfect retrieval\naugmentation, the absolute performance still remains to be unsatisfactory. Among all baselines, no\nsingle method consistently outperforms others across all datasets. This observation highlights that\nthese baselines are tailored to distinct settings and may not be universally applicable. For instance,\nInstructRAG is more effective on TriviaQA, achieving the best performance among all baselines with\nboth Claude and Gemini. In contrast, Self-Route performs better than InstructRAG on both NQ and\nBioASQ. Moreover, RobustRAG achieves very different performance when applied to Gemini and\nClaude. Through in-depth analysis, we find that RobustRAG with Gemini exhibits a high refusal rate\n(refuse to answer) in responses. We attribute this instability to the varying method designs of the\nbaselines, which are tailored for different scenarios, resulting in inconsistent improvement across\ndatasets. Overall, InstructRAG and Self-Route demonstrates the best performance among all baselines\nwhen applied to Claude and Gemini respectively. We also note that increasing the number of API calls\ndoes not necessarily correlate with improved performance.\nAstute RAG consistently outperforms baselines across all datasets of different properties. The\noverall improvement compared with the best baseline is relatively 6.85% on Claude and 4.13% on\nGemini, and the improvements in domain-specific questions are much higher. These results highlight\nthe effectiveness ofAstute RAG in overcoming imperfect retrieval augmentation. On Claude,\nadding more iteration of knowledge consolidation leads to consist improvement. The improvement\nmargin becomes lower when\ud835\udc61 becomes larger. This is because after each iteration, the remaining\nimprovement space for knowledge consolidation becomes smaller. On Gemini, increasing\ud835\udc61 primarily\nbenefitsBioASQandPopQA.Thesetwodatasetsrelymoreheavilyonexternalknowledge, anditerative\nknowledge consolidation helps mitigate noise within this external information. Performance on NQ\nand TriviaQA does not improve further when\ud835\udc61 reaches 3. We attribute this to the less critical role of\nexternal knowledge in these datasets. For setting consistency and efficiency, we set the parameter\u02c6\ud835\udc5a", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "b0eead99-110a-4a00-9577-c09ec66bfa39": {"text": "and TriviaQA does not improve further when\ud835\udc61 reaches 3. We attribute this to the less critical role of\nexternal knowledge in these datasets. For setting consistency and efficiency, we set the parameter\u02c6\ud835\udc5a\nto a smaller value, limiting the influence of internal knowledge.\n4.3. Analyses\nPerformance by retrieval precision.We compare the performance ofAstute RAG and baselines\nacrossdifferentsubsetspartitionedbytheirretrievalprecision, onourcollecteddatawithClaudeasthe\nLLM. As shown in Figure 4,Astute RAG achieves consistently better performance than all baselines\nacross different retrieval precision, indicating its effectiveness in improving RAG trustworthiness in\nbroad scenarios. Notably,Astute RAG does not sacrifice performance gain under high retrieval\nquality in exchange for improvement under low retrieval quality. When the retrieval quality is\nextremely low (close to zero retrieval precision), all other RAG variants underperforms the \u2019No RAG\u2019\nbaseline, except for the proposedAstute RAG . This observation aligns with the worst-case results\non RGB. It demonstrates the difficulty in overcoming imperfect retrieval augmentation, and verify the\neffectiveness ofAstute RAG in doing so.\nEffectiveness in addressing knowledge conflicts.We split our collected data in to three subset\naccording to the answers from Claude, with and without RAG. The answers from two inference\nmethods can be both correct, both incorrect, or conflicting with one being correct. These three subsets\nrepresents the three situations between internal and external knowledge. The results are shown\nin Figure 4. On the conflicting subset,Astute RAG successfully chooses the correct answer in\napproximately 80% of cases, being the most effective method in addressing knowledge conflicts.\nNotably,Astute RAG even brings performance improvement on the subset where neither internal\nnor external knowledge alone leads to the correct answer. This indicates thatAstute RAG can\neffectively combine partially-correct information from LLM-internal and external knowledge, to\n9", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "d8a05b43-5cc9-497e-ad08-13ab63bdf998": {"text": "Astute RAG : Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models\n(a) Performance by Retrieval Precision\n(b) Performance by Knowledge Conflicts\n(c) Worst-case Performance on RGB\nFigure 4|Analysis results. (a) Performance of Claude across different buckets ranked by retrieval\nprecision. (b) Performance of Claude on conflicting and consistent instances between No RAG\nand RAG. (c) Worst-case performance of Claude on RGB, where all retrieved passages are negative.\nAstute RAG reaches a performance close to No RAG, while other RAG systems are far behind.\nachieve the correct answer through collective information across them.\nWorst-case performance on RGB..Figure 4 presents the results under the worst-case setting on RGB\nwhere all retrieved documents are negative. It demonstrates the noise robustness ofAstute RAG\nand baseline RAG methods. The performance gap between RAG and No RAG exceeds 50 points,\nhighlighting the detrimental impact of imperfect retrieval results and emphasizing the importance of\nproviding robust safeguards against worst-case scenarios. While the baseline RAG methods outperform\nthe original RAG, they still obviously fall behind No RAG.Astute RAG is the only RAG method\nthat reaches a performance close to No RAG under the worst-case scenario, further supporting its\neffectiveness in addressing imperfect retrieval augmentation.\nQualitative study.In Figure 5, we present two representative examples showing the intermediate\noutputs ofAstute RAG . In the first example, LLM without RAG generates a wrong answer, while\nRAG returns a correct answer.Astute RAG successfully identified the incorrect information in\nits generated passage and an external passage, avoiding confirmation bias Tan et al. (2024). In\nthe second example, LLM alone is correct, while RAG is incorrect due to the noisy retrieval results.\nAstute RAG detected the correct answer from noisy retrieved information by checking with its\ninternal knowledge.\n5. Related Work\nRetrieval augmented generation (RAG) seeks to address the inherent knowledge limitation of LLMs\nwith passages retrieved from external sources of information such as private corpora or public\nknowledge bases (Borgeaud et al., 2022; Guu et al., 2020; Lewis et al., 2020). Given the widespread\nadoption of RAG in various real-world applications, including risk-sensitive domains, the negative\nimpact of noisy information within retrieved passages has garnered increasing attention (Cuconasu", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "2e7e4e91-9ac0-4dce-8a17-f21e73a7ee8a": {"text": "adoption of RAG in various real-world applications, including risk-sensitive domains, the negative\nimpact of noisy information within retrieved passages has garnered increasing attention (Cuconasu\net al., 2024). Recent work has sought to enhance the robustness of RAG systems against noise from\nvarious perspectives, including training LLMs with noisy context (Fang et al., 2024; Pan et al., 2024;\nYoran et al., 2024; Yu et al., 2023b), training small models to filter out irrelevant passages (Wang\net al., 2023c; Xu et al., 2023), passage reranking (Glass et al., 2022; Yu et al., 2024), dynamic and\niterative retrieval (Asai et al., 2023; Jiang et al., 2023; Yan et al., 2024), query rewriting (Ma et al.,\n10", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "0ee6405c-07e9-46ff-bee4-c2606b3b03c3": {"text": "Astute RAG : Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models\nFigure 5|Qualitative examples.Top: Astute RAG identified the error in internal knowledge (i.e.,\ngenerated passage) by confirming with external sources.Bottom: Astute RAG detected the correct\nanswer from noisy retrieved information by checking with its internal knowledge. Standard RAG does\nnot provide an answer because the retrieved passages are too noisy.\n2023), and speculative drafting (Wang et al., 2024). These studies focus on distinct modules or stages\nof RAG systems and are orthogonal to our work.\nOur work focuses on enhancing RAG robustness at the post-retrieval stage, after retrieved passages\nhave been provided. On this topic, RobustRAG (Xiang et al., 2024) aggregates answers from each\nindependent passage to provide certifiable robustness. InstructRAG (Wei et al., 2024) instructs the\nLLM to provide a rationale connecting the answer with information in passages. MADRA (Wang\net al., 2023b) applies multi-agent debate to select helpful evidence. However, these works do not\nexplicitly incorporate internal knowledge to recover from RAG failures and may therefore collapse\nwhen the majority of retrieved passages are negative. In terms of emphasizing internal knowledge of\nLLMs in RAG, recent work has explored using LLM-generated passage as context (Yu et al., 2023a;\nZhang et al., 2023), adaptively switching between LLMs with and without RAG (Jeong et al., 2024;\nMallen et al., 2023; Xu et al., 2024a), and combining answers from internal and external knowledge\n11", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "20b9cf33-a4f3-49fe-967a-74d67a505355": {"text": "Astute RAG : Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models\nthrough contrastive decoding (Jin et al., 2024; Zhao et al., 2024). We focus on a black-box setting\nwhere no further training is required, directly addressing knowledge conflicts to combine the helpful\ninformation from both sides and achieve more reliable answers.\n6. Conclusion\nOur paper investigates the impact of imperfect retrieval on the performance of RAG systems and\nidentifies knowledge conflicts as a key challenge. To address this, we introduceAstute RAG , a\nnovel approach that leverages the internal knowledge of LLMs and iteratively refines the generated\nresponses by consolidating internal and external knowledge in a source way. Our empirical results\ndemonstratetheeffectivenessof Astute RAG inmitigatingthenegativeeffectsofimperfectretrieval\nand improving the robustness of RAG systems, particularly in challenging scenarios with unreliable\nexternal sources.\nAmong the limitations,Astute RAG \u2019s effectiveness hinges on the capabilities of advanced LLMs\nwith strong instruction-following and reasoning abilities, hence potentially more limited applicability\nwith less sophisticated LLMs. As an important future direction, extending the experimental setup to\ninclude longer outputs would be important, where the challenges of imperfect retrieval and knowledge\nconflicts may be even more pronounced. Furthermore, a comprehensive analysis of the impact of\nvarious context types (Balachandran et al., 2024) would enhance the understanding of the proposed\nmethod\u2019s effectiveness. Future work can also extend our method beyond LLMs and RAG, such as\naddressing knowledge conflicts in multimodal settings (Zhu et al., 2024).\nAcknowledgement\nWe would like to thank Jinsung Yoon for valuable discussions and insights that helped to improve\nthis paper. We would also like to thank all other colleagues from Google Cloud AI Research for their\nvaluable feedback.\nReferences\nA. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi. Self-rag: Learning to retrieve, generate, and critique\nthrough self-reflection. InThe Twelfth International Conference on Learning Representations, 2023.\nY. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McK-\ninnon, et al. Constitutional ai: Harmlessness from ai feedback.arXiv preprint arXiv:2212.08073,\n2022.\nV. Balachandran, J. Chen, N. Joshi, B. Nushi, H. Palangi, E. Salinas, V. Vineet, J. Woffinden-Luey,", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "b864796c-61f2-4dc3-bea3-e9153675b20d": {"text": "innon, et al. Constitutional ai: Harmlessness from ai feedback.arXiv preprint arXiv:2212.08073,\n2022.\nV. Balachandran, J. Chen, N. Joshi, B. Nushi, H. Palangi, E. Salinas, V. Vineet, J. Woffinden-Luey,\nand S. Yousefi. Eureka: Evaluating and understanding large foundation models.arXiv preprint\narXiv:2409.10566, 2024.\nS. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. B. Van Den Driessche,\nJ.-B. Lespiau, B. Damoc, A. Clark, et al. Improving language models by retrieving from trillions of\ntokens. InInternational conference on machine learning, pages 2206\u20132240. PMLR, 2022.\nJ. Chen, H. Lin, X. Han, and L. Sun. Benchmarking large language models in retrieval-augmented\ngeneration. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages\n17754\u201317762, 2024a.\n12", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "c5ad6145-e62e-486d-a5c4-ae540a5767e8": {"text": "Astute RAG : Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models\nX. Chen, R. Aksitov, U. Alon, J. Ren, K. Xiao, P. Yin, S. Prakash, C. Sutton, X. Wang, and D. Zhou.\nUniversalself-consistencyforlargelanguagemodels. In ICML2024WorkshoponIn-ContextLearning ,\n2024b.\nF. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano, Y. Maarek, N. Tonellotto, and\nF. Silvestri. The power of noise: Redefining retrieval for rag systems. InProceedings of the 47th\nInternational ACM SIGIR Conference on Research and Development in Information Retrieval, pages\n719\u2013729, 2024.\nS. Dai, C. Xu, S. Xu, L. Pang, Z. Dong, and J. Xu. Unifying bias and unfairness in information\nretrieval: A survey of challenges and opportunities with large language models.arXiv preprint\narXiv:2404.11457, 2024.\nF. Fang, Y. Bai, S. Ni, M. Yang, X. Chen, and R. Xu. Enhancing noise robustness of retrieval-augmented\nlanguage models with adaptive adversarial training.arXiv preprint arXiv:2405.20978, 2024.\nM. Glass, G. Rossiello, M. F. M. Chowdhury, A. Naik, P. Cai, and A. Gliozzo. Re2g: Retrieve, rerank,\ngenerate. InProceedings of the 2022 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pages 2701\u20132715, 2022.\nK. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang. Retrieval augmented language model pre-training.\nIn International conference on machine learning, pages 3929\u20133938. PMLR, 2020.\nS. Jeong, J. Baek, S. Cho, S. J. Hwang, and J. C. Park. Adaptive-rag: Learning to adapt retrieval-\naugmented large language models through question complexity. InProceedings of the 2024 Confer-\nence of the North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies (Volume 1: Long Papers), pages 7029\u20137043, 2024.\nZ. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang, J. Callan, and G. Neubig. Active\nretrieval augmented generation. InProceedings of the 2023 Conference on Empirical Methods in\nNatural Language Processing, pages 7969\u20137992, 2023.\nZ. Jin, P. Cao, Y. Chen, K. Liu, X. Jiang, J. Xu, L. Qiuxia, and J. Zhao. Tug-of-war between knowledge:\nExploringandresolvingknowledgeconflictsinretrieval-augmentedlanguagemodels. In Proceedings\nof the 2024 Joint International Conference on Computational Linguistics, Language Resources and\nEvaluation (LREC-COLING 2024), pages 16867\u201316878, 2024.\nM. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer. Triviaqa: A large scale distantly supervised challenge", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "003bb1d0-8d14-4ddd-b589-fcfaf508410a": {"text": "Evaluation (LREC-COLING 2024), pages 16867\u201316878, 2024.\nM. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer. Triviaqa: A large scale distantly supervised challenge\ndataset for reading comprehension. InProceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 1601\u20131611, 2017.\nT. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin,\nJ.Devlin, K.Lee, etal. Naturalquestions: abenchmarkforquestionansweringresearch. Transactions\nof the Association for Computational Linguistics, 7:453\u2013466, 2019.\nP. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. K\u00fcttler, M. Lewis, W.-t. Yih,\nT. Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks.Advances\nin Neural Information Processing Systems, 33:9459\u20139474, 2020.\nS. Longpre, K. Perisetla, A. Chen, N. Ramesh, C. DuBois, and S. Singh. Entity-based knowledge\nconflicts in question answering. InProceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, 2021.\n13", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "42ef21ec-3f6d-4085-ae7a-76959c0dbcb4": {"text": "Astute RAG : Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models\nX. Ma, Y. Gong, P. He, H. Zhao, and N. Duan. Query rewriting in retrieval-augmented large language\nmodels. InProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,\npages 5303\u20135315, 2023.\nA. Mallen, A. Asai, V. Zhong, R. Das, D. Khashabi, and H. Hajishirzi. When not to trust language\nmodels: Investigating effectiveness of parametric and non-parametric memories. InProceedings of\nthe 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npages 9802\u20139822, 2023.\nR. Pan, B. Cao, H. Lin, X. Han, J. Zheng, S. Wang, X. Cai, and L. Sun. Not all contexts are equal:\nTeaching llms credibility-aware generation.arXiv preprint arXiv:2404.06809, 2024.\nR. Shao, J. He, A. Asai, W. Shi, T. Dettmers, S. Min, L. Zettlemoyer, and P. W. Koh. Scaling retrieval-\nbased language models with a trillion-token datastore.arXiv preprint arXiv:2407.12854, 2024.\nH. Su, H. Yen, M. Xia, W. Shi, N. Muennighoff, H.-y. Wang, H. Liu, Q. Shi, Z. S. Siegel, M. Tang, et al.\nBright: A realistic and challenging benchmark for reasoning-intensive retrieval.arXiv preprint\narXiv:2407.12883, 2024.\nH. Tan, F. Sun, W. Yang, Y. Wang, Q. Cao, and X. Cheng. Blinded by generated contexts: How\nlanguage models merge generated and retrieved contexts for open-domain qa?arXiv preprint\narXiv:2401.11911, 2024.\nN. Thakur, N. Reimers, A. R\u00fcckl\u00e9, A. Srivastava, and I. Gurevych. Beir: A heterogeneous benchmark for\nzero-shotevaluationofinformationretrievalmodels. In Thirty-fifthConferenceonNeuralInformation\nProcessing Systems Datasets and Benchmarks Track (Round 2), 2024.\nG. Tsatsaronis, G. Balikas, P. Malakasiotis, I. Partalas, M. Zschunke, M. R. Alvers, D. Weissenborn,\nA. Krithara, S. Petridis, D. Polychronopoulos, et al. An overview of the bioasq large-scale biomedical\nsemantic indexing and question answering competition.BMC bioinformatics, 16:1\u201328, 2015.\nF. Wang, W. Mo, Y. Wang, W. Zhou, and M. Chen. A causal view of entity bias in (large) language\nmodels. InFindings of the Association for Computational Linguistics: EMNLP 2023, pages 15173\u2013\n15184, 2023a.\nH. Wang, X. Du, W. Yu, Q. Chen, K. Zhu, Z. Chu, L. Yan, and Y. Guan. Apollo\u2019s oracle: Retrieval-\naugmented reasoning in multi-agent debates.arXiv preprint arXiv:2312.04854, 2023b.\nZ. Wang, J. Araki, Z. Jiang, M. R. Parvez, and G. Neubig. Learning to filter context for retrieval-\naugmented generation.arXiv preprint arXiv:2311.08377, 2023c.", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "6c1519f9-b171-4bf1-96bf-36a1ebed1dfa": {"text": "augmented reasoning in multi-agent debates.arXiv preprint arXiv:2312.04854, 2023b.\nZ. Wang, J. Araki, Z. Jiang, M. R. Parvez, and G. Neubig. Learning to filter context for retrieval-\naugmented generation.arXiv preprint arXiv:2311.08377, 2023c.\nZ. Wang, Z. Wang, L. Le, H. S. Zheng, S. Mishra, V. Perot, Y. Zhang, A. Mattapalli, A. Taly, J. Shang,\net al. Speculative rag: Enhancing retrieval augmented generation through drafting.arXiv preprint\narXiv:2407.08223, 2024.\nZ. Wei, W.-L. Chen, and Y. Meng. Instructrag: Instructing retrieval-augmented generation with explicit\ndenoising. arXiv preprint arXiv:2406.13629, 2024.\nC. Xiang, T. Wu, Z. Zhong, D. Wagner, D. Chen, and P. Mittal. Certifiably robust rag against retrieval\ncorruption. arXiv preprint arXiv:2405.15556, 2024.\nJ. Xie, K. Zhang, J. Chen, R. Lou, and Y. Su. Adaptive chameleon or stubborn sloth: Revealing the\nbehavior of large language models in knowledge conflicts. InThe Twelfth International Conference\non Learning Representations, 2024.\n14", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "a7c12e2b-b644-49cd-bd72-48bd0204dd1b": {"text": "Astute RAG : Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models\nF. Xu, W. Shi, and E. Choi. Recomp: Improving retrieval-augmented lms with compression and\nselective augmentation.arXiv preprint arXiv:2310.04408, 2023.\nP. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian, E. Bakhturina, M. Shoeybi, and\nB. Catanzaro. Retrieval meets long context large language models. InThe Twelfth International\nConference on Learning Representations, 2024a.\nR. Xu, Z. Qi, C. Wang, H. Wang, Y. Zhang, and W. Xu. Knowledge conflicts for llms: A survey.arXiv\npreprint arXiv:2403.08319, 2024b.\nS.-Q. Yan, J.-C. Gu, Y. Zhu, and Z.-H. Ling. Corrective retrieval augmented generation.arXiv preprint\narXiv:2401.15884, 2024.\nX. Yang, K. Sun, H. Xin, Y. Sun, N. Bhalla, X. Chen, S. Choudhary, R. D. Gui, Z. W. Jiang, Z. Jiang,\net al. Crag\u2013comprehensive rag benchmark.arXiv preprint arXiv:2406.04744, 2024.\nO. Yoran, T. Wolfson, O. Ram, and J. Berant. Making retrieval-augmented language models robust to\nirrelevant context. InThe Twelfth International Conference on Learning Representations, 2024.\nW. Yu, D. Iter, S. Wang, Y. Xu, M. Ju, S. Sanyal, C. Zhu, M. Zeng, and M. Jiang. Generate rather\nthan retrieve: Large language models are strong context generators. InThe Eleventh International\nConference on Learning Representations, 2023a.\nW. Yu, H. Zhang, X. Pan, K. Ma, H. Wang, and D. Yu. Chain-of-note: Enhancing robustness in\nretrieval-augmented language models.arXiv preprint arXiv:2311.09210, 2023b.\nY. Yu, W. Ping, Z. Liu, B. Wang, J. You, C. Zhang, M. Shoeybi, and B. Catanzaro. Rankrag: Unifying\ncontext ranking with retrieval-augmented generation in llms.arXiv preprint arXiv:2407.02485,\n2024.\nY. Zhang, M. Khalifa, L. Logeswaran, M. Lee, H. Lee, and L. Wang. Merging generated and retrieved\nknowledge for open-domain qa. InThe 2023 Conference on Empirical Methods in Natural Language\nProcessing, 2023.\nZ. Zhao, E. Monti, J. Lehmann, and H. Assem. Enhancing contextual understanding in large language\nmodels through contrastive decoding. InProceedings of the 2024 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1:\nLong Papers), pages 4225\u20134237, 2024.\nT. Zhu, Q. Liu, F. Wang, Z. Tu, and M. Chen. Unraveling cross-modality knowledge conflict in large\nvision-language models.arXiv preprint arXiv:2410.03659, 2024.\nW. Zou, R. Geng, B. Wang, and J. Jia. Poisonedrag: Knowledge poisoning attacks to retrieval-", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "481a0c55-c8e1-41b3-bed8-dbab5be5c5bb": {"text": "vision-language models.arXiv preprint arXiv:2410.03659, 2024.\nW. Zou, R. Geng, B. Wang, and J. Jia. Poisonedrag: Knowledge poisoning attacks to retrieval-\naugmented generation of large language models.arXiv preprint arXiv:2402.07867, 2024.\n15", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "845be6a1-0fa2-41a3-81e8-c1a43f419320": {"text": "Astute RAG : Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models\nA. Prompt Template forAstute RAG\nPrompt for Adaptive Passage Generation (\ud835\udc5d\ud835\udc54\ud835\udc52\ud835\udc5b)\nGenerate a document that provides accurate and relevant information to answer the given\nquestion. If the information is unclear or uncertain, explicitly state \u2019I don\u2019t know\u2019 to avoid any\nhallucinations.\nQuestion: {question} Document:\nPrompt for Iterative Knowledge Consolidation (\ud835\udc5d\ud835\udc50\ud835\udc5c\ud835\udc5b)\nTask: Consolidate information from both your own memorized documents and externally\nretrieved documents in response to the given question.\n* For documents that provide consistent information, cluster them together and sum-\nmarize the key details into a single, concise document.\n* For documents with conflicting information, separate them into distinct documents, ensuring\neach captures the unique perspective or data.\n* Exclude any information irrelevant to the query.\nFor each new document created, clearly indicate:\n* Whether the source was from memory or an external retrieval.\n* The original document numbers for transparency.\nInitial Context: {context}\nLast Context: {context}\nQuestion: {question}\nNew Context:\n16", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "f17da396-4eea-4699-800f-34496a61f6fb": {"text": "Astute RAG : Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models\nPrompt for Knowledge Consolidation and Answer Finalization (\ud835\udc5d\ud835\udc4e\ud835\udc5b\ud835\udc60)\nTask: Answer a given question using the consolidated information from both your own\nmemorized documents and externally retrieved documents.\nStep 1: Consolidate information\n* For documents that provide consistent information, cluster them together and summarize\nthe key details into a single, concise document.\n* For documents with conflicting information, separate them into distinct documents, ensuring\neach captures the unique perspective or data.\n* Exclude any information irrelevant to the query.\nFor each new document created, clearly indicate:\n* Whether the source was from memory or an external retrieval.\n* The original document numbers for transparency.\nStep 2: Propose Answers and Assign Confidence\nFor each group of documents, propose a possible answer and assign a confidence score based\non the credibility and agreement of the information.\nStep 3: Select the Final Answer\nAfter evaluating all groups, select the most accurate and well-supported answer.\nHighlight your exact answer within<ANSWER> your answer</ANSWER>.\nInitial Context: {context_init}\n[Consolidated Context: {context}] # optional\nQuestion: {question}\nAnswer:\nB. Data Collection\nEncompassing adiverse range ofnatural questions, our benchmark consists ofrealistic retrieval results\nwith Google Search9 as the retriever and the Web as the corpus. Notably, we do not select questions\nor annotate answers based on the retrieval results. This setting allows us to analyze the severity of\nimperfect retrieval in real-world RAG. It distinguishes our benchmark from previous ones that employ\nsynthetic retrieval corruptions or that unintentionally reduce the frequency of imperfect retrieval\nwith biased construction protocols (Chen et al., 2024a; Yang et al., 2024). Overall, our benchmark\ncontains 1,042 short-form question-answer pairs, each paired with 10 retrieved passages.\nQuestion-answer pairs. We consider question-answer pairs from four datasets of different\nproperties spanning across general questions, domain-specific questions, and long-tail questions. NQ\n(Kwiatkowskietal.,2019)andTriviaQA(Joshietal.,2017)aretwowidely-studiedquestion-answering\n(QA) datasets in general domains. BioASQ (Tsatsaronis et al., 2015) is from biomedical domain\nthat has demonstrated significant benefits from RAG when general-purpose LLMs are considered.", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "cf8a178a-1f19-4961-9326-ab291b93eeb2": {"text": "(QA) datasets in general domains. BioASQ (Tsatsaronis et al., 2015) is from biomedical domain\nthat has demonstrated significant benefits from RAG when general-purpose LLMs are considered.\nPopQA (Mallen et al., 2023) focuses on long-tail knowledge and has been shown to be challenging\nfor even advanced LLMs to solve without external knowledge. All these datasets contain questions\nwith short-form answers and most of them list all valid answer variants. This format can support\nautomatic verification of answer appearance in retrieved passages and model responses, leading to\nmore precise evaluations.\n9https://developers.google.com/custom-search/v1/overview\n17", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "21bc5716-cc44-4f1d-bdf8-28f82afc63bd": {"text": "Astute RAG : Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models\nRetrieval process.For each question in our benchmark, we query Google Search to retrieve the\ntop 30 results and select the first 10 accessible websites. From each retrieved website, we extract the\nparagraph corresponding to the snippet provided in Google Search results as the retrieved passage.\nWe do not consider enhancements to the retrieval side, such as query rewriting, as such enhancements\nare typically already incorporated into commercial information retrieval systems.\n18", "metadata": {"title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models", "pdf_url": "http://arxiv.org/pdf/2501.05249v1"}}, "f76c5700-555b-4085-9efa-714200109f60": {"text": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for\nMulti-Hop Queries\nYixuan Tangand Yi Yang\nHong Kong University of Science and Technology\n{yixuantang,imyiyang}@ust.hk\nAbstract\nRetrieval-augmented generation (RAG) aug-\nments large language models (LLM) by re-\ntrieving relevant knowledge, showing promis-\ning potential in mitigating LLM hallucinations\nand enhancing response quality, thereby facil-\nitating the great adoption of LLMs in prac-\ntice. However, we find that existing RAG sys-\ntems are inadequate in answering multi-hop\nqueries, which require retrieving and reasoning\nover multiple pieces of supporting evidence.\nFurthermore, to our knowledge, no existing\nRAG benchmarking dataset focuses on multi-\nhop queries. In this paper, we develop a novel\ndataset, MultiHop-RAG, which consists of a\nknowledge base, a large collection of multi-\nhop queries, their ground-truth answers, and\nthe associated supporting evidence. We detail\nthe procedure of building the dataset, utiliz-\ning an English news article dataset as the un-\nderlying RAG knowledge base. We demon-\nstrate the benchmarking utility of MultiHop-\nRAG in two experiments. The first experiment\ncompares different embedding models for re-\ntrieving evidence for multi-hop queries. In the\nsecond experiment, we examine the capabili-\nties of various state-of-the-art LLMs, includ-\ning GPT-4, PaLM, and Llama2-70B, in rea-\nsoning and answering multi-hop queries given\nthe evidence. Both experiments reveal that ex-\nisting RAG methods perform unsatisfactorily\nin retrieving and answering multi-hop queries.\nWe hope MultiHop-RAG will be a valuable re-\nsource for the community in developing effec-\ntive RAG systems, thereby facilitating greater\nadoption of LLMs in practice. The MultiHop-\nRAG and implemented RAG system is publicly\navailable athttps://github.com/yixuantt/\nMultiHop-RAG/.\n1 Introduction\nThe emergence of large language models (LLMs),\nsuch as ChatGPT, has fostered a wide range of inno-\nvations, powering intelligent chatbots and other nat-\nural language processing (NLP) applications (Ope-\nFigure 1: RAG with multi-hop query.\nnAI, 2023). One promising use case is Retrieval-\nAugmented Generation (RAG) (Asai et al., 2023),\nwhich optimizes the output of a large language\nmodel by referencing an external knowledge base\noutside of the LLM training data sources before\ngenerating a response. RAG improves LLM\u2019s re-\nsponse (Borgeaud et al., 2022) and also mitigates\nthe occurrence of hallucinations, thereby enhancing\nthe models\u2019 credibility (Gao et al., 2023). LLM-", "metadata": {"title": "In Defense of RAG in the Era of Long-Context Language Models", "pdf_url": "http://arxiv.org/pdf/2409.01666v1"}}, "bb90b2f0-9fcd-4008-8a73-ef04b4023a6c": {"text": "outside of the LLM training data sources before\ngenerating a response. RAG improves LLM\u2019s re-\nsponse (Borgeaud et al., 2022) and also mitigates\nthe occurrence of hallucinations, thereby enhancing\nthe models\u2019 credibility (Gao et al., 2023). LLM-\nbased frameworks, such as LlamaIndex (Liu, 2022)\nand LangChain (Chase, 2022), specialize in sup-\nporting RAG pipelines.\nIn real-world Retrieval-Augmented Generation\n(RAG) applications, a user\u2019s query often necessi-\ntates retrieving and reasoning over evidence from\nmultiple documents, a process known asmulti-hop\nquery. For instance, consider financial analysis us-\ning a database of financial reports. A financial ana-\nlyst might query, Which company among Google,\nApple, and Nvidia reported the largest profit mar-\ngins in their third-quarter reports for 2023? or\ninquire about a specific company\u2019s performance\nover time, such as How does Apple\u2019s sales trend\nlook over the past three years? These queries re-\nquire evidence from multiple documents to formu-\nlate an answer. Due to the multifaceted nature of\nsuch queries, involving information from various\nsources, traditional similarity matching methods\nlike cosine similarity between query and financial\narXiv:2401.15391v1  [cs.CL]  27 Jan 2024", "metadata": {"title": "In Defense of RAG in the Era of Long-Context Language Models", "pdf_url": "http://arxiv.org/pdf/2409.01666v1"}}, "8ba1f2ee-0971-474c-81aa-a691e5b54f21": {"text": "News source Fortune Magazine The Sydney Morning Herald\nEvidence Back then, just like today, home prices had boomed\nfor years before Fed officials were ultimately forced\nto hike interest rates aggressively in an attempt to\nfight inflation.\nPostponements of such reports could complicate\nthings for the Fed, which has insisted it will make\nupcoming decisions on interest rates based on what\nincoming data say about the economy.\nClaim Federal Reserve officials were forced to aggressively\nhike interest rates to combat inflation after years of\nbooming home prices.\nThe Federal Reserve has insisted that it will base its\nupcoming decisions on interest rates on the incoming\neconomic data.\nBridge-Topic Interest rate hikes to combat inflation Interest rate decisions based on economic data\nBridge-Entity Federal Reserve Federal Reserve\nQuery Does the article from Fortune suggest that the Federal Reserve\u2019s interest rate hikes are a response to past\nconditions, such as booming home prices, while The Sydney Morning Herald article indicates that the\nFederal Reserve\u2019s future interest rate decisions will be based on incoming economic data?\nAnswer Yes\nTable 1: An example of a multi-hop query, including supporting evidence from two news articles, the paraphrased\nclaim, the bridge-topic and bridge-entity, and the corresponding answer.\nreport chunk embeddings might not yield optimal\nresults. We demonstrate this multi-hop retrieval\nprocess in Figure 1.\nHowever, existing RAG benchmarks, such as\nRGB (Chen et al., 2023) and RECALL (Liu et al.,\n2023), mainly evaluate a simple case where the an-\nswer of a query can be retrieved and solved using\none single piece of evidence. None of these bench-\nmarks assess the retrieval and reasoning capability\nof LLMs for complex multi-hop queries. To ad-\ndress this gap and make RAG benchmarking more\nclosely resemble real-world scenarios, in this paper,\nwe introduce MultiHop-RAG. To our knowledge,\nMultiHop-RAG is one of the first RAG datasets\nfocusing specifically on multi-hop queries.\nBased on the RAG queries commonly encoun-\ntered in real-world scenarios, we first categorize\nmulti-hop queries into four types: Inference query,\nComparison query , Temporal query , and Null\nquery. The first three types \u2014 Inference, Com-\nparison, and Temporal \u2014 require the retrieval and\nanalysis of evidence from multiple sources, encom-\npassing tasks like inferring relationships, compar-\ning data points, and sequencing events over time.\nThe Null query represents a scenario where the\nquery cannot be derived from the knowledge base.", "metadata": {"title": "In Defense of RAG in the Era of Long-Context Language Models", "pdf_url": "http://arxiv.org/pdf/2409.01666v1"}}, "3965646b-8363-4ad0-bcb7-a8e38afc295c": {"text": "analysis of evidence from multiple sources, encom-\npassing tasks like inferring relationships, compar-\ning data points, and sequencing events over time.\nThe Null query represents a scenario where the\nquery cannot be derived from the knowledge base.\nThis category is crucial for assessing whether an\nLLM might hallucinate an answer to a multi-hop\nquery when the retrieved text lacks relevance.\nWe construct our RAG knowledge base using a\ncollection of news articles. Using GPT-4 as a data\ngenerator, we then take an extensive procedure to\nconstruct a diverse set of multi-hop queries, each\nrequiring the retrieval and reasoning over multiple\ndocuments. An example of query construction is\nshown in Table 1. First, we begin by extracting\nfactual sentences from each news article as evi-\ndence. For example, an extracted piece of evidence\nfrom an article may state: \u201cBack then, just like\ntoday, home prices had boomed for years before\nFed officials were ultimately forced to hike interest\nrates aggressively in an attempt to fight inflation.\u201d\nSecond, we input each evidence piece into GPT-4,\nprompting it to rephrase the evidence into a claim.\nThis claim is clarified with a disambiguated topic\nand entity. For instance, GPT-4 might rephrase the\naforementioned evidence into: \u201cFederal Reserve\nofficials were forced to aggressively hike interest\nrates to combat inflation after years of booming\nhome prices\u201d, identifying \u201cInterest rate hikes to\ncombat inflation\u201d as the topic and \u201cFederal Re-\nserve\u201d as the entity. These topics and entities act as\nbridges for constructing multi-hop queries, known\nas bridge-topic or bridge-entity. Next, we use GPT-\n4 to generate specific multi-hop queries related to\nthe same bridge-topic or bridge-entity, accompa-\nnied by the correct answers. Lastly, we undertake\na validation step to ensure the data quality.\nWe demonstrate the benchmarking capabilities\nof MultiHop-RAG using two experiments, utilizing\na RAG system implemented with LlamaIndex (Liu,\n2022). The first experiment involves a comparison\nof different embedding models for retrieving rele-\nvant evidence for multi-hop queries. In the second\nexperiment, we assess the reasoning and answering\nabilities of various state-of-the-art LLMs, including\nGPT-4, GPT-3.5, PaLM, Claude-2, Llama2-70B,\nand Mixtral-8x7B, for multi-hop queries when re-\ntrieved text is provided. The results from both ex-\nperiments indicate that the current RAG implemen-\ntations are inadequate for effectively retrieving and\nanswering multi-hop queries. We publicly release", "metadata": {"title": "In Defense of RAG in the Era of Long-Context Language Models", "pdf_url": "http://arxiv.org/pdf/2409.01666v1"}}, "a3f053ab-a08b-4e95-ab8d-9b06593e6658": {"text": "this challenging MultiHop-RAG dataset and hope it\nwill be a valuable resource for the community in de-\nveloping and benchmarking RAG systems, thereby\nunleashing the great potential of generative AI in\npractice.\n2 RAG with multi-Hop queries\n2.1 Retrieval-augmented Generation (RAG)\nIn an RAG application, we utilize an external cor-\npus, denoted as D, which comprises multiple docu-\nments and serves as the knowledge base. Each doc-\nument within this corpus, represented as di \u2208 D, is\nsegmented into a set of chunks.These chunks are\nthen transformed into vector representations using\nan embedding model and stored in an embedding\ndatabase. Given a user query q, the system typi-\ncally retrieves the top-K chunks that best match the\nquery. These chunks constitute the retrieval set\nfor query q, represented as Rq = {r1, r2, ..., rK}.\nThe retrieved chunks, combined with the query\nand an optional prompt, are then fed into an LLM\nto generate a final answer, following the format:\nLLM(q, Rq, prompt) \u2192 answer.\n2.2 Multi-Hop Query\nWe define a multi-hop query as one that requires\nretrieving and reasoning over multiple pieces of\nsupporting evidence to provide an answer. In other\nwords, for a multi-hop query q, the chunks in the\nretrieval set Rq collectively provide an answer\nto q. For example, the query \"Which company\namong Google, Apple, and Nvidia reported the\nlargest profit margins in their third-quarter reports\nfor 2023?\" requires 1) retrieving relevant pieces of\nevidence related to profit margins from the reports\nof the three companies; 2) generating an answer by\ncomparing and reasoning from the multiple pieces\nof retrieved evidence. This differs from a single-\nhop query such as \"What is Google\u2019s profit margin\nin the third-quarter reports for 2023,\" where the\nanswer can be directly derived from a single piece\nof evidence.\nBased on the queries commonly used in real-\nworld RAG systems, we identify four types of\nmulti-hop queries. For each type, we present a\nhypothetical query within the context of a financial\nRAG system, where the knowledge base consists\nof a collection of annual reports.\nInference query:For such a query q, the answer\nis deduced through reasoning from the retrieval\nset Rq. An example of an inference query might\nbe: Which report discusses the supply chain risk of\nApple, the 2019 annual report or the 2020 annual\nreport?\nComparison query: For such a query q, the an-\nswer requires a comparison of evidence within the\nretrieval set Rq. For instance, a comparison query\nmight ask: Did Netflix or Google report higher", "metadata": {"title": "In Defense of RAG in the Era of Long-Context Language Models", "pdf_url": "http://arxiv.org/pdf/2409.01666v1"}}, "88723282-75ea-4e09-930e-8c6876c1af42": {"text": "report?\nComparison query: For such a query q, the an-\nswer requires a comparison of evidence within the\nretrieval set Rq. For instance, a comparison query\nmight ask: Did Netflix or Google report higher\nrevenue for the year 2023?\"\nTemporal query:For such a query q, the answer\nrequires an analysis of the temporal information\nof the retrieved chunks. For example, a temporal\nquery may ask: Did Apple introduce the AirTag\ntracking device before or after the launch of the 5th\ngeneration iPad Pro?\nNull query:For such as queryq, the answer cannot\nbe derived from the retrieved set Rq. We include\nthe null query to assess the generation quality, es-\npecially regarding the issue of hallucination. For a\nnull query, even though a retrieved set is provided,\nan LLM should produce a null response instead\nof hallucinating an answer. For example, assum-\ning ABCD is a non-existent company, a null query\nmight ask: What are the sales of company ABCD\nas reported in its 2022 and 2023 annual reports?\n2.3 Evaluation Metrics\nAn RAG system handling multi-hop queries can be\nassessed from two key aspects: retrieval evaluation\nand generation evaluation.\nRetrieval Evaluation:Evidently, the quality of\nthe retrieval set Rq determines the final genera-\ntion quality. We compare the retrieved set with\nthe ground truth evidence associated with each\nquery, except for the null queries, as they have\nno evidence to derive from. Assuming the top-\nK chunks are retrieved, i.e., |Rq| = K, we use\nretrieval evaluation metrics including Mean Aver-\nage Precision at K (MAP@K), Mean Reciprocal\nRank at K (MRR@K), and Hit Rate at K (Hit@K).\nMAP@K measures the average top-K retrieval pre-\ncision across all queries. MRR@K calculates the\naverage of the reciprocal ranks of the first relevant\nchunk for each query, considering the top-K re-\ntrieved set. Hit@K metric measures the fraction of\nevidence that appears in the top-K retrieved set.\nResponse Evaluation: Since the multi-hop\nquery requires reasoning over multiple pieces of\nretrieved chunks, we can also evaluate the reason-\ning capability of the LLM by comparing the LLM\nresponse with the ground truth answer of the query.", "metadata": {"title": "In Defense of RAG in the Era of Long-Context Language Models", "pdf_url": "http://arxiv.org/pdf/2409.01666v1"}}, "9f32c421-7ef5-4178-9c19-039c528c16c5": {"text": "Figure 2: MultiHop-RAG Construction Pipeline.\n3 A Benchmarking Dataset:\nMultiHop-RAG\nIn this section, we provide detailed information\non the construction of the MultiHop-RAG dataset.\nSpecifically, we describe the process of creating a\nset of multi-hop queries, along with the correspond-\ning ground truth evidence sets and answers derived\nfrom a collection of news articles.\n3.1 MultiHop-RAG Construction\nStep 1: Dataset Collection.We download a news\ndataset using the mediastack API 1, a REST API in-\nterface delivering worldwide news data. The news\ndata source comprises various English-language\nwebsites covering a range of news categories: en-\ntertainment, business, sports, technology, health,\nand science. To mimic real-world RAG scenarios,\nwhere the knowledge base data, such as an enter-\nprise\u2019s internal data, may differ from the LLMs\u2019\ntraining data, we select news articles published\nfrom September 26, 2023, to December 26, 2023.\nThis timeframe extends beyond the knowledge cut-\noff of some widely-used LLMs, including Chat-\nGPT and LLaMA, as of the time of writing. This\nselection also helps in teasing out the possibility\nof the underlying LLM having been exposed to\nthese news articles. We only keep articles with a\ntoken length greater than or equal to 1,024. Every\n1https://mediastack.com/\nnews article is paired with metadata, including the\ntitle, publish date, author, category, URL, and news\nsource.\nStep 2: Evidence Extraction.For each article, we\nextract factual or opinion sentences using a trained\nlanguage model 2. These factual sentences are later\nused as evidence for answering multi-hop queries.\nWe retain only those news articles containing ev-\nidence that may have overlapping keywords with\nother news articles. This allows us to later create\nmulti-hop queries where the answer\u2019s evidences\nare drawn from multiple sources.\nStep 3: Claim, Bridge-Entity, Bridge-Topic Gen-\neration. Our goal is to use GPT-4 to automatically\ngenerate high-quality multi-hop queries using the\nevidence set. However, the raw evidence obtained\nfrom Step 2 is not ideal for query generation due\nto inconsistency in linguistic structure. For exam-\nple, some pieces of evidence use pronouns to refer\nto subjects and lack the actual entity in the text.\nTo address this, we employ GPT-4 to paraphrase\nthe evidence, which we refer to as claims, given\nthe original evidence and its context. To ensure\nconsistency between the generated claim and the\nevidence, we further perform fact-checking using\nthe UniEval (Zhong et al., 2022) framework to ver-", "metadata": {"title": "In Defense of RAG in the Era of Long-Context Language Models", "pdf_url": "http://arxiv.org/pdf/2409.01666v1"}}, "1ecd0724-837c-4dd9-8118-f4f828e0629e": {"text": "the evidence, which we refer to as claims, given\nthe original evidence and its context. To ensure\nconsistency between the generated claim and the\nevidence, we further perform fact-checking using\nthe UniEval (Zhong et al., 2022) framework to ver-\nify the alignment between the evidence and claim.\nAppendix A presents the prompt used for GPT-4\nfor claim generation.\nBridge-Entity and Bridge-Topic:The shared en-\ntity or topic across pieces of evidence is referred to\nas the bridge-entity or bridge-topic. These bridge-\nentities or bridge-topics can be used to link dif-\nferent pieces of evidence from which a multi-hop\nquery\u2019s answer is derived. For example, in a claim\nsuch as \u201cGoogle reports its third-quarter results for\n2023, showcasing a detailed overview of its finan-\ncial performance, including revenue growth, profit\nmargins\u201d, the term profit margin can be viewed as\na bridge-topic and the term Google can be viewed\nas a bridge-entity that links the different pieces of\nevidence. We prompt GPT-4 to identify the bridge-\nentity and bridge-topic for each claim. Appendix A\nalso presents the prompt used for GPT-4 for bridge\ngeneration.\nStep 4: Query and Answer Generation.In this\nstep, we leverage the bridge-entity or bridge-topic\nto generate multi-hop queries. Specifically, we first\ngroup the claims having the same bridge-entity or\n2https://huggingface.co/lighteternal/fact-or-opinion-xlmr-\nel", "metadata": {"title": "In Defense of RAG in the Era of Long-Context Language Models", "pdf_url": "http://arxiv.org/pdf/2409.01666v1"}}, "495d371c-0f91-47b3-a504-987b4480c651": {"text": "bridge-topic into a claim set. We restrict the claim\nset to have at least two claims but no more than four\nclaims. For each type of query, we feed the claim\nset to GPT-4 and prompt it with an instruction to\ngenerate a query with information from each claim.\nBelow, we explain the specifications for different\nmulti-hop query types. In the construction of each\nquery, we also include the source of the news article\nwhere the supporting evidence is associated with\nto mimic real-world RAG scenarios. Appendix\nA presents the prompts used for GPT-4 for query\ngeneration.\nInference Query:These queries are formulated\nby synthesizing the various characterizations of the\nbridge-entity across multiple claims, with the final\nanswer being the identification of the entity itself.\nComparison Query: These queries are struc-\ntured to compare the similarities and differences\nrelated to the bridge entity or topic. The resultant\nanswer to such queries is typically a definitive \u201cyes\u201d\nor \u201cno\u201d, based on the comparison.\nTemporal Query: These queries explore the\ntemporal ordering of events across different points\nin time. The answer to such queries is typically a\n\u201cyes\u201d or \u201cno\u201d or a single temporal indicator word\nlike \u201cbefore\u201d or \u201cafter\u201d.\nNull Query: Null query is a query whose an-\nswer cannot be derived from the retrieved set. To\ncreate null queries, we generate multi-hop queries\nusing entities that do not exist in the existing bridge-\nentities. To add complexity, we also include fic-\ntional news source metadata when formulating\nthese questions, ensuring that the questions do not\nreference any contextually relevant content from\nthe knowledge base. The answer to the null query\nshould be \u201cinsufficient information\u201d or similar.\nStep 5: Quality Assurance.Finally, we use two\napproaches to reassure the dataset quality. First, we\nmanually review a subset sample of the generated\nmulti-hop queries, their corresponding evidence\nsets, and the final answers. The results of the man-\nual examination indicate a high degree of accuracy\nand data quality. Second, we utilize GPT-4 to as-\nsess each example in the dataset against the follow-\ning criteria: 1) The generated query must utilize\nall provided evidence in formulating the response;\n2) The query should be answerable solely based\non the provided evidence; 3) The response to the\ngenerated query should be either a single word or\na specific entity; 4) The query must conform to its\ndesignated query type.\nCategory Avg. Tokens Entry Count\ntechnology 2262.3 172\nentertainment 2084.3 114\nsports 2030.6 211", "metadata": {"title": "In Defense of RAG in the Era of Long-Context Language Models", "pdf_url": "http://arxiv.org/pdf/2409.01666v1"}}, "94fd41f7-8326-44c0-9891-89df92324677": {"text": "generated query should be either a single word or\na specific entity; 4) The query must conform to its\ndesignated query type.\nCategory Avg. Tokens Entry Count\ntechnology 2262.3 172\nentertainment 2084.3 114\nsports 2030.6 211\nscience 1745.5 21\nbusiness 1723.8 81\nhealth 1481.1 10\ntotal 2046.5 609\nTable 2: Descriptive statistics of the news article knowl-\nedge base in MultiHop-RAG.\nQuery Category Entry Count Percentage\nInference Query 816 31.92%\nComparison Query 856 33.49%\nTemporal Query 583 22.81%\nNull Query 301 11.78%\nTotal 2,556 100.00 %\nTable 3: The distribution of query types in MultiHop-\nRAG.\n3.2 Descriptive Statistics\nThe MultiHop-RAG dataset contains six different\ntypes of news articles, covering 609 distinct news,\nwith an average of 2,046 tokens. The distribution of\nthe news categories is shown in Table 2. MultiHop-\nRAG contains four types of multi-hop queries and\nthe distribution of these queries is shown in Table\n3. In total, about 88% of queries in the dataset are\nnon-null queries where answers can be retrieved\nand reasoned from the knowledge base. In addition,\nthe form of queries exhibits considerable diversity.\nApproximately 27% of interrogative queries start\nwith \"does,\" around 15% initiate with \"what,\" a\nsimilar proportion start \"which,\" and 14% begin\nwith \"who,\" with the remainder incorporating a\nsmall percentage of other interrogative words such\nas \"when.\" Moreover, the number of evidence re-\nquired to answer a multi-hop query varies. Table\n4 shows the distribution of evidence numbers for\neach query in the dataset. Around 42% of queries\ncan be answered using two pieces of evidence,\nwhile approximately 30% and 15% of queries can\nbe answered using three or four pieces of evidence,\nrespectively.\n4 Benchmarking RAG system using\nMultiHop-RAG\nMultiHop-RAG can be used as a benchmark for var-\nious RAG-related tasks. Broadly speaking, RAG-", "metadata": {"title": "In Defense of RAG in the Era of Long-Context Language Models", "pdf_url": "http://arxiv.org/pdf/2409.01666v1"}}, "4676f727-48b3-49d5-95ac-c0c809502a9d": {"text": "Num. of Evidence Needed Count Percentage\n0 (Null Query) 301 11.78%\n2 1078 42.18%\n3 779 30.48%\n4 398 15.56%\nTotal 2,556 100.00 %\nTable 4: The distribution of the number of evidence\nrequired to answer multi-hop queries in MultiHop-RAG.\nrelated tasks can be categorized as retrieval-related\ntasks and generation-related tasks. A retrieval-\nrelated task focuses on retrieving relevant text from\nthe knowledge base, while a generation-related task\nfocuses on generating high-quality responses given\nthe retrieved text. In this section, we showcase two\nuse cases for each task where MultiHop-RAG can\nbe employed.\n4.1 Retrieval-related Task\nAn important design choice in an RAG system is\nthe selection of the embedding model. An embed-\nding model converts data into numerical vectors\nand subsequently stores these vectors in embedding\ndatabases. In this experiment, we evaluate differ-\nent embedding models by examining their retrieval\nquality.\nExperiment Setup:We implement an RAG sys-\ntem using the LlamaIndex framework (Liu, 2022).\nWe partition the documents in the MultiHop-RAG\nknowledge base into chunks, each consisting of 256\ntokens. We then convert the chunks using an em-\nbedding model and save the embeddings into a vec-\ntor database. Similarly, in the retrieval step, we con-\nvert a query using the same embedding model and\nretrieve the top-K most relevant chunks that have\nthe highest cosine similarity with the query embed-\nding. In this experiment, we test a variety set of em-\nbedding models, including the ada-embeddings by\nOpenAI (text-embedding-ada-002, text-search-ada-\nquery-001), voyage-02 3, llm-embedder (Zhang\net al., 2023), bge-large-en-v1.5 (Xiao et al., 2023),\njina-embeddings-v2-base-en (G\u00fcnther et al., 2023),\ne5-base-v2 (Wang et al., 2022), and instructor-large\n(Su et al., 2023). NULL queries are excluded in\nthis experiment because there is no matching evi-\ndence to the query. Additionally, we also include\na Reranker module to examine the retrieval perfor-\nmance, using bge-reranker-large (Xiao et al., 2023).\nAfter retrieving 20 related chunks using the em-\n3https://www.voyageai.com/\nbedding model, we further select the top-K chunks\nusing the Reranker.\nExperiment Result:Table 5 shows the retrieval\nresult of using different embedding models. It\nshows that there is still a significant gap in retriev-\ning relevant evidence for the multi-hop queries.\nWhile Rerank can effectively improve retrieval rel-\nevance, the highest Hits@10 is only 0.7467 when\nthe Reranker technique is used. Moreover, the drop", "metadata": {"title": "In Defense of RAG in the Era of Long-Context Language Models", "pdf_url": "http://arxiv.org/pdf/2409.01666v1"}}, "f22d23a1-083f-4812-b634-1b6f1a0c6d9d": {"text": "ing relevant evidence for the multi-hop queries.\nWhile Rerank can effectively improve retrieval rel-\nevance, the highest Hits@10 is only 0.7467 when\nthe Reranker technique is used. Moreover, the drop\nin the highest Hits@4 to 0.6625 is worrisome. In\npractical RAG systems, the underlying LLM of-\nten has a context window limit. As a result, the\nnumber of retrieved chunks is usually restricted to\na small number. The low values of the retrieval\nmetrics highlight the challenges in retrieving rele-\nvant pieces of evidence for multi-hop queries when\nusing direct similarity matching between the multi-\nhop query and text chunks.\n4.2 Generation-related Task\nThe underlying LLMs play a crucial role in gen-\nerating responses in an RAG system. In this ex-\nperiment, we evaluate the quality of generated re-\nsponses under two different settings. In the first\nsetting, we employ the best-performing retrieval\nmodel, namely voyage-02 with bge-reranker-large,\nas indicated in Table 5, to retrieve the top-K texts\nand then feed them into the LLM. In the second\nsetting, we use the ground-truth evidence associ-\nated with each query as the retrieved text for the\nLLM. This setting represents a ceiling performance\nfor testing the LLM\u2019s response capabilities, as it\nutilizes the actual evidences.\nExperiment Setup: In the first experiment, we\nretrieve top-6 chunks so that the total length of the\nretrieved text does not exceed 2,048. All queries\nin MultiHop-RAG are tested in the experiment.\nIn the second experiment, since the null queries\ndo not have associated evidence, we exclude this\ntype of query in the experiment. For the LLMs\nused in the experiment, we consider state-of-the-\nart commercial models, including GPT-4 (OpenAI,\n2023), GPT-3.5, Claude-2 (Anthropic, 2023), and\nGoogle-PaLM (Google, 2023). We obtain answers\nusing the provided API of the respective models.\nWe also assess some open-source models, includ-\ning Mixtral-8x7b-instruct (Jiang et al., 2024) and\nLlama-2-70b-chat-hf (Touvron et al., 2023).\nExperiment Results:Table 6 shows the response\naccuracy of different LLMs. First, we can see\nthat the response accuracy rate using the retrieved", "metadata": {"title": "In Defense of RAG in the Era of Long-Context Language Models", "pdf_url": "http://arxiv.org/pdf/2409.01666v1"}}, "dbf627bd-e916-445e-8257-d2d494af4d66": {"text": "Embedding Without Reranker With bge-reranker-large\nMRR@10 MAP@10 Hits@10 Hits@4 MRR@10 MAP@10 Hits@10 Hits@4\ntext-embedding-ada-002 0.4203 0.3431 0.6381 0.504 0.5477 0.4625 0.7059 0.6169\ntext-search-ada-query-001 0.4203 0.3431 0.6399 0.5031 0.5483 0.4625 0.7064 0.6174\nllm-embedder 0.2558 0.1725 0.4499 0.3189 0.425 0.3059 0.5478 0.4756\nbge-large-en-v1.5 0.4298 0.3423 0.6718 0.5221 0.563 0.4759 0.7183 0.6364\njina-embeddings-v2-base-en 0.0621 0.031 0.1479 0.0802 0.1412 0.0772 0.1909 0.1639\nintfloat/e5-base-v2 0.1843 0.1161 0.3556 0.2334 0.3237 0.2165 0.4176 0.3716\nvoyage-02 0.3934 0.3143 0.6506 0.4619 0.586 0.4795 0.7467 0.6625\nhkunlp/instructor-large 0.3458 0.265 0.5717 0.4229 0.5115 0.4118 0.659 0.5775\nTable 5: Retrieval performance of different embedding models.\nModels Accuracy\nRetrieved Chunk Ground-truth Chunk\nGPT-4 0.56 0.89\nChatGPT 0.44 0.57\nLlama-2-70b-chat-hf 0.28 0.32\nMixtral-8x7B-Instruct 0.32 0.36\nClaude-2.1 0.52 0.56\nGoogle-PaLM 0.47 0.74\nTable 6: Generation accuracy of LLMs.\nchunks is not satisfactory, with the state-of-the-\nart GPT-4 model achieving only 0.56 accuracy.\nThis is expected, because the retrieval component\nfalls short in retrieving relevant evidences from the\nknowledge base. Second, even when we provide\nthe LLM with the ground-truth evidences, we can\nsee that the response accuracy is far from being per-\nfect. Open source LLM such as Llama02-70B and\nMixtral-8x7B only achieve an accuracy of 0.32 and\n0.36 respectively. GPT-4 achieves strong reason-\ning capability with an accuracy of 0.89, followed\nby the second-based LLM Google-PaLM with an\naccuracy of 0.74.\nFigure 3 shows the detailed results of different\nquery types for GPT-4 and Mixtral-8x7B-instruct.\nBoth models show relatively high robustness on\nnull queries, meaning they are generally good at\ndetermining when a query cannot be answered\nbased on the retrieved text. This is encouraging be-\ncause one benefit of RAG is to mitigating the LLM\nhallucination issue by augmenting LLM with re-\ntrieval knowledge. However, Mixtral-8x7B model\nperforms significantly worse than the GPT-4 in\ncomparison and temporal queries. Upon reviewing\nthe incorrect responses, we find that Mixtral-8x7B\nfails to accurately handle logical negation, leading\nto misinterpretation of statements and thus a low\nperformance in the comparison queries. In addi-\ntion, Mixtral-8x7B often fails to correctly identify\nFigure 3: Generation accuracy for different query types.\nthe chronological order of events, which is crucial\nfor answering temporal queries where timing is a", "metadata": {"title": "In Defense of RAG in the Era of Long-Context Language Models", "pdf_url": "http://arxiv.org/pdf/2409.01666v1"}}, "9c523e3b-051c-4193-8f59-da13ee9729c0": {"text": "tion, Mixtral-8x7B often fails to correctly identify\nFigure 3: Generation accuracy for different query types.\nthe chronological order of events, which is crucial\nfor answering temporal queries where timing is a\nkey factor. Taken together, this experiment demon-\nstrates that there is still room for improvement in\nthe reasoning capabilities of LLMs, particularly\nthose that are open-source, for multi-hop queries.\n4.3 Other Use Cases\nBeyond embedding models and LLM generation,\nthere are other areas worth exploring. For exam-\nple, query decomposition is a widely utilized tech-\nnique in RAG frameworks, such as LLamaIndex.\nThis process involves breaking down the query\ninto smaller segments; it targets a single document\nfor retrieval and integrates the information subse-\nquently, thereby potentially enhancing retrieval ac-\ncuracy. Another advanced and promising approach\ninvolves building LLM-based agents that can au-\ntomatically plan and execute multi-hop queries,\nsuch as AutoGPT (Gravitas, 2023). Another area\nof interest is the hybrid retrieval approach, which\ncombines keyword and embedding matching tech-", "metadata": {"title": "In Defense of RAG in the Era of Long-Context Language Models", "pdf_url": "http://arxiv.org/pdf/2409.01666v1"}}, "abe9a26e-f501-4454-a53f-fa6eb9bb7faf": {"text": "niques. We believe that there are many potential\nareas for enhancing RAG\u2019s performance on multi-\nhop queries, and the curated dataset MultiHop-\nRAG can be a valuable resource to the community.\n5 Related Work\nRAG Evaluation:As RAG systems gain increas-\ning popularity, a variety of RAG benchmarking\ndatasets and evaluation tools have been developed.\nFor instance, RGB (Chen et al., 2023) and RE-\nCALL (Liu et al., 2023) evaluate the performance\nof LLMs in generating responses for RAG systems\nunder conditions involving noisy, integrative, and\ncounterfactual queries. However, both datasets pri-\nmarily focus on evaluating the generation aspect\nof RAG systems without specifically addressing\ntheir retrieval accuracy. In addition, recent ad-\nvancements have been made in automated RAG\nevaluation tools, such as ARES (Saad-Falcon et al.,\n2023) and RAGAS (Es et al., 2023). These tools\nutilize LLMs to automatically assess the quality of\nRAG generation, yet they do not introduce bench-\nmarking datasets. Our work introduces one of the\nfirst RAG benchmarking datasets, consisting of a\nknowledge base, a large collection of multi-hop\nqueries, their ground-truth answers, and the associ-\nated supporting evidence, thereby complementing\nexisting RAG evaluations.\nRetrieval datasets: Apart from the context of\nRAG, several benchmarking datasets exist for in-\nformation retrieval evaluation. The FEVER (Fact\nExtraction and VERification) dataset, for instance,\ncontains claims classified as Supported, Refuted,\nor NotEnoughInfo by the given Wikipedia article\n(Thorne et al., 2018). Similarly, the SciFact dataset\ncomprises scientific claims paired with evidence-\ncontaining abstracts (Wadden et al., 2020). How-\never, the claims in both datasets are single-hop\nstatements, and the supporting evidence is from one\nsingle article, in contrast to the multi-hop queries\ndiscussed in this paper. Another dataset, HoVer,\ninvolves claims that require extracting and reason-\ning from multiple Wikipedia articles (Jiang et al.,\n2020). However, unlike our dataset, HoVer focuses\nsolely on classifying claims as either supported or\nnot supported by the articles without evaluating\nan LLM generation step. Moreover, in HoVer, the\nWikipedia articles from which evidence is drawn\nare given for claim verification, which is signifi-\ncantly different from our setting, where relevant\npieces of evidence need to be extracted from a\nlarge knowledge base. Separately, (Kamalloo et al.,\n2023) evaluates a range of commercial embedding\nAPIs for information retrieval, but this evaluation", "metadata": {"title": "In Defense of RAG in the Era of Long-Context Language Models", "pdf_url": "http://arxiv.org/pdf/2409.01666v1"}}, "b77ef1e6-5f84-498b-92c3-48288980ef8f": {"text": "cantly different from our setting, where relevant\npieces of evidence need to be extracted from a\nlarge knowledge base. Separately, (Kamalloo et al.,\n2023) evaluates a range of commercial embedding\nAPIs for information retrieval, but this evaluation\nis not contextualized within the framework of RAG\nsystems either.\nMulti-document QA datasets: Question-\nanswering (QA) is a fundamental task in NLP, and\nseveral popular benchmarks, such as HotpotQA\n(Yang et al., 2018), MultiRC (Khashabi et al.,\n2018), and 2WikiMultiHopQA (Ho et al., 2020),\naim to achieve QA from multiple sources of\ndocuments. This task is similar to our multi-hop\nquery RAG task, as both involve reasoning from\nmultiple sources of information. However, these\ndatasets primarily focus on assessing a model\u2019s\nreasoning skills, and they do not emphasize the\nretrieval of evidence from a knowledge base.\nAdditionally, their primary data sources Wikipedia,\nsignificantly overlap with the training data of\nmost existing LLMs. If we use these sources for\nbenchmarking RAG systems, there is a potential\nconcern that LLM responses might rely on training\nknowledge rather than reasoning from the retrieved\nknowledge base.\n6 Conclusion\nIn this work, we introduce MultiHop-RAG, a novel\nand unique dataset designed for queries that re-\nquire retrieval and reasoning from multiple pieces\nof supporting evidence. These types of multi-hop\nqueries represent user queries commonly encoun-\ntered in real-world scenarios. MultiHop-RAG con-\nsists of a knowledge base, a large collection of\nmulti-hop queries, their ground-truth answers, and\nthe associated supporting evidence. This paper\ndetails the creation process of MultiHop-RAG, em-\nploying a hybrid approach that integrates human\neffort with GPT-4. Additionally, we explore two\nuse cases of MultiHop-RAG in the benchmarking\nof RAG systems, thereby highlighting the potential\napplications of this dataset. By publicly releas-\ning MultiHop-RAG, we aim to provide a valuable\nresource to the community, contributing to the ad-\nvancement and benchmarking of RAG systems.\nLimitations\nThis work has several limitations that can be im-\nproved in future research. First, our ground truth\nanswers are restricted to simple responses such as\n\u201cyes\", \u201cno\", entity names, or temporal indicators\nlike \u201cbefore\" or \u201cafter\" to facilitate the use of a", "metadata": {"title": "In Defense of RAG in the Era of Long-Context Language Models", "pdf_url": "http://arxiv.org/pdf/2409.01666v1"}}, "a7624dc8-ece8-4fec-9254-8233e816c0c3": {"text": "straightforward accuracy metric for evaluating gen-\neration performance. Future work could consider\nallowing free text as answers and employing more\nsophisticated metrics to assess generation quality.\nSecond, the current dataset limits supporting ev-\nidence for a query to a maximum of four pieces.\nFuture work can extend the dataset by including\nqueries that require retrieving and reasoning from\neven more evidence. Lastly, while our experiments\nutilize a basic RAG framework using LlamaIndex,\nfuture work could involve evaluating the answering\nof multi-hop queries using more advanced RAG\nframeworks or LLM-agent frameworks.\nReferences\nAnthropic. 2023. Claude 2.1 (May version). https:\n//api.anthropic.com/v1/messages. Claude 2.1.\nAkari Asai, Sewon Min, Zexuan Zhong, and Danqi\nChen. 2023. Retrieval-based language models and\napplications. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 6: Tutorial Abstracts), pages 41\u201346.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, Diego\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman\nRing, Tom Hennigan, Saffron Huang, Loren Mag-\ngiore, Chris Jones, Albin Cassirer, Andy Brock,\nMichela Paganini, Geoffrey Irving, Oriol Vinyals,\nSimon Osindero, Karen Simonyan, Jack Rae, Erich\nElsen, and Laurent Sifre. 2022. Improving language\nmodels by retrieving from trillions of tokens. In\nProceedings of the 39th International Conference\non Machine Learning, volume 162 of Proceedings\nof Machine Learning Research , pages 2206\u20132240.\nPMLR.\nHarrison Chase. 2022. LangChain.\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\n2023. Benchmarking large language models in\nretrieval-augmented generation.\nShahul Es, Jithin James, Luis Espinosa-Anke, and\nSteven Schockaert. 2023. Ragas: Automated evalua-\ntion of retrieval augmented generation.\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\n2023. Enabling large language models to generate\ntext with citations.\nGoogle. 2023. PaLM 2 (May version).\nhttps://generativelanguage.googleapis.\ncom/v1beta2/models/. Chat-bison-002.\nSignificant Gravitas. 2023. Autogpt. https://github.\ncom/Significant-Gravitas/AutoGPT.\nMichael G\u00fcnther, Jackmin Ong, Isabelle Mohr, Alaed-\ndine Abdessalem, Tanguy Abel, Mohammad Kalim\nAkram, Susana Guzman, Georgios Mastrapas, Saba\nSturua, Bo Wang, Maximilian Werk, Nan Wang,\nand Han Xiao. 2023. Jina embeddings 2: 8192-", "metadata": {"title": "In Defense of RAG in the Era of Long-Context Language Models", "pdf_url": "http://arxiv.org/pdf/2409.01666v1"}}, "7805b16e-498c-4a3b-b0cf-f9a42cbf40bd": {"text": "Michael G\u00fcnther, Jackmin Ong, Isabelle Mohr, Alaed-\ndine Abdessalem, Tanguy Abel, Mohammad Kalim\nAkram, Susana Guzman, Georgios Mastrapas, Saba\nSturua, Bo Wang, Maximilian Werk, Nan Wang,\nand Han Xiao. 2023. Jina embeddings 2: 8192-\ntoken general-purpose text embeddings for long doc-\numents.\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\nand Akiko Aizawa. 2020. Constructing a multi-\nhop QA dataset for comprehensive evaluation of\nreasoning steps. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 6609\u20136625, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine\nRoux, Arthur Mensch, Blanche Savary, Chris\nBamford, Devendra Singh Chaplot, Diego de las\nCasas, Emma Bou Hanna, Florian Bressand, Gi-\nanna Lengyel, Guillaume Bour, Guillaume Lam-\nple, L\u00e9lio Renard Lavaud, Lucile Saulnier, Marie-\nAnne Lachaux, Pierre Stock, Sandeep Subramanian,\nSophia Yang, Szymon Antoniak, Teven Le Scao,\nTh\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang,\nTimoth\u00e9e Lacroix, and William El Sayed. 2024. Mix-\ntral of experts.\nYichen Jiang, Shikha Bordia, Zheng Zhong, Charles\nDognin, Maneesh Singh, and Mohit Bansal. 2020.\nHoVer: A dataset for many-hop fact extraction and\nclaim verification. In Findings of the Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nEhsan Kamalloo, Xinyu Zhang, Odunayo Ogundepo,\nNandan Thakur, David Alfonso-Hermelo, Mehdi\nRezagholizadeh, and Jimmy Lin. 2023. Evaluat-\ning embedding apis for information retrieval. arXiv\npreprint arXiv:2305.06300.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nBeyond the Surface: A Challenge Set for Reading\nComprehension over Multiple Sentences. In Proc. of\nthe Annual Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(NAACL).\nJerry Liu. 2022. LlamaIndex.\nYi Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao\nZhou, Fandong Meng, Jie Zhou, and Xu Sun. 2023.\nRecall: A benchmark for llms robustness against\nexternal counterfactual knowledge.\nOpenAI. 2023. GPT4 (Nov 7 version). https://chat.\nopenai.com/chat. gpt-4-1106-preview.\nJon Saad-Falcon, Omar Khattab, Christopher Potts, and\nMatei Zaharia. 2023. Ares: An automated evalua-\ntion framework for retrieval-augmented generation\nsystems.", "metadata": {"title": "In Defense of RAG in the Era of Long-Context Language Models", "pdf_url": "http://arxiv.org/pdf/2409.01666v1"}}, "7332e89c-e5cb-4c65-b5e1-1432f0567d08": {"text": "Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang,\nYushi Hu, Mari Ostendorf, Wen tau Yih, Noah A.\nSmith, Luke Zettlemoyer, and Tao Yu. 2023. One\nembedder, any task: Instruction-finetuned text em-\nbeddings.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFever: a large-scale dataset for fact extraction and\nverification.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-\ntuned chat models.\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu\nWang, Madeleine van Zuylen, Arman Cohan, and\nHannaneh Hajishirzi. 2020. Fact or fiction: Verifying\nscientific claims. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 7534\u20137550, Online. As-\nsociation for Computational Linguistics.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing\nJiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. 2022. Text embeddings by weakly-\nsupervised contrastive pre-training. arXiv preprint\narXiv:2212.03533.\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\nMuennighoff. 2023. C-pack: Packaged resources\nto advance general chinese embedding.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018. HotpotQA: A dataset\nfor diverse, explainable multi-hop question answer-\ning. In Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nPeitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou,\nand Jian-Yun Nie. 2023. Retrieve anything to aug-\nment large language models.", "metadata": {"title": "In Defense of RAG in the Era of Long-Context Language Models", "pdf_url": "http://arxiv.org/pdf/2409.01666v1"}}, "8cb69154-4b36-4839-af61-f78da3642a40": {"text": "ing. In Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nPeitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou,\nand Jian-Yun Nie. 2023. Retrieve anything to aug-\nment large language models.\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu\nJiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and\nJiawei Han. 2022. Towards a unified multi-\ndimensional evaluator for text generation.\nA Appendix A: GPT-4 Prompts Used for\nData Generation\nWe present the prompts used for guiding GPT-4 for\ndata generation. Table 7 shows the prompt used for\nclaim generation, along with the corresponding top-\nics and entities within these claims. Table 8, Table\n9, and Table 10 respectively show the prompts used\nfor generating multi-hop queries of the inference,\ncomparison, and temporal types.\nB Appendix B: Dataset Examples\nIn this appendix, we present an example of each\ntype of multi-hop query included in the MultiHop-\nRAG dataset. These examples are illustrated in the\nrespective tables: Table 12 for Inference Queries,\nTable 13 for Comparison Queries, Table 14 for\nTemporal Queries, and Table 15 for Null Queries.\nEach query is paired with a ground-truth answer\nfor the evaluation of generation accuracy, while\nmultiple pieces of supporting evidence are included\nfor assessing retrieval performance. Additionally,\nmetadata such as the title, source, and publication\ntime of the news articles are provided as references.", "metadata": {"title": "In Defense of RAG in the Era of Long-Context Language Models", "pdf_url": "http://arxiv.org/pdf/2409.01666v1"}}, "8ccc4b4d-96cc-45d3-b105-22e5413b2eaf": {"text": "A \"claim\" is a statement or assertion made within a text expressing a belief, opinion, or fact. Given\nevidence from the original context, please extract one claim and its associated topics.\nNote: The claim should not contain ambiguous references, such as \u2019he\u2019,\u2019 she,\u2019 and\u2019 it\u2019, and should use\ncomplete names. If there are multiple topics, give the most dominant one. The target of the claim (one\nentity)is the specific individual, group, or organization that the statement or assertion within a text is\ndirected towards or about which it is making a case. The topic of the claim should be a simple phrase\nrepresenting the claim\u2019s central argument concept. If there is no claim, please leave it blank. Please\ngenerate a claim based on the given evidence. Don\u2019t generate the evidence yourself.\nPlease give the response following this format:\nEvidence: [original context]\nClaims: [extract claim]\nClaim Target: [target]\nClaim Topic: [topic]\nHere are examples:\n<examples>\nNow, it\u2019s your turn.\n<News>\n<evidence>\nTable 7: Claim Generation Prompting\nA multi-hop question is a query requiring multiple inferential leaps or accessing several pieces of\ninformation from different locations or sources to arrive at an answer. The following are news articles\u2019\nmetadata and claims come from the articles. All the claims from the article are related to a similar\ntarget. Your task is to generate one multi-hop inference question based on the claims. Here are some\ninstructions:\n1. Find the Connection: The connection between claims is <target>, which is how these key pieces of\ninformation are related or how they can be combined to form a more complex idea.\n2. Formulate the Question: Create a question that cannot be answered by relying on just one of the\nsentences but instead requires understanding and linking the information from all of the sources. The\nanswer is <target>.\n3. Ensure Coherence: Make sure the question flows logically from the combined information and is\nclear and unambiguous.\n4. Use the keywords: <key set>\n<examples>\nContext:\n<Context>\nTable 8: Inference Query Generation Prompting", "metadata": {"title": "In Defense of RAG in the Era of Long-Context Language Models", "pdf_url": "http://arxiv.org/pdf/2409.01666v1"}}, "8f805959-0b18-4313-830c-3f043afb4fe9": {"text": "<Context>\nThe above are news articles\u2019 metadata and claims come from the articles. All the claims from the\narticles are related to a similar target. Your task is to generate one comparison question based on all the\nclaims from different sources. This question needs to compare some factual elements of the claims that\nare explicitly stated to find where they agree or differ. The correct answer to this question is expressed\nas a comparative adjective, a statement of alignment, a simple yes or no. To generate a comparative\nquestion from claims, you need to use the following keywords: <key set>\nThe Good Comparison Questions:\n<examples>\nYour Comparison Question:\nTable 9: Comparison Query Generation Prompting\n<Context>\nPlease create a time-sensitive comparison question using metadata and excerpts from multiple news\narticles. That is to compare the consistency or sequence of reports on similar topics at multiple different\ntime points. If it is to compare the consistency, please clearly mention the news source and time in the\nquestion using <time frame>. If it is to compare sequences of reports, just clearly mention the news\nsource and do not mention the timeline. Utilize the following keywords provided in the <key set> to\nconstruct the question. The correct answer should based on the factual excerpts and is only one word.\n<examples>\nYour time-sensitive comparison question:\nTable 10: Temporal Query Generation Prompting\nA multi-hop question is a query requiring multiple inferential leaps or accessing several pieces of\ninformation from different locations or sources to arrive at an answer. Considering you have read\nat least two news articles on <entity>, construct a multi-hop question that incorporates all the news\nsources. The source of the news should be stated in the question. Also, ensure that the answer to the\nquestion is a single word/entity. Do not answer this question directly. Just give me the question:\nTable 11: Null Query Generation Prompting", "metadata": {"title": "In Defense of RAG in the Era of Long-Context Language Models", "pdf_url": "http://arxiv.org/pdf/2409.01666v1"}}, "29cf7460-e590-4422-a3ae-c70df52134b7": {"text": "Query: Which platform is at the center of discussions in articles from Music Business Worldwide,\nPolygon, and FOX News - Health, concerning the policing of AI-driven voice replication, the debate\nover \"reaction\" content, and being the most used app overnight by young people?\nAnswer: YouTube\nEvidence List:\nTitle: Sony Music\u2019s artists aren\u2019t involved in YouTube\u2019s new voice-cloning AI experiment.\nSource: Music Business Worldwide\nPublished Time: 2023-11-23T18:48:48+00:00\nFact: During this period of discussion, YouTube has made a number of positive announcements\nregarding the biggest issue for any rightsholder regarding AI-driven voice replication of artists: their\nability to police it.\nTitle: YouTube demonetizes popular content creator SSSniperwolf after doxxing accusations\nSource: Polygon\nPublished Time: 2023-10-25T18:18:06+00:00\nFact: The debate over \"reaction\" content on YouTube has been brewing for years, but a recent incident\nbetween two creators has refueled the urgency of the conversation.\nTitle: Cell phone shocker as 97% of kids use their device during school hours and beyond, says study\nSource: FOX News - Health\nPublished Time: 2023-10-01T09:05:26+00:00\nFact: Overnight phone use was primarily spent engaging with the same media, although YouTube\nappeared to be the longest-running app because videos were often left playing during the night.\nTable 12: The example of inference questions\nQuery: Did the Cnbc | World Business News Leader report on Nike\u2019s net income and the article from\nThe Age on the 10-year Treasury yield both report a decrease in their respective financial metrics?\nAnswer: Yes\nEvidence List:\nTitle: Nike misses revenue expectations for the first time in two years, beats on earnings and gross\nmargin\nSource: Cnbc | World Business News Leader\nPublished Time: 2023-09-28T20:31:00+00:00\nFact: The company\u2019s reported net income for the three-month period that ended August 31 was $1.45\nbillion, or 94 cents per share, compared with $1.47 billion, or 93 cents per share, a year earlier.\nTitle: ASX set to open higher as Wall Street rebounds; $A rises\nSource: The Age\nPublished Time: 2023-10-04T21:01:01+00:00\nFact: The yield on the 10-year Treasury, which is the centrepiece of the bond market, pulled back from\nits highest level since 2007, down to 4.73 per cent from 4.80 per cent late on Tuesday.\nTable 13: The example of comparison questions", "metadata": {"title": "In Defense of RAG in the Era of Long-Context Language Models", "pdf_url": "http://arxiv.org/pdf/2409.01666v1"}}, "be18bd49-9777-4f22-9f14-85979824489b": {"text": "Query: Was the performance of the Chicago Bears\u2019 defense reported as improved by Yardbarker after\nSporting News highlighted a sack by the Bears\u2019 defense on Joshua Dobbs during the NFL \u2019Monday\nNight Football\u2019 game?\nAnswer: Yes\nEvidence List:\nTitle: Bears vs. Vikings live score, updates, highlights from NFL \u2019Monday Night Football\u2019 game\nSource: Sporting News\nPublished Time: 2023-11-27T23:32:04+00:00\nFact: The Bears answer right back and sack Dobbs, with Sweat and Brisker in there to take him down.\nTitle: Hottest seat on each NFC team: Buns burning for these four head coaches\nSource: Yardbarker\nPublished Time: 2023-11-30T22:29:33+00:00\nFact: In his second season as HC, the defense has improved, but positive results are hard to come by\nbehind a lackluster offense ranked 19th in yards (323.2) and 21st in points per game (20.2).\nTable 14: The example of time-sensitive questions\nQuery: What is the first letter of the CEO\u2019s last name in the news article from Bloomberg on TomTom,\nand what is the first letter of the city where the company\u2019s headquarters is located in the news article\nfrom Reuters?\nAnswer: Insufficient information.\nTable 15: The example of negative rejection questions", "metadata": {"title": "In Defense of RAG in the Era of Long-Context Language Models", "pdf_url": "http://arxiv.org/pdf/2409.01666v1"}}, "447f2df0-6fc5-4c6b-8cb3-32b18b2d7fab": {"text": "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented\nInstructions\nWanlong Liu2\u2020, Junying Chen1\u2020, Ke Ji1, Li Zhou1, Wenyu Chen2, Benyou Wang1*\n1 The Chinese University of Hong Kong, Shenzhen,\n2 University of Electronic Science and Technology of China\nwangbenyou@cuhk.edu.cn\nAbstract\nRetrieval-Augmented Generation (RAG) has\nemerged as a key paradigm for enhancing large\nlanguage models (LLMs) by incorporating\nexternal knowledge. However, current RAG\nmethods face two limitations: (1) they only\ncover limited RAG scenarios. (2) They suffer\nfrom limited task diversity due to the lack\nof a general RAG dataset. To address these\nlimitations, we propose RAG-Instruct, a\ngeneral method for synthesizing diverse and\nhigh-quality RAG instruction data based on\nany source corpus. Our approach leverages\n(1) five RAG paradigms , which encompass\ndiverse query-document relationships, and\n(2) instruction simulation , which enhances\ninstruction diversity and quality by utilizing\nthe strengths of existing instruction datasets.\nUsing this method, we construct a 40K\ninstruction dataset from Wikipedia, compre-\nhensively covering diverse RAG scenarios\nand tasks. Experiments demonstrate that\nRAG-Instruct effectively enhances LLMs\u2019\nRAG capabilities, achieving strong zero-shot\nperformance and significantly outperforming\nvarious RAG baselines across a diverse set of\ntasks. RAG-Instruct is publicly available at\nhttps://github.com/FreedomIntelligence/RAG-\nInstruct.\n1 Introduction\nRetrieval-Augmented Generation (RAG) (Guu\net al., 2020; Asai et al., 2024b) enhances large\nlanguage models (LLMs) by integrating exter-\nnal knowledge through document retrieval, effec-\ntively reducing hallucinations and improving per-\nformance across diverse tasks (Asai et al., 2023;\nJin et al., 2024; Lu et al., 2022; Liu et al., 2024a).\nSince retrievers are not perfect, and consider-\nable research has shown that noisy retrieval can\nadversely impact LLM performance (Petroni et al.,\n*Corresponding author. \u2020Equal Contribution.\n2020; Shi et al., 2023; Maekawa et al., 2024), nu-\nmerous studies have focused on enhancing the ro-\nbustness of RAG in handling noisy retrieval con-\ntexts (Wei et al., 2024; Chan et al., 2024). On the\none hand, some studies involve adaptive retrieval\nbased on query analysis (Asai et al., 2024a; Jeong\net al., 2024), or query reformulation (Chan et al.,\n2024; Ma et al., 2023) to enhance the robustness\nof LLM-based RAG systems. On the other hand,\n(Zhang et al., 2024; Liu et al., 2024b; Yoran et al.,\n2024) enhance the robustness of models\u2019 naive", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "8021df6c-fd89-4e98-9f54-b2cdaef963c9": {"text": "et al., 2024), or query reformulation (Chan et al.,\n2024; Ma et al., 2023) to enhance the robustness\nof LLM-based RAG systems. On the other hand,\n(Zhang et al., 2024; Liu et al., 2024b; Yoran et al.,\n2024) enhance the robustness of models\u2019 naive\nRAG capabilities by training them to adapt to irrel-\nevant and noisy documents.\nHowever, existing RAG methods have two limi-\ntations: (1) Limited RAG scenarios. Real-world\nRAG scenarios are complex: Given the query, the\nretrieved information may directly contain the an-\nswer, offer partial help, or be helpless. Some an-\nswers can be obtained from a single document,\nwhile others require multi-hop reasoning across\nmultiple documents. Our preliminary study demon-\nstrates existing RAG methods cannot adequately\nhandle all such scenarios (Chan et al., 2024; Asai\net al., 2024a; Liu et al., 2024b). (2) Limited\ntask diversity. Due to the lack of a general RAG\ndataset, most current RAG methods (Wei et al.,\n2024; Zhang et al., 2024) are fine-tuned on task-\nspecific datasets (e.g., NQ (Kwiatkowski et al.,\n2019), TrivialQA (Joshi et al., 2017)), which suffer\nfrom limited question diversity and data volume.\nTo address these limitations, we propose RAG-\nInstruct, a general method for synthesizing diverse\nand high-quality RAG instruction data based on any\nsource corpus. Using this method, we construct a\n40K synthetic instruction dataset from Wikipedia\ntailored for RAG. Our method emphasizes the di-\nversity in two aspects:\n1. Defining diverse RAG paradigms: we define\nfive RAG query paradigms that encompass\nvarious query-document relationships to adapt\narXiv:2501.00353v1  [cs.CL]  31 Dec 2024", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "7b56f631-df65-4eae-b9f8-e3aeaf7978d0": {"text": "to different RAG scenarios, considering both\ndocument usefulness and the number of useful\ndocuments. Based on these modes, we prompt\nLLMs to synthesize RAG-specific instructions\nand responses using external documents.\n2. Enhancing task diversity and data quality:\nwe incorporate exemplar data from existing\ninstruction datasets, such as SlimOrca (Mi-\ntra et al., 2023) and Evol Instruct (Xu et al.,\n2023), to guide the generation of RAG in-\nstructions. This approach is inspired by re-\ncent advancements in synthetic instruction\ndatasets which have two key advantages: (1)\nhigh-quality instruction-following responses\ngenerated by proprietary LLMs, and (2) di-\nverse instructions that cover a wide range of\nreal-world tasks. We refer to this approach\nas \u201cInstruction Simulation\u201d, which leverages\nthe strengths of existing instruction datasets\nto improve the diversity and quality of the\nsynthesized data.\nOur contributions are summarized as follows:\n\u2022 We introduce RAG-Instruct, a general\nmethod for synthesizing diverse and high-\nquality RAG instruction data from any given\ncorpus. Using this method, we construct the\nRAG-Instruct dataset (based on Wikipedia),\nthe first dataset to comprehensively cover di-\nverse RAG scenarios and tasks.\n\u2022 We define five RAG paradigms to cover di-\nverse query-document relationships and in-\ntroduce Instruction Simulation, a technique\nthat enhances instruction diversity and quality\nby utilizing the strengths of existing instruc-\ntion datasets. These techniques ensure the\ndiversity of synthesized datasets across RAG\nscenarios and tasks.\n\u2022 Empirical experiments on 11 tasks, includ-\ning knowledge-intensive QA, multi-step rea-\nsoning, and domain-specific benchmarks,\ndemonstrate that RAG-Instruct significantly\nenhances the model\u2019s RAG capabilities. It sig-\nnificantly outperforms previous state-of-the-\nart methods such as Self-RAG (Asai et al.,\n2024a) and RQ-RAG (Chan et al., 2024). Fur-\nthermore, ablation studies demonstrate that\nboth Instruction Simulation and the five RAG\nquery paradigms significantly contribute to\nthe performance of RAG-Instruct.\nMethod TriviaQA (Single-hop) HotpotQA (Multi-hop)\nHelpful Midhelp Helpless Helpful Midhelp\nLlama2-7b 71.0 48.0 17.1 51.2 21.2\nLlama3-8b 76.4 51.0 20.2 61.4 21.4\nSelf-RAG (2-7b) 77.3 42.4 14.7 45.1 16.6\nRQ-RAG (2-7b) 80.9 52.6 18.7 57.9 24.0\nChatQA-1.5 (3-8b) 83.5 54.9 21.4 65.1 23.9\nChatQA-2.0 (3-8b) 82.4 51.5 20.1 61.4 19.9\nRAG-Instruct (3-8b)86.9 72.6 40.5 73.1 42.2\nTable 1: Preliminary study of limited RAG scenarios.", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "6414772e-a05c-41a3-982b-f05dd5f8121a": {"text": "RQ-RAG (2-7b) 80.9 52.6 18.7 57.9 24.0\nChatQA-1.5 (3-8b) 83.5 54.9 21.4 65.1 23.9\nChatQA-2.0 (3-8b) 82.4 51.5 20.1 61.4 19.9\nRAG-Instruct (3-8b)86.9 72.6 40.5 73.1 42.2\nTable 1: Preliminary study of limited RAG scenarios.\nAccuracy (%) is reported. We divided TriviaQA and\nHotPotQA into multiple subsets. More information for\neach subset is shown in Appendix B.1\n.\nDataset Data Size RAG ScenariosTask Diversityr0 r1 r2 r3 r4\nLongAlpaca 12K \u2717 \u2717 \u2717 \u2713 \u2717 \u2713\nSQuAD2.0 130K \u2717 \u2717 \u2717 \u2713 \u2717 \u2717\nNarrativeQA 15K \u2717 \u2717 \u2717 \u2713 \u2717 \u2717\nRAG-12000 12K \u2717 \u2717 \u2717 \u2713 \u2717 \u2717\nSelf-RAG Data 150K\u2713 \u2717 \u2717 \u2713 \u2717 \u2717\nRQ-RAG Data 40K \u2717 \u2717 \u2717 \u2713 \u2713 \u2713\nRAG-Instruct 40K \u2713 \u2713 \u2713 \u2713 \u2713 \u2713\nTable 2: Comparision with existing RAG datasets.r0 to\nr4 represent the five RAG scenario paradigms defined\nin Table 3.\n2 Preliminary Study\nSince retrievers are not perfect, the helpfulness of\nretrieved documents to the query varies in real-\nworld scenarios. This raises the question: Can\nexisting RAG methods handle complex and var-\nious RAG scenarios?\nTo investigate this, we first define five RAG\nscenarios based on query-document relationships,\nwhich we believe cover the majority of RAG use\ncases: Single-Doc Answer (helpful), Single-Doc\nSupport (midhelp), Useless Doc (helpless), Multi-\nDoc Answer (helpful), and Multi-Doc Support\n(midhelp). Detailed definitions for each scenario\nare provided in \u00a7 3.1.\nNext, we evaluate the performance of existing\nRAG methods across these five scenarios. Us-\ning GPT-4o (Achiam et al., 2023), we catego-\nrize questions from two question answering (QA)\ndatasets, Single-hop QA (TriviaQA) and Multi-hop\nQA (HotPotQA (Yang et al., 2018)), into relevant\nsubsets based on the defined RAG scenarios1. De-\ntailed prompts for categorization are provided in\nthe Appendix B.1. Then we choose some robust\nRAG methods, including Self-RAG (Asai et al.,\n2024a), RQ-RAG (Chan et al., 2024), ChatQA-1.5\nand ChatQA-2.0 (Liu et al., 2024b) as baselines\n1We choose these datasets for their large number of ques-\ntions and subsets, which reduces bias.", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "1e12525d-2864-4a74-a7f0-dc146c72aadf": {"text": "Diverse synthetic datasets\nRAG-Instruct\nRAGParadigm\n() Single-documentSupporting Information\nRelationship Between        and    \nSupporting\nExplicit\nUseless\n() Multi-documentSupporting Information() Single-documentExplicit Answer() Multi-documentExplicit Answer\n() Useless Documents\nSourceDocuments\nShareGPTEvol-Instruct\nAlpacaSlimOrca\nInstructionSimulationFilter\nGPT-4o\nKnowledge-intensive Insturctions \nGPT-4o\nRAG  ScenariosDiversity\nInstructionDiversityTarget\nFigure 1: The main process of synthesizing data with RAG-Instruct. RAG-Instruct ensures instruction data diversity\nthrough five RAG paradigms and Instruction Simulation.\nto explore their performance across the five RAG\nscenarios.\nAs shown in Table 1, existing RAG methods im-\nprove primarily in helpful scenarios, while gains\nin mid-helpful and helpless scenarios are minimal,\nwith some, such as Self-RAG, even underperform-\ning the baseline. This indicates that existing RAG\nmethods are still unable to handle complex and di-\nverse RAG scenarios effectively. In comparison,\nour RAG-Instruct method demonstrates significant\nimprovements across all five scenarios, highlight-\ning its effectiveness and adaptability to complex\nand diverse RAG scenarios.\nComparision with existing RAG datasets. We\nreview several representative non-task-specific\nRAG datasets, including long-context instruction\ndatasets such as LongAlpaca (Chen et al., 2023b),\nSQuAD2.0 (Rajpurkar et al., 2018), and Narra-\ntiveQA (Ko\u02c7cisk`y et al., 2018), which have been\nused in ChatQA for RAG training, as well as\ndatasets from classic RAG approaches including\nSelf-RAG data, RQ-RAG data and ChatQA data.\nAs shown in Table 2, existing RAG datasets fail\nto balance both scenario and task diversity. Long-\ncontext instruction datasets are limited to a nar-\nrow range of RAG scenarios and focus primar-\nily on reading comprehension tasks. Addition-\nally, previous state-of-the-art RAG methods such as\nSelf-RAG perform poorly on multi-hop reasoning\nbenchmarks due to their neglect of multi-hop sce-\nnarios. These shortcomings are reflected in Table 1.\nIn contrast, our RAG-Instruct effectively balances\nboth RAG scenario and task diversity, demonstrat-\ning superior generalization and robustness.\n3 Method\nThis section outlines the RAG-Instruct process, fo-\ncusing on constructing diverse and high-quality\nsynthetic RAG datasets. The detailed architecture\nis illustrated in Figure 1.\n3.1 RAG-Instruct\nSynthesizing RAG Instructions. Recent propri-\netary models like GPT-4o (Achiam et al., 2023)", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "0e687dd9-826b-4555-80a5-81ba87caebcb": {"text": "cusing on constructing diverse and high-quality\nsynthetic RAG datasets. The detailed architecture\nis illustrated in Figure 1.\n3.1 RAG-Instruct\nSynthesizing RAG Instructions. Recent propri-\netary models like GPT-4o (Achiam et al., 2023)\nhave demonstrated remarkable capabilities, and\nmany works (Zheng et al., 2023b; Xu et al., 2023;\nChen et al., 2023a) based on synthetic datasets\nhave achieved notable success. Therefore, we use\nGPT-4o to synthesize RAG instructions by lever-\naging source documents D\u22172 to create context-rich\ninstructions. Specifically, GPT-4o synthesizes an\ninstruction q\u2217 based on D\u2217, followed by a response\na\u2217 referencing D\u2217, which can be formalized as:\n(q\u2217, a\u2217) =LLM(D\u2217). (1)\nInspired by work (Zhang et al., 2024), we intro-\nduce documents D\u2212 unrelated to q\u2217, which serve\nas additional noise to enhance the robustness. Then\nour target RAG instruction is as follows.\nD\u2217, D\u2212, q\u2217 \u2192 a\u2217.\nHowever, RAG instructions generated this way\nlack diversity in both RAG scenarios and tasks. To\naddress this, we define five RAG paradigms and\nintroduce Instruction Simulation.\n2We will explain how D\u2217 are obtained in the following\nInstruction Simulation section.", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "f62ce201-16b3-4a8c-8b1c-be3193ddf287": {"text": "D\u2217-q\u2217 Relationship Usefulness\nof D\u2217 |D\u2217| Relationship Description\n(r0)\nUseless Doc Useless 1 D\u2217 offers no help in answering q\u2217, even if related.\n(r1)\nSingle-Doc Support Supporting 1 One doc (|D\u2217| = 1) aids q\u2217, providing supporting info or clues\nwithout explicit answers.\n(r2)\nMulti-Doc Support Supporting \u2265 2\nMultiple documents (|D\u2217| \u22652) support q\u2217 by providing clues or\nsupporting information without explicitly answering it, requiring\nintegration (multi-hop reasoning).\n(r3)\nSingle-Doc Answer Explicit 1 One doc ( |D\u2217| = 1) directly provides the answer a\u2217 to q\u2217.\n(r4)\nMulti-Doc Answer Explicit \u2265 2 Multiple docs (|D\u2217| \u22652) provide a full answer to q\u2217, requiring\nintegration (multi-hop reasoning).\nTable 3: Descriptions of 5 RAG paradigms. See Appendix B.2 for specific prompts.\n<Documents>\n[1] {<document 1>}\n[2] {<document 2>}\n[3] ...\n</Documents>\nYour task is to generate an English question q* and a corresponding response a* based on the provided <Documents>.\nPlease note that the question q* can take various forms, not limited to questions with a question mark, but also including\nstatements, instructions, and other formats. You need to follow the requirements below to generate the q* and a* (RAG\nParadigms):\n1. The answer to q* can be derived from multiple documents within <Documents>, involving multi-hop reasoning or\nthe integration of information from several documents.\n2. a* should leverage the information in <Documents> to provide an accurate answer to q*, ensuring that the response\nis accurate, detailed, and comprehensive.\nAdditionally, to ensure diversity, richness, and high quality in the question q* you generate, we will randomly provide\na question for you to emulate. In other words, while satisfying the requirements above, make q* similar in task\nrequirement and expression to the <Simulated Instruction> below:\n<Simulated Instruction>\n{<Simulated Instruction>}\n</Simulated Instruction>\nPlease directly generate the question-answer pair (q*, a*) following all the rules above in the format of {\"q*\": ..., \"a*\":\n...}. Ensure the quality of the generated (q*, a*).\nFigure 2: The prompt of RAG-Instruct. <document> and <Simulated Instruction> represent input variables for\nthe document and simulated instruction, respectively. (Blue text) indicates RAG Paradigms, illustrating the prompt\nfor r4; other paradigms are shown in Appendix B.2. (Red text) represents Instruction Simulation.\nRAG Paradigms. Real-world RAG scenarios are\ncomplex: Given the q\u2217, D\u2217 may directly contain\nthe answer, offer partial help, or be helpless. Some", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "a71eb1d7-d3aa-4744-8ab2-4a5f7824bf94": {"text": "for r4; other paradigms are shown in Appendix B.2. (Red text) represents Instruction Simulation.\nRAG Paradigms. Real-world RAG scenarios are\ncomplex: Given the q\u2217, D\u2217 may directly contain\nthe answer, offer partial help, or be helpless. Some\nanswers can be obtained from a single document in\nD\u2217, while others require multi-hop reasoning across\nmultiple documents. To address this, we define\nRAG paradigms R, where each r \u2208 R characterizes\nthe relationship between D\u2217 and q\u2217. As in Table\n3, these RAG paradigms consider both document\nutility and the count of useful documents.\nInstruction Simulation. Generating (q\u2217, a\u2217)\nfrom D\u2217 faces the challenge of instruction\nmonotony. Although q\u2217 is related to D\u2217, the task,\nphrasing, and difficulty of the instructions can be-\ncome repetitive with a similar synthesis prompt.\nPrevious datasets address this by broadly collect-\ning instructions (Izacard et al., 2023) or using self-\ninstruct (Wang et al., 2023b). In our approach, we\nleverage diverse, high-quality instructions to diver-\nsify q\u2217, a process we term Instruction Simulation.\nIn this process, we use questions from synthetic\ndatasets including ShareGPT (Wang et al., 2023a),\nAlpaca (hin Cheung and Lam, 2023), WizardLM-\n70K (Xu et al., 2023), Lmsys-chat-1M (Zheng\net al., 2023a), and SlimOrca (Mitra et al., 2023)\nas exemplar data. These datasets cover a wide\nrange of tasks, diverse phrasing styles, and vary-\ning levels of instruction difficulty. Since RAG is", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "a57f6270-6280-4c96-afc8-4d64f2da5019": {"text": "Useless Doc (\ud835\ud835)\nSingle-Doc \n Support (\ud835\ud835)\nMulti-Doc Support (\ud835\ud835)\nSingle-Doc \nAnswer (\ud835\ud835)\nMulti-Doc \nAnswer (\ud835\ud835)\n(a) Distributions of RAG Paradigms\nGPT4-Alpaca\nW izardLM\nLmsys-Chat-1M\nShareGPT_V3\nSlimOcar (b) Distributions of Data Sources\nFigure 3: The distributions of RAG paradigms and simulated instruction sources.\nmost effective in knowledge-intensive task scenar-\nios (Maekawa et al., 2024; Shi et al., 2023), we use\nGPT-4o to filter knowledge-intensive instructions\nfrom these synthetic datasets (details of the prompt\nare provided in Appendix A.1).\nThen for each synthesis, an instruction q\u2032 \u2208 Q\nis randomly sampled for simulation. Given a cor-\npus D containing multiple documents d \u2208 D, the\nsource documents D\u2217 \u2282 D are retrieved based on\nq\u2032. Subsequently, (q\u2217, a\u2217) can be synthesized as\nfollows:\n(q\u2217, a\u2217) =LLM(D\u2217, q\u2032, r), (2)\nwhere r denotes the sampled RAG paradigm, and\nthe synthesis prompt is illustrated in Figure 2. Here,\nD\u2217 controls the topic of q\u2217, while q\u2032 shapes its\nformat and task requirements.\n3.2 Dataset Construction\nWe construct RAG-Instruct using Wikipedia corpus.\nFor each synthesis, we sample an RAG paradigm\nr, a simulated instruction q\u2032, and retrieved source\ndocuments D\u2217 to generate (q\u2217, a\u2217) using GPT-4o.\nTo incorporate unrelated documents D\u2212, we ran-\ndomly sample documents retrieved based onq\u2217 and\nranked beyond the top 200 asD\u2212. Additionally, for\ncases where |D\u2217| \u22652, we ensure that the number\nof source documents is fewer than 5. Subsequently,\nD\u2217, D\u2212, q\u2217 \u2192 a\u2217 is set as the training objective to\nform RAG-Instruct. In total, we build a dataset\nof 40K instructions, with the distributions of RAG\nparadigms and simulated instructions illustrated in\nFigure 3. More dataset construction details are\nshown in Appendix A.1.\n4 Experiments\n4.1 Experimental Settings\nEvaluation Tasks. We conduct evaluations of\nour RAG-Instruct and various baselines across 10\ntasks in four major categories: (1) Open-Ended\nTasks, including WebQA (WQA) (Berant et al.,\n2013), PopQA (PQA) (Mallen et al., 2023), and\nTriviaQA-unfiltered (TQA) (Joshi et al., 2017),\nwhere models answer open-domain factual ques-\ntions with accuracy as the metric. (2) Closed-\nSet Tasks, including OpenbookQA (OBQA) (Mi-\nhaylov et al., 2018), PubHealth (Pub) (Zhang et al.,\n2023) and ARC-Challenge (ARC) (Clark et al.,\n2018), involving multiple-choice QA with Extract\nMatch (EM) as the metric. (3) Multi-Hop Tasks,\nincluding 2WikiMultiHopQA (2WIKI) (Ho et al.,\n2020), HotpotQA (HotQ) (Yang et al., 2018), and\nMusique (MSQ) (Trivedi et al., 2022), requiring", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "af09640a-0a12-4ee6-9bb8-bcc4eb9846a8": {"text": "2018), involving multiple-choice QA with Extract\nMatch (EM) as the metric. (3) Multi-Hop Tasks,\nincluding 2WikiMultiHopQA (2WIKI) (Ho et al.,\n2020), HotpotQA (HotQ) (Yang et al., 2018), and\nMusique (MSQ) (Trivedi et al., 2022), requiring\nmulti-hop reasoning with accuracy as the metric.\n(4) Domain-Specific Tasks, CFQA (Chen et al.,\n2022) in the financial domain and PubMedQA (Jin\net al., 2019) in the medical domain, with EM as the\nmetric. We perform zero-shot evaluations through-\nout these experiments, providing task instructions\nwithout few-shot demonstrations. Reasoning de-\ntails and prompts are provided in Appendix A.2.\nBaselines. We compare our method against a di-\nverse set of baselines, grouped into two main cate-\ngories: (1) Closed-Source LLMs without RAG,\nincluding GPT-4o and GPT-4o-mini. We test them\nusing OpenAI\u2019s official APIs. (2) Open-source\nmodel baselines with RAG , including Llama2-\n7b (Touvron et al., 2023), Llama3-8b (Dubey et al.,\n2024). Additionally, we also compare with compet-\nitive open-source instruction-tuned LMs, such as\nLlama3-8b-Instruct, Llama3-70B-Instruct, Llama2-", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "7f8afd0d-9af4-4a54-9627-8c02f439ae1c": {"text": "Open-ended Closed-set Multi-hop Domain-specific\nWQA PQA TQA OBQA Pub ARC 2WIKI HotP MSQ CFQA PubMed\n(acc) (acc) (acc) (EM) (EM) (EM) (acc) (acc) (acc) (EM) (EM)\nClosed-Source LLMs without RAG\nGPT-4o 68.5 60.3 79.4 88.6 87.7 88.0 88.0 54.6 31.4 63.0 77.0\nGPT-4o-mini 63.5 62.2 77.2 89.6 87.0 84.1 74.4 54.5 30.8 60.7 73.0\nRAG-Specific Models with RAG\nRQ-RAG (Llama2-7B) 56.5 57.1 70.2 80.6 71.8 68.3 53.7 43.1 18.2 21.9 55.6\nSelf-RAG (Llama2-7B) 49.0 55.8 69.3 78.0 72.4 73.1 48.4 35.8 11.5 21.5 49.8\nChatQA-1.5 (Llama3-8B) 53.8 55.4 73.0 70.8 77.0 66.0 63.6 46.2 20.1 56.0 61.7\nChatQA-2.0 (Llama3-8B) 50.5 58.3 72.5 72.6 75.8 65.6 59.0 42.3 16.1 51.8 61.3\nOpen-Source Base Models with RAG\nLlama-2-7B 49.8 51.4 62.6 56.8 36.5 48.0 55.8 38.2 17.8 22.3 58.6\nLlama-3-8B 59.4 57.8 71.9 58.6 50.1 50.5 62.3 42.2 23.9 44.6 62.3\nLlama-3.1-8B 56.7 56.8 71.5 72.4 57.6 61.4 60.7 45.5 23.5 53.1 63.0\nQwen2.5-7B 61.0 58.5 71.7 70.6 56.6 65.2 59.8 46.2 22.2 52.8 67.4\nLlama-3.1-70B 62.4 58.5 76.5 76.6 59.2 66.0 67.9 49.9 26.6 53.8 65.9\nOpen-Source Instruction-Tuned Models with RAG\nLlama-3-8B-Instruct 62.1 62.0 72.4 75.0 58.2 67.4 65.9 45.0 19.1 54.9 72.8\nLlama-3.1-8B-Instruct 61.9 62.8 73.9 77.2 56.8 70.3 66.8 45.5 19.0 53.7 73.6\nQwen2.5-7B-Instruct 64.1 62.0 75.6 74.2 74.2 75.7 66.5 49.5 20.8 58.7 62.6\nLlama-3.1-70B-Instruct 64.9 63.3 75.4 85.0 75.4 84.7 73.5 47.5 26.6 59.1 77.2\nLlama-2-7B +RAG-Instruct 67.2 62.4 77.4 71.4 75.9 74.8 68.1 53.5 21.8 29.7 71.2\nLlama-3-8B +RAG-Instruct 68.6 65.3 79.5 79.6 75.0 78.4 76.0 58.1 32.0 57.4 78.2\nLlama-3.1-8B +RAG-Instruct 69.7 68.4 80.0 82.4 77.2 79.6 76.8 59.6 33.7 57.3 77.0\nQwen2.5-7B +RAG-Instruct 66.1 63.7 78.1 78.4 76.4 78.0 74.8 54.6 27.7 55.0 72.7\nLlama-3.1-70B +RAG-Instruct70.6 69.4 82.2 88.6 78.8 84.2 82.8 63.9 41.2 61.6 78.5\nTable 4: Zero-shot performance of different instruction datasets on RAG Benchmarks. Bold and underline indicate\nthe best and second-best experimental results. The datasets were fine-tuned using identical hyperparameters.\n7b-chat, Llama-3.1-8B-Instruct and Qwen2.5-7B-\nInstruct (Yang et al., 2024) to evaluate the ad-\nvantages of our RAG instruction dataset. For\ninstruction-tuned LMs, we use the official system\nprompts or instruction formats from their train-\ning process when publicly available. (3) RAG-\nspecific baselines, including Self-RAG, RQ-RAG,\nChatQA-1.5, ChatQA-2.0. For these methods, we\nevaluate using publicly released model weights and\nprompts provided by their respective works.\nTraining settings. We train our model using the", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "8639071c-ba92-4a81-ab43-005fa4af3151": {"text": "specific baselines, including Self-RAG, RQ-RAG,\nChatQA-1.5, ChatQA-2.0. For these methods, we\nevaluate using publicly released model weights and\nprompts provided by their respective works.\nTraining settings. We train our model using the\nRAG-Instruct dataset (wikipedia), which features\ndiverse instruction-following input-output pairs.\nDuring the dataset construction, we employ the off-\nthe-shelf Contriever-MS MARCO (Izacard et al.)\nas the retriever. For each data entry, we ensure\nthe use of all source documents D\u2217 and supple-\nment them with enough unrelated documents D\u2212\nto total 10 documents. For training, we use Llama2-\n7B, Llama3-8B, Llama3.1-8B, Llama3.1-70B, and\nQwen2.5-7B as the base models. Additional train-\ning details are provided in Appendix A.1.\nInference settings. We use vLLM (Kwon et al.,\n2023) for memory-efficient inference and adopt a\ngreedy decoding strategy for model generation. For\nevaluation benchmarks, we utilize Wikipedia as the\nretrieval corpus and use the Contriever retriever\nfor document retrieval. More detailed inference\nspecifications can be found in Appendix A.2.\n4.2 RAG Capability Gains\nComparison against closed-source LLMs. As\nshown in Table 4, compared to powerful propri-\netary models like GPT-4o and GPT-4o-mini, our\nRAG-Instruct, trained on base 8B models, matches\nor even outperforms them on several tasks, in-\ncluding open-ended tasks (PQA and TQA), multi-\nhop tasks (HotQA and MSQ), and domain-specific\ntasks (PubMedQA). This demonstrates that our\nRAG-Instruct significantly enhances the model\u2019s\nRAG capabilities.\nComparison against RAG-specific models. As\nshown in Table 4, RAG-specific models such as\nSelf-RAG, and RQ-RAG show significant improve-\nments over the base models on open-ended and\nclosed-set tasks. However, they underperform com-", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "99ea3b12-d6e8-4964-ab45-c03207453c87": {"text": "RAG \nParadigms Source Documents Generated Question \n(w.o. Instruction Simulation) Example Question Gnerated Question\n(w. Instruction Simulation)\n\ud8350\n[1] know and understand the Creed, the Lord's \nPrayer, and the Ten Commandments, and be \nable to answer the other questions in the \nChurch Catechism. ...\nWhat is the significance of \nconfirmation within The \nChurch of Jesus Christ of \nLatter-day Saints?\nClaim: \"It's important for some Christians that their \nbabies have a Baptism.\u201d. Is the claim above correct, \nand can it be verified by human common sense and \nwithout a web search? Options: yes - no \nClaim: 'Baptism in some Christian traditions is considered \nnecessary for salvation.' Is the claim above correct, and \ncan it be verified by human common sense and without a \nweb search? Options: - yes - no\n\ud8351\n[1] The capital of Heilongjiang, is one of \nChina's biggest cities with nearly ten million \nurban residents. It is also dependent on the its \nwater supply. ...\nWhat role does the Songhua \nRiver play in the capital of \nHeilongjiang?\nDo these two sentences from wikipedia have the \nsame meaning? Choose your answer from: \nA  no   B. yes. The answer is:\nSelect the main industrial highlight of Harbin: \nA) Textile Manufacturing B) Steam Turbine Production  C) \nAgriculture\n\ud8352\n[1] In Tier 2, the main purpose of progress \nmonitoring is to determine whether \ninterventions are successful in helping \nstudents learn at an....\n[2] Entities receiving grant money are given a \nfair amount of autonomy. Each plan devised....\nWhat is the main purpose of \nprogress monitoring in Tier \n2 interventions?\nImagine you are designing a program that analyzes \nfactors like socio-economic status. The program \nshould provide recommendations for study habits, \ntutoring, while also ensuring ongoing monitoring \nand collaboration with teachers, families, and \ncommunity organizations.\nImagine you are an educational program designer tasked \nwith creating a comprehensive intervention strategy aimed \nat improving student academic performance. What \nelements should be included in your strategy to ensure \nsuccess, considering the different factors that can impact \nstudent learning outcomes?\n\ud8353\n[1] Soil moisture Current or past data \ncollection: Point framing, Above ground plant \ntraits, Soil moisture, Transplant experiments, \nNutrients; (Transplanted) seedling survival; ...\n Which plant genera are \nstudied in the OTC plots?\nTell me the temperature,  sunshine rate, rainfall, \nhumidity rate, soil type for handkerchief tree seed", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "71e712d3-4a08-49ae-aef9-dea256f6be2e": {"text": "Nutrients; (Transplanted) seedling survival; ...\n Which plant genera are \nstudied in the OTC plots?\nTell me the temperature,  sunshine rate, rainfall, \nhumidity rate, soil type for handkerchief tree seed \nin bullets 2 words answer in number \nSummarize the main focus of the experiment and its \ngeographical scope in one sentence.\n\ud8354\n[1] facilitate data use by policy makers and \nresearchers. It provides statistical standards, ...\n[2] The birth rate percentages over the age of \n30 and under the age of 30 are also var...\n[3]  Data can also be transformed to make \nthem easier to visualize. For example, suppose \n  What role do population \npyramids play in comparing \ndemographic trends across \ndifferent countries?\nHow can I generate a web page that displays a chart \nshowing the population growth rate of different \ncountries using Python code? Can you provide me \nwith some sample code to get started?\nHow might data transformation influence the visualization \nof population statistics on a web platform?\nFigure 4: Some cases of RAG-Instruct for each RAG scenario. We compare the generated questions with and\nwithout using Instruction Simulation.\npared to the base models on domain-specific and\nmulti-hop tasks. In contrast, our RAG-Instruct\nachieves significant improvements across all four\ncategories of tasks compared to the base models\nand outperforms all previous SOTA RAG-specific\nmodels, particularly in multi-hop and domain-\nspecific tasks. This highlights its superior robust-\nness and generalization across a broader range of\nRAG scenarios and tasks.\nComparison against Open-source instruction-\ntuned models. We also compare our method with\nopen-source instruction-tuned models, which ex-\nhibit strong RAG capabilities. As shown in Table 4,\nmodels trained with RAG-Instruct on base models\noutperform these instruction-tuned models across\nvarious tasks, demonstrating that the RAG instruc-\ntion dataset effectively enhances the model\u2019s RAG\nperformance.\n4.3 Impact of Instruction Simulation\nTo investigate the impact of Instruction Simulation,\nwe design a comparative experiment. We randomly\nsample a subset Ds containing 20,000 entries from\nour RAG-Instruct dataset and create another subset\nD\u2032\ns without using Instruction Simulation. To ensure\na fair comparison, Ds and D\u2032\ns share the same source\ndocuments D\u2217 and include all five RAG scenario\nparadigms. We then train two models on Llama3-\n8B using Ds and D\u2032\ns with identical hyperparameters.\nAs shown in Table 5, removing the Instruction\nSimulation process results in performance declines", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "6707413a-f748-4b90-ad2b-c52862716d7f": {"text": "documents D\u2217 and include all five RAG scenario\nparadigms. We then train two models on Llama3-\n8B using Ds and D\u2032\ns with identical hyperparameters.\nAs shown in Table 5, removing the Instruction\nSimulation process results in performance declines\nacross all tasks. The drop is smaller for open-ended\ntasks but significantly larger for closed-set, multi-\nhop, and domain-specific tasks. We observe that\nwithout Instruction Simulation, GPT-4o tends to\ngenerate overly simple and uniform questions, re-\nsembling open-ended ones, leading to minimal im-\npact on closed-set evaluation. However, the di-\nverse formats of closed-set, multi-hop, and domain-\nspecific tasks, such as multiple-choice and multi-\nhop reasoning, pose challenges that the model\nstruggles to handle. This highlights the critical role\nof Instruction Simulation in enabling the model to\nadapt to a wide variety of tasks.\nAdditionally, we provide specific cases, as\nshown in Figure 4, demonstrating that Instruction\nSimulation generates questions that closely resem-\nble exemplar questions, significantly enhancing\ndiversity compared to those produced without it.\nGiven the high quality and diversity of the synthe-\nsized dataset, Instruction Simulation ensures both\nattributes effectively.\n4.4 Role of RAG Paradigms\nTo evaluate the role of RAG paradigms, we design\nan ablation experiment to verify the effectiveness\nof the five RAG scenarios in RAG-Instruct. Specif-\nically, we remove the data corresponding to each\nparadigm from RAG-Instruct one at a time and\ntrain models on Llama3-8B using identical training\nhyperparameters, respectively.\nAs shown in Table 6, when a single RAG\nparadigm (e.g. r0) is removed from RAG-Instruct,\nwe observe a noticeable performance drop in evalu-\nation benchmarks corresponding to that specific\nRAG scenario. This indicates that each RAG\nparadigm plays a critical role in enhancing the\nmodel\u2019s RAG capabilities across different scenar-\nios. Furthermore, we observe that removing multi-\ndocument paradigms (r2 and r4) leads to a signif-\nicant decline in multi-hop performance. Notably,", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "8b4e4094-3a05-48a9-bd37-1785c91794e0": {"text": "Open-ended Closed-set Multi-hop Domain-specific\nWQA PQA TQA OBQA Pub ARC 2WIKI HotP MSQ CFQA PubMed\nRAG-Instruct20k(Llama3-8B) 64.6 64.8 77.0 80.2 76.0 79.4 73.0 53.1 29.7 55.4 77.2\nw.o. Simulation20k 63.4 63.1 75.9 74.2 71.4 70.4 62.5 47.7 25.0 47.4 70.4\nRAG-Instructw.o. Retrieval 57.6 28.4 64.2 61.2 60.6 62.8 47.7 35.4 10.1 - -\nTable 5: Ablation Study on RAG-Instruct. w.o. Simulation indicates the removal of the Instruction Simulation\nprocess, while w.o. Retrieval indicates the performance in non-retrieval scenarios.\nwhen all multi-document paradigms (r2 and r4) are\nremoved, the model\u2019s performance on multi-hop\ntasks drops significantly. In contrast, removing all\nsingle-document paradigms (r0, r1, r3) results in a\nrelatively small decline in single-hop performance.\nThis suggests that multi-document RAG paradigm\ndata can partially enhance the model\u2019s RAG capa-\nbilities in single-hop scenarios.\nMethod TriviaQA (Single) HotpotQA (Multi)\nHelpful Midhelp Helpless Helpful Midhelp\nRAG-Instruct 86.9 72.6 40.5 73.1 42.2w.o.r0 86.4 69.6 36.4\u2013 74.1 39.3w.o.r1 86.5 66.5\u2013 40.9 72.4 41.3w.o.r2 86.2 71.8 39.7 68.2 29.8 \u2013\nw.o.r3 83.5\u2013 70.6 39.6 72.8 42.2w.o.r4 85.2 72.1 39.5 65.4 \u2013 38.8w.o.r0,r1,r3 84.3 68.1\u2013 36.5\u2013 74.8 41.4w.o.r2,r4 85.0 71.4 38.8 63.5 \u2013 26.6\u2013\nTable 6: Ablation study on role of query paradigms.\nAll experiments are conducted based on the Llama3-8B\nmodel using identical hyperparameters. \u2018 \u2013\u2019 indicates\nlarge performance drops for each paradigm.\n4.5 Further Analysis\nPerformance in non-retrieval scenarios. Since\nour RAG-Instruct is built on the Wikipedia cor-\npus, the performance improvements on evaluation\nbenchmarks may stem from knowledge injection\nduring the supervised fine-tuning stage. To investi-\ngate whether our approach genuinely enhances the\nmodel\u2019s RAG capabilities, we compare the perfor-\nmance in both retrieval and non-retrieval scenarios\n(based on the Llama3-8B model trained on RAG-\nInstruct). As shown in Table 5, performance in\nnon-retrieval scenarios is significantly lower across\nall benchmarks compared to retrieval scenarios,\ndemonstrating that RAG-Instruct effectively en-\nhances the model\u2019s capabilities in RAG scenarios.\nDifferent retrieval source. To further explore\nthe generalization of our method, we investigate the\nimpact of using different retrieval sources. Specifi-\ncally, we further evaluate our method on four single-\nhop QA tasks, including ARC, PQA, TQA and\nOBQA, utilizing DuckDuckGo, and Bing Search\nas retrieval sources during inference. The results", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "32158c2a-2c9e-4eaf-9ef6-4e79957ae59a": {"text": "impact of using different retrieval sources. Specifi-\ncally, we further evaluate our method on four single-\nhop QA tasks, including ARC, PQA, TQA and\nOBQA, utilizing DuckDuckGo, and Bing Search\nas retrieval sources during inference. The results\n(detailed in Appendix C.) suggest that all retrieval\nsources effectively improve task performance, with\nminimal variation in performance across different\nsources. This demonstrates the robustness of our\napproach to enhancing RAG capabilities.\n5 Related Work\nRetrieval-augmented generation (RAG) is a widely\nadopted approach for supplementing the parametric\nknowledge of large language models (LLMs) with\nexternal information sources. Due to the imperfec-\ntions of retrievers, the retrieved information often\nfails to align well with the LLM\u2019s needs, which can\nnegatively impact LLM performance (Petroni et al.,\n2020; Shi et al., 2023; Maekawa et al., 2024).\nTo enhance LLM-based RAG capabilities, some\nstudies focus on aligning retrievers with LLM\nneeds (Shi et al., 2024; Lin et al., 2023) through\nmulti-step retrieval processes (Trivedi et al., 2023;\nJiang et al., 2023; Jeong et al., 2024; Shao et al.,\n2023; Yu et al., 2023; Asai et al., 2024a; Wei et al.,\n2024) and query reformulation (Ma et al., 2023;\nJeong et al., 2024). On the other hand, several\nstudies focus on enhancing the RAG capabilities\nof LLMs by improving their robustness in noisy\nretrieval contexts. Research such as (Chan et al.,\n2024; Zhang et al., 2024; Liu et al., 2024b; Yoran\net al., 2024) trains models with additional irrelevant\nor noisy documents to better handle such scenarios.\nHowever, these approaches consider only a limited\nrange of RAG scenarios. Furthermore, the lack of\na general RAG dataset forces many works, such as\nRAFT (Zhang et al., 2024), to fine-tune models on\ntask-specific datasets, leading to poor task general-\nization. This highlights the need for a dataset that\ncovers diverse RAG scenarios and tasks.", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "97d66cc3-091d-4461-825f-6558bdd12922": {"text": "6 Conclusion\nThis work introduces RAG-Instruct, a method for\nsynthesizing diverse and high-quality RAG instruc-\ntion data from any source corpus. It incorpo-\nrates five RAG paradigms to capture diverse query-\ndocument relationships and uses instruction simula-\ntion to enhance data quality and diversity by lever-\naging existing datasets. Using this approach, we\nconstruct a 40K instruction dataset from Wikipedia,\ncovering diverse RAG scenarios and tasks. For\nfuture work, we plan to expand the instructions\nin RAG-Instruct to incorporate chain-of-thought\n(CoT) characteristics, enabling models to perform\nplanned retrieval based on the query.\nLimitations\nGranularity of RAG Paradigms While RAG-\nInstruct introduces five distinct RAG query\nparadigms to handle various query-document re-\nlationships, this relationship is of a coarse granu-\nlarity. Specifically, the current set of paradigms\nfocuses on broad categories but does not explore\nmore granular or specialized paradigms that could\nbetter capture nuanced retrieval tasks. For instance,\nfor multi-hop queries, the number of hops could be\nspecified, and relevance might have more granular\noptions. Expanding the range of RAG paradigms to\ncover finer distinctions could enhance the model\u2019s\nability to handle complex, diverse, and edge-case\nretrieval situations, thereby improving its robust-\nness and performance.\nReliance on Synthetic Data Our approach re-\nlies on synthetic data generation, which inherently\ncarries the risk of introducing errors or biases, even\nwhen using powerful large language models like\nGPT-4. While the use of large-scale instruction\ndatasets such as SlimOrca and Evol Instruct im-\nproves the diversity and quality of the generated\ndata, it is still possible for GPT-4 to produce flawed\nor inconsistent RAG instructions that may nega-\ntively impact downstream tasks. As synthetic data\ngeneration becomes more prevalent, ensuring the\naccuracy and reliability of such data remains an on-\ngoing challenge, especially in high-stakes domains\nwhere the correctness of information is critical.\nReferences\nAchiam et al. 2023. Gpt-4 technical report. arXiv\npreprint arXiv:2303.08774.\nAkari Asai, Sewon Min, Zexuan Zhong, and Danqi\nChen. 2023. Retrieval-based language models and\napplications. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 6: Tutorial Abstracts), pages 41\u201346.\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. 2024a. Self-rag: Learning to", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "073ac472-f650-49c7-bb3e-53e6823b95f7": {"text": "ing of the Association for Computational Linguistics\n(Volume 6: Tutorial Abstracts), pages 41\u201346.\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. 2024a. Self-rag: Learning to\nretrieve, generate, and critique through self-reflection.\nIn The Twelfth International Conference on Learning\nRepresentations.\nAkari Asai, Zexuan Zhong, Danqi Chen, Pang Wei\nKoh, Luke Zettlemoyer, Hannaneh Hajishirzi, and\nWen-tau Yih. 2024b. Reliable, adaptable, and at-\ntributable language models with retrieval. arXiv\npreprint arXiv:2403.03187.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. In Proceedings of the 2013\nconference on empirical methods in natural language\nprocessing, pages 1533\u20131544.\nChi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo,\nWei Xue, Yike Guo, and Jie Fu. 2024. Rq-rag: Learn-\ning to refine queries for retrieval augmented genera-\ntion. arXiv preprint arXiv:2404.00610.\nJunying Chen, Xidong Wang, Ke Ji, Anningzhe Gao,\nFeng Jiang, Shunian Chen, Hongbo Zhang, Dingjie\nSong, Wenya Xie, Chuyi Kong, et al. 2023a.\nHuatuogpt-ii, one-stage training for medical adap-\ntion of llms. arXiv preprint arXiv:2311.09774.\nYukang Chen, Shaozuo Yu, Shengju Qian, Haotian Tang,\nXin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2023b.\nLong alpaca: Long-context instruction-following\nmodels. https://github.com/dvlab-research/\nLongLoRA.\nZhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma,\nSameena Shah, and William Yang Wang. 2022. Con-\nvfinqa: Exploring the chain of numerical reasoning\nin conversational finance question answering. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing , pages 6279\u2013\n6292.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. arXiv\npreprint arXiv:1803.05457.\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and\nChristopher R\u00e9. 2022. Flashattention: Fast and\nmemory-efficient exact attention with io-awareness.\nAdvances in Neural Information Processing Systems,\n35:16344\u201316359.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, et al. 2024. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783.", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "5d077295-a41f-4735-ba10-e7de767615f9": {"text": "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In International confer-\nence on machine learning, pages 3929\u20133938. PMLR.\nTsun hin Cheung and Kin Man Lam. 2023. Factllama:\nOptimizing instruction-following language models\nwith external knowledge for automated fact-checking.\n2023 Asia Pacific Signal and Information Processing\nAssociation Annual Summit and Conference (APSIPA\nASC), pages 846\u2013853.\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\nand Akiko Aizawa. 2020. Constructing a multi-hop\nqa dataset for comprehensive evaluation of reasoning\nsteps. In Proceedings of the 28th International Con-\nference on Computational Linguistics, pages 6609\u2013\n6625.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin,\nand Edouard Grave. Unsupervised dense informa-\ntion retrieval with contrastive learning. Transactions\non Machine Learning Research.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\nYu, Armand Joulin, Sebastian Riedel, and Edouard\nGrave. 2023. Atlas: Few-shot learning with retrieval\naugmented language models. Journal of Machine\nLearning Research, 24(251):1\u201343.\nSoyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju\nHwang, and Jong C Park. 2024. Adaptive-rag: Learn-\ning to adapt retrieval-augmented large language mod-\nels through question complexity. In Proceedings of\nthe 2024 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies (Volume 1: Long Pa-\npers), pages 7029\u20137043.\nZhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing\nSun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\nJamie Callan, and Graham Neubig. 2023. Ac-\ntive retrieval augmented generation. arXiv preprint\narXiv:2305.06983.\nJiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang,\nand Zhicheng Dou. 2024. Flashrag: A modular\ntoolkit for efficient retrieval-augmented generation\nresearch. arXiv preprint arXiv:2405.13576.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William\nCohen, and Xinghua Lu. 2019. Pubmedqa: A dataset\nfor biomedical research question answering. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 2567\u20132577.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "d177ed53-2313-4ce8-b222-cf8881e80e5a": {"text": "ternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 2567\u20132577.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1601\u20131611.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769\u20136781.\nTom\u00e1\u0161 Ko\u02c7cisk`y, Jonathan Schwarz, Phil Blunsom, Chris\nDyer, Karl Moritz Hermann, G\u00e1bor Melis, and Ed-\nward Grefenstette. 2018. The narrativeqa reading\ncomprehension challenge. Transactions of the Asso-\nciation for Computational Linguistics, 6:317\u2013328.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics , 7:453\u2013\n466.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph Gon-\nzalez, Hao Zhang, and Ion Stoica. 2023. Efficient\nmemory management for large language model serv-\ning with pagedattention. In Proceedings of the 29th\nSymposium on Operating Systems Principles, pages\n611\u2013626.\nXi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi,\nMaria Lomeli, Rich James, Pedro Rodriguez, Jacob\nKahn, Gergely Szilvasy, Mike Lewis, et al. 2023.\nRa-dit: Retrieval-augmented dual instruction tuning.\narXiv preprint arXiv:2310.01352.\nWanlong Liu, Enqi Zhang, Li Zhou, Dingyi Zeng, Shao-\nhuan Cheng, Chen Zhang, Malu Zhang, and Wenyu\nChen. 2024a. A compressive memory-based re-\ntrieval approach for event argument extraction. arXiv\npreprint arXiv:2409.09322.\nZihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Moham-\nmad Shoeybi, and Bryan Catanzaro. 2024b. Chatqa:\nBuilding gpt-4 level conversational qa models. arXiv\npreprint arXiv:2401.10225.\nShuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-\nwon Hwang, and Alexey Svyatkovskiy. 2022. Reacc:\nA retrieval-augmented code completion framework.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 6227\u20136240.\nXinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao,", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "9560f9ca-418b-4109-aa44-5a0705c1bd7d": {"text": "A retrieval-augmented code completion framework.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 6227\u20136240.\nXinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao,\nand Nan Duan. 2023. Query rewriting in retrieval-\naugmented large language models. In Proceedings\nof the 2023 Conference on Empirical Methods in\nNatural Language Processing, pages 5303\u20135315.\nSeiji Maekawa, Hayate Iso, Sairam Gurajada, and Nikita\nBhutani. 2024. Retrieval helps or hurts? a deeper\ndive into the efficacy of retrieval augmentation to\nlanguage models. In Proceedings of the 2024 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies (Volume 1: Long Papers), pages\n5506\u20135521.", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "8e66ac05-c053-4505-8473-c9319c349506": {"text": "Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023.\nWhen not to trust language models: Investigating\neffectiveness of parametric and non-parametric mem-\nories. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 9802\u20139822.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question an-\nswering. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2381\u20132391.\nArindam Mitra, Luciano Del Corro, Shweti Mahajan,\nAndres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi\nChen, Anastasia Razdaibiedina, Erik Jones, Kriti\nAggarwal, et al. 2023. Orca 2: Teaching small\nlanguage models how to reason. arXiv preprint\narXiv:2311.11045.\nFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim\nRockt\u00e4schel, Yuxiang Wu, Alexander H Miller, and\nSebastian Riedel. 2020. How context affects lan-\nguage models\u2019 factual predictions. In Automated\nKnowledge Base Construction.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,\nand Yuxiong He. 2020. Zero: Memory optimizations\ntoward training trillion parameter models. In SC20:\nInternational Conference for High Performance Com-\nputing, Networking, Storage and Analysis, pages 1\u2013\n16. IEEE.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don\u2019t know: Unanswerable ques-\ntions for SQuAD. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 784\u2013789,\nMelbourne, Australia. Association for Computational\nLinguistics.\nZhihong Shao, Yeyun Gong, Yelong Shen, Minlie\nHuang, Nan Duan, and Weizhu Chen. 2023. En-\nhancing retrieval-augmented large language models\nwith iterative retrieval-generation synergy. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2023, pages 9248\u20139274.\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan\nScales, David Dohan, Ed H Chi, Nathanael Sch\u00e4rli,\nand Denny Zhou. 2023. Large language models can\nbe easily distracted by irrelevant context. In Inter-\nnational Conference on Machine Learning , pages\n31210\u201331227. PMLR.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Richard James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. 2024. Replug: Retrieval-\naugmented black-box language models. In Proceed-\nings of the 2024 Conference of the North American\nChapter of the Association for Computational Lin-", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "7ded0298-43c1-4ad8-9ea9-da54d5e061c0": {"text": "joon Seo, Richard James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. 2024. Replug: Retrieval-\naugmented black-box language models. In Proceed-\nings of the 2024 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies (Volume 1:\nLong Papers), pages 8364\u20138377.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2022. Musique: Multi-\nhop questions via single-hop question composition.\nTransactions of the Association for Computational\nLinguistics, 10:539\u2013554.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2023. Interleaving retrieval\nwith chain-of-thought reasoning for knowledge-\nintensive multi-step questions. In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n10014\u201310037.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack\nHessel, Tushar Khot, Khyathi Raghavi Chandu,\nDavid Wadden, Kelsey MacMillan, Noah A. Smith,\nIz Beltagy, and Hannaneh Hajishirzi. 2023a. How\nfar can camels go? exploring the state of instruction\ntuning on open resources. ArXiv, abs/2306.04751.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023b. Self-instruct: Aligning language\nmodels with self-generated instructions. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 13484\u201313508.\nZhepei Wei, Wei-Lin Chen, and Yu Meng. 2024.\nInstructrag: Instructing retrieval-augmented gen-\neration with explicit denoising. arXiv preprint\narXiv:2406.13629.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\nPu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. 2023. Wizardlm: Empowering large lan-\nguage models to follow complex instructions. ArXiv,\nabs/2304.12244.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2\ntechnical report. arXiv preprint arXiv:2407.10671.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D Manning. 2018. Hotpotqa: A dataset for\ndiverse, explainable multi-hop question answering.", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "1f5d362f-c215-4fbf-b692-64d4d2ae8ab7": {"text": "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D Manning. 2018. Hotpotqa: A dataset for\ndiverse, explainable multi-hop question answering.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2369\u20132380.\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan Be-\nrant. 2024. Making retrieval-augmented language\nmodels robust to irrelevant context. In The Twelfth\nInternational Conference on Learning Representa-\ntions.", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "82a54603-088b-4641-ab16-d0de6d20b1dd": {"text": "W. Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma,\nHongwei Wang, and Dong Yu. 2023. Chain-of-\nnote: Enhancing robustness in retrieval-augmented\nlanguage models. ArXiv, abs/2311.09210.\nTianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei\nFang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu,\nDanny Fox, Helen Meng, and James Glass. 2023. In-\nterpretable unified language checking. arXiv preprint\narXiv:2304.03728.\nTianjun Zhang, Shishir G Patil, Naman Jain, Sheng\nShen, Matei Zaharia, Ion Stoica, and Joseph E Gonza-\nlez. 2024. Raft: Adapting language model to domain\nspecific rag. arXiv preprint arXiv:2403.10131.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle\nLi, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZhuohan Li, Zi Lin, Eric P Xing, et al. 2023a. Lmsys-\nchat-1m: A large-scale real-world llm conversation\ndataset. arXiv preprint arXiv:2309.11998.\nYang Zheng, Adam W Harley, Bokui Shen, Gordon Wet-\nzstein, and Leonidas J Guibas. 2023b. Pointodyssey:\nA large-scale synthetic dataset for long-term point\ntracking. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 19855\u2013\n19865.", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "f49f8e6a-171e-48d1-a372-a9d4d215d4f7": {"text": "A Experimental Details\nA.1 More Details of Training\nDataset Construction. Our RAG-Instruct cor-\npus is built using Wikipedia. Following the ap-\nproach (Karpukhin et al., 2020), each document is\na disjoint text block of up to 100 words extracted\nfrom a Wikipedia article. Following work (Shi\net al., 2023), we generate Wikipedia document em-\nbeddings.\nFor exemplar data, we select datasets such as\nShareGPT (Wang et al., 2023a), Alpaca (hin Che-\nung and Lam, 2023), WizardLM-70K (Xu et al.,\n2023), Lmsys-chat-1M (Zheng et al., 2023a), and\nSlimOrca (Mitra et al., 2023). First, we remove\noverly short, overly long, and low-quality data from\nthese datasets. Then, we randomly sample 120K\nquestions from the filtered data. Since RAG is\nmost effective in knowledge-intensive task scenar-\nios (Maekawa et al., 2024; Shi et al., 2023), we use\nGPT-4o to further filter for knowledge-intensive\ninstructions from these synthetic datasets. The spe-\ncific prompt used is shown in Figure 5.\nTraining Details. We train our models using 8\nNvidia A800 GPUs, each with 80GB of memory.\nAll models are trained for 3 epochs with a total\nbatch size of 128, a peak learning rate of 5e-6,\n3% warmup steps, and linear weight decay. The\nmaximum token length is set to 4096 for all mod-\nels. We leverage DeepSpeed Stage 3 (Rajbhandari\net al., 2020) for multi-GPU distributed training with\nBFloat16 precision enabled. FlashAttention (Dao\net al., 2022) is employed to improve efficiency dur-\ning long-context training.\nA.2 More Details of Inference\nWe conduct evaluations of our RAG-Instruct and\nvarious baselines across a wide range of down-\nstream tasks, covering 11 tasks in four major cat-\negories. Throughout these experiments, we per-\nform zero-shot evaluations, providing task instruc-\ntions without few-shot demonstrations. For RAG-\nspecific models, we follow the original papers\u2019\nweights and prompts for inference. For our model\nand other baselines, reasoning details and prompts\nare provided in Table 8.\nOpen-Ended Tasks include three open-domain\nquestion-answering datasets, WebQA (WQA) (Be-\nrant et al., 2013), PopQA (PQA) (Mallen et al.,\n2023), and TriviaQA-unfiltered (TQA) (Joshi et al.,\n2017), where models are required to answer arbi-\ntrary questions based on factual knowledge. We re-\ntrieve the top 10 most relevant documents from the\ncorpus as candidate documents. Following (Asai\net al., 2024a), we evaluate the performance based\non accuracy, assessing whether gold answers are\nincluded in the model output.\nClosed-Set Tasks include two multiple-choice", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "f379da8a-93b3-4b45-8503-1ddee720be6c": {"text": "corpus as candidate documents. Following (Asai\net al., 2024a), we evaluate the performance based\non accuracy, assessing whether gold answers are\nincluded in the model output.\nClosed-Set Tasks include two multiple-choice\nquestion-answering datasets: OpenbookQA\n(OBQA) (Mihaylov et al., 2018), PubHealth\n(Pub) (Zhang et al., 2023) and ARC-Challenge\n(ARC) (Clark et al., 2018). We retrieve the top\n5 most relevant documents from the corpus as\ncandidate documents. Extract Match (EM) is used\nas the evaluation metric, and results are reported\non the test set for both datasets.\nMulti-Hop Tasksinclude three multi-hop question-\nanswering datasets: 2WikiMultiHopQA (2WIKI),\nHotpotQA (HotQ), and Musique (MSQ). Follow-\ning (Chan et al., 2024), we adopt a reading compre-\nhension setup for these datasets, using candidate\ndocuments from their original sources. Each ques-\ntion is linked to 10 passages, with only a few (2 for\nHotQ and 2 or 4 for 2WIKI) being relevant. MSQ\nis more challenging, requiring 2, 3, or 4 reasoning\nhops to answer. We use accuracy as the evaluation\nmetric.\nDomain-Specific Tasks include two datasets:\nCFQA (Chen et al., 2022) in the financial domain\nand PubMedQA (Jin et al., 2019) in the medical do-\nmain. For both, we adopt a reading comprehension\nsetup, utilizing the provided context as candidate\ndocuments. Exact Match (EM) is used as the eval-\nuation metric.\nB Detailed Prompts in our Experiments\nB.1 Prompts for dividing the datasets into five\nRAG scenarios.\nTo explore the performance of RAG methods across\nfive different scenarios, we use GPT-4o to catego-\nrize questions from two QA datasets: Single-hop\nQA (TriviaQA) and Multi-hop QA (HotPotQA),\ninto relevant subsets based on the defined RAG\nscenarios. The prompts used for categorization are\nshown in Figure 6 (Single-hop QA) and Figure 7\n(Multi-hop QA). The final data volume for each\nsubset is shown in Table 9.\nB.2 Prompts for synthesizing data for five\nRAG scenarios.\nWe construct five RAG paradigms as described in\nFigure 8, Figure 9, Figure 10, Figure 11, and Fig-", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "2297d787-6fa0-49c3-b32a-3d5b56f54943": {"text": "Method ARC PQA OBQA WQA A VG.( \u2191) V AR.(\u2193)\nSelf-RAG (Llama2-7B)\n+ DuckDuckGo 72.1 56.7 76.4 48.1\n62.9 1.9 + WIKI 73.1 55.8 78.0 49.0\n+ BingSearch 68.6 53.2 76.8 46.4\nRQ-RAG (Llama2-7B)\n+ DuckDuckGo 69.0 58.3 79.8 52.4\n65.2 1.6 + WIKI 68.3 57.1 80.6 56.5\n+ BingSearch 68.9 55.6 78.8 57.4\nRAG-Instruct (Llama2-7B)\n+ DuckDuckGo 75.1 63.0 74.4 68.1\n69.7 0.7+ WIKI 74.8 62.4 71.4 67.2\n+ BingSearch 75.5 63.8 72.0 69.0\nTable 7: Performance comparison of different retrieval sources. A VG. represents the mean, and V AR. represents the\nvariance.\nKnowledge-Intensive Data Selection Prompt\n{Question}\nPlease determine if retrieving external information would help answer the above question. If it helps, answer \"True\", otherwise\nanswer \"False\".\nFigure 5: The prompt of filtering knowledge-intensive instructions from synthetic datasets\nDividing Prompt for Single-hop Question.\nDocuments:\n{Doucments}\nQuestion:\n{Question}\nAnswer:\n{Answer}\nBased on the question and its answer, along with the provided documents, carefully review the documents to assess their\noverall usefulness in answering the question. Avoid evaluating each document individually; instead, consider the documents\nas a whole. Choose the most accurate option based on how much the documents contribute to the answer: 1. Very helpful:\nThe answer is directly provided in the documents. 2. Partially helpful: The documents offer supporting information or clues\nbut do not provide an explicit answer. 3. Not helpful: The documents do not contribute to answering the question. Please\ndirectly respond with only the chosen option (1, 2, or 3).\nFigure 6: The prompt for dividing the single-hop question answering datasets into five RAG scenarios.\nure 12. To generate data for each RAG paradigm,\nwe simply provide the randomly selected source\ndocuments <Documents> and the simulated instruc-\ntion <Simulated Instruction>.\nC Additionally Experiments\nC.1 Experiments on Different Retrieval\nSource\nTo further explore the generalization of our method,\nwe investigate the impact of using different re-\ntrieval sources. Specifically, we further evaluate\nour method on four single-hop QA tasks, includ-\ning ARC, PQA, TQA, and OBQA, utilizing Duck-\nDuckGo, Wikipedia, and Bing Search as retrieval", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "530db349-48eb-40d4-bcb1-06cb62b01b35": {"text": "Dividing Prompt for Multi-hop Question.\nDocuments:\n{Doucments}\nQuestion:\n{Question}\nAnswer:\n{Answer}\nBased on the question and answer provided, carefully review the given documents and assess their overall usefulness in\naddressing the question. Avoid evaluating each document individually; instead, consider the documents as a whole. Choose\nthe most accurate option based on how much the documents contribute to the answer: 1. Very helpful: The answer can be\ndirectly derived from multiple documents. 2. Partially helpful: The documents offer supporting information or clues but do\nnot provide an explicit answer. It needs further reasoning or more knowledge. Please directly respond with only the chosen\noption (1, or 2).\nFigure 7: The prompt for dividing the multi-hop question answering datasets into five RAG scenarios.\nsources during inference. As shown in Table 7,\nour RAG-Instruct method demonstrates strong re-\nsilience to changes in retrieval sources compared\nto Self-RAG and RQ-RAG. We use the official API\nto obtain retrieval results.\nWhile Self-RAG, primarily curated using\nWikipedia, shows notable performance drops (3-\n5%) when switching to Bing Search (with a vari-\nance of 1.9), and RQ-RAG similarly experiences\nperformance inconsistencies (variance of 1.6), our\nRAG-Instruct method exhibits minimal perfor-\nmance fluctuations across different data sources.\nSpecifically, the average performance of RAG-\nInstruct remains consistently high (69.7) with a\nvariance of only 0.7, even when employing Duck-\nDuckGo, Wikipedia, or Bing Search for retrieval.\nThis demonstrates that RAG-Instruct not only\nachieves higher overall performance but also main-\ntains exceptional robustness and stability across\ndiverse retrieval sources, highlighting its superior\ngeneralization capabilities compared to existing\nmethods.", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "4577cd4a-3bb6-4f2d-815d-b26dca515b66": {"text": "Useless Doc (r0)\n<Documents>\n[1] {<Document 1>}\n</Documents>\nYour task is to generate an English question q* and a corresponding response a* based on the provided <Documents>. Please\nnote that the question q* can take various forms, not limited to questions with a question mark, but also including statements,\ninstructions, and other formats. You need to follow the requirements below to generate the q* and a* (RAG Paradigms):\n1. q* should be related to the <Documents>, but the <Documents> can not provide any useful information for answering q*.\n2. a* should be able to answer q*, ensuring that the response a* is accurate, detailed, and comprehensive.\nAdditionally, to ensure diversity, richness, and high quality in the question q* you generate, we will randomly provide a\nquestion for you to emulate. In other words, while satisfying the requirements above, make q* similar in task requirement\nand expression to the <Simulated Instruction> below:\n<Simulated Instruction>\n{<Simulated Instruction>}\n</Simulated Instruction>\nPlease directly generate the question-answer pair (q*, a*) following all the rules above in the format of {\"q*\": ..., \"a*\": ...}.\nEnsure the quality of the generated (q*, a*).\nFigure 8: The prompt for synthesizing Useless Doc (r0) data.\nSingle-Doc Support (r1)\n<Documents>\n[1] {<Document 1>}\n</Documents>\nYour task is to generate an English question q* and a corresponding response a* based on the provided <Documents>. Please\nnote that the question q* can take various forms, not limited to questions with a question mark, but also including statements,\ninstructions, and other formats. You need to follow the requirements below to generate the q* and a* (RAG Paradigms):\n1. <Documents> can support q* by providing useful information or hints, but they do not contain explicit answers.\n2. a* should use useful information from <Documents> to aid in answering q*, ensuring that the response is accurate,\ndetailed, and comprehensive.\nAdditionally, to ensure diversity, richness, and high quality in the question q* you generate, we will randomly provide a\nquestion for you to emulate. In other words, while satisfying the requirements above, make q* similar in task requirement\nand expression to the <Simulated Instruction> below:\n<Simulated Instruction>\n{<Simulated Instruction>}\n</Simulated Instruction>\nPlease directly generate the question-answer pair (q*, a*) following all the rules above in the format of {\"q*\": ..., \"a*\": ...}.\nEnsure the quality of the generated (q*, a*).", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "b2ea4353-e377-4cf8-bf88-59689b9a0ee7": {"text": "{<Simulated Instruction>}\n</Simulated Instruction>\nPlease directly generate the question-answer pair (q*, a*) following all the rules above in the format of {\"q*\": ..., \"a*\": ...}.\nEnsure the quality of the generated (q*, a*).\nFigure 9: The prompt for synthesizing Single-Doc Support (r1) data.", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "ba60ccf4-7039-4298-a790-f15e7dc9c7ae": {"text": "Task Template\nOpen-ended\n### Instruction:\nReference Document:\n{RETRIEVED DOCUMENTS}\nPlease refer to the documents above and answer the following question:\n{QUESTION}\n### Response:\nDomain-specific\nOBQA & ARC\n### Instruction:\nReference Document:\n{RETRIEVED DOCUMENTS}\nGiven four answer candidates, A, B, C and D, choose the best answer choice for the question.\nPlease refer to the documents above and answer the following question:\n{QUESTION (Including Options) }\n### Response:\nPub (FEVER)\n### Instruction:\nReference Document:\n{RETRIEVED DOCUMENTS}\nIs the following statement correct or not? Say true if it\u2019s correct; otherwise, say false.\nPlease refer to the documents above and answer the following question:\n{QUESTION}\n### Response:\nMulti-hop\n### Instruction:\nReference Document:\n{RETRIEVED DOCUMENTS}\nPlease refer to the documents above and answer the following question:\n{QUESTION}\n### Response:\nDomain-specific\nCFQA\n### Instruction:\nReference Document:\n{RETRIEVED DOCUMENTS}\nPlease refer to the documents above and answer the following question:\n{PREVIOUS QUESTIONS ANSWERS}\n{QUESTION}\n### Response:\nPubMed\n### Instruction:\nReference Document:\n{RETRIEVED DOCUMENTS}\nPlease refer to the documents above and answer the following question:\nAnswer the question with \u201cyes\u201d or \u201cno\u201d or \u201cmaybe\u201d.\n{QUESTION}\n### Response:\nTable 8: Prompt templates in our Evaluation. For Open-ended and Close-set datasets, RETRIEVED DOCUMENTS\nare sourced from the retrieval corpus (e.g., Wikipedia). For Multi-hop and Domain-specific datasets, RETRIEVED\nDOCUMENTS come from the context provided in datasets.\nTriviaQA(Single-hop QA) HotpotQA (Multi-hop QA)\nHelpful Midhelpful Helpless Helpful Midhelpful\nMumber of Data 5628 894 791 4015 3390\nTable 9: Detailed information on dataset subsets categorized into five RAG scenarios.", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "34f0fcec-47fa-4d26-9412-a34c34776edf": {"text": "Multi-Doc Support (r2)\n<Documents>\n[1] {<Document 1>}\n[2] {<Document 2>}\n[3] ...\n</Documents>\nYour task is to generate an English question q* and a corresponding response a* based on the provided <Documents>. Please\nnote that the question q* can take various forms, not limited to questions with a question mark, but also including statements,\ninstructions, and other formats. You need to follow the requirements below to generate the q* and a* (RAG Paradigms):\n1. Multiple documents within <Documents> can support q* by providing useful information or hints, but they do not contain\nexplicit answers.\n2. a* should use useful information from <Documents> to aid in answering q*, ensuring that the response is accurate,\ndetailed, and comprehensive.\nAdditionally, to ensure diversity, richness, and high quality in the question q* you generate, we will randomly provide a\nquestion for you to emulate. In other words, while satisfying the requirements above, make q* similar in task requirement\nand expression to the <Simulated Instruction> below:\n<Simulated Instruction>\n{<Simulated Instruction>}\n</Simulated Instruction>\nPlease directly generate the question-answer pair (q*, a*) following all the rules above in the format of {\"q*\": ..., \"a*\": ...}.\nEnsure the quality of the generated (q*, a*).\nFigure 10: The prompt for synthesizing Multi-Doc Support (r2) data.\nSingle-Doc Answer (r3)\n<Documents>\n[1] {<Document 1>}\n</Documents>\nYour task is to generate an English question q* and a corresponding response a* based on the provided <Documents>. Please\nnote that the question q* can take various forms, not limited to questions with a question mark, but also including statements,\ninstructions, and other formats. You need to follow the requirements below to generate the q* and a* (RAG Paradigms):\n1. Ensure that q* can be answered directly using the content of <Documents>, meaning its answer can be fully derived from\n<Documents>.\n2. a* should use the information from <Documents> to answer q* accurately, ensuring that the response is accurate, detailed,\nand comprehensive.\nAdditionally, to ensure diversity, richness, and high quality in the question q* you generate, we will randomly provide a\nquestion for you to emulate. In other words, while satisfying the requirements above, make q* similar in task requirement\nand expression to the <Simulated Instruction> below:\n<Simulated Instruction>\n{<Simulated Instruction>}\n</Simulated Instruction>", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "0d2603f2-eed6-4e32-8f8a-6363fd74b03b": {"text": "question for you to emulate. In other words, while satisfying the requirements above, make q* similar in task requirement\nand expression to the <Simulated Instruction> below:\n<Simulated Instruction>\n{<Simulated Instruction>}\n</Simulated Instruction>\nPlease directly generate the question-answer pair (q*, a*) following all the rules above in the format of {\"q*\": ..., \"a*\": ...}.\nEnsure the quality of the generated (q*, a*).\nFigure 11: The prompt for synthesizing Single-Doc Answer (r3) data.", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "64da7818-fdfa-4a69-b697-25d16f663ebd": {"text": "Multi-Doc Answer (r4)\n<Documents>\n[1] {<Document 1>}\n[2] {<Document 2>}\n[3] ...\n</Documents>\nYour task is to generate an English question q* and a corresponding response a* based on the provided <Documents>. Please\nnote that the question q* can take various forms, not limited to questions with a question mark, but also including statements,\ninstructions, and other formats. You need to follow the requirements below to generate the q* and a* (RAG Paradigms):\n1. The answer to q* can be derived from multiple documents within <Documents>, involving multi-hop reasoning or the\nintegration of information from several documents.\n2. a* should leverage the information in <Documents> to provide an accurate answer to q*, ensuring that the response is\naccurate, detailed, and comprehensive.\nAdditionally, to ensure diversity, richness, and high quality in the question q* you generate, we will randomly provide a\nquestion for you to emulate. In other words, while satisfying the requirements above, make q* similar in task requirement\nand expression to the <Simulated Instruction> below:\n<Simulated Instruction>\n{<Simulated Instruction>}\n</Simulated Instruction>\nPlease directly generate the question-answer pair (q*, a*) following all the rules above in the format of {\"q*\": ..., \"a*\": ...}.\nEnsure the quality of the generated (q*, a*).\nFigure 12: The prompt for synthesizing Multi-Doc Answer (r4) data.", "metadata": {"title": "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge\n  Update", "pdf_url": "http://arxiv.org/pdf/2410.20299v1"}}, "5e7fd8ba-6524-490f-95da-d9d8c93ed55b": {"text": "RAG-WM: An Efficient Black-Box Watermarking Approach for\nRetrieval-Augmented Generation of Large Language Models\nPeizhuo Lv\nInstitute of Information Engineering,\nChinese Academy of Sciences\nChina\nlvpeizhuo@gmail.com\nMengjie Sun\nInstitute of Information Engineering,\nChinese Academy of Sciences\nChina\nsunmengjie@iie.ac.cn\nHao Wang\nSchool of Cyber Science and\nTechnology, Shandong University\nChina\n202437082@mail.sdu.edu.cn\nXiaofeng Wang\nIndiana University Bloomington\nUSA\nxw7@iu.edu\nShengzhi Zhang\nDepartment of Computer Science,\nMetropolitan College, Boston\nUniversity\nUSA\nshengzhi@bu.edu\nYuxuan Chen\nSchool of Cyber Science and\nTechnology, Shandong University\nChina\nchenyuxuan@sdu.edu.cn\nKai Chen\nInstitute of Information Engineering,\nChinese Academy of Sciences\nChina\nchenkai@iie.ac.cn\nLimin Sun\nInstitute of Information Engineering,\nChinese Academy of Sciences\nChina\nsunlimin@iie.ac.cn\nAbstract\nIn recent years, tremendous success has been witnessed in Retrieval-\nAugmented Generation (RAG), widely used to enhance Large Lan-\nguage Models (LLMs) in domain-specific, knowledge-intensive, and\nprivacy-sensitive tasks. However, attackers may steal those valu-\nable RAGs and deploy or commercialize them, making it essential\nto detect Intellectual Property (IP) infringement. Most existing\nownership protection solutions, such as watermarks, are designed\nfor relational databases and texts. They cannot be directly applied\nto RAGs because relational database watermarks require white-\nbox access to detect IP infringement, which is unrealistic for the\nknowledge base in RAGs. Meanwhile, post-processing by the ad-\nversary\u2019s deployed LLMs typically destructs text watermark infor-\nmation. To address those problems, we propose a novel black-box\n\u201cknowledge watermark\u201d approach, named RAG-WM, to detect IP\ninfringement of RAGs. RAG-WM uses a multi-LLM interaction\nframework, comprising a Watermark Generator, Shadow LLM &\nRAG, and Watermark Discriminator, to create watermark texts\nbased on watermark entity-relationship tuples and inject them into\nthe target RAG. We evaluate RAG-WM across three domain-specific\nand two privacy-sensitive tasks on four benchmark LLMs. Experi-\nmental results show that RAG-WM effectively detects the stolen\nRAGs in various deployed LLMs. Furthermore, RAG-WM is robust\nagainst paraphrasing, unrelated content removal, knowledge inser-\ntion, and knowledge expansion attacks. Lastly, RAG-WM can also\nevade watermark detection approaches, highlighting its promising\napplication in detecting IP infringement of RAG systems.", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "eb0f895d-d880-4823-8bc4-cdfda2f83452": {"text": "tion, and knowledge expansion attacks. Lastly, RAG-WM can also\nevade watermark detection approaches, highlighting its promising\napplication in detecting IP infringement of RAG systems.\n1 Introduction\nLarge Language Models (LLMs), such as GPT [47] and Llama [43],\nhave gained significant attention and are applied across diverse\nfields, including healthcare [ 35, 59], content generation [ 73], fi-\nnance [72], etc. However, they face considerable challenges, espe-\ncially with tasks that are domain-specific or knowledge-intensive.\nThey are particularly prone to generating \u201challucinations\u201d [ 70]\nwhen responding to queries outside their training data. Addition-\nally, many users are unwilling to upload their sensitive data to\nthird-party platforms for LLM training due to data privacy con-\ncerns. To address these problems, Retrieval-Augmented Generation\n(RAG), consisting of a retriever model and a knowledge base, is\nproposed to improve LLMs by retrieving relevant information from\nthe knowledge bases according to semantic similarity. For example,\nMicrosoft has incorporated RAG into its Azure OpenAI service [45],\nand the Llama models developed by Meta support RAG integra-\ntion in certain applications [44]. Additionally, users can implement\na team-specific RAG knowledge base on a preferred model (e.g.,\nLlama) using the AnythingLLM AI application [23].\nBuilding an RAG system, particularly its knowledge bases, re-\nquires significant investment in resources such as data collection,\ncleaning, organization, updates, and maintenance by skilled per-\nsonnel. For example, as noted in [48], CyC [36] costs about $120M;\nDBpedia [20] is developed at the cost of $5.1M; YAGO [64] costs\n$10M. Therefore, Intellectual Property (IP) protection of the RAG\nsystem is essential to protect the investment of the original RAG\ndevelopers. Digital watermarking is a content-based, information-\nhiding technique for embedding/detecting digital information (usu-\nally related to the owner\u2019s identifier) into/from carrier data and\nhas been demonstrated successful in relational databases [4, 30, 38],\ntexts [31, 46, 50], DNN models [3], etc. However, those watermark-\ning approaches cannot be directly used to protect the IP of RAG\nsystems. On the one hand, when the owner utilizes the model water-\nmarking approach like [3] to embed a watermark into the retriever\nmodel to protect the IP of the RAG system, attackers can easily\narXiv:2501.05249v1  [cs.CR]  9 Jan 2025", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "0c9cb2ee-3c85-4f2c-9460-e1422b9a1946": {"text": "bypass such IP protection by replacing the watermarked retriever\nwith a clean retriever without any watermark embedded. On the\nother hand, applying the database watermarking approach like [30]\nto protect the IP of the knowledge base (the core component of\nRAG) requires direct access to the contents of the database for\nverification, i.e., the \u201cwhite-box\u201d access. However, owners often\nonly have the \u201cblack-box\u201d access to the suspicious RAG systems\ndeployed by adversaries, allowing users to view outputs of LLM\nwithout direct access to the underlying knowledge base.\nRecently, WARD [29] was proposed to detect unauthorized usage\nof RAG using an LLM red-green list watermark to paraphrase all\ntexts of RAGs. However, LLM red-green list watermarks are not\nrobust against paraphrasing attacks [33, 40, 49, 68]. Additionally,\nWARD is vulnerable to piracy attacks, where attackers paraphrase\nall texts in the stolen RAG using their own red-green list, thus\nfraudulently claiming ownership. More importantly, WARD did\nnot consider or discuss the unique challenges posed by RAG for\neffective IP protection. To protect the IP of their RAGs, owners\nhave to embed watermarks (e.g., format-based, syntactic-based,\nand red-green list-based) [14, 31, 42] into the knowledge database\nsince embedding watermarks into retrievers can be easily bypassed.\nAfter stealing the RAG, attackers often deploy it with LLMs of\ntheir choice, making direct access to the outputs of the knowledge\ndatabase impossible. Instead, the owners can only access the post-\nprocessed outputs by attackers\u2019 LLMs, which might have destroyed\nthe watermark embedded in the knowledge database.\nRAG-WM. To solve these challenges, we propose RAG-WM, a\nnovel black-box watermarking method for RAG systems. It protects\nthe IP of valuable RAGs and enables IP infringement detection.\nSpecifically, our method embeds a \u201cknowledge watermark\u201d into\nthe knowledge base, considering that knowledge can be success-\nfully retrieved and remain intact even after being processed by\nLLMs. To generate watermark texts, we first extract entities and\nrelationships from the knowledge base, identify the high-frequency\nentities and relationships, and use them to generate tuples of wa-\ntermark entities and corresponding relations. Since the watermark\nentities and relationships are derived from the original RAG, this\neffectively enhances the watermark\u2019s stealthiness. This process\ninvolves a keyed hash function, with the secret key only known\nto the owner, thus enhancing the security of the watermark. We", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "3675aca9-e805-4ae5-a92d-a9b6c4023eaf": {"text": "effectively enhances the watermark\u2019s stealthiness. This process\ninvolves a keyed hash function, with the secret key only known\nto the owner, thus enhancing the security of the watermark. We\nthen employ a multi-LLM interaction watermarking technique that\ncomprises a Watermark Generator, Shadow LLM&RAG, and Wa-\ntermark Discriminator, to produce watermark texts based on these\nentity-relationship tuples. This framework significantly improves\nthe quality of the watermark texts, ensuring that the watermark\nknowledge information remains intact and retrievable even after\nbeing processed by adversary-deployed LLMs. Finally, we propose a\nrelevant-text concatenation technique to inject the watermark text\ninto a position that facilitates easy retrieval, thus generating the\nwatermarked RAG. Whenever IP infringement detection is needed,\ne.g., a suspicious LLM exhibits good performance in a domain where\nthe owner\u2019s RAG contains specialized knowledge, we query the\nsuspicious LLM with the watermark question and apply a binomial\ntest to the responses to detect IP infringement.\nWe evaluate our watermark and demonstrate its effectiveness for\nRAG systems in three domain-specific tasks (NQ, HotpotQA, and\nMS-MARCO), two privacy-sensitive tasks (TREC-COVID and NF-\nCorpus), and across four benchmark LLMs: GPT-3.5-Turbo, PaLM 2,\nLlama-2-7B, and Vicuna-13B. First, RAG-WM effectively detects IP\ninfringement of stolen RAGs across various LLM models, achiev-\ning 100% verification success and demonstrating its effectiveness.\nMoreover, RAG-WM does not falsely detect IP infringement of inno-\ncent RAGs without the embedded watermark, demonstrating high\nintegrity. Second, the main performance alignment between the wa-\ntermarked RAG and the clean RAG is 97.87% on average, indicating\ngood fidelity. Third, RAG-WM is robust against attacks that aim to\ndestroy any embedded watermark, such as Paraphrasing, Unrelated\nContent Removal, Knowledge Insertion, and Knowledge Expan-\nsion Attacks. After these attacks, the watermarks still achieve 100%\nverification success. Fourth, RAG-WM is stealthy and not easily\ndetectable by watermark detection methods (Detection by Perplex-\nity and Duplicate Text Filtering), with detection success rates of\n13.05% and 0%, respectively. Finally, we conduct extensive evalua-\ntions using various parameters of RAG system and RAG-WM, and\nRAG-WM achieves 100% verification success.\nContributions. We summarize our contributions as below:\n\u2022We propose RAG-WM, a novel \u201cknowledge watermark\u201d method", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "df869ea6-6ab4-42d5-9253-80e6ac394d87": {"text": "tions using various parameters of RAG system and RAG-WM, and\nRAG-WM achieves 100% verification success.\nContributions. We summarize our contributions as below:\n\u2022We propose RAG-WM, a novel \u201cknowledge watermark\u201d method\nfor RAG systems, which generates high-quality watermark texts\nby the proposed Multi-LLM Interaction technique, effectively pro-\ntecting the IP of RAGs. It ensures reliable watermark verification\nand causes minimal degradation in clean data performance.\n\u2022We comprehensively evaluate the proposed approach on four\ndifferent LLMs and five datasets, and the results demonstrate\neffective watermark performance and good main task perfor-\nmance preservation. We release our watermark implementation\non GitHub [2], contributing to the RAG community to protect IP.\n2 Background and Related Work\n2.1 Retrieval-Augmented Generation (RAG)\nNaive RAGs. Retrieval-augmented generation (RAG) enhances\nlarge language models by integrating external knowledge databases,\nwhich improves accuracy and credibility in knowledge-intensive\ntasks like question-answering [ 7, 53], medical applications [ 37],\ndialogue systems [60], etc. An RAG system comprises three com-\nponents: a knowledge database, a retriever, and a large language\nmodel (LLM). The RAG process involves two main steps: relevant\nknowledge retrieval and answer generation.\nRelevant Knowledge Retrieval. When presented with a question\n\ud835\udc44, RAG retrieves the \ud835\udc58 text records most relevant to \ud835\udc44 from the\nknowledge database \ud835\udc3e\ud835\udc37. The retriever first encodes the question\nusing the text encoder \ud835\udc52 to produce the embedding vector \ud835\udc52(\ud835\udc44).\nIt then applies a similarity metric \ud835\udc60\ud835\udc56\ud835\udc5a (e.g., Cosine Similarity, Eu-\nclidean Distance) to assess the similarity between \ud835\udc52(\ud835\udc44)and each\ntext record \ud835\udc52(\ud835\udc5f\ud835\udc56)in \ud835\udc3e\ud835\udc37, where \ud835\udc5f\ud835\udc56 \u2208\ud835\udc3e\ud835\udc37. Finally, RAG selects \ud835\udc58 text\nrecords {\ud835\udc5f1,...,\ud835\udc5f \ud835\udc58}that are most relevant to question \ud835\udc44 as below:\n{\ud835\udc5f1,\ud835\udc5f2,...,\ud835\udc5f \ud835\udc58}= top-\ud835\udc58(\ud835\udc60\ud835\udc56\ud835\udc5a(\ud835\udc52(\ud835\udc44),\ud835\udc52(\ud835\udc5f\ud835\udc56))),\ud835\udc5f\ud835\udc56 \u2208\ud835\udc3e\ud835\udc37 (1)\nAnswer Generation. Give the question \ud835\udc44, the \ud835\udc58 most relevant\ntext records {\ud835\udc5f1,...,\ud835\udc5f \ud835\udc58}, and an LLM \ud835\udc3f\ud835\udc3f\ud835\udc40(), the output answer is\nobtained by inputting the question and texts into the LLM:\n\ud835\udc4e\ud835\udc5b\ud835\udc60\ud835\udc64\ud835\udc52\ud835\udc5f = \ud835\udc3f\ud835\udc3f\ud835\udc40(\ud835\udc44,{\ud835\udc5f1,...,\ud835\udc5f \ud835\udc58}) (2)\n2", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "87fd0b28-bc53-4dca-942f-d793759da016": {"text": "The knowledge database of RAG is accessible to users in a black-\nbox manner. As a result, the owner can only detect IP infringement\nin a black-box manner. In addition, the adversary may deploy a\nvariety of LLMs, which are unknown to the owner.\nAdvanced RAGs. However, the naive RAG techniques face some\nchallenges in complex deployed scenarios. To solve this problem,\nsome advanced RAG techniques are proposed to improve the per-\nformance of the RAG schedule. Self-RAG [11] trains an LLM that\nadaptively retrieves contexts on-demand and reflects on both re-\ntrieved contexts and their generations to improve the quality of\ngenerated answers. The core idea of these advanced RAG techniques\nis to enhance the relevance of retrieved texts, thereby improving\nthe accuracy of LLM-generated answers. For example, CRAG [65]\nintroduces a lightweight retrieval evaluator that assesses the quality\nof retrieved contexts and provides a confidence score to determine\nwhen knowledge retrieval actions should be triggered, thereby en-\nhancing the robustness and accuracy of RAG. FLARE [28] predicts\nthe upcoming sentence to anticipate future content that is then\nused as the query to retrieve the related documents. IRCoT [ 56]\nintegrates chain-of-thought (CoT) with the retrieval process, using\nCoT to guide retrieval and leveraging the results to enhance CoT.\n2.2 Watermarks of RAGs\n[29] proposes WARD, a black-box RAG dataset inference method\nbased on LLM watermarking to detect unauthorized usage of RAG.\nWARD uses a red-green list watermark to paraphrase all RAG texts\nand detects the watermark\u2019s presence by calculating the green\ntoken ratio in the LLM\u2019s response. However, this watermark lacks\nrobustness against strong text transformations. For instance, an\nattacker can perform paraphrasing attacks to modify the texts of\nthe stolen RAG, which have been shown to effectively remove\nthe red-green list watermark and evade detection [33, 40, 49, 68].\nWARD is also vulnerable to piracy attacks. Since WARD relies on\nparaphrasing, an attacker can use their own LLM and red/green list\n(derived by their hash function and key) to paraphrase all texts to\nfraudulently claim ownership. In contrast, our RAG-WM is robust\nagainst paraphrasing (Section 5.4.1) and piracy attack (Section 6.3).\n[9, 39] propose RAG membership inference attacks, which may\nbe extended as watermarking approaches to detect IP infringement\nof RAG. [39] queries the LLM with a target sample, obtains the\nresponse, and compares two scores ( i.e., the degree of similarity", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "513a2cd6-5665-4762-9931-fba092410e5f": {"text": "be extended as watermarking approaches to detect IP infringement\nof RAG. [39] queries the LLM with a target sample, obtains the\nresponse, and compares two scores ( i.e., the degree of similarity\nbetween the response and the target sample, and the perplexity\nof the output ) for membership inference. However, this attack is\na gray-box attack [29], as it requires gray-box access to the LLM\nfor perplexity calculation. [9] directly prompts the LLM to check if\nthe target sample is present in the context to perform the attack.\nHowever, this attack can be defended by designing secure system\ninstructions to influence the LLM\u2019s output [ 9]. Similarly, if this\nattack is used as a watermark, it can be removed using system\ninstructions. Thus, these approaches are not effective or robust\nenough to serve as reliable black-box watermarks for RAGs.\n2.3 Watermarks on Relational Databases\nFor traditional relational databases, some watermarks [4, 5, 15, 30,\n38, 51, 52, 57, 71] are proposed for ownership protection, proving\ndata integrity, etc. Most current database watermarks are white-box\napproaches, i.e., the owner needs to access the suspicious databases\u2019\ninner information (e.g., values, tuples, and attributes). According\nto [30], these watermarks can be classified into three categories:\nBit-resetting watermarks, data statistic-modifying watermarks, and\nconstrained data content-modifying watermarks. Bit-resetting wa-\ntermarks [4, 5] select some bits of the data values from the target\ndatabase and reset them by a systematic watermarking process.\nFor example, [5] proposes to embed a watermark bit into a tuple\nby computing a hash value after applying a hash function on the\nprimary key and a secret key, and if the hash value is even, \ud835\udc57\ud835\udc61\u210e LSB\n(least significant bits) of the attribute values is set to 0; otherwise,\nit is set to 1. Data statistics-modifying watermarks aim to embed a\npattern (e.g., the bit pattern [51, 52] or the image pattern [57, 71],\nacting as the watermark) into the data statistics (e.g., mean, vari-\nance, and distribution) of the target databases. Constrained data\ncontent-modifying watermarks have been proposed, which embed\nwatermarks by altering the data content. For instance, [15, 38] in-\ntroduce methods for watermarking databases at the tuple level. For\nexample, [15] embeds watermarks by manipulating the ordering\nof database tuples. Notably, relational datasets contain structured\ndata, which differs significantly from the knowledge base of RAGs.\nAdditionally, most of these watermarks are white-box watermarks.", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "7211bc6b-cd66-4c7f-b27b-8c564b0e64e3": {"text": "of database tuples. Notably, relational datasets contain structured\ndata, which differs significantly from the knowledge base of RAGs.\nAdditionally, most of these watermarks are white-box watermarks.\nThus, such watermarks cannot be applied to RAGs, as they are\naccessible to the owner only in a black-box manner.\n2.4 Watermarks of Texts\nText watermarking algorithms are proposed to protect the copy-\nright of textual content. For example, [14, 50] propose format-based\nwatermarks that change the text format to embed watermarks.\nThe format of watermark can be line/word shift, Unicode space\ncharacters (e.g., whitespace (U+0020)), etc. [46, 54, 66] propose wa-\ntermarks by replacing the selected words with their synonyms\nwithout changing the syntactic structure of sentences. In addition,\nsome syntactic-based watermarks [12, 42] are proposed, These wa-\ntermarks introduce the syntax transformations (e.g., Movement,\nClefting, Passivization) to embed watermark. The owner will de-\ntect the watermark by first converting the original and watermark\ntexts to syntax trees and then comparing the structure difference\nfor watermark information extraction. Generation-based water-\nmarks [22, 26, 62, 69] utilize the pre-trained language models to\ndirectly generate the watermark texts from the original texts and\nthe watermark messages. Recently, with the development of large\nlanguage models, some techniques [ 22, 31, 32, 62] are proposed\nto inject watermarks during the text generation process of LLMs.\nKGWs [31] is the most classic work, it partitions the vocabulary\ninto a red list and a green list at each token position, using a hash\nfunction that depends on the preceding token, to inject a watermark.\nThen KGW utilizes z-metric (based on z-test) to calculate the green\ntoken ratio, for the ownership verification. However, these water-\nmarking methods can not directly apply to Retrieval-Augmented\nGeneration (RAG) systems. When such watermarks are embedded\nin the text content of RAGs, post-processing by an adversary\u2019s\ndeployed LLM typically removes the watermark information, in-\ncluding format, syntax, or the red/green token table. Additionally,\nparaphrasing attacks [33, 40, 49, 68] pose significant threats to these\nwatermarks.\n3", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "cdc17770-58de-4905-ad6a-a6cb8b4340e7": {"text": "3 Problem Statement\n3.1 Threat Model\nThe developer or the owner of an RAG system can embed a water-\nmark to detect IP infringement, ensuring it does not compromise its\navailability. If an LLM exhibits exceptional performance in a domain\nwhere the owner\u2019s RAG holds specialized knowledge, the owner\nmay suspect it is using a stolen version of their RAG. To confirm\nthis, the owner can query the LLM and obtain the corresponding\noutputs (through black-box access to the RAG) to extract the water-\nmark from the RAG\u2019s knowledge base for IP infringement detection.\nAn attacker might steal the RAG through an insider attack (e.g., col-\nluding administrators) or an intrusion (e.g., malware infection) and\nintegrate the stolen RAG with their LLM for commercial purposes.\nWe assume that the attacker lacks both the expertise to build an\nRAG system independently, including the specialized knowledge\nin the target RAG\u2019s knowledge base, and the financial resources\nto do so. Otherwise, they would create their own RAG instead of\nstealing. However, the attacker may attempt to detect and remove\nwatermarks from the RAG\u2019s knowledge base to avoid potential law-\nsuits. Following prior studies, we consider that the adversaries can\napply the following techniques to attack the watermarked RAG:\nParaphrasing Attack. Paraphrasing [33, 40, 49, 68] indicates that\nthe adversary can paraphrase the retrieved texts from RAG to per-\nturb the watermark information to evade the verification. This\ntechnique has been applied in defending against RAG poisoning,\nprompt injection, jailbreaking attacks, etc. We extend it as an attack\nmethod against RAG-WM, treating it as a technique to modify the\ndatabase contents without degrading system performance.\nUnrelated Content Removal. Considering that the watermark\ncontent is extra information related to the ownership verification,\nwhich may not be related to the core subject matter of the main\ncontent. The adversary can also manipulate the retrieved text by\nanalyzing the text and removing any incoherent or unrelated sen-\ntences for watermark information removal.\nKnowledge Insertion Attack. Knowledge Insertion Attack involves\nthe adversary inserting additional knowledge or misleading infor-\nmation directly into the RAG\u2019s knowledge base. This added knowl-\nedge can mislead the RAG\u2019s retrieval process against the watermark\nqueries, leading to outputs that either obscure the watermark or\nintroduce noise, thereby undermining the reliability of ownership\nverification in RAG-based systems. Such an attack is similar to the", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "b26cc204-bb7e-4bda-abbb-c3c9209d35e0": {"text": "queries, leading to outputs that either obscure the watermark or\nintroduce noise, thereby undermining the reliability of ownership\nverification in RAG-based systems. Such an attack is similar to the\ntraditional database insertion attack [30].\nKnowledge Expansion Attack. The adversary can effectively di-\nlute the presence of the watermark by increasing the volume of\nnon-watermarked information in the retrieved texts. Specifically,\nRAG-WM injects at most \ud835\udc41\ud835\udc64\ud835\udc5a watermark texts into a knowledge\ndatabase for each watermark question. If the adversary retrieves\n\ud835\udc58 texts, with \ud835\udc58 > \ud835\udc41\ud835\udc64\ud835\udc5a, then it is very likely that at least \ud835\udc58\u2212\ud835\udc41\ud835\udc64\ud835\udc5a\ntexts would be clean ones. As a result, the watermark\u2019s effectiveness\nmay be significantly reduced.\nDetection by Perplexity. The embedding of watermark informa-\ntion may degrade the text quality of the RAG, thus the adversary\ncan detect the low-quality text contents as the the suspicious water-\nmark content. Particularly, perplexity is used to measure the text\u2019s\nquality, and a large perplexity of a text means it is of low quality.\nHMAC \nFunction{E, R}\nMulti-LLM Interaction Watermarking \nWM-Gen\nWM-Disc RAG\n{\ud835\ud835\udc98, \ud835\ud835\udc98}\nKnowledge \nBase of RAG Watermark Information Generation\n(Harry Potter, Include, Pirates of the Caribbean ) \nShadow-LLM&RAG\nHarry Potter and Pirates \nof the Caribbean have an \ninclusive relationship, in \nwhich the adventure spirit \nand fantasy elements of \nboth intertwine.\nFigure 1: Workflow of RAG-WM.\nAdversaries can exploit this by detecting high-perplexity texts to\nattack RAG-WM effectively.\nDuplicate Text Filtering. To increase the success rate of watermark\ncontent retrieval, the owner may inject multiple instances of the\nsame watermark information. However, the adversary could detect\nand filter out duplicate texts from the knowledge database to bypass\nwatermark verification.\n3.2 Requirements of RAG Watermarks\nAn ideal RAG watermarking solution should achieve the following\nproperties: (i) effectiveness: watermark information should be suc-\ncessfully retrieved and remain intact, even after being processed by\nLLMs deployed by adversaries. (ii) robustness: watermarks should\nstill be detected by owners from stolen RAG systems even if the\nRAGs are manipulated in various ways, e.g, paraphrasing, knowl-\nedge insertion, and other attacks; (iii) security: it should be difficult\nfor attackers to forge a new watermark for the stolen RAG; (iv) in-\ntegrity: it should be highly unlikely for owners to detect IP infringe-\nment over innocent RAGs; (v) stealthiness: it should be difficult", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "d283118a-6e4c-41ab-a75f-4137a81f49c5": {"text": "for attackers to forge a new watermark for the stolen RAG; (iv) in-\ntegrity: it should be highly unlikely for owners to detect IP infringe-\nment over innocent RAGs; (v) stealthiness: it should be difficult\nfor attackers to learn the existence of watermark from the stolen\nRAG; and (vi) fidelity: watermark-embedding should introduce little\nimpact on the performance of the original RAGs.\n4 Watermarking Approach\nFigure 1 illustrates the workflow of our \u201cknowledge watermark\u201d\napproach (RAG-WM). First, based on a well-constructed RAG sys-\ntem, the owner extracts entities and relations from the knowledge\ndatabase, generating a list of entities and relations {\ud835\udc38,\ud835\udc45}as can-\ndidates for watermarking. To create the watermark entities and\nrelations {\ud835\udc38\ud835\udc64\ud835\udc5a,\ud835\udc45\ud835\udc64\ud835\udc5a}(i.e., the watermark-related knowledge), we\napply an HMAC function to {\ud835\udc38,\ud835\udc45}using the owner\u2019s signature as\nthe secret key. For each tuple of watermark entities and their corre-\nsponding relations (\ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a,\ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a,\ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a)in {\ud835\udc38\ud835\udc64\ud835\udc5a,\ud835\udc45\ud835\udc64\ud835\udc5a}, we employ a\nmulti-LLM interaction technique to generate watermark texts. This\ntechnique consists of three components: WM-Gen (Watermark Gen-\nerator), Shadow-LLM&RAG, and WM-Disc (Watermark Discrimina-\ntor). Through their interaction, we produce high-quality watermark\ntexts embedded with the watermark knowledge (\ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a,\ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a,\ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a).\nThese generated watermark texts are then integrated into the RAG\nsystem to create a watermarked version.\n4", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "9167c777-baea-4cdc-bc55-9cd9a656ac11": {"text": "4.1 Watermark Information Generation\nTo inject watermarks into an RAG system, the owner can manipu-\nlate its key components: the knowledge base, retriever, and LLM.\nInjecting watermarks into the LLM or retriever is not ideal, as at-\ntackers can easily replace these models with clean, unwatermarked\nmodels. However, the knowledge base, which contains crucial doc-\nument chunks, is the most valuable part of the RAG. An adversary\ncannot remove the watermark without damaging the knowledge\nbase and rendering the system unusable. Therefore, we inject a\n\u201cknowledge watermark\u201d into the knowledge base.\nThe watermark knowledge is injected into the knowledge base\nas texts, which can be abstracted into entities and relations. That\nis, we first represent the watermark as a set of tuples in the form\n(\ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a,\ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a,\ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a), where \ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a and \ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a are entities, and \ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a de-\nnotes the relation between them. This structured form will simplify\nboth watermark generation and IP infringement detection. Impor-\ntant, the watermark injection process must preserve the RAG\u2019s\navailability and the effectiveness of the watermarks. Moreover,\nsince the adversary lacks expertise in the target RAG\u2019s knowledge\nbase (as outlined in Section 3.1), we should inject watermark in-\nformation that includes entities or relations originally part of the\nknowledge base, enhancing the watermark\u2019s stealthiness. Addition-\nally, to verify the watermark, the injected entities and their relations\nmust be authentic but include deliberate inaccuracies known only\nto the owner. These \u201cintentional inaccuracies\u201d can then be extracted\nfrom the LLM\u2019s outputs to detect IP infringement.\nEntities and Relations Extraction. We begin by extracting enti-\nties and relations from the original knowledge base \ud835\udc3e\ud835\udc37, which will\nserve as candidates for constructing the watermark\u2019s entities and\nrelations. Specifically, we employ a large language model (LLM)\n(e.g., LLM Graph Transformer) to parse and categorize entities and\ntheir relations from the text documents within \ud835\udc3e\ud835\udc37. However, ex-\ntracting all entities and relations from the extensive text documents\nin \ud835\udc3e\ud835\udc37 would be costly due to their sheer volume. Therefore, we\ncan randomly select a subset of text documents to create the entity\nlist \ud835\udc38and relations list \ud835\udc45as follows:\n{\ud835\udc38,\ud835\udc45}= ParseER(Sample(\ud835\udc3e\ud835\udc37,\ud835\udc51 )) (3)\nwhere Sample(\ud835\udc3e\ud835\udc37,\ud835\udc51 )represents a random sample of \ud835\udc51 documents\nfrom the knowledge base \ud835\udc3e\ud835\udc37, and ParseER(\u00b7)denotes the process\nof parsing entities and relations from the sampled documents using", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "a0f2f93c-8acc-4cde-afdc-e688ffcdc783": {"text": "{\ud835\udc38,\ud835\udc45}= ParseER(Sample(\ud835\udc3e\ud835\udc37,\ud835\udc51 )) (3)\nwhere Sample(\ud835\udc3e\ud835\udc37,\ud835\udc51 )represents a random sample of \ud835\udc51 documents\nfrom the knowledge base \ud835\udc3e\ud835\udc37, and ParseER(\u00b7)denotes the process\nof parsing entities and relations from the sampled documents using\nthe LLM. Since the raw entity list\ud835\udc38and relations list \ud835\udc45may include\nrare types of entities and relations, we reduce the lists by focusing\non high-frequency entities and relationships to avoid outliers, en-\nhancing the watermark\u2019s stealthiness. Thus, we generate the final\nentity list \ud835\udc38and relations list \ud835\udc45, with the size as |\ud835\udc38|and |\ud835\udc45|.\nWatermark Entities and Relations Generation. To construct\nthe watermark texts, we generate a set of tuples in the format\n(\ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a,\ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a,\ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a)based on the entity list \ud835\udc38and relations list \ud835\udc45. For\nIP infringement detection, the owner\u2019s signature or identifier (ID)\nmust be embedded within these watermark tuples. However, em-\nbedding the signature or ID directly into the entities or relations\nwould compromise stealthiness and reduce the quality of the wa-\ntermark. To address this, we use the signature as a secret key\ud835\udc58\ud835\udc52\ud835\udc66\nin an HMAC (keyed cryptographic hash function) to generate the\nwatermark tuples (\ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a,\ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a,\ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a). Specifically, we generate a wa-\ntermark entities list \ud835\udc38\ud835\udc64\ud835\udc5a for embedding watermark information.\nThe first entity \ud835\udc520\ud835\udc64\ud835\udc5a is initialized as \ud835\udc41\ud835\udc62\ud835\udc59\ud835\udc59, and the subsequent enti-\nties are generated as follows:\n\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65(\ud835\udc52\ud835\udc56+1\n\ud835\udc64\ud835\udc5a)= (\ud835\udc3b\ud835\udc40\ud835\udc34\ud835\udc36(\ud835\udc58\ud835\udc52\ud835\udc66,\ud835\udc52\ud835\udc56\n\ud835\udc64\ud835\udc5a))% |\ud835\udc38| (4)\nwhere \ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a \u2208\ud835\udc38\ud835\udc64\ud835\udc5a is the \ud835\udc56-th watermark entity. This Equation\nindicates that we perform a hash operation using the current entity\n\ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a and the secret \ud835\udc58\ud835\udc52\ud835\udc66, then compute the index of next entity\ud835\udc52\ud835\udc56+1\ud835\udc64\ud835\udc5a\nby applying modulo operation over the size of the entity list \ud835\udc38.\nNext, based on \ud835\udc38\ud835\udc64\ud835\udc5a, we need to establish the relations between\nthese entities. Entities can be treated as nodes in a graph, with\nthe relations representing edges. The existence of a relation be-\ntween two entities (\ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a and \ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a) is determined probabilistically\naccording to a specific probability \ud835\udc5d1. When a relation exists, it is\ngenerated as follows:\n\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65(\ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a)= (\ud835\udc3b\ud835\udc40\ud835\udc34\ud835\udc36(\ud835\udc58\ud835\udc52\ud835\udc66,\ud835\udc52\ud835\udc56\n\ud835\udc64\ud835\udc5a,\ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a))% |\ud835\udc45| (5)\nwhere \ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a \u2208\ud835\udc45\ud835\udc64\ud835\udc5a is the relation constructed based on adjacent\nentities (\ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a and \ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a) and the secret \ud835\udc58\ud835\udc52\ud835\udc66. This process contin-\nues until the target number of watermark tuples is generated. By\nverifying the relations between entities within the watermark, we\ncan confirm ownership while preventing fraudulent claims. The\nsecurity of this approach lies in the difficulty an attacker has in\nforging a valid HMAC, generating an accurate entity-relationship", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "bbe54066-8c68-4c27-a81b-9b9911fe3c12": {"text": "can confirm ownership while preventing fraudulent claims. The\nsecurity of this approach lies in the difficulty an attacker has in\nforging a valid HMAC, generating an accurate entity-relationship\nlist, and providing the secret key. HMAC\u2019s cryptographic nature\nensures a unique and tamper-resistant output, making it an effec-\ntive tool for watermarking and protecting intellectual property, as\ndemonstrated in applications like watermarking DNN models [3]\nand texts [31, 32].\n4.2 Watermark Injection\nGiven the structural watermark knowledge (including the water-\nmark entities and the corresponding relations), we must convert\nit into high-quality watermark document chunks and place them\nappropriately in the knowledge base. First, if the generated wa-\ntermark texts are of low quality, this will complicate verification,\nparticularly in two ways: (i) low-quality watermark text may not\nbe retrievable by RAG systems; (ii) even if retrievable, an attacker\u2019s\nLLM might fail to extract crucial watermark knowledge, hindering\nIP infringement detection. Second, strategically placing the gener-\nated watermark text will improve its retrieval success rate, allowing\nthe owner to extract the watermark more easily while enhancing\nits robustness and stealthiness. To address these challenges, we\npropose the Multi-LLM interaction watermarking approach for\ngenerating these watermark texts.\nMulti-LLM interaction watermarking Technique. There are\nthree components in the interaction framework, including WM-\nGen, Shadow-LLM&RAG, and WM-Disc, as shown in Figure 1. Dur-\ning the interaction process, WM-Gen generates multiple watermark\ntexts and stores them into the RAG of Shadow-LLM&RAG system.\nWM-Disc then utilizes the watermark verification question \ud835\udc4a\ud835\udc44 to\nquery the Shadow-LLM&RAG system, obtaining the Answer \ud835\udc34\ud835\udc5b\ud835\udc60.\nIt checks whether the \ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a (the relation between \ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a and \ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a)\n1The probability \ud835\udc5dis set 0.05 in our evaluation.\n5", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "74c6c121-d9fe-454d-9b6a-12b9fe714ae4": {"text": "is present in the processed answer \ud835\udc34\ud835\udc5b\ud835\udc60. If \ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a is detected, it indi-\ncates that WM-Gen successfully embeds the watermark knowledge\n(\ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a,\ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a,\ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a)into the RAG. Otherwise, WM-Disc provides feed-\nback to WM-Gen to retry watermark generation until successful\nembedding of the information or the maximum number of interac-\ntion epochs is reached. Through this iterative process, we can suc-\ncessfully embed all watermark tuples\u2019 information for{\ud835\udc38\ud835\udc64\ud835\udc5a,\ud835\udc45\ud835\udc64\ud835\udc5a}\ninto the RAG.\n\u2022Shadow-LLM&RAG. The owner constructs a local Shadow LLM\nand watermarked RAG to simulate the scenario where adversaries\ndeploy the stolen RAG with their LLM. The Shadow LLM is de-\nployed to mimic the adversarial system, thus it can differ from the\nadversaries\u2019 actual LLM.\n\u2022Watermark Discriminator (WM-Disc). WM-Disc can query this\nshadow system to improve the quality of the watermark text\ud835\udc4a\ud835\udc47\n(generated by WM-Gen based on (\ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a,\ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a,\ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a)) using a set of\nwatermark verification questions \ud835\udc4a\ud835\udc44 (e.g., \u201cWhat can you tell me\nabout \ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5aand \ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a?\u201d or \u201cHow are\ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5aand \ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5arelated?\u201d). The sys-\ntem retrieves the most related content from the RAG based on \ud835\udc4a\ud835\udc44\nand then uses the shadow LLM to generate processed answers \ud835\udc34\ud835\udc5b\ud835\udc60.\nEquation 6 defines this retrieval and answer generation process:\n\ud835\udc34\ud835\udc5b\ud835\udc60= Shadow-LLM(\ud835\udc4a\ud835\udc44, RAG(\ud835\udc4a\ud835\udc44)) (6)\nWM-Disc then analyzes whether the relation \ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a between en-\ntities \ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a and \ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a is present in the processed answers \ud835\udc34\ud835\udc5b\ud835\udc60 of\nShadow-LLM&RAG system. This step ensures watermark relations\nare properly embedded and can be successfully retrieved from\nLLM\u2019s responses. Discrimination function \ud835\udc37 is defined as below:\n\ud835\udc37 = Disc(\ud835\udc34\ud835\udc5b\ud835\udc60,\ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a) (7)\nwhere \ud835\udc37 checks if the processed answer \ud835\udc34\ud835\udc5b\ud835\udc60contains the correct\nwatermark relation \ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a, confirming the validity of the watermark.\n\u2022Watermark Generator (WM-Gen). WM-Gen firstly generates wa-\ntermark text \ud835\udc4a\ud835\udc47 and then inserts \ud835\udc4a\ud835\udc47 into RAG to generate the\nwatermarked RAG\ud835\udc64\ud835\udc5a. First, WM-Gen uses an LLM to generate\nwatermark texts \ud835\udc4a\ud835\udc47, ensuring that each text includes the entities\n\ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a and \ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a, along with the relation\ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a between them, for all tu-\nples (\ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a,\ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a,\ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a)in the set of watermark entity-relation pairs\n{\ud835\udc38\ud835\udc64\ud835\udc5a,\ud835\udc45\ud835\udc64\ud835\udc5a}. This process can be defined as follows:\n\ud835\udc4a\ud835\udc47 = LLM\ud835\udc64\ud835\udc5a(\ud835\udc52\ud835\udc56\n\ud835\udc64\ud835\udc5a,\ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a,\ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a). (8)\nWhere LLM\ud835\udc64\ud835\udc5a is the LLM configured with our watermark gen-\neration prompt (as detailed in Appendix B.2.1). Additionally, con-\nsidering that RAG can retrieve top k text highly relevant to the\nquestion from the knowledge base, to improve the watermark text", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "be0aa8d2-efa3-4297-bb68-08f32a122d76": {"text": "eration prompt (as detailed in Appendix B.2.1). Additionally, con-\nsidering that RAG can retrieve top k text highly relevant to the\nquestion from the knowledge base, to improve the watermark text\nsuccess retrieval rate, we can generate and inject multiple diverse\n\ud835\udc4a\ud835\udc47 according to the same (\ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a,\ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a,\ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a)by setting the prompt.\nNext, we inject the watermark text \ud835\udc4a\ud835\udc47 into the original RAG\nand generate the watermarked RAG\ud835\udc64\ud835\udc5a. It is crucial that the water-\nmark can be successfully retrieved during the verification process.\nTherefore, we should inject the watermark in a position that fa-\ncilitates easy retrieval. We propose a watermark injection based\non relevant-text concatenation. Specifically, to embed the water-\nmark text \ud835\udc4a\ud835\udc47 generated from (\ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a,\ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a,\ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a), we first use the\nwatermark query \ud835\udc4a\ud835\udc44 related to (\ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a,\ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a)to retrieve the most\nrelevant text \ud835\udc47\ud835\udc38\ud835\udc4b\ud835\udc47 from the original RAG. Although it is unlikely\nthat the original RAG contains content directly linking both \ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a\nand \ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a together, we can always find the most similar content for\nwatermark embedding. The retrieval process is defined as follows:\n\ud835\udc47\ud835\udc38\ud835\udc4b\ud835\udc47 = RAG-Retrieve(\ud835\udc4a\ud835\udc44), \ud835\udc4a\ud835\udc44for (\ud835\udc52\ud835\udc56\n\ud835\udc64\ud835\udc5a,\ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a) (9)\nOnce \ud835\udc47\ud835\udc38\ud835\udc4b\ud835\udc47 is retrieved, WM-Gen performs a text concatenation\noperation to entangle the watermark text \ud835\udc4a\ud835\udc47 with the retrieved\ntext \ud835\udc47\ud835\udc38\ud835\udc4b\ud835\udc47 as follows:\n\ud835\udc47\ud835\udc38\ud835\udc4b\ud835\udc47 \u2295\ud835\udc4a\ud835\udc47 = Concatenate(\ud835\udc47\ud835\udc38\ud835\udc4b\ud835\udc47,\ud835\udc4a\ud835\udc47 ) (10)\nThis improves watermark retrieval performance and makes it harder\nfor adversaries to remove the watermark without disrupting the\noriginal knowledge in the RAG.\nTo ensure the quality of the concatenated text\ud835\udc47\ud835\udc38\ud835\udc4b\ud835\udc47 \u2295\ud835\udc4a\ud835\udc47, WM-\nGen further utilizes an LLM to evaluate the semantic coherence of\nthe result. This step is crucial for preserving the fluency and logical\nstructure of the combined text. If the semantic coherence check\npasses, the concatenated text is considered valid. If not, WM-Gen\nadjusts the generated text to improve the quality. Finally, we embed\nthe generated the watermark text \ud835\udc4a\ud835\udc47 into the RAG and generated\n\ud835\udc45\ud835\udc34\ud835\udc3a\ud835\udc64\ud835\udc5a as below:\n\ud835\udc45\ud835\udc34\ud835\udc3a\ud835\udc64\ud835\udc5a = RAG \u222a\ud835\udc4a\ud835\udc47 (11)\nwhere \u222arepresents relevant-text concatenation.\n4.3 IP Infringement Detection\nIf a suspicious LLM demonstrates exceptional performance in a do-\nmain where the owner\u2019s RAG contains specialized knowledge, the\nowner may suspect the LLM is using a stolen version of\ud835\udc45\ud835\udc34\ud835\udc3a\ud835\udc64\ud835\udc5a. IP\ninfringement detection can be conducted using the WM-Disc com-\nponent. Specifically, we randomly select\ud835\udc5btuples of (\ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a,\ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a,\ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a)\nfrom {\ud835\udc38\ud835\udc64\ud835\udc5a,\ud835\udc45\ud835\udc64\ud835\udc5a}, generate the corresponding watermark queries\n\ud835\udc4a\ud835\udc44, and execute the watermark discrimination operation (as Equa-", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "45e5727f-5e98-4fe5-96f5-81f6a56628df": {"text": "ponent. Specifically, we randomly select\ud835\udc5btuples of (\ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a,\ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a,\ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a)\nfrom {\ud835\udc38\ud835\udc64\ud835\udc5a,\ud835\udc45\ud835\udc64\ud835\udc5a}, generate the corresponding watermark queries\n\ud835\udc4a\ud835\udc44, and execute the watermark discrimination operation (as Equa-\ntion (6) and Equation (7)). This process calculates how many water-\nmark entity pairs (i.e., \ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a,\ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a) successfully retrieve their correct\nrelation \ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a, resulting in a count\ud835\udc50\ud835\udc64\ud835\udc5a. We can verify the watermark\nby Binomial Test.\nNull Hypothesis \ud835\udc3b0: The suspicious LLM is not equipped with\nour watermarked RAG, so the probability of outputting relation\ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a\nis \ud835\udc5d0 (\ud835\udc5d0 = 1\n\ud835\udc5b\ud835\udc5f , where\ud835\udc5b\ud835\udc5f is the total number of relations in the RAG);\nAlternative Hypothesis \ud835\udc3b1: The suspicious LLM is equipped with\nour watermarked RAG, and the probability of outputting relation\n\ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a is significantly greater than \ud835\udc5d0. This is a one-tailed test, as\nwe are interested in whether the probability of outputting relation\n\ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a is greater than that of a random LLM (i.e., \ud835\udc5d0). The calculated\np-value from the binomial test is:\n\ud835\udc43(\ud835\udc4b = \ud835\udc50\ud835\udc64\ud835\udc5a)=\n\u0012 \ud835\udc5b\n\ud835\udc50\ud835\udc64\ud835\udc5a\n\u0013\n\ud835\udc5d\ud835\udc50\ud835\udc64\ud835\udc5a\n0 (1 \u2212\ud835\udc5d0)\ud835\udc5b\u2212\ud835\udc50\ud835\udc64\ud835\udc5a . (12)\nwhere \ud835\udc5brepresents the number of queries, and \ud835\udc50\ud835\udc64\ud835\udc5a represents the\ncount of successfully retrieved watermark relations. If the p-value is\nsignificantly lower than the common significance level\ud835\udefc= 0.05, we\nreject the null hypothesis2. This suggests that the probability of the\nsuspicious model outputting relation \ud835\udc5f\ud835\udc56,\ud835\udc57\n\ud835\udc64\ud835\udc5a is significantly greater\n2\ud835\udefc = 0.05 is commonly used in hypothesis testing [61].\n6", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "8a74c3cd-fa8d-4420-99af-af8113fca296": {"text": "than \ud835\udc5d0, i.e., the suspect LLM is deployed with our watermark RAG\n(i.e., \ud835\udc45\ud835\udc34\ud835\udc3a\ud835\udc64\ud835\udc5a).\nMainstream knowledge bases typically contain numerous rela-\ntions, e.g., TREC-COVID (\ud835\udc5b\ud835\udc5f = 127,764), NFCorpus (\ud835\udc5b\ud835\udc5f = 75,179),\nand NQ (\ud835\udc5b\ud835\udc5f = 41,763), as shown in Table 11. Given that \ud835\udc5b\ud835\udc5f gen-\nerally exceeds 100, \ud835\udc5d0 becomes smaller than 1\n100 . For example, in\na knowledge base with 100 relations, querying the suspect RAGs\nwith \ud835\udc5b = 30 watermark queries and using \ud835\udc5d0 = 1\n100 allows us to\nreject the null hypothesis if\ud835\udc50\ud835\udc64\ud835\udc5a > 2. This corresponds to a p-value\nsmaller than 4 \u00d710\u22123, which is significantly below the significance\nlevel \ud835\udefc = 0.05. Thus, the watermark is detected when three or more\nqueries produce outputs that match the watermarked relations.\n5 Evaluation\nBased on the watermark destruction approaches discussed in Threat\nModel (Section 3.1), we evaluate RAG-WM in the following aspects.\n(i) Effectiveness (Section 5.2). Watermarks should be embedded into\nRAG and detected by the owners from the stolen RAG in a black-box\nmanner (i.e., the owner queries the adversary\u2019s deployed LLM and\nRAG systems), and the watermark task should have little impact on\nthe original task\u2019s performance of watermarked RAGs. (ii) Impact\nof Parameters (Section 5.3). We evaluate how the parameters of\nRAG and RAG-WM influence the performance of RAG-WM. (iii) Ro-\nbustness (Section 5.4). The watermark should still be detected even\nif the encoders suffer from watermark attacks, e.g., paraphrasing,\nunrelated content removal, knowledge insertion, and knowledge\nexpansion. (iv) Stealthiness (Section 5.5). The watermark should be\nstealthy against detection methods (e.g., detection by perplexity, and\nduplicate text filtering). (v) Advanced RAG Systems (Section 5.6).\nIn addition to the naive RAG, our watermark should effectively\nprotect the IP of the state-of-the-art advanced RAG systems, e.g.,\nSelf-RAG, CRAG.\n5.1 Experimental Setup\nDatasets & LLMs. We use five benchmark datasets commonly\nemployed in RAG for question-answering tasks. NQ [ 34], Hot-\npotQA [67], and MS-MARCO [13] are widely used datasets, and\nwatermarks help protect the significant effort invested in their cre-\nation. TREC-COVID [58] and NFCorpus [16] are privacy-sensitive\ndatasets, and watermarks support IP infringement detection. A de-\ntailed introduction to these datasets is provided in Appendix A. We\nutilize four mainstream and representative large language models,\nincluding the black-box LLMs (GPT-3.5-Turbo [17] and PaLM 2 [10])\nby calling their APIs and the white-box LLMs (Llama-2-7B [55] and", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "7081482c-0b24-438b-923a-e313411d8807": {"text": "utilize four mainstream and representative large language models,\nincluding the black-box LLMs (GPT-3.5-Turbo [17] and PaLM 2 [10])\nby calling their APIs and the white-box LLMs (Llama-2-7B [55] and\nVicuna-13B [18]) to evaluate the effectiveness of our RAG-WM.\nRAG Systems. We evaluate RAG systems by configuring various\ntypes of retrievers and incorporating diverse expertise into the\nknowledge database.\n\u2022Retriever. We deploy three commonly used retriever models, in-\ncluding Contriever [24], Contriever-ms [24], and ANCE [63] to gen-\nerate the sentence embeddings. To measure the distance between\nthe query and the retrieved documents, we apply three distance\nmetrics: Euclidean distance, Inner Product, and Cosine similarity.\n\u2022Knowledge Database. We store the text content of each of the\nfive dataset into the knowledge dataset for RAG. Specifically, we\nTable 1: Watermark Verification\nDataset Metrics LLMs\nGPT-3.5 Llama Vicuna PaLM\nTREC- WSN 18 18 20 17\nCOVID WIRR 96.67% 96.67% 100.00% 100.00%\nNFCorpus WSN 26 24 24 20\nWIRR 100.00% 100.00% 100.00% 93.33%\nNQ WSN 24 19 18 20\nWIRR 93.33% 93.33% 90.00% 90.00%\nHotpotQA WSN 27 20 22 19\nWIRR 100.00% 100.00% 100.00% 100.00%\nMS- WSN 23 19 18 20\nMARCO WIRR 90.00% 96.67% 90.00% 90.00%\nuse the open-source embedding database Chroma [6] to store text\nembeddings and associated metadata.\nWatermark Setting. Due to space limitations, we introduce the\nwatermark setting in Appendix B.\nEvaluation Metrics. We evaluate RAG-WM using these metrics.\n\u2022Watermark Information Retrieval Ratio (WIRR) measures the pro-\nportion of watermark queries that successfully retrieve the embed-\nded watermark texts from the total number of watermark queries.\n\u2022Watermark Success Number (WSN) evaluates the number that a\nLLM correctly classifies watermark queries as the target watermark\nrelations label. In particular, we utilize 30 watermark queries for IP\ninfringement detection. As long as WSN is larger than 2, we can\nsuccessfully detect the IP infringement of Watermarked RAG3, as\nintroduced in Section 4.3.\n\u2022Clean Data Performance Alignment (CDPA). Clean Data Perfor-\nmance (CDP) evaluates main task performance of the LLM deployed\nthe RAG system. CDPA measures the proportion of questions for\nwhich the clean RAG and the watermarked RAG produce the same\nanswer, calculated as the ratio of such questions to the total number\nof clean questions.\n\u2022Clean Information Retrieval Alignment (CIRA) measures the re-\ntrieval alignment of main task texts when queried from the water-", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "3a989ff0-15da-4d29-9c53-ffdde506e79c": {"text": "answer, calculated as the ratio of such questions to the total number\nof clean questions.\n\u2022Clean Information Retrieval Alignment (CIRA) measures the re-\ntrieval alignment of main task texts when queried from the water-\nmarked knowledge database versus the clean knowledge database.\nIt represents the proportion of clean questions for which the clean\nRAG and watermarked RAG retrieve the same text.\nUnless otherwise specified, we use GPT-3.5-Turbo, configured\nwith prompts detailed in Appendix B.2.2 and Appendix B.3, to help\nmeasure the WSN and CDPA. Additionally, human evaluations are\nconducted, showing similar performance results to those of the\nLLM-based evaluation, as shown in Section 5.2.\nPlatform. All experiments are conducted on a server with 64-bit\nUbuntu 20.04 LTS system with Intel(R) Xeon(R) Silver 4214 CPU @\n2.20GHz, 692GB memory, and four Tesla V100 GPUs (32GB each).\n5.2 Effectiveness\nIn this subsection, we evaluate the effectiveness of RAG-WM in\nterms of watermark verification, main-task performance, integrity,\ntime consumption, and human evaluation.\n3We test various values for watermark queries (i.e., \ud835\udc5b \u2208 [10,200]) and record the\nwatermark success number (\ud835\udc50\ud835\udc64\ud835\udc5a). Using these values, we calculate the p-value, which\nis always less than the significance level\ud835\udefc = 0.05, indicating 100% verification success.\nReferring to [27, 41], which use 30 watermark queries for verification, we adopt the\nsame number of queries in our evaluation.\n7", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "228f390e-ad05-4c06-8e31-f322c073c4d2": {"text": "Table 2: Main Task Performance\nDataset TREC- NFCorpus NQ HotpotQA MS-\nCOVID MARCO\nCDPA 98.00% 96.90% 99.60% 97.70% 97.10%\nCIRA 96.40% 89.16% 94.66% 96.94% 98.68%\nEffectiveness of Watermark Verification. To evaluate the effec-\ntiveness of RAG-WM, we inject watermark texts into the knowledge\nbases of TREC-COVID, NFCorpus, NQ, HotpotQA, and MS-MARCO\ntasks. Specifically, we insert 237, 246, 184, 191, and 230 watermark\ntexts, occupying 0.1383%, 6.7713%, 0.0069%, 0.0036%, and 0.0026% of\nthe original databases, respectively. Next, we simulate an adversary\ndeploying the stolen, watermarked database with their LLMs (in-\ncluding GPT-3.5-Turbo, PaLM 2, Llama-2, and Vicuna-13B) to create\nRAG systems. Acting as the database owner, we then randomly\nselect 30 injected watermark tuples and use watermark questions\n(\u201cWhat is the relationship between \ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a and \ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a?\u201d) to query these\nRAG systems (equipped with various LLMs and knowledge bases)\nfor IP infringement detection. The experimental results in Table 1\nindicate that RAG-WM achieves effective watermark verification\nacross various LLMs and datasets. The minimum watermark success\ncounts (WSN) for the TREC-COVID, NFCorpus, NQ, HotpotQA, and\nMS-MARCO datasets are 18, 20, 18, 19, and 18, respectively. These\nvalues are far higher than the threshold of 2 required to detect IP in-\nfringement of the watermarked RAG. Besides, the results also show\nthat the watermark texts are successfully retrieved from the water-\nmarked knowledge database, achieving a watermark information\nretrieval ratio (WIRR) of greater than or equal to 90% across vari-\nous databases. This high WIRR highlights the effectiveness of our\nwatermark embedding method, which integrates watermark texts\ninto the most relevant database entries to ensure efficient retrieval.\nThe WIRR for MS-MARCO is lower than for the other knowledge\nbases, likely due to its large size, which makes watermark retrieval\nmore challenging.\nMain Task Performance (Fidelity). We evaluate the clean data\nperformance alignment (i.e., CDPA) between watermarked and\nclean RAGs using the main task questions from these five datasets.\nTo achieve this, we retrieve relevant texts for these questions from\nthe knowledge bases of the evaluated RAGs and input them into the\nLlama-2-7B and Vicuna-13B models. These models are selected for\ntheir white-box nature, which allows for easier control (e.g., token\nsampling strategies) compared to black-box models like GPT-3.5-\nTurbo and PaLM-2, which are less stable due to factors such as latent", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "dafe6e06-810c-460d-968e-b12c74f16ebd": {"text": "their white-box nature, which allows for easier control (e.g., token\nsampling strategies) compared to black-box models like GPT-3.5-\nTurbo and PaLM-2, which are less stable due to factors such as latent\nvariable states4. Moreover, since the evaluation results for Llama-2-\n7B and Vicuna-13B are similar, we only present the results for Llama-\n2-7B in Table 2. We can see that the average CDPA across these\ntasks is 97.87%, demonstrating the good performance in maintaining\nthe main task. Moreover, we also assess the clean information\nretrieval alignment (i.e., CIRA) between clean and watermarked\nRAGs. The value is 95.17% on average. We analyze the primary\nfactor affecting CIRA: when entities in clean queries overlap with\nthose in watermark texts, the watermark texts can interfere with\nclean information retrieval. However, this interference decreases\nas the dataset size grows. For example, MS-MARCO, with 8,841,823\n4GPT-3.5-Turbo may produce different answers to the same question because of non-\ndeterministic sampling during inference.\ntexts, has a CIRA of 97.34%, while NFCorpus, with 3,633 texts, has\na CIRA of 89.16%.\nIntegrity.We evaluate the integrity of RAG-WM by testing whether\nit will detect IP infringement over innocent RAGs not stolen from\nours. In this experiment, we assess RAG-WM on clean RAGs using\nfour LLMs across five tasks. Ideally, no IP infringement should be\ndetected in these clean RAGs, meaning their WSN should be less\nthan or equal to 2. The evaluation results show that the WSN of\nRAG-WM in clean RAGs is always 0, indicating that RAG-WM does\nnot falsely detect IP infringement in these clean RAGs.\nHuman Evaluation on Watermark Verification. The above eval-\nuation results for WSN are obtained with the help of GPT-3.5-Turbo\n(configured with prompts detailed in Appendix B.2.2). To validate\nthis approach, we performed a human evaluation. This involves\nmanually verifying whether watermark relationships are contained\nin the responses produced by the adversary\u2019s deployed LLMs and\nRAG systems. The evaluation results are shown in Figure 10 of\nAppendix. Notably, the results of the LLM-based evaluation align\nclosely with those of the human evaluation, with a difference of no\nmore than 3 in WSN, demonstrating high consistency. Such a high\nconsistency between LLM-based and human evaluations highlights\nthe LLMs as a reliable tool for watermark detection.\nTime Consumption. The watermarking process introduces ad-\nditional time compared to a clean RAG. We use GPT-3.5-Turbo", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "32c872be-8b46-4c95-af09-9879617816d3": {"text": "consistency between LLM-based and human evaluations highlights\nthe LLMs as a reliable tool for watermark detection.\nTime Consumption. The watermarking process introduces ad-\nditional time compared to a clean RAG. We use GPT-3.5-Turbo\nto generate the watermark texts. Table 3 shows that the average\ntime for generating each watermark text is 9.40 seconds, which is\nacceptable for the owner, as watermarking the RAG\u2019s knowledge\nbase is a one-time task for the owner.\nTable 3: Time Consumption\nDataset TREC- NFCorpus NQ HotpotQA MS-\nCOVID MARCO\nTime 5.90 9.99 10.11 10.94 10.05(seconds)\n5.3 Impact of Parameters\nThe performance of our watermark approach is related to several\nfactors, including the parameters of the RAGs (retriever models,\nsimilarity metrics, and the\ud835\udc58value for retrieval top\ud835\udc58related texts, as\ndefined in equation (1)), as well as the parameters of RAG-WM (e.g.,\nthe number of injected watermark tuples, the number of watermark\ntexts per tuple, and the watermark queries). We evaluate the impact\nof these factors using three datasets: REC-COVID, NFCorpus, and\nMS-MARCO, which differ in scale and knowledge domain. LLaMA-\n2-7B is used as the adversary\u2019s LLM for the RAG system. Due to\nspace limitations, we present the results of impact of watermark\nqueries in Appendix E.\n5.3.1 Impact of Retriever Models We evaluate three widely used re-\ntriever models: Contriever [24], Contriever-ms [24], and ANCE [63].\nThe three retriever models employ mainstream strategies such as\nunsupervised training, supervised fine-tuning, and retrieval with\napproximate nearest neighbor optimization, allowing for a thor-\nough performance evaluation. Figure 2 shows the evaluation results\nof the effectiveness of RAG-WM on these retrievers. Our results\nshow that RAG-WM consistently performs well across all retrievers,\n8", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "33b66a0b-fcc9-4cd2-a708-8f4ef8f78064": {"text": "with average watermark success numbers (WSN) of 20, successfully\ndetecting IP infringement. This is because the watermark texts are\nsemantically aligned with the watermark queries, achieving an av-\nerage retrieval rate of 95.93% (i.e., WIRR), making them more likely\nto be retrieved by various retrievers.\nContrieverContriever-msANCE\n0\n10\n20\n30\nWSN\n(a) TREC-COVID\nContrieverContriever-msANCE\n(b) NFCorpus\nContrieverContriever-msANCE\n(c) MS-MARCO\nFigure 2: Impact of Retrievers.\n5.3.2 Impact of Similarity Metrics To assess the impact of simi-\nlarity metrics, we evaluate three metrics: cosine similarity, inner\nproduct, and Euclidean distance to calculate the similarity of embed-\nding vectors when retrieving texts from a watermarked knowledge\ndatabase for a watermark query. Figure 3 shows the results of these\nsimilarity metrics. We observe that RAG-WM produces consistent\nresults across the three tasks. Specifically, the WSN values are simi-\nlar for each similarity metric across tasks, with differences of no\nmore than 3 for the same task. Furthermore, these WSN values are\nsignificantly higher than 2, effectively protecting the IP of RAGs.\n5.3.3 Impact of k RAG returns the top\ud835\udc58text records most relevant\nto the querying question to LLMs. This parameter is typically set by\nthe adversary. We evaluate the impact of\ud835\udc58by setting its value within\nthe range of [1,5]. The evaluation results are shown in Figure 4.\nWe can see that the WSN of RAG-WM remains significantly high\n(well above the threshold of 2) when \ud835\udc58 is between 1 and 5. This is\nbecause most of the retrieved texts contain watermark information,\nresulting in a high WIRR, 99.11% on average. Additionally, with\nan increase in the number of retrieved texts ( \ud835\udc58), both the WSN\nand WIRR increase. For example, the WSN and WIRR are typically\nhigher for \ud835\udc58 = 5 than for \ud835\udc58 = 1. Thus, we use \ud835\udc58 = 1 in other\nevaluations, as it represents the worst-case scenario.\nCosine Inner productEuclidean\n0\n10\n20\n30\nWSN\n(a) TREC-COVID\nCosine Inner productEuclidean\n(b) NFCorpus\nCosine Inner productEuclidean\n(c) MS-MARCO\nFigure 3: Impact of Similarity Metrics.\n5.3.4 The Number of Injected Watermark Tuples Watermark tuples\ncontain the watermark information, i.e., the embedded entities and\ntheir relationships. The number of injected watermark tuples can\naffect the subsequent watermark verification process. To detect IP\ninfringement, the owner queries a suspicious RAG system with\n1 2 3 4 5\n15\n20\n25\n30\nWSN\n\ud835\udc58Value\n(a) TREC-COVID\n1 2 3 4 5\n\ud835\udc58Value\nWIRRWSN\n(b) NFCorpus\n1 2 3 4 5\n\ud835\udc58Value\n90%\n95%\n100%\nWIRR", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "8a26a41c-525b-44b4-93c2-fe5ddfc75b4a": {"text": "affect the subsequent watermark verification process. To detect IP\ninfringement, the owner queries a suspicious RAG system with\n1 2 3 4 5\n15\n20\n25\n30\nWSN\n\ud835\udc58Value\n(a) TREC-COVID\n1 2 3 4 5\n\ud835\udc58Value\nWIRRWSN\n(b) NFCorpus\n1 2 3 4 5\n\ud835\udc58Value\n90%\n95%\n100%\nWIRR\n(c) MS-MARCO\nFigure 4: Impact of \ud835\udc58.\n30 randomly selected watermark questions related to the injected\nwatermark tuples. We assess how the number of embedded water-\nmark tuples, specifically 40, 50, 60, 80, and 100, affects verification\nperformance. The evaluation results are in Figure 5. The verification\nsuccess rates remain stable across the 30 queries, with the WSN\nconsistently around 20 and the WIRR exceeding 95%, regardless of\nthe number of embedded watermark tuples (40, 50, 60, 80, or 100).\nThese results suggest that our watermark achieves stable perfor-\nmance with a limited number of tuples. The main task performance\n(i.e., CDPA) remains stable, exceeding 94% when embedding dif-\nferent numbers of watermark tuples. As the number of watermark\ntuples increases, the clean information retrieval alignment (CIRA)\ndecreases in the NFCorpus task, due to the injection of more water-\nmark texts and the smaller size of its knowledge base. In contrast,\nfor larger knowledge bases (e.g., TREC-COVID and MS-MARCO),\nthe CIRA remains stable because the number of watermark texts is\nminimal compared to the total number of task-related texts. Based\non these evaluations, we default to embedding 50 tuples in the ex-\nperiments of this paper, as this number achieves good performance.\n40 60 80 100\n15\n20\n25\n30\nWSN\nNumber\n(a) TREC-COVID\n40 60 80 100\nNumber\nWIRRWSN\n(b) NFCorpus\n40 60 80 100\nNumber\n90%\n95%\n100%\nWIRR\n(c) MS-MARCO\n40 60 80 100\n85%\n90%\n95%\n100%\nCIRA\nNumber\n(d) TREC-COVID\n40 60 80 100\nNumber\nCDPACIRA\n(e) NFCorpus\n40 60 80 100\nNumber\n90%\n95%\n100%\nCDPA\n(f) MS-MARCO\nFigure 5: Impact of Number of Watermark Tuples.\n5.3.5 The Number of Injected Texts per Watermark Tuple For a\ngiven question, the RAG system retrieves the top \ud835\udc58 most relevant\ntexts. For each watermark tuple, we can generate multiple texts\nwith the same semantics but different content, thereby increas-\ning the proportion of watermark texts retrieved. We evaluate the\nimpact of varying the number (i.e., \ud835\udc41\ud835\udc64\ud835\udc5a) of injected watermark\n9", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "e24f9337-baf4-44cc-b0c6-5a0e39044275": {"text": "1 2 3 4 5\n15\n20\n25\n30\nWSN\nNumber\n(a) TREC-COVID\n1 2 3 4 5\nNumber\nWSNCDPA\n(b) NFCorpus\n1 2 3 4 5\nNumber\n90%\n95%\n100%\nCDPA\n(c) MS-MARCO\nFigure 6: Impact of Number of Injected Texts per Watermark\nTuple.\ntexts per tuple. Specifically, we set \ud835\udc58 = 1 (i.e., the number of re-\ntrieved most related texts) and assess watermark performance for\n\ud835\udc41\ud835\udc64\ud835\udc5a values ranging from 1 to 5. Figure 6 shows the results. We\nobserve that the WSN value increases as the number of injected\nwatermark texts increases. This is because adding more watermark\ntexts raises the likelihood of retrieving the watermark text, making\nthe watermark relation more likely to appear in the LLM outputs.\nTherefore, we can generate multiple watermark texts for each wa-\ntermark tuple and inject them into RAG to improve watermark\nperformance. Moreover, adversaries cannot reliably detect multiple\nwatermark texts for the same watermark tuple using duplicate text\nfiltering, as demonstrated in Section 5.5.2). Additionally, CDPA re-\nmains stable for \ud835\udc41\ud835\udc64\ud835\udc5a = [1,5], consistently above 95%, indicating\ngood performance in maintaining the main task.\n5.4 Robustness\nAfter stealing RAGs, attackers may attempt to remove the water-\nmark. The adversary can utilize paraphrasing, removing unrelated\ncontent, inserting knowledge, and expanding knowledge attacks.\n5.4.1 Paraphrasing Attack Paraphrasing has been employed as a\nstrategy to evade watermark detection in LLM-generated water-\nmark texts [19, 32, 32, 33], thus it may be an effective technique to\nevade the watermark verification in RAG-WM. Specifically, when\na watermark query is issued, the adversary can use an LLM to au-\ntomatically paraphrase the retrieved texts from the watermarked\nknowledge database, thereby removing the watermark information\nand further evading verification. For paraphrasing, we follow the\napproach in [32], employing GPT-3.5-Turbo with the prompt \u201cpara-\nphrase the following sentences\u201d, a temperature setting of 0.7, and a\nmaximum output length of 200 tokens. Table 4 shows the results of\nthe attack. The average WSN value is 14, well above 2, indicating\nthat RAG-WM remains detectable even after paraphrasing. This\nsuggests that paraphrasing cannot effectively remove our RAG-WM.\nThis is because paraphrasing can modify the wording and structure\nof the text, but it cannot alter the knowledge (i.e., the entities and\nrelationship types) present in the content. In contrast, [29], which\nis based on a red-green list watermark, is not robust against para-\nphrasing attacks. The adversary can paraphrase any word in the", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "288e1c91-80dc-4bf8-b551-5a952c04256c": {"text": "relationship types) present in the content. In contrast, [29], which\nis based on a red-green list watermark, is not robust against para-\nphrasing attacks. The adversary can paraphrase any word in the\nRAG texts by replacing words from the green list with those from\nthe red list, rendering the WARD watermark ineffective.\n5.4.2 Unrelated Content Removal Watermark content serves as ex-\ntra information for detecting IP infringement. Such content may be\nremoved by excluding specific sentences from the retrieved content\nTable 4: Paraphrasing Attack\nDataset TREC-COVID NFCorpus MS-MARCO\nWSN 13 17 12\nin RAGs. Since removing content closely related to the main task\nwould degrade performance, an adversary would desire to remove\nonly unrelated content to minimize the impact on the main task. To\nattack RAG-WM, we adapt this method to remove such sentences\nfrom retrieved texts from the watermarked knowledge database for\na watermark query. Specifically, GPT-3.5-Turbo is used to remove\nunrelated content based on a designed prompt (details in Appen-\ndix D). The experimental results are shown in Table 5. After the\nattack, the average WSN is 12, satisfying IP infringement detection.\nThus, RAG-WM is robust against unrelated content removal.\nTable 5: Unrelated Content Removal Attack\nDataset TREC-COVID NFCorpus MS-MARCO\nWSN 12 14 10\n5.4.3 Knowledge Insertion Attack Adversaries can insert mislead-\ning information (acting as noise) into an RAG\u2019s knowledge base\nto interfere with the watermark retrieval process [21]. Since our\nwatermark text encode information about two entities and their\nrelation, the adversary can maximize the interference by inserting\nrecords with randomly selected entities and relations. Such inser-\ntion may involve the watermark entities and introduce random\nrelations (different from the watermark relations), potentially caus-\ning the failure of watermark relation retrieval. To launch such an\nattack, we generate attack texts by randomly combining two texts\nfrom the watermarked RAG knowledge base and directly injecting\nthem into the watermarked RAG. We vary the number of inserted\ntexts from 0 to 2500 under default settings and retrieve the top\nrelevant texts (evaluating both top 1 and top 5 retrieval cases) for\neach watermark query. The results are shown in Figure 7. In the\ntop 1 case, with no texts injected, the WSN values for TREC-COVID,\nNFCorpus, and MS-MARCO are 18, 24, and 19, respectively. With\n2,500 injected texts, the WSN values decrease to 12, 13, and 12.", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "cbe80a7f-8ff2-4958-8feb-bb13957083bc": {"text": "top 1 case, with no texts injected, the WSN values for TREC-COVID,\nNFCorpus, and MS-MARCO are 18, 24, and 19, respectively. With\n2,500 injected texts, the WSN values decrease to 12, 13, and 12.\nWhile the WSN decreases as the number of injected texts increases,\nit remains above 2, indicating that the attack does not compromise\nthe IP infringement detection of RAG-WM. The decrease in WSN is\nmainly due to the interference from the injected texts, which affects\nthe retrieval of watermark texts. For instance, when 2,500 texts are\ninjected (approximately 70% of the original NFCorpus texts), the\nWIRR gradually drops from 100.00% to 46.67%. Furthermore, we\nobserve that the decrease in WSN is smaller when retrieval is per-\nformed using the top 5 texts compared to the top 1 text. Specifically,\nWSN values decrease from 24, 27, and 23 (with no texts injected) to\n21, 17, and 19 (with 2,500 texts injected). This indicates that RAG-\nWM is more robust against knowledge insertion attacks when the\nadversary retrieves more texts for the LLMs.\n5.4.4 Knowledge Expansion Attack The adversary can reduce the\neffectiveness of the watermark by increasing the proportion of\nnon-watermarked information in the retrieved texts. Specifically,\nRAG-WM injects up to \ud835\udc41\ud835\udc64\ud835\udc5a watermark texts into a knowledge\n10", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "55fca813-c19f-4a94-b0b8-1dc759dd17de": {"text": "0 500 1000 1500 2000 250010\n15\n20\n25\n30\nNumber\nWSN\n(a) TREC-COVID\n0 500 1000 1500 2000 2500\nNumber\n(b) NFCorpus\n0 500 1000 1500 2000 2500\nNumber\n\ud835\udc58=1\n\ud835\udc58=5\n40%\n60%\n80%\n100%\nWIRR\n(c) MS-MARCO\nFigure 7: Knowledge Insertion Attack.\ndatabase for each watermark query. If the adversary retrieves \ud835\udc58\ntexts, where \ud835\udc58 > \ud835\udc41\ud835\udc64\ud835\udc5a, it is likely that at least \ud835\udc58 \u2212\ud835\udc41\ud835\udc64\ud835\udc5a of these\ntexts will be clean, thereby undermining RAG-WM\u2019s effectiveness.\nWe evaluate this attack by varying the number of retrieved texts\nfrom 3 to 50. Due to the input text length limit of the LLM, water-\nmark text verification cannot be performed when the text is too\nlong. Therefore, we exclude Llama-2-7B and Vicuna-13B from our\nevaluation. Instead, we use PaLM-2 for the RAG system, which\nsupports an input token limit of 8,192. Figure 8 shows the evalu-\nation results. As the proportion of non-watermarked information\nincreases, the WSN of RAG-WM remains stable. Even with\ud835\udc5a= 50,\nwhere only 10% of the retrieved texts are watermarked (with \ud835\udc58 = 5\nwatermark texts per query), RAG-WM maintains a minimum WSN\nof 19 on MS-MARCO. Additionally, this attack leads to significant\ncomputational costs on the LLM, as longer contexts require more\nresources to generate responses.\n3 10 20 30 40 50\n15\n20\n25\n30\nWSN\nNumber\nTREC-COVID NFCorpus MS-MARCO\nFigure 8: Knowledge Expansion Attack.\n5.5 Stealthiness\nAdversaries might use perplexity analysis or duplicate text filtering\ntechniques to detect the watermark. We evaluate our watermark\nagasint them in this subsection.\n5.5.1 Detection by Perplexity Text perplexity (PPL), commonly\nused to measure text quality, is the average negative log likelihood\nof each token\u2019s occurrence. A model\u2019s perplexity increases if a\ngiven sequence is disfluent, contains grammatical errors, or lacks\nlogical coherence with prior inputs. PPL has served as a defense\nmechanism against attacks on LLMs [8, 25]. Considering that the\nembedding of watermark information may degrade the text quality\nof the RAG, adversaries could detect this low-quality content as\nsuspicious watermark data. In our experiment, we calculate the\nperplexity of the Llama-2-7B model, as it is a white-box model.\nWe randomly select 2,000 clean texts and 200 watermark texts\nfrom the knowledge base. We calculate the perplexity values for\nboth sets and apply the K-means algorithm to partition their PPL\nvalues into two clusters. The smaller cluster is identified as the\noutlier, corresponding to the watermark texts. The F1-scores for\nthis evaluation are only 15.88%, 6.93%, 16.36% on the TREC-COVID,", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "64a3dd50-f1fc-40e5-bbc1-8290b4c788c9": {"text": "values into two clusters. The smaller cluster is identified as the\noutlier, corresponding to the watermark texts. The F1-scores for\nthis evaluation are only 15.88%, 6.93%, 16.36% on the TREC-COVID,\nNFcorpus, and MS-MARCO tasks, respectively. We also analyze the\nfrequency distribution of perplexity values for both sets, focusing on\nwhether the perplexity of watermark texts is significantly higher\nthan that of clean texts. The results are visualized as heatmaps\nin Figure 11 of Appendix, with watermark texts highlighted in\nwhite. Interestingly, the perplexity distribution of watermark texts\nclosely overlaps with regions of low perplexity and high-frequency\nvalues in clean texts. These evaluations show that watermark texts\ngenerated by RAG-WM exhibit high quality, making them difficult\nto distinguish from clean texts based solely on perplexity. As a\nresult, using perplexity is not an effective method for detecting\nwatermark texts.\n5.5.2 Duplicate Text Filtering The owner may inject multiple wa-\ntermark texts for each watermark tuple to improve the success rate\nof watermark retrieval in response to a watermark query. These\nwatermark texts may be identical, which allows the adversary to\nfilter out duplicate texts from the knowledge base, thus evading\nwatermark verification. Referring to [74], we conduct experiments\nto filter duplicate texts on watermarked RAG systems. Specifically,\nfor each watermark query, we compute the hash value (using the\nSHA-256 hash function) for each text in the top 50 retrieval re-\nsults from the watermarked knowledge base and remove any texts\nwith the same hash value. Table 6 compares the results with and\nwithout the attack. We observe that both WSN and WIRR remain\nunchanged after the attack, indicating that duplicate text filtering\nis ineffective at detecting and removing watermark texts. This is be-\ncause the watermark texts generated by the multi-LLM interaction\nwatermarking technique differ for each watermark tuple.\n11", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "cf7724fe-0e4e-45c7-9892-814ba204d2f2": {"text": "Table 6: Duplicate Text Filtering Attack\nDataset TREC-COVID NFCorpus MS-MARCO\nWSN 18 24 19\nWIRR 96.67% 100.00% 96.67%\n5.6 Effectiveness in Advanced RAGs\nThe above experiments are evaluated against naive RAG systems.\nRecently, some advanced RAG techniques [11, 65] have been pro-\nposed, to solve the naive RAGs\u2019 disadvantages, e.g., retrieval chal-\nlenges, generation difficulties, and augmentation hurdles. Referring\nto [74], we evaluate our RAG-WM in two commonly used advanced\nRAG systems, i.e., Self-RAG [11] and CRAG [65]. Table 7 shows\nthat RAG-WM achieves high WSNs (average 22 WSN, well above\nthe threshold of 2), demonstrating its ability to protect the IP of\nadvanced RAGs. This effectiveness is due to the core idea behind\nthese advanced RAG techniques: enhancing the relevance of re-\ntrieved texts to improve the accuracy of LLM-generated answers.\nMeanwhile, the crafted watermark texts are designed to be relevant\nto watermark queries, allowing the LLM to generate correct water-\nmark relationships based on the retrieved watermark contexts.\nTable 7: Advanced RAGs\nDataset Self-RAG CRAG\nWSN WIRR WSN WIRR\nTREC-WIRR 23 100.00% 23 96.67%\nNFCorpus 26 100.00% 19 100.00%\nMS-MARCO 23 100.00% 20 100.00%\n6 Discussion\n6.1 Watermark Injection Approach\nWe propose a watermark injection method based on relevant-text\nconcatenation. Alternatively, a direct insertion approach can be\nconsidered, where the owner embeds the watermark text \ud835\udc4a\ud835\udc47 as\na separate record in RAG\ud835\udc64\ud835\udc5a. This method has the advantage of\nintroducing minimal disruption to the structure and content of the\noriginal RAG. However, it does not fully confirm that the watermark\ntexts can be retrieved and detected during verification. In contrast,\nrelevant-text concatenation injects \ud835\udc4a\ud835\udc47 into the most relevant text\n\ud835\udc47\ud835\udc38\ud835\udc4b\ud835\udc47 of the RAG through pre-retrieval, improving detectability\nand extraction. To evaluate direct insertion, we generate the same\nnumber of watermark texts as in Section 5.2 using the multi-LLM\ninteraction watermarking technique and directly insert them into\nRAG\ud835\udc64\ud835\udc5a. The results in Table 8 indicate that the WSN and WIRR\nvalues for direct insertion are lower than those for relevant-text\nconcatenation. This is because the pre-retrieval step in the latter\nimproves watermark retrieval performance and increases WSN\nvalues. Notably, the performance of direct insertion is poor in large-\nscale knowledge bases (e.g., NQ, HotpotQA, MSMARCO), and it\nworsens as the size of the base increases. This is because the vast\namount of text in the knowledge base introduces more noise, while", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "6a2ffadd-f006-486c-9570-ec08f01680f2": {"text": "scale knowledge bases (e.g., NQ, HotpotQA, MSMARCO), and it\nworsens as the size of the base increases. This is because the vast\namount of text in the knowledge base introduces more noise, while\ndirect insertion lacks retrieval guarantees, making the watermark\nsusceptible to interference from other texts.\nTable 8: Direct Insertion Approach\nDataset Metrics LLMs\nGPT-3.5 LLama Vicuna PaLM\nTREC-COVID WSN 13 14 15 12\nWIRR 80.00% 80.00% 80.00% 80.00%\nNFCorpus WSN 17 18 16 15\nWIRR 93.33% 93.33% 93.33% 93.33%\nNQ WSN 9 12 10 11\nWIRR 66.67% 66.67% 66.67% 66.67%\nHotpotQA WSN 8 9 8 9\nWIRR 56.67% 56.67% 56.67% 56.67%\nMS- WSN 7 8 7 7\nMARCO WIRR 46.67% 46.67% 46.67% 46.67%\n6.2 Knowledge Graph Distillation Attack\nSince our watermark relies on the construction of watermark enti-\nties and relations, an adversary can launch a knowledge graph distil-\nlation attack against the watermarked domain-specific knowledge\nbase (i.e., Domain-specific Knowledge Distillation Attack). This\ninvolves extracting entities and relations from the knowledge base\nand constructing a knowledge graph based on the extracted infor-\nmation. The adversary can then distill information from this graph\nby sorting entities according to their degree within the dataset\ncorpus and generating dense subgraphs based on these entities and\ntheir relationships.\nWe evaluate this attack against NFCorpus task by generating\nsubgraphs of different granularities for high-degree entities (with\nthe entities\u2019 distillation (preservation) rate of 5%, 10%, 20%, 40%,\n80%, 100% ) and their corresponding relations. The evaluation re-\nsults are shown in Figure 9. We can see that if the watermarked\nRAGs are distilled at a lower distillation rate, the RAGs have been\ndistilled to be considered as \u201cfail\u201d on the main tasks (e.g., with 20%\ndistillation rate, the CDPA is only 25.39%). The WSN of RAG-WM\nis 22, far larger than 2. When the distillation rate is high (above\n80%), the performance of the main tasks is maintained. The WSN\nvalue of RAG-WM is far more extensive than 2, which can effec-\ntively detect IP infringement of the stolen RAG. This robustness\narises because our watermark texts are generated based on high-\nfrequency watermark entities and relations. Since the attack focus\non distilling important entities and relations from the knowledge\ngraph, the high degree of these watermark entities ensures their\neffective retention in the extracted graph, making the watermark\nrobust against domain-specific knowledge distillation attack.", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "deab9380-03e1-4785-9a33-ab36e4c46b20": {"text": "graph, the high degree of these watermark entities ensures their\neffective retention in the extracted graph, making the watermark\nrobust against domain-specific knowledge distillation attack.\nAdditionally, for a general knowledge base with extensive do-\nmain knowledge (e.g., Wikidata [1], DBpedia [20], YAGO [64]), the\nadversary might attempt to distill part of the knowledge base for the\ntarget domain. To counter this, we can refer to traditional relational\ndatabase watermarking techniques to partition the knowledge base\ninto groups [30], with each group containing domain-specific texts.\nWatermarks are then embedded in each group using RAG-WM. This\npartitioning approach enhances the robustness of our RAG-WM\nagainst this attack. In summary, RAG is robust against knowledge\ngraph distillation attack.\n6.3 Piracy Attack\nAttackers can use our watermark embedding algorithm to insert a\npirated watermark into stolen RAGs and fraudulently claim owner-\nship. However, due to the robustness (Section 5.4) and stealthiness\n12", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "0237aba9-317e-43a2-aad4-fa0faefe8adf": {"text": "0% 20% 40% 60% 80% 100%0\n10\n20\n30\nDistillation Rate\nWSN\n0\n20\n40\n60\n80\n100\nCDPA\nFigure 9: Knowledge Graph Distillation Attack.\n(Section 5.5) of our watermark, attackers cannot remove our wa-\ntermark or create an RAG with only their own watermark. As a\nresult, attackers can only present an RAG containing both their\nwatermark and the owner\u2019s watermark, while the true owner can\npresent an RAG with only their own watermark. It is clear who is\nthe true owner. Thus, our watermark is robust against piracy attack\n(achieving security).\n7 Conclusion\nIn this paper, we propose a novel black-box \u201cknowledge watermark\u201d\napproach RAG-WM, to protect the intellectual property of RAGs.\nRAG-WM generates watermarked RAGs by leveraging a multi-LLM\ninteraction watermarking technique, which creates watermark texts\nbased on watermark entity-relationship tuples. These watermarks\nare then injected into the target RAG, and IP infringement is de-\ntected by querying the suspect LLM and RAG systems with the\nwatermark queries in a black-box manner. Moreover, we evaluate\nour watermark on both domain-specific and privacy-sensitive tasks.\nThe results demonstrate that our watermark can effectively detect\nIP infringement of RAGs in various adversary\u2019s deployed LLMs,\nand is robust against various watermark attacks.\nReferences\n[1] 2024. Wikipedia. https://www.wikidata.org/wiki/Wikidata:Main_Page.\n[2] 2025. Code of RAG-WM . https://anonymous.4open.science/r/ragwm-DFF7.\n[3] Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet.\n2018. Turning your weakness into a strength: Watermarking deep neural net-\nworks by backdooring. In 27th USENIX security symposium (USENIX Security 18) .\n1615\u20131631.\n[4] Rakesh Agrawal, Peter J Haas, and Jerry Kiernan. 2003. A system for watermark-\ning relational databases. In Proceedings of the 2003 ACM SIGMOD international\nconference on Management of data . 674\u2013674.\n[5] Rakesh Agrawal and Jerry Kiernan. 2002. Watermarking relational databases. In\nVLDB\u201902: Proceedings of the 28th International Conference on Very Large Databases .\nElsevier, 155\u2013166.\n[6] Chroma AI. 2025. Chroma. https://docs.trychroma.com/.\n[7] Ahmet Yusuf Alan, Enis Karaarslan, and \u00d6mer Aydin. 2024. A rag-based question\nanswering system proposal for understanding islam: Mufassirqas llm. arXiv\npreprint arXiv:2401.15378 (2024).\n[8] Gabriel Alon and Michael Kamfonas. 2023. Detecting language model attacks\nwith perplexity. arXiv preprint arXiv:2308.14132 (2023).\n[9] Maya Anderson, Guy Amit, and Abigail Goldsteen. 2024. Is My Data in Your", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "1ee33b1b-b477-4f6f-9163-aeb69eecd394": {"text": "preprint arXiv:2401.15378 (2024).\n[8] Gabriel Alon and Michael Kamfonas. 2023. Detecting language model attacks\nwith perplexity. arXiv preprint arXiv:2308.14132 (2023).\n[9] Maya Anderson, Guy Amit, and Abigail Goldsteen. 2024. Is My Data in Your\nRetrieval Database? Membership Inference Attacks Against Retrieval Augmented\nGeneration. arXiv preprint arXiv:2405.20446 (2024).\n[10] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,\nAlexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,\net al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403 (2023).\n[11] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.\nSelf-rag: Learning to retrieve, generate, and critique through self-reflection.arXiv\npreprint arXiv:2310.11511 (2023).\n[12] Mikhail J Atallah, Victor Raskin, Michael Crogan, Christian Hempelmann, Flo-\nrian Kerschbaum, Dina Mohamed, and Sanket Naik. 2001. Natural language\nwatermarking: Design, analysis, and a proof-of-concept implementation. In In-\nformation Hiding: 4th International Workshop, IH 2001 Pittsburgh, PA, USA, April\n25\u201327, 2001 Proceedings 4 . Springer, 185\u2013200.\n[13] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu,\nRangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. 2016.\nMs marco: A human generated machine reading comprehension dataset. arXiv\npreprint arXiv:1611.09268 (2016).\n[14] Mahbuba Begum and Mohammad Shorif Uddin. 2020. Digital image watermark-\ning techniques: a review. Information 11, 2 (2020), 110.\n[15] Sukriti Bhattacharya, Agostino Cortesi, et al. 2009. A Distortion Free Watermark\nFramework for Relational Databases.. In ICSOFT (2) . Citeseer, 229\u2013234.\n[16] Vera Boteva, Demian Gholipour, Artem Sokolov, and Stefan Riezler. 2016. A\nfull-text learning to rank dataset for medical information retrieval. InAdvances in\nInformation Retrieval: 38th European Conference on IR Research, ECIR 2016, Padua,\nItaly, March 20\u201323, 2016. Proceedings 38 . Springer, 716\u2013722.\n[17] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877\u20131901.\n[18] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,\nLianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023.\nVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "93023948-3337-435f-8407-81ab55c1e9a1": {"text": "[18] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,\nLianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023.\nVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See\nhttps://vicuna. lmsys. org (accessed 14 April 2023) 2, 3 (2023), 6.\n[19] Miranda Christ, Sam Gunn, and Or Zamir. 2024. Undetectable watermarks for\nlanguage models. In The Thirty Seventh Annual Conference on Learning Theory .\nPMLR, 1125\u20131139.\n[20] DBpedia Community. 2024. DBpedia. https://www.dbpedia.org/.\n[21] Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare\nCampagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. 2024. The\npower of noise: Redefining retrieval for rag systems. In Proceedings of the 47th\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval. 719\u2013729.\n[22] Zhengmian Hu, Lichang Chen, Xidong Wu, Yihan Wu, Hongyang Zhang, and\nHeng Huang. 2023. Unbiased watermark for large language models. arXiv\npreprint arXiv:2310.10669 (2023).\n[23] Mintplex Labs Inc. 2025. Anything LLM AI . https://anythingllm.com/.\n[24] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-\njanowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised dense in-\nformation retrieval with contrastive learning. arXiv preprint arXiv:2112.09118\n(2021).\n[25] Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchen-\nbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and\nTom Goldstein. 2023. Baseline defenses for adversarial attacks against aligned\nlanguage models. arXiv preprint arXiv:2309.00614 (2023).\n[26] Eric Jang, Shixiang Gu, and Ben Poole. 2016. Categorical reparameterization\nwith gumbel-softmax. arXiv preprint arXiv:1611.01144 (2016).\n[27] Hengrui Jia, Christopher A Choquette-Choo, Varun Chandrasekaran, and Nicolas\nPapernot. 2021. Entangled watermarks as a defense against model extraction. In\n30th USENIX security symposium (USENIX Security 21) . 1937\u20131954.\n[28] Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu,\nYiming Yang, Jamie Callan, and Graham Neubig. 2023. Active retrieval augmented\ngeneration. arXiv preprint arXiv:2305.06983 (2023).\n[29] Nikola Jovanovi\u0107, Robin Staab, Maximilian Baader, and Martin Vechev. 2024.\nWard: Provable RAG Dataset Inference via LLM Watermarks. arXiv preprint\narXiv:2410.03537 (2024).\n[30] Muhammad Kamran and Muddassar Farooq. 2018. A comprehensive survey", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "f899a0f4-457c-4cd2-9b49-b27dd9b0d074": {"text": "[29] Nikola Jovanovi\u0107, Robin Staab, Maximilian Baader, and Martin Vechev. 2024.\nWard: Provable RAG Dataset Inference via LLM Watermarks. arXiv preprint\narXiv:2410.03537 (2024).\n[30] Muhammad Kamran and Muddassar Farooq. 2018. A comprehensive survey\nof watermarking relational databases research. arXiv preprint arXiv:1801.08271\n(2018).\n[31] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and\nTom Goldstein. 2023. A watermark for large language models. In International\nConference on Machine Learning . PMLR, 17061\u201317084.\n[32] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi\nKong, Kasun Fernando, Aniruddha Saha, Micah Goldblum, and Tom Goldstein.\n2023. On the reliability of watermarks for large language models. arXiv preprint\narXiv:2306.04634 (2023).\n[33] Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit\nIyyer. 2024. Paraphrasing evades detectors of ai-generated text, but retrieval is an\neffective defense. Advances in Neural Information Processing Systems 36 (2024).\n[34] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur\nParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton\nLee, et al. 2019. Natural questions: a benchmark for question answering research.\nTransactions of the Association for Computational Linguistics 7 (2019), 453\u2013466.\n[35] John Snow Labs. 2025. John Snow Labs .\nhttps://www.johnsnowlabs.com/healthcare-llm/.\n[36] Douglas B Lenat. 1995. CYC: A large-scale investment in knowledge infrastruc-\nture. Commun. ACM 38, 11 (1995), 33\u201338.\n[37] Mingchen Li, Halil Kilicoglu, Hua Xu, and Rui Zhang. 2024. Biomedrag: A retrieval\naugmented large language model for biomedicine.arXiv preprint arXiv:2405.00465\n13", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "9e3dd641-4850-49f7-8e09-d06175885a02": {"text": "(2024).\n[38] Yingjiu Li, Huiping Guo, and Sushil Jajodia. 2004. Tamper detection and localiza-\ntion for categorical data using fragile watermarks. In Proceedings of the 4th ACM\nworkshop on Digital rights management . 73\u201382.\n[39] Yuying Li, Gaoyang Liu, Chen Wang, and Yang Yang. 2024. Generating Is Be-\nlieving: Membership Inference Attacks against Retrieval-Augmented Generation.\narXiv preprint arXiv:2406.19234 (2024).\n[40] Zhe Lin, Yitao Cai, and Xiaojun Wan. 2021. Towards document-level para-\nphrase generation with sentence rewriting and reordering. arXiv preprint\narXiv:2109.07095 (2021).\n[41] Peizhuo Lv, Pan Li, Shenchen Zhu, Shengzhi Zhang, Kai Chen, Ruigang Liang,\nChang Yue, Fan Xiang, Yuling Cai, Hualong Ma, et al. 2024. Ssl-wm: A black-box\nwatermarking approach for encoders pre-trained by self-supervised learning. In\nProceedings of the 2024 Annual Network and Distributed System Security Sympo-\nsium, NDSS\u201924 .\n[42] Hasan Mesut Meral, B\u00fclent Sankur, A Sumru \u00d6zsoy, Tunga G\u00fcng\u00f6r, and Emre\nSevin\u00e7. 2009. Natural language watermarking via morphosyntactic alterations.\nComputer Speech & Language 23, 1 (2009), 107\u2013125.\n[43] Meta. 2025. Llama. https://www.llama.com/.\n[44] Meta. 2025. Llama RAG. https://ai.meta.com/blog/meta-llama-3-1/.\n[45] Microsoft. 2025. Azure. https://learn.microsoft.com/zh-cn/azure/ai-\nstudio/concepts/retrieval-augmented-generation.\n[46] Travis Munyer, Abdullah Tanvir, Arjon Das, and Xin Zhong. 2023. DeepTextMark:\nA Deep Learning-Driven Text Watermarking Approach for Identifying Large\nLanguage Model Generated Text. arXiv preprint arXiv:2305.05773 (2023).\n[47] OpenAI. 2025. GPT. https://openai.com/index/gpt-4/.\n[48] Heiko Paulheim. 2018. How much is a triple. In IEEE International Semantic Web\nConference.\n[49] Saksham Rastogi and Danish Pruthi. 2024. Revisiting the Robustness of Water-\nmarking to Paraphrasing Attacks. arXiv preprint arXiv:2411.05277 (2024).\n[50] Ryoma Sato, Yuki Takezawa, Han Bao, Kenta Niwa, and Makoto Yamada. 2023.\nEmbarrassingly simple text watermarks. arXiv preprint arXiv:2310.08920 (2023).\n[51] Mohamed Shehab, Elisa Bertino, and Arif Ghafoor. 2007. Watermarking relational\ndatabases using optimization-based techniques. IEEE transactions on Knowledge\nand Data Engineering 20, 1 (2007), 116\u2013129.\n[52] Radu Sion, Mikhail Atallah, and Sunil Prabhakar. 2003. Rights protection for\nrelational data. In Proceedings of the 2003 ACM SIGMOD international conference\non Management of data . 98\u2013109.\n[53] Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kalu-", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "5a3e847e-c4c4-4ebc-9464-6e9075a6064d": {"text": "relational data. In Proceedings of the 2003 ACM SIGMOD international conference\non Management of data . 98\u2013109.\n[53] Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kalu-\narachchi, Rajib Rana, and Suranga Nanayakkara. 2023. Improving the domain\nadaptation of retrieval augmented generation (RAG) models for open domain\nquestion answering. Transactions of the Association for Computational Linguistics\n11 (2023), 1\u201317.\n[54] Umut Topkara, Mercan Topkara, and Mikhail J Atallah. 2006. The hiding virtues of\nambiguity: quantifiably resilient watermarking of natural language text through\nsynonym substitutions. In Proceedings of the 8th workshop on Multimedia and\nsecurity. 164\u2013174.\n[55] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-\nmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288 (2023).\n[56] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.\n2022. Interleaving retrieval with chain-of-thought reasoning for knowledge-\nintensive multi-step questions. arXiv preprint arXiv:2212.10509 (2022).\n[57] Meng-Hsiun Tsai, Fang-Yu Hsu, Jun-Dong Chang, and Hsien-Chu Wu. 2007.\nFragile database watermarking for malicious tamper detection using support\nvector regression. In Third International Conference on Intelligent Information\nHiding and Multimedia Signal Processing (IIH-MSP 2007) , Vol. 1. IEEE, 493\u2013496.\n[58] Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman,\nWilliam R Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, and Lucy Lu Wang. 2021.\nTREC-COVID: constructing a pandemic information retrieval test collection. In\nACM SIGIR Forum , Vol. 54. ACM New York, NY, USA, 1\u201312.\n[59] Calvin Wang, Joshua Ong, Chara Wang, Hannah Ong, Rebekah Cheng, and\nDennis Ong. 2024. Potential for GPT technology to optimize future clinical\ndecision-making using retrieval-augmented generation. Annals of Biomedical\nEngineering 52, 5 (2024), 1115\u20131118.\n[60] Hongru Wang, Wenyu Huang, Yang Deng, Rui Wang, Zezhong Wang, Yufei\nWang, Fei Mi, Jeff Z Pan, and Kam-Fai Wong. 2024. Unims-rag: A unified multi-\nsource retrieval-augmented generation for personalized dialogue systems. arXiv\npreprint arXiv:2401.13256 (2024).\n[61] Wikipedia. 2024. Statistical Hypothesis Test .\nhttps://en.wikipedia.org/wiki/Statistical_hypothesis_test.\n[62] Yihan Wu, Zhengmian Hu, Hongyang Zhang, and Heng Huang. 2023. Dipmark:", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "0e804773-ecce-456a-b407-1d11045a9150": {"text": "preprint arXiv:2401.13256 (2024).\n[61] Wikipedia. 2024. Statistical Hypothesis Test .\nhttps://en.wikipedia.org/wiki/Statistical_hypothesis_test.\n[62] Yihan Wu, Zhengmian Hu, Hongyang Zhang, and Heng Huang. 2023. Dipmark:\nA stealthy, efficient and resilient watermark for large language models. arXiv\npreprint arXiv:2310.07710 (2023).\n[63] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett,\nJunaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neighbor nega-\ntive contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808\n(2020).\n[64] YAGO. 2024. YAGO Knowledge. https://yago-knowledge.org/.\n[65] Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. Corrective retrieval\naugmented generation. arXiv preprint arXiv:2401.15884 (2024).\n[66] Xi Yang, Kejiang Chen, Weiming Zhang, Chang Liu, Yuang Qi, Jie Zhang, Han\nFang, and Nenghai Yu. 2023. Watermarking text generated by black-box language\nmodels. arXiv preprint arXiv:2305.08883 (2023).\n[67] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan\nSalakhutdinov, and Christopher D Manning. 2018. HotpotQA: A dataset for di-\nverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600\n(2018).\n[68] Hanlin Zhang, Benjamin L Edelman, Danilo Francati, Daniele Venturi, Giuseppe\nAteniese, and Boaz Barak. 2023. Watermarks in the sand: Impossibility of strong\nwatermarking for generative models. arXiv preprint arXiv:2311.04378 (2023).\n[69] Ruisi Zhang, Shehzeen Samarah Hussain, Paarth Neekhara, and Farinaz Koushan-\nfar. 2024. {REMARK-LLM}: A robust and efficient watermarking framework for\ngenerative large language models. In 33rd USENIX Security Symposium (USENIX\nSecurity 24) . 1813\u20131830.\n[70] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting\nHuang, Enbo Zhao, Yu Zhang, Yulong Chen, et al . 2023. Siren\u2019s song in the\nAI ocean: a survey on hallucination in large language models. arXiv preprint\narXiv:2309.01219 (2023).\n[71] Zhi-hao Zhang, Xiao-Ming Jin, Jian-Min Wang, and De-Yi Li. 2004. Watermarking\nrelational database using image. In Proceedings of 2004 International Conference\non Machine Learning and Cybernetics (IEEE Cat. No. 04EX826) , Vol. 3. IEEE, 1739\u2013\n1744.\n[72] Huaqin Zhao, Zhengliang Liu, Zihao Wu, Yiwei Li, Tianze Yang, Peng Shu,\nShaochen Xu, Haixing Dai, Lin Zhao, Gengchen Mai, et al. 2024. Revolutionizing\nfinance with llms: An overview of applications and insights. arXiv preprint\narXiv:2401.11641 (2024).", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "b986822c-1952-431b-89d1-3ed8412434ab": {"text": "Shaochen Xu, Haixing Dai, Lin Zhao, Gengchen Mai, et al. 2024. Revolutionizing\nfinance with llms: An overview of applications and insights. arXiv preprint\narXiv:2401.11641 (2024).\n[73] Pengyuan Zhou, Lin Wang, Zhi Liu, Yanbin Hao, Pan Hui, Sasu Tarkoma, and\nJussi Kangasharju. 2024. A survey on generative ai and llm for video generation,\nunderstanding, and streaming. arXiv preprint arXiv:2404.16038 (2024).\n[74] Wei Zou, Runpeng Geng, Binghui Wang, and Jinyuan Jia. 2024. Poisonedrag:\nKnowledge poisoning attacks to retrieval-augmented generation of large lan-\nguage models. arXiv preprint arXiv:2402.07867 (2024).\nAppendix\nA Datasets\n\u2022TREC-COVID is a dataset based on COVID-19 literature to support\nnew research and technologies in the pandemic search. It contains\n1,713,332 texts [58].\n\u2022NFCorpus is a full-text English dataset for medical information re-\ntrieval, containing 3,633 documents primarily sourced from PubMed.\nThe queries in the dataset are derived from the NutritionFacts.org\nwebsite [16].\n\u2022NQ [34] includes questions from real users and 2,681,468 texts\nfrom Wikipedia articles to advance development in open-domain\nquestion answering.\n\u2022HotpotQA dataset, collected from Wikipedia, consists of 5,233,329\ntexts designed for natural, multi-hop questions. It provides strong\nsupervision for supporting facts, aiming to create more explainable\nquestion-answering systems [67].\n\u2022MS-MARCO [13] is a question-answering dataset containing\n8,841,823 text samples, collected from web documents using the\nMicrosoft Bing search engine.\nTable 9 shows the statistics of these datasets. For each dataset,\nwe construct a knowledge base and embed watermarks into it.\nB Watermark Setting\nB.1 Watermarking Experimental Setup\nWatermark Setting. For the size of the entity list \ud835\udc38and relations\nlist \ud835\udc45, we set as |\ud835\udc38|= 100 and |\ud835\udc45|= 20. Based on \ud835\udc38 and \ud835\udc45, we\n14", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "a3f319be-50c1-4a76-a43f-709d3f6d586a": {"text": "GPT-3.5 Llama Vicuna PaLM\n0\n10\n20\n30\nWSN\n(a) TREC-COVID\nGPT-3.5 Llama Vicuna PaLM\n(b) NFCorpus\nGPT-3.5 Llama Vicuna PaLM\nLLM Human\n(c) NQ\nGPT-3.5 Llama Vicuna PaLM\n(d) HotpotQA\nGPT-3.5 Llama Vicuna PaLM\n(e) MS-MARCO\nFigure 10: Comparison of Human and LLM Evaluations for WSN Values.\nTable 9: Statistics of Datasets\nDataset Questions\nTexts in\nKnowledge\nBase\nWatermark\nTexts\nTREC-COVID 50 171,332 237\nNFCorpus 323 3,633 246\nNQ 3,452 2,681,468 184\nHotpotQA 7,405 5,233,329 191\nMS-MARCO 502,939 8,841,823 230\nutilize SHA-256 as the\ud835\udc3b\ud835\udc40\ud835\udc34\ud835\udc36()function (with the randomly gener-\nated \ud835\udc58\ud835\udc52\ud835\udc665 by the pseudo-random number generator), and generate\n50 watermark tuples of (\ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a,\ud835\udc5f\ud835\udc64\ud835\udc5a,\ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a)for watermark injection.\nTo ensure effective watermark retrieval, we generate up to 5 wa-\ntermark texts for each tuple, with the total number of injected\nwatermark texts listed in Table 9. In Multi-LLM interaction frame-\nwork, the prompts of WM-Gen, Shadow-LLM&RAG, and WM-Disc\nare shown in Appendix B.2. We use GPT-3.5-Turbo to generate wa-\ntermark texts in our experiments, where the temperature parameter\nis set to 0.1. The \ud835\udc40\ud835\udc34\ud835\udc4b_\ud835\udc52\ud835\udc5d\ud835\udc5c\ud835\udc50\u210e\ud835\udc60 of interaction is 10. We select the\ntop 1 text retrieved from the knowledge base, and also evaluate\nusing more texts retrieved from the knowledge base (as shown in\nSection 5.3.3). However, retrieving fewer texts makes watermark\nretrieval and verification more challenging. Therefore, we default\nto selecting the top 1 text from the knowledge base. For the water-\nmark verification, unless otherwise specified, we randomly select\n30 watermark tuples and utilize \u201cWhat is the relationship between\n\ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a and \ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a?\u201d as query questions to check for the presence of wa-\ntermarking relationships against suspicious RAGs. We also evaluate\nmultiple different watermark queries in Appendix E.\nIn our evaluation, we do not assume control over the adversary\u2019s\nLLMs to inject a watermark, so RAG-WM should be effective in\nboth black-box and white-box LLMs. These four LLMs (GPT-3.5-\nTurbo, PaLM 2, Llama-2-7B, and Vicuna-13B ) are tested as potential\nadversary-deployed models in Section 5.2, and we default set the\ntemperature parameter of these LLMs to be 0.1. In other experi-\nments, unless otherwise specified, we use Llama-2-7B as the default\nLLM deployed in the adversary\u2019s RAG system. In addition, we de-\nploy the GPT-3.5-Turbo as the shadow model due to its powerful\ncapability of comprehensively understanding text content, making\nit an ideal candidate for improving the effectiveness of our water-", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "db950de3-9925-460a-a20c-e27c3d9e1ff5": {"text": "ploy the GPT-3.5-Turbo as the shadow model due to its powerful\ncapability of comprehensively understanding text content, making\nit an ideal candidate for improving the effectiveness of our water-\nmark. Moreover, unless otherwise specified, we utilize Contriever\nwith cosine similarity as the default retriever in our evaluation.\n5\u201c3d9fe9a618e 46f9697e7fad814e8fe27b0954cfad82ad5e05db5aead3439b1e\u201d\nB.2 Prompt for Multi-LLM Interaction\nWatermarking Technique\nB.2.1 Prompt for WM-Gen The prompt of WM-Gen for optimizing\nwatermark texts.\nPrompt for WM-Gen\nYou are a watermark generator, a knowledge graph expert,\nand a linguist. In a given knowledge graph, two entities\n(E1) and (E2) are connected by a relationship (R1). Your task\nis to generate watermark text (WT) that clearly encodes\nthis relationship (R1) between (E1) and (E2), ensuring that\nthe watermark text is coherent and related to the database\ncontent (TEXT).\nThe generated watermark text will undergo two stages of\nprocessing:\n1. **Direct Evaluation**: - **Watermark Discriminator 1\n(WD1)**: This model evaluates whether the watermark\ntext (WT) accurately implies the relationship (R1) between\n(E1) and (E2).\n2. **Extractor-Based Evaluation**: - **Watermark Extractor\n(WE)**: This model attempts to extract the relationship\n(R1) between (E1) and (E2) based on the restored water-\nmark text (WT) and additional database content (TEXT).\n- **Watermark Discriminator 2 (WD2)**: After the extrac-\ntion, this model assesses whether the relationship (R1) is\nstill clearly and accurately implied.\nYour objective is to refine the watermark text (WT) to\nensure: 1. The relationship (R1) between (E1) and (E2) re-\nmains clear and accurate after processing by the extractor.\n2. Both discriminators (WD1 and WD2) confirm that the\nrelationship (R1) is correctly encoded. 3. The generated\nwatermark text (WT) should be approximately 30 words\nlong. 4. Ensure that appending the watermark text (WT)\nto (TEXT) does not result in incoherent or unrelated sen-\ntences that could be discarded.\n**Input:** - Restored watermark text: WT - Extractor out-\nput: WE - Discriminator feedback (WD1): {WD1} - Discrim-\ninator feedback (WD2): {WD2} - Relationship (R1): {R1} -\nEntity 1 (E1): {E1} - Entity 2 (E2): {E2} - Database retrieval\noutput (text): {TEXT}\n**Output:** Return the refined watermark text in JSON\nformat: [{{\"watermark_text\": \"Your refined text\"}}]\n15", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "5f5dafd7-f704-460e-b717-19e90546932f": {"text": "B.2.2 Prompt used for WM-Disc The following are the watermark\nqueries used by WM-Disc to extract watermark information.\nPrompt for Watermark Queries by WM-Disc\n(i) What is the relationship between {E1} and {E2} ?\n(ii) What is the text sentence that is related to both {E1}\nand {E2}?\n(iii) What text is related to both {E1} and {E2}?\nThe prompt used by WM-Disc for watermark verification, which\nis also used to measure WSN, is as follows:\nPrompt for Watermark Verification by WM-Disc\nYou are an expert in knowledge graphs and linguistics. Your\ntask is to evaluate the text: (rag_doc). Identify whether it\nsuggests a relationship (R1) exists between the entities (E1)\nand (E2).\nInput:\n- rag_doc: ({rag_doc})\n- R1: ({R1})\n- E1: ({E1})\n- E2: ({E2})\nOutput: Reply strictly with \"yes\" if the relationship is im-\nplied, or \"no\" if it is not. No additional information is re-\nquired.\nB.2.3 Prompt for Shadow-LLM&RAG The following is the prompt\nused in the Shadow-LLM&RAG.\nPrompt for Shadow-LLM&RAG\nYou are a helpful assistant. Below are some relevant con-\ntexts. Use only the information provided in these contexts\nto answer the question. If you cannot find the answer to the\nquestion within the contexts, simply say \u2019I do not know\u2019.\nContexts: [context]\nQuestion: [question]\nAnswer:\nB.3 Prompt for Measuring CDPA\nThe following prompt is used to evaluate the main task performance\nof RAGs (i.e., CDPA).\nPrompt for Measuring CDPA\nGiven two sentences, determine if they convey the same\nmeaning. If they are similar in meaning, return \u2019yes\u2019; oth-\nerwise, return \u2019no\u2019.\nThe following situations are also considered as the two\nsentences expressing the same meaning:\n1. One sentence includes the meaning expressed in the\nother sentence.\n2. The two sentences express the same central idea but in\ndifferent ways.\nSentence 1: {baseline_item[\"llm_text\"]}\nSentence 2: {llm_text_item[\"llm_text\"]}\nOutput: \u2019yes\u2019 or \u2019no\u2019 only, No explanations, no extra text.\nC Some Examples for RAG-WM\nC.1 Examples of Injected Watermark\nWe show some examples of injected watermarks, including the\nwatermark tuple and watermark texts, for the five evaluated tasks.\nWatermark Tuple and Its Texts in TREC-COVID\nWatermark tuple: [\u201cAngiogenesis\u201d, \u201cUSES\u201d, \u201cAntagonists\u201d]\nWatermark texts: 1) Angiogenesis utilizes antagonists to\nregulate blood vessel formation. 2) Antagonists are em-\nployed by angiogenesis to modulate its processes. 3) An-\ntagonists play a crucial role in regulating the multi-step\nprocess of angiogenesis, thus serving as key components", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "648033c0-56c7-4b27-84dd-5e71b4f5e4f6": {"text": "regulate blood vessel formation. 2) Antagonists are em-\nployed by angiogenesis to modulate its processes. 3) An-\ntagonists play a crucial role in regulating the multi-step\nprocess of angiogenesis, thus serving as key components\nin the process. 4) The process of angiogenesis makes use\nof antagonists for its functionality. 5) Antagonists are used\nin angiogenesis to influence the development of new blood\nvessels.\nWatermark Tuple and Its Texts in NFCorpus\nWatermark tuple: [\u201cIdiopathic Urticaria\u201d, \u201cASSOCIATION\u201d,\n\u201cCalciferols\u201d]\nWatermark texts: 1) Idiopathic Urticaria is associated with\nCalciferols. 2) Calciferols are associated with Idiopathic\nUrticaria. 3) An association exists between Idiopathic Ur-\nticaria and Calciferols. 4) Calciferols and Idiopathic Ur-\nticaria have an association. 5) The relationship between\nIdiopathic Urticaria and Calciferols is one of association.\nWatermark Tuple and Its Texts in NQ\nWatermark tuple: [\u201cPreston\u201d, \u201cINVOLVES\u201d, \u201cLiquidity\u201d]\nWatermark texts: 1) Preston is connected to liquidity. 2)\nPreston is involved with Liquidity. 3) The involvement\nof Preston in liquidity is notable. 4) Liquidity, in which\nPreston is involved, is crucial. 5) Preston\u2019s involvement\nwith liquidity is clearly demonstrated.\n16", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "85f5d448-08ab-4726-a4ef-cc6f1277d5e8": {"text": "5 10 15 20 25\nPPL Value\n0.0\n0.1\n0.2\nDensity\n(a) TREC-COVID\n5 10 15 20 25\nPPL Value\n0.0\n0.1\n0.2\nDensity\n(b) NFCorpus\n5 10 15 20 25\nPPL Value\n0.00\n0.05\n0.10\nDensity\n(c) MS-MARCO\nFigure 11: Detection by Perplexity.\nWatermark Tuple and Its Texts in HotpotQA\nWatermark tuple: [\u201cShamshad Hussain\u201d,\n\u201cHAS_OCCUPATION\u201d, \u201cBoston University\u201d]\nWatermark texts: 1) Shamshad Hussain has an occupa-\ntion at Boston University. 2) Boston University is where\nShamshad Hussain works. 3) Shamshad Hussain is em-\nployed at Boston University. 4) Shamshad Hussain holds\na position at Boston University. 5) Shamshad Hussain\u2019s\noccupation is at Boston University.\nWatermark Tuple and Its Texts in MS-MARCO\nWatermark tuple: [\u201cMozilla\u201d, \u201cCAUSES\u201d, \u201cScrofula\u201d]\nWatermark texts: 1) Mozilla is known to cause scrofula. 2)\nMozilla can cause Scrofula. 3) AMozilla causes the occur-\nrence of scrofula. 4) There is a causal relationship where\nMozilla leads to scrofula. 5) Scrofula results from the influ-\nence of Mozilla.\nD Prompt for Unrelated Content Removal\nThe following is the prompt used to remove unrelated content in\nretrieved texts for the unrelated content removal attack.\nPrompt for Unrelated Content Removal Attack\nYou are a helpful assistant, below is a text which may\ncontain unrelated sentences. Please analyze the text and\nremove any incoherent or unrelated sentences. The text:\n{TEXT}\nE The Watermark Queries\nTo detect IP infringement or the misuse of sensitive data, the owner\ncan create watermark queries to obtain the watermark information\nfrom the responses of the suspect LLM and RAG systems. These\nqueries can vary, for example: (Type 1) What is the relationship\nbetween \ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a and \ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a? (Type 2) Please introduce the most relevant\ncontent of \ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a and \ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a. (Type 3) \ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a and \ud835\udc52\ud835\udc57\n\ud835\udc64\ud835\udc5a have a correlation,\nplease provide an introduction. We evaluate the effectiveness of\nthese queries, and the results are shown in Table 10. The queries\nyield similar performance, with average WSN values of 20, 19, 20\nand average WIRR values of 97.33%, 98.00%, 96.00%, respectively.\nThus, the owner can use diverse queries for watermark detection.\nF Entities and Relations\nTable 11 presents the number of entities and relations extracted\nfrom various datasets to create the entity and relation list {\ud835\udc38,\ud835\udc45}.\nFor the NFCorpus dataset, all items are fully processed, while other\ndatasets are extracted at varying rates due to their larger sizes. The\nextraction ratios and the corresponding numbers of entities and\nrelations are shown in Table 11.\nTable 10: Watermark Queries", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "35382242-5ef0-42f1-8133-91f2d8b29915": {"text": "datasets are extracted at varying rates due to their larger sizes. The\nextraction ratios and the corresponding numbers of entities and\nrelations are shown in Table 11.\nTable 10: Watermark Queries\nDataset Metrics Type 1 Type 2 Type 3\nTREC- WSN 18 19 20\nCOVID WIRR 96.67% 100.00% 96.67%\nNFCorpus WSN 24 20 25\nWIRR 100.00% 100.00% 100.00%\nMS- WSN 19 19 20\nMARCO WIRR 96.67% 93.33% 90.00%\nNQ WSN 19 22 21\nWIRR 93.33% 96.67% 96.67%\nHotPotQA WSN 20 15 16\nWIRR 100.00% 100.00% 96.67%\nTable 11: The Number of Extracted Entities and\nRelations\nDataset Extraction\nRate Entities Relations\nTREC-COVID 5.00% 74,176 127,764\nNFCorpus 100.00% 38,194 75,179\nNQ 0.19% 34,659 41,763\nHotpotQA 0.096% 25,300 27,707\nMS-MARCO 0.057% 29,530 36,167\n17", "metadata": {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "pdf_url": "http://arxiv.org/pdf/2401.15391v1"}}, "bd4377a5-7b86-4388-a620-3f7b62cbec00": {"text": "EACO-RAG: Edge-Assisted and Collaborative RAG with\nAdaptive Knowledge Update\nJiaxing Li\nSimon Fraser University\nBurnaby, BC, Canada\njla641@sfu.ca\nChi Xu\nSimon Fraser University\nBurnaby, BC, Canada\nchix@sfu.ca\nLianchen Jia\nTsinghua University\nBeijing, China\njlc21@mails.tsinghua.edu.cn\nFeng Wang\nUniversity of Mississippi\nUniversity, MS, USA\nfwang@cs.olemiss.edu\nCong Zhang\nJiangxing Intelligence Inc.\nShenzhen, China\nvcongzc@gmail.com\nJiangchuan Liu\nSimon Fraser University\nBurnaby, BC, Canada\njcliu@sfu.ca\nABSTRACT\nLarge Language Models are revolutionizing Web, mobile, and Web\nof Things systems, driving intelligent and scalable solutions. How-\never, as Retrieval-Augmented Generation (RAG) systems expand,\nthey encounter significant challenges related to scalability, includ-\ning increased delay and communication overhead. To address these\nissues, we propose EACO-RAG, an edge-assisted distributed RAG\nsystem that leverages adaptive knowledge updates and inter-node\ncollaboration. By distributing vector datasets across edge nodes and\noptimizing retrieval processes, EACO-RAG significantly reduces\ndelay and resource consumption while enhancing response accu-\nracy. The system employs a multi-armed bandit framework with\nsafe online Bayesian methods to balance performance and cost.\nExtensive experimental evaluation demonstrates that EACO-RAG\noutperforms traditional centralized RAG systems in both response\ntime and resource efficiency. EACO-RAG effectively reduces delay\nand resource expenditure to levels comparable to, or even lower\nthan, those of local RAG systems, while significantly improving\naccuracy. This study presents the first systematic exploration of\nedge-assisted distributed RAG architectures, providing a scalable\nand cost-effective solution for large-scale distributed environments.\n1 INTRODUCTION\nLarge Language Models (LLMs) are reshaping Web, mobile, and Web\nof Things (WoT) systems by enabling more intelligent, adaptive,\nand scalable solutions. Their integration enhances user experience\nwith accurate, real-time, and context-aware responses, improving\nsystem efficiency and unlocking new capabilities for autonomous\nprocessing [1], recommendations [2, 3], and decision-making [4, 5].\nRetrieval-Augmented Generation (RAG) extends LLMs\u2019 capa-\nbilities by integrating retrieval mechanisms that provide relevant\ncontext from a knowledge base, further improving the accuracy of\ngenerated responses. Its expanding presence in healthcare, educa-\ntion, and legal services is driving rapid adoption, with the market", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "34fb97aa-7e15-4ab2-a865-8968967e6c0b": {"text": "context from a knowledge base, further improving the accuracy of\ngenerated responses. Its expanding presence in healthcare, educa-\ntion, and legal services is driving rapid adoption, with the market\nprojected to grow at a compound annual growth rate of 44.7% be-\ntween 2024 and 2030 [6].\nAs RAG services continue to expand rapidly, scalability chal-\nlenges can result in Quality of Service (QoS) degradation, with typ-\nical concerns including reduced answer quality [7] and increased\nresponse delay [8, 9]. Commonly, RAG systems convert text into\nvector representations stored in databases and are hosted alongside\nlanguage models in centralized data centers. However, as service\nFigure 1: Overview of Edge-assisted and Collaborative RAG\n(EACO-RAG), together with a comparison to other solutions,\nshowing its improvements in answer quality, response time,\nand cost efficiency.\ndemand grows, these centralized architectures encounter problems\nsuch as increased delay, communication overhead with end users,\nand inefficient global database searches. A notable trend is the\ndeployment of LLMs at the edge [ 10, 11], which shows promis-\ning potential to reduce generation delay. In light of this, we ar-\ngue that shifting RAG services toward edge-assisted distributed\narchitectures could be a highly effective approach to address these\nscalability and performance challenges.\nMore specifically, edge integration could help by strategically\nplanning retrieval and generation based on end-user behavioral\npatterns in proximity, reducing delay, and improving performance.\nEdge nodes can analyze behaviors locally to preemptively optimize\nretrievals. In a cascaded fashion, simpler tasks may be processed at\nthe edge, while complex queries are escalated to central databases\nin the cloud. This approach can not only reduce communication\noverhead and enhance system efficiency, but also allow edge nodes\nto dynamically update their knowledge bases, enhancing their lo-\ncalized query handling capabilities and further reducing communi-\ncations with central cloud servers.\nDespite these benefits, integrating edge computing with RAG\nsystems presents challenges. For example, user queries span a wide\nrange of topics, requiring a knowledge base that potentially encom-\npasses global information. Maintaining such vast data locally at\nedge nodes is impractical due to storage and resource constraints.\nFurther, in distributed environments, optimizing retrieval processes\narXiv:2410.20299v1  [cs.DC]  27 Oct 2024", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "67027d79-e7b6-468d-ab13-da611a1cc876": {"text": "Li et al.\n0.5b 1.5b 3b 7b 14b 32b\nModel\n0.0\n1.5\n3.0\n4.5\n6.0Average FLOPs (TFLOPs)\nAverage FLOPs\nMobile Edge Limit\nEdge Server Limit\n(a) Models VS FLOPs\n0.5b 1.5b 3b 7b 14b 32b\nModel\n0\n20\n40\n60\n80Accuracy (%)\n0\n1\n2\n3\n4\nGeneration Time (s)\nAccuracy\nGeneration Time\nMobile Edge Limit\nEdge Server Limit (b) Models VS Accuracy and Generation Time\n0 5000 10000 15000\nDistance (km)\n0\n100\n200\n300RTT Delay (ms)\nCity A\nSF NY\nCBG\nBJ\nSARTT Delay (ms) (c) Cloud Model Deploy Location VS RTT\nFigure 2: Performance and bottleneck overview of LLM-only applications across varying parameter sizes.\n300 600 900 1200 1500\nChunk Size\n0\n20\n40\n60\n80Accuracy (%)\nAccuracy\n(a) Chunk Size VS Accuracy\n0 20 40 60 80\nT op K\n0\n20\n40\n60\n80Accuracy (%)\nAccuracy Avg Gen Time\n0\n2\n4\n6\nAvg Gen Time (s)\n (b) Top K VS Accuracy, Generation Time\nFigure 3: Impact of RAG parameters on performance: chunk\nsize and Top K vs. accuracy and generation time.\nto minimize resource consumption while ensuring real-time perfor-\nmance can be complicated, where such factors as physical distance\nto data centers, data volume, and node load all can influence re-\ntrieval delays.\nTraditional distributed databases and search engines have ad-\ndressed data scalability but often exhibit static architectures lacking\nflexibility, making it difficult to adapt to dynamic user needs in the\nRAG context. Additionally, centralized query processing increases\ndelay and communication overhead, while the absence of intelligent,\ndynamic updates limits efficient responses to changing demands.\nThese limitations further complicate the balance between efficient\ndata retrieval and adaptive performance in edge environments.\nIn this paper, we explore the following research question: \"Can\nRAG systems be optimized to minimize resource consumption while\nmaintaining performance through adaptive knowledge updates and\ncollaboration at edge nodes?\" Addressing this question provides in-\nsights into designing scalable, efficient, and intelligent RAG systems\ncapable of meeting complex and dynamic user needs.\nWe provide a positive answer by introducing EACO-RAG, a dis-\ntributed RAG system that fully integrates edge assistance through\nadaptive knowledge updates and collaboration at the edge. EACO-\nRAG distributes vector datasets across multiple edge nodes, optimiz-\ning the retrieval process by dynamically updating local knowledge\nbases. By leveraging edge computing and inter-node collaboration,\nthe system addresses key challenges such as scalability, delay reduc-", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "3ffc97ac-c101-40d7-a2db-21a369e630ad": {"text": "ing the retrieval process by dynamically updating local knowledge\nbases. By leveraging edge computing and inter-node collaboration,\nthe system addresses key challenges such as scalability, delay reduc-\ntion, and resource efficiency in large-scale distributed environments.\nIn this context, optimizing the RAG system involves the critical\nchallenge of minimizing costs while meeting strict accuracy and\ndelay constraints.\nTo tackle this, we model the optimization problem as a multi-\narmed bandit, utilizing safe online Bayesian methods to manage the\ntrade-off between accuracy, delay, and cost. As shown in Figure 1,\nEACO-RAG effectively reduces both delay and resource expendi-\nture to levels comparable to, or even lower than, those of local\nRAG systems, while significantly improving accuracy. In our exper-\niments, EACO-RAG achieves a 76.7% reduction in cost and a 74.2%\nreduction in delay compared to KGRAG-3B, with only an 11.5%\nsacrifice in accuracy. Our contributions are the following:\n\u2022 To our best knowledge, we are the first efforts to systemati-\ncally propose and investigate an edge-assisted distributed\nRAG architecture, EACO-RAG, which uses adaptive knowl-\nedge updates and collaborations at edge nodes to minimize\ndelay and communication overhead, offering a cost-efficient\nsolution for large-scale distributed environments.\n\u2022We propose adaptive knowledge update mechanisms that\nenable edge nodes to dynamically adjust their local knowl-\nedge bases, adapting to user behavior and evolving demands\nin real-time.\n\u2022We design optimized retrieval processes that are capable\nto integrate edge collaborations to better balance real-time\nperformance with resource efficiency, ensuring scalability\nacross distributed systems.\n\u2022We conduct extensive experiments evaluating EACO-RAG,\nwhich demonstrate the superiority of our EACO-RAG solu-\ntion in response times and resource utilization compared\nto traditional centralized RAG systems.\n2 MOTIVATION\nIn this section, we first perform a series of trace-driven analyses to\nidentify the bottlenecks, strengths, and limitations of existing RAG\nsystems, which motivate us to propose our EACO-RAG solution.\n2.1 Tradeoffs in RAG Systems\nTable 1: LLM Information.\nID Model Name Model Size Deploy\n1 qwen2.5:0.5b 398MB Mobile Edge\n2 qwen2.5:1.5b 986MB Mobile Edge\n3 qwen2.5:3b 1.9GB Mobile Edge\n4 qwen2.5:7b 4.7GB Edge Server\n5 qwen2.5:14b 9GB Edge Server\n6 qwen2.5:32b 20GB Cloud Server\n7 qwen2.5:72b 47GB Cloud Server\nA typical RAG system consists of two main components: the gen-", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "82c42b2c-328d-4010-90b1-fa020808dacf": {"text": "3 qwen2.5:3b 1.9GB Mobile Edge\n4 qwen2.5:7b 4.7GB Edge Server\n5 qwen2.5:14b 9GB Edge Server\n6 qwen2.5:32b 20GB Cloud Server\n7 qwen2.5:72b 47GB Cloud Server\nA typical RAG system consists of two main components: the gen-\nerator (i.e., LLM) and the knowledge database. In our analysis, we\nfocus on the bottlenecks of a generalized RAG system, specifically in\nterms of accuracy, delay and computational resource consumption,", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "c9cb3bd6-f9e1-4b4a-84f5-05b0a9dba097": {"text": "EACO-RAG: Edge-Assisted and Collaborative RAG with\nAdaptive Knowledge Update\nas determined by these two components. The models presented in\nTable 1 represent a selection of LLM configurations chosen for this\npaper. Note that here we highlight the importance of deploying\nLLM-based RAG systems at the edge [12], evaluating them in terms\nof cost, accuracy, and delay.\nGenerator: For the generator, increasing the number of parame-\nters typically enhances accuracy but at the cost of longer generation\ntimes and higher computational requirements. To evaluate these\ntrade-offs, we conducted experiments with 500 question-answer\n(QA) pairs using an NVIDIA RTX 4090, which represents a typical\nedge configuration, reflecting the computational power commonly\navailable at the edge [ 13]. The results, presented in Figures 2(a)\nand 2(b), show how model size impacts accuracy, generation time,\nand resource consumption, where the accuracy was measured by\ncomparing the generated responses with correct answers using\nGPT-4o1 [14].\nSome key trends can be observed from the figure: accuracy im-\nproves significantly as models scale from small to medium (0.5B\u20137B\nparameters) and from medium to large (7B\u201332B parameters). How-\never, these accuracy gains come with substantial trade-offs. For\ninstance, while generation times remain under 1 second for models\nup to 14B parameters, they surpass 4 seconds for the 32B model\ndue to the limitations of the RTX 4090. Moreover, deploying models\nlarger than 7B on mobile devices is impractical due to their high\ncomputational demands. The 32B model, in particular, exceeds the\ncapacity of typical edge servers. As shown in Figure 2(a), the aver-\nage TFLOP consumption per query increases dramatically beyond\nthe 7B parameter threshold.\nThese computational constraints suggest that larger models are\noften better suited for cloud deployment, but this introduces addi-\ntional network delays. Figure 2(c) depicts the network delay mea-\nsured via ICMP protocol over varying distances (with \"City A\"\nrepresenting the local region, and other locations anonymized).\nThe delay rises from under 10ms in local regions to over 100ms,\nand occasionally over 200ms, for more distant regions. While cloud-\nbased models benefit from faster generation times due to superior\ncomputational resources, their overall service time is predominantly\naffected by network delays, especially across longer distances. This\nunderscores the challenge of balancing model accuracy with real-\ntime performance.", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "9d3b0b52-3e2f-4a08-8d6b-40b5c11b1ca5": {"text": "computational resources, their overall service time is predominantly\naffected by network delays, especially across longer distances. This\nunderscores the challenge of balancing model accuracy with real-\ntime performance.\nGiven the delay and cost issues associated with cloud-based large\nmodels, deploying LLMs at the edge becomes critical. However,\nedge deployment presents accuracy challenges, making the use of\nan integrated knowledge database in RAG systems essential for\nimproving performance.\nKnowledge Base In RAG systems, two key parameters for op-\ntimizing performance are chunk size and the number of retrieved\ndocuments per query (Top K). These parameters directly influ-\nence retrieval efficiency and the quality of the generated responses.\nChunk size defines how the knowledge base is segmented (mea-\nsured in token count), while Top K determines the number of docu-\nments (or chunks) retrieved per query [15]. Figure 3(a) explores the\nrelationship between chunk size and accuracy. While larger chunk\nsizes can reduce redundancy and speed up retrieval, they may also\n1https://openai.com/index/hello-gpt-4o/\n1 2 3 4 5 6 7 8 9 1011121314151617181920\nClusters\n0\n25\n50\n75\n100Accuracy (%)\nLLM RAG KGRAG\n(a) Accuracy in Current RAG Approaches\n1 2 3 4 5 6 7 8 9 1011121314151617181920\nClusters\n0.0\n2.0\n4.0\n6.0\n8.0Generation Time (s)\nLLM RAG KGRAG\n(b) Generation Time in Current RAG Approaches\nFigure 4: Comparison of different RAG approaches: LLM\nonly, local RAG, and cloud KGRAG in terms of accuracy and\ngeneration time.\nlower accuracy by increasing the likelihood of irrelevant informa-\ntion being included in the LLM\u2019s context. Figure 3(b) examines the\ntrade-off between the number of retrieved chunks (Top K) and sys-\ntem performance. Increasing Top K enhances the LLM\u2019s contextual\nknowledge but also leads to higher delay, computational costs and\ngenerating hallucinations from excessive context. Through compar-\native analysis, we determined that a chunk size of 300 tokens and a\nTop K of 20 achieve an optimal balance between retrieval efficiency\nand answer accuracy for edge-deployed LLM-based RAG systems.\n2.2 Advantages and Disadvantages of Current\nRAG Systems\nAfter identifying the bottlenecks in current RAG systems, we now\nexamine their strengths and weaknesses in context-dependent sce-\nnarios. To do this, we collected 500 QA from a public dataset about\nthe Harry Potter series2. These questions were categorized into 20\nclusters based on their semantic correlations. We evaluated three", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "8aec1d0c-dc8f-41be-9c5e-3fdc98f9b886": {"text": "narios. To do this, we collected 500 QA from a public dataset about\nthe Harry Potter series2. These questions were categorized into 20\nclusters based on their semantic correlations. We evaluated three\nsystems: an LLM-only, a local RAG, and a cloud Knowledge Graph-\nbased RAG (KGRAG). Figure 4(a) shows the accuracy of these sys-\ntems across the different classes. It can be seen that over 50% of\nthe questions were context-dependent, meaning that when domain-\nspecific knowledge was required, the accuracy of standalone LLMs\ndropped to nearly zero (e.g., in clusters such as 5, 9 and 17). While\nRAG systems partially mitigate this issue, improving accuracy by\n15% to 20% over LLMs, they are still constrained by the scope of\ntheir local knowledge bases. If relevant information is missing, the\nbenefit of RAG is minimal. In contrast, KGRAG systems, which use\nstructured knowledge graphs, showed a significant increase in accu-\nracy\u2014an average improvement over 40% compared to LLMs\u2014when\nthe required information was available in the knowledge base.\n2https://huggingface.co/datasets/saracandu/harrypotter-trivia-ai-new", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "c27c8a8b-73e4-4b84-9f63-c78bfc87678d": {"text": "Li et al.\nHowever, these gains do not come for free. Although KGRAG\ndramatically enhances accuracy, it also notably increases generation\ntime. As shown in Figure 4(b), while generation times for LLM and\nbasic RAG systems remain relatively stable across various classes,\nthe generation times for KGRAG fluctuate due to the overhead of\nretrieving from huge knowledge bases. Depending on the retrieval\ncomplexity, generation times can spike from the usual 0.5 seconds to\nover 7 seconds (as seen in clusters 10, 11, 13, 16 and 19). On average,\nKGRAG\u2019s collaborative retrieval process extends generation time\nby an additional 1.95 seconds compared to standalone LLMs.\nThese results highlight both the strengths and limitations of\ntraditional RAG systems, reinforcing the need for more efficient de-\nsigns. While RAG systems improve accuracy by integrating domain-\nspecific knowledge, their overall effectiveness is hampered by issues\nlike fluctuating generation times and limited knowledge base cov-\nerage. This observation inspired us to explore a more adaptive\napproach. By dynamically updating the knowledge base based on\nuser queries and leveraging the vast number of distributed edge de-\nvices, we can harness the power of each edge node\u2019s local database.\nThrough collaboration between these distributed edge resources,\nqueries can be addressed more effectively at the edge, improving\ncontextual understanding while maintaining reasonable generation\ntimes. This approach is particularly valuable for context-dependent\napplications. In the next section, we will delve into the design of\nour distributed edge RAG system with adaptive knowledge update.\n3 EACO-RAG OVERVIEW AND SYSTEM\nDESIGN\nHigh-performance models, such as O1 models3, present significant\nchallenges for service providers due to their heavy computational\nrequirements and extended generation times, both of which nega-\ntively affect the overall quality of service (QoS). Furthermore, these\nmodels often underperform when handling context-dependent\nqueries, which are common in industrial applications. Resolving\nsuch queries typically necessitates additional model fine-tuning or\nthe use of retrieval-augmented generation (RAG), further compli-\ncating the operational workflow.\nTo address these challenges, in EACO-RAG, we propose an on-\nline adaptive knowledge update system that optimizes response\nquality, generation time, and costs by adaptively selecting from\nLLMs, RAG, and knowledge-graph RAG (KGRAG) based on con-\ntextual information. Moreover, in this system, simpler queries are", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "3737c119-cd6f-4313-b784-72018d4454ae": {"text": "quality, generation time, and costs by adaptively selecting from\nLLMs, RAG, and knowledge-graph RAG (KGRAG) based on con-\ntextual information. Moreover, in this system, simpler queries are\nprocessed by edge nodes located near the user, while more com-\nplex queries are escalated to the cloud for handling, operating in a\ncascading structure to efficiently manage resource allocation.\nAs depicted in Figure 5, user queries are initially stored in the\ncloud as they pass through the gate in edge nodes, with this data be-\ning used to update the local databases at the edge. Once a sufficient\nnumber of queries are collected, the system generates an abstract\nusing an LLM (e.g., GPT-4o), producing summaries such as \u201cNames\nand functions of spells in the wizarding world\u201d or \u201cRules, history, and\nkey events in Quidditch\u201d , which reflect common topics queried by\nnearby users. These abstracts are then compared to community\nsummaries stored in the knowledge graph by embedding both sets\nof data and assessing their similarity. The most relevant data chunks\n3https://openai.com/o1/\nEdge 1\nCloud\nQuery\nResponse\nEdge Queries\nGPT-4o\nAbstractEdge Abstracts\nEmbeddingEdge 1\nEdge 2\nEdge 3\nEdge 1\nEdge 2\nEdge 3\nSimilarity\nCommunity\nSummary\nKnowledge Graph\nEmbedding\nCloud Database\nChunk DataGate\n GraphRAGEdge DBEdge LM\nEdge 2\nQuery\nResponseGate\n Edge DBEdge LM\nConnected\nEdge 3 Edge 4 Edge 5\nAll other Edges have the same config.\nFigure 5: Workflow of the EACO-RAG system design.\nare identified and downloaded to the edge for local RAG operations.\nIf the abstract changes based on recent user queries at the edge, the\nlocal database is updated accordingly.\nOn the other hand, when a new query is received at the edge, the\nsystem determines the optimal retrieval and generation strategy\nbased on query complexity, network delay, and similarity to other\nedge databases. However, query complexity and similarity to other\nedge databases are not straightforward to obtain. Specifically, query\ncomplexity can be assessed using a simple classifier [ 16], while\nsimilarity to other edge databases is determined by comparing the\nembedding of the user query with the summaries from each edge.\nThis approach facilitates efficient retrieval from relevant edge nodes\nand enhances overall system stability.\nFurthermore, the gate mechanism plays a vital role in reducing\ngeneration time and operational costs while preserving service\nquality. It determines the necessity of retrieval and selects the most\nefficient approach\u2014whether local, edge, cloud retrieval or bypassing", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "836e86d4-356f-47f1-be8b-78b0a8bc286e": {"text": "generation time and operational costs while preserving service\nquality. It determines the necessity of retrieval and selects the most\nefficient approach\u2014whether local, edge, cloud retrieval or bypassing\nretrieval for simpler queries. Additionally, the gate decides whether\nto generate responses using a local model or delegate to the more\npowerful cloud models. The local knowledge base is continuously\nupdated, and the gate dynamically optimizes the RAG retrieval and\ngeneration path, minimizing costs while ensuring that response\naccuracy and generation delay meet the required thresholds.\nAs a provider of LLM-based services, our primary objectives are\nto ensure response quality, reduce generation delay, and minimize\noperational costs. To address this, we model the problem as a con-\ntextual multi-armed bandit and employ a gate architecture based\non Safe Online Bayesian Optimization. This approach minimizes\ntotal costs while maintaining the necessary accuracy and delay\nstandards, effectively balancing the use of both edge and cloud\nresources. Detailed implementation strategies are presented in the\nsubsequent sections.\n4 COST MODELLING AND OPTIMIZATION\n4.1 Cost Modelling with Contextual\nMulti-Armed Bandit\nOur objective is to minimize the total operational costs incurred by\nthe service provider, which include both resource usage and delay\ncosts. Resource costs arise from computational expenses related\nto model generation and knowledge base retrieval, while delay\ncosts reflect the opportunity cost of increased response times due\nto network latency and model generation delays. The system must", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "77d00e46-c3d4-4218-9a63-07d8d864689c": {"text": "EACO-RAG: Edge-Assisted and Collaborative RAG with\nAdaptive Knowledge Update\nsatisfy two key constraints: (i) answer accuracy must exceed a\ndefined threshold \ud835\udf0cmin, and (ii) response time must remain below a\nreal-time limit \u210emax.\nContext: At each time \ud835\udc61, the context is represented as \ud835\udc50\ud835\udc61 :=\n[\ud835\udc51\ud835\udc61\ncloud,\ud835\udc51\ud835\udc61\nedge,\ud835\udc60\ud835\udc61,\ud835\udc5e\ud835\udc61]\u2208C , where\ud835\udc51\ud835\udc61\ncloud and \ud835\udc51\ud835\udc61\nedge represent network\ndelays to the cloud and edge servers, respectively, \ud835\udc60\ud835\udc61 indicates the\nsimilarity between the query and the summaries in the edge data-\nbase, and \ud835\udc5e\ud835\udc61 reflects the complexity of the query. The context space\nis denoted by C.\nControl Policies: Let \ud835\udc51\ud835\udc61 := [\ud835\udc5f\ud835\udc61,\ud835\udc54\ud835\udc61]\u2208D represent the control\npolicy at time \ud835\udc61, where \ud835\udc5f\ud835\udc61 denotes the retrieval location and \ud835\udc54\ud835\udc61\nspecifies the generation location. Retrieval \ud835\udc5f\ud835\udc61 can occur locally, at\nthe edge, from the cloud or non-retrieval, while generation \ud835\udc54\ud835\udc61 can\ntake place at either the edge or cloud. These policies are dynamically\nadjusted based on the real-time context to optimize cost.\nThe system carefully balances service provider costs, delay, and\naccuracy. Local and edge-based databases provide low-cost, low-\ndelay responses that are well-suited for simple, highly relevant\nqueries, which can be efficiently processed at the local level. For\nmore complex queries that demand broader knowledge or greater\naccuracy, the system retrieves data from the cloud-based knowledge\ngraph. While this incurs higher costs and greater delay, it delivers\nmore accurate and comprehensive responses. Similarly, for response\ngeneration, the system balances between the faster but less powerful\nedge models, which suffice for basic queries, and the more accurate\ncloud models, which, though more costly, ensure the precision\nneeded for more complex queries.\nThe total cost function is defined as:\n\ud835\udc62\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61)= \ud835\udeff1 \u00b7\ud835\udc62\ud835\udc61\nr (\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61)+\ud835\udeff2 \u00b7\ud835\udc62\ud835\udc61\nd (\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61), (1)\nwhere \ud835\udeff1 and \ud835\udeff2 are weights for resource and delay costs, respec-\ntively. \ud835\udc62\ud835\udc61r (\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61)represents the computational cost incurred by the\nservice provider during model generation and data retrieval, while\n\ud835\udc62\ud835\udc61\nd (\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61)accounts for delay time costs, including those related to\nnetwork network and model generation delays, as excessive delays\ncan reduce system efficiency and increase operational overhead for\nthe provider.\nThe optimization problem is formulated as:\nmin\n{\ud835\udc51\ud835\udc61 }\ud835\udc47\n\ud835\udc61=1\n\ud835\udc47\u2211\ufe01\n\ud835\udc61=1\n\ud835\udc62\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61)\ns.t. \ud835\udf0c\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61)\u2265 \ud835\udf0cmin, \u2200\ud835\udc61 \u2264\ud835\udc47,\n\u210e\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61)\u2264 \u210emax, \u2200\ud835\udc61 \u2264\ud835\udc47,\n(2)\nwhere \ud835\udf0c\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61)is the accuracy of the generated answer, and\n\u210e\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61)represents the model response time at time \ud835\udc61.\nThis formulation focuses on minimizing the service provider\u2019s", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "79f51534-5ac1-4102-a7b1-e93884ef950f": {"text": "\ud835\udc61=1\n\ud835\udc62\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61)\ns.t. \ud835\udf0c\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61)\u2265 \ud835\udf0cmin, \u2200\ud835\udc61 \u2264\ud835\udc47,\n\u210e\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61)\u2264 \u210emax, \u2200\ud835\udc61 \u2264\ud835\udc47,\n(2)\nwhere \ud835\udf0c\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61)is the accuracy of the generated answer, and\n\u210e\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61)represents the model response time at time \ud835\udc61.\nThis formulation focuses on minimizing the service provider\u2019s\noperational costs while ensuring that both accuracy and delay\nconstraints are met. The system adapts dynamically to evolving\nnetwork conditions and database content using online learning\ntechniques to continuously refine decision-making strategies. As\nnetwork delay or database content changes, the system adjusts\nretrieval and generation strategies in real time to maintain opti-\nmal performance while minimizing costs. In the next section, we\nwill explore the use of Safe Online Bayesian Optimization to de-\nvelop algorithms that ensure these constraints are respected, while\nminimizing service provider costs and maintaining high quality of\nservice.\n4.2 Safe Online Bayesian Optimization\nIn this part, we present our solution to the contextual multi-armed\nbandit problem discussed earlier to further enhance the system\u2019s\ndecision-making process in dynamic environments. To this end, we\nintroduce a Bayesian online optimization approach, summarized\nin Algorithm 1, which utilizes Gaussian Processes (GPs) to cap-\nture the nonlinearities and correlations within the system, while\nalso quantifying the uncertainty in function estimation. This algo-\nrithm enables the system to dynamically balance exploration and\nexploitation, leading to performance improvements over time with\nminimal cost and delay. Further details on the algorithm, including\nits implementation and operation, are provided in the following\nsection.\nFunction Approximator. We use Gaussian Processes (GPs) to\nestimate the cost and constraint functions, modeling them as collec-\ntions of random variables with joint Gaussian distributions [17, 18].\nLet \ud835\udc65 \u2208X = C\u00d7D represent a context-decision pair. Each un-\nknown function is modeled as a sample from GP(\ud835\udf07(\ud835\udc65),\ud835\udc58(\ud835\udc65,\ud835\udc65\u2032)),\nwhere \ud835\udf07(\ud835\udc65)is the mean function and \ud835\udc58(\ud835\udc65,\ud835\udc65\u2032)is the kernel func-\ntion [18] that captures the covariance between points \ud835\udc65 and \ud835\udc65\u2032.\nWe assume a prior distribution where \ud835\udf07(\ud835\udc65)= 0 and \ud835\udc58(\ud835\udc65,\ud835\udc65\u2032)< 1,\nmeaning the distribution is unconditioned by data.\nGiven this prior and a set of observed data, we compute the\nposterior distribution of the functions in closed form. Let \ud835\udc4b\ud835\udc47 =\n{\ud835\udc651,...,\ud835\udc65 \ud835\udc47}represent the observed context-decision pairs up to\ntime \ud835\udc47. The observations for cost, accuracy, and response time are\ndenoted as \ud835\udc66(0)\n\ud835\udc47 = {\ud835\udc621,...,\ud835\udc62 \ud835\udc47}, \ud835\udc66(1)\n\ud835\udc47 = {\ud835\udf0c1,...,\ud835\udf0c \ud835\udc47}, and \ud835\udc66(2)\n\ud835\udc47 =", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "ea6cc7c4-346e-4e8b-af89-034517921b5b": {"text": "{\ud835\udc651,...,\ud835\udc65 \ud835\udc47}represent the observed context-decision pairs up to\ntime \ud835\udc47. The observations for cost, accuracy, and response time are\ndenoted as \ud835\udc66(0)\n\ud835\udc47 = {\ud835\udc621,...,\ud835\udc62 \ud835\udc47}, \ud835\udc66(1)\n\ud835\udc47 = {\ud835\udf0c1,...,\ud835\udf0c \ud835\udc47}, and \ud835\udc66(2)\n\ud835\udc47 =\n{\u210e1,...,\u210e \ud835\udc47}, respectively, assuming independent Gaussian noise\nwith variance \ud835\udf0e2. The posterior distribution remains Gaussian, with\nupdated mean \ud835\udf07(\ud835\udc56)\n\ud835\udc47 (\ud835\udc65)and covariance \ud835\udc58(\ud835\udc56)\n\ud835\udc47 (\ud835\udc65,\ud835\udc65\u2032)for each function\n\ud835\udc56(cost, accuracy, response time) given by:\n\ud835\udf07(\ud835\udc56)\n\ud835\udc47 (\ud835\udc65)= \ud835\udc58(\ud835\udc56)\n\ud835\udc47 (\ud835\udc65)\ud835\udc47(\ud835\udc3e(\ud835\udc56)\n\ud835\udc47 +\ud835\udf0e2\ud835\udc3c\ud835\udc47)\u22121\ud835\udc66(\ud835\udc56)\n\ud835\udc47 (3)\n\ud835\udc58(\ud835\udc56)\n\ud835\udc47 (\ud835\udc65,\ud835\udc65\u2032)= \ud835\udc58(\ud835\udc56)(\ud835\udc65,\ud835\udc65\u2032)\u2212\ud835\udc58(\ud835\udc56)\n\ud835\udc47 (\ud835\udc65)\ud835\udc47(\ud835\udc3e(\ud835\udc56)\n\ud835\udc47 +\ud835\udf0e2\ud835\udc3c\ud835\udc47)\u22121\ud835\udc58(\ud835\udc56)\n\ud835\udc47 (\ud835\udc65\u2032), (4)\nwhere \ud835\udc58(\ud835\udc56)\n\ud835\udc47 (\ud835\udc65)= [\ud835\udc58(\ud835\udc56)(\ud835\udc651,\ud835\udc65),...,\ud835\udc58 (\ud835\udc56)(\ud835\udc65\ud835\udc47,\ud835\udc65)]\ud835\udc47 is the covariance\nvector between \ud835\udc65 and the observed points in \ud835\udc4b\ud835\udc47, \ud835\udc3e(\ud835\udc56)\n\ud835\udc47 is the kernel\nmatrix [\ud835\udc58(\ud835\udc56)(\ud835\udc65,\ud835\udc65\u2032)]\ud835\udc65,\ud835\udc65\u2032\u2208\ud835\udc4b\ud835\udc47 , and \ud835\udc3c\ud835\udc47 is the identity matrix of size \ud835\udc47.\nThe index \ud835\udc56 corresponds to different objective functions: \ud835\udc56 = 0 for\ncost, \ud835\udc56 = 1 for accuracy, and \ud835\udc56 = 2 for response time.\nFor any unobserved \ud835\udc65 \u2208X, the posterior mean and covariance\nfor function \ud835\udc56 are derived from the prior, observed data \ud835\udc4b\ud835\udc47, and\ncorresponding observations \ud835\udc66(\ud835\udc56)\n\ud835\udc47 using the equations above.\nSafe Set. Identifying the safe set, consisting of control policies\nthat meet system constraints in a given context, is essential for\nmaintaining performance within acceptable limits. The safe set\ndepends on both the control policies and the context. For instance,\nin our problem, a context change such as a complex query or high\nnetwork delay may require shifting from local resources to cloud-\nbased resources, increasing response time. As a result, policies that\nperform well under low-delay conditions might not satisfy the\nresponse time constraint in high-delay scenarios.", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "cc3d1224-b5b0-4a70-9093-d16529bee78c": {"text": "Li et al.\nWe define the safe set \ud835\udc46\ud835\udc61 at time \ud835\udc61 as the set of context-decision\npairs that satisfy all system constraints:\n\ud835\udc46\ud835\udc61 = {\ud835\udc65 \u2208X| \ud835\udf07(1)\n\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)\u2212\ud835\udefd\ud835\udf0e(1)\n\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)\u2265 \ud835\udf0cmin\n\u2227\ud835\udf07(2)\n\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)+\ud835\udefd\ud835\udf0e(2)\n\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)\u2264 \u210emax}\n(5)\nWhere \ud835\udf07(\ud835\udc56)\n\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)and \ud835\udf0e(\ud835\udc56)\n\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)represent the mean and uncer-\ntainty (standard deviation) of the Gaussian Process predictions for\nfunction \ud835\udc56 (accuracy or response time) at time \ud835\udc61. The exploration\nparameter \ud835\udefd balances exploration and exploitation, while \ud835\udf0cmin and\n\u210emax denote the minimum accuracy and maximum response time,\nrespectively.\nDetermining the safe set is challenging due to the noisy nature\nof system performance indicators, such as accuracy and response\ntime, which are affected by the stochastic environment. To address\nthis, we use Gaussian Processes (GPs) to estimate the safe set based\non observed data. At each time step\ud835\udc61, after observing the context\ud835\udc50\ud835\udc61\nand decision \ud835\udc65\ud835\udc61, the GP models are updated, refining the estimates\nof the mean \ud835\udf07(\ud835\udc56)\n\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)and uncertainty \ud835\udf0e(\ud835\udc56)\n\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)for both cost and\nconstraint functions. Because of the correlations between nearby\npoints in the context-decision space, updating the posterior for\none point influences the estimates for surrounding points, which\naffects the composition of the safe set in the following time step,\n\ud835\udc61 +1. This dynamic update process allows the system to contin-\nually adjust its estimation of the safe set, ensuring that it adapts\nto changing contexts while maintaining performance within the\nrequired constraints.\nStep One: Exploration . The first phase of the AdaptiveEdge\nSafeOBO algorithm serves as a warm-up step, focusing on explor-\ning the control space Xwithout initially considering the safe set.\nThe purpose of this warm-up phase is to gather foundational data on\nthe cost, accuracy, and response time functions by randomly select-\ning context-decision pairs from X. At each time step \ud835\udc61, a context \ud835\udc50\ud835\udc61\nis observed, and the algorithm randomly selects a decision \ud835\udc65\ud835\udc61 from\nthe control space. This exploration allows the algorithm to collect\nsufficient observations of system performance under varying con-\nditions, creating a solid basis for more informed decision-making\nin later steps. Formally, the decision \ud835\udc65\ud835\udc61 is chosen as:\n\ud835\udc65\ud835\udc61 = Randomly select from X. (6)\nOnce a decision is made, the corresponding accuracy \ud835\udf0c\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61),\nresponse time\u210e\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61), and total cost\ud835\udc62\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61)are observed. These\nobservations are used to update the Gaussian Process (GP) posteri-\nors for the cost, accuracy, and response time functions, setting the\nfoundation for the exploitation phase.", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "8a195056-75d2-425d-ad5b-f579490c32bf": {"text": "response time\u210e\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61), and total cost\ud835\udc62\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61)are observed. These\nobservations are used to update the Gaussian Process (GP) posteri-\nors for the cost, accuracy, and response time functions, setting the\nfoundation for the exploitation phase.\nStep Two: Exploitation and Safe Set Optimization . In the\nsecond phase, the algorithm transitions from random exploration\nto strategic decision-making, utilizing the data gathered in the\nexploration phase. At this point, AdaptiveEdge SafeOBOesti-\nmates a safe set \ud835\udc46\ud835\udc61 of context-decision pairs that meet the system\u2019s\nconstraints on accuracy and response time. Specifically, the safe\nset includes decisions expected to satisfy both the minimum accu-\nracy \ud835\udf0cmin and the maximum response time \u210emax, with confidence\nbounds based on the GP posteriors:\nAlgorithm 1: AdaptiveEdge SafeOBO Algorithm\n1 Inputs: Control space X, Safe seed set \ud835\udc460, kernel \ud835\udc58, \ud835\udf0cmin (minimum\naccuracy), \u210emax (maximum response time), exploration parameter\n\ud835\udefd, cost weights \ud835\udeff1, \ud835\udeff2\n2 Initialization: \ud835\udc4d0 = \u2205,\ud835\udc66(0)\n0 = \u2205,\ud835\udc66(1)\n0 = \u2205,\ud835\udc66(2)\n0 = \u2205;\n3 for \ud835\udc61 = 1,...,\ud835\udc47 0 (Exploration phase) do\n4 Observe context \ud835\udc50\ud835\udc61 ;\n5 Randomly select \ud835\udc65\ud835\udc61 from X(random exploration);\n6 Observe \u210e\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61 )(response time), \ud835\udf0c\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61 )(accuracy),\n\ud835\udc62\ud835\udc61r (\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61 )(resource cost), \ud835\udc62\ud835\udc61\nd (\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61 )(delay cost);\n7 Compute total cost:\n\ud835\udc62\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61 )= \ud835\udeff1 \u00b7\ud835\udc62\ud835\udc61r (\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61 )+\ud835\udeff2 \u00b7\ud835\udc62\ud835\udc61\nd (\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61 );\n8 Update \ud835\udc4d\ud835\udc61 = \ud835\udc4d\ud835\udc61\u22121 \u222a{\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61 };\n9 Update GP posteriors:\n10 \ud835\udc66(0)\n\ud835\udc61 \u2190\ud835\udc66(0)\n\ud835\udc61\u22121 \u222a\ud835\udc62\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61 )(update cost posterior);\n11 \ud835\udc66(1)\n\ud835\udc61 \u2190\ud835\udc66(1)\n\ud835\udc61\u22121 \u222a\ud835\udf0c\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61 )(update accuracy posterior);\n12 \ud835\udc66(2)\n\ud835\udc61 \u2190\ud835\udc66(2)\n\ud835\udc61\u22121 \u222a\u210e\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61 )(update response time posterior);\n13 end\n14 for \ud835\udc61 = \ud835\udc470 +1,...,\ud835\udc47 (Exploitation phase) do\n15 Observe context \ud835\udc50\ud835\udc61 ;\n16 Compute \ud835\udf07(\ud835\udc56)\n\ud835\udc61\u22121 (\ud835\udc50\ud835\udc61,\ud835\udc65)and \ud835\udf0e(\ud835\udc56)\n\ud835\udc61\u22121 (\ud835\udc50\ud835\udc61,\ud835\udc65)for all \ud835\udc56 = 0,1,2, using\nthe posterior from the previous iteration;\n17 Estimate the safe set:\n18 \ud835\udc46\ud835\udc61 = \ud835\udc460 \u222a{\ud835\udc65 \u2208X| \ud835\udf07(1)\n\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)\u2212 \ud835\udefd\ud835\udf0e(1)\n\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)\u2265 \ud835\udf0cmin\n19 \u2227\ud835\udf07(2)\n\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)+ \ud835\udefd\ud835\udf0e(2)\n\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)\u2264 \u210emax };\n20 Select \ud835\udc65\ud835\udc61 = arg min\ud835\udc65 \u2208\ud835\udc46\ud835\udc61 \ud835\udf07(0)\n\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)\u2212\n\u221a\ufe01\n\ud835\udefd\ud835\udc61 \ud835\udf0e(0)\n\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)\n(acquisition function);\n21 Observe \u210e\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61 )(response time), \ud835\udf0c\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61 )(accuracy),\n\ud835\udc62\ud835\udc61r (\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61 )(resource cost), \ud835\udc62\ud835\udc61\nd (\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61 )(delay cost);\n22 Compute total cost:\n\ud835\udc62\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61 )= \ud835\udeff1 \u00b7\ud835\udc62\ud835\udc61r (\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61 )+\ud835\udeff2 \u00b7\ud835\udc62\ud835\udc61\nd (\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61 );\n23 Update \ud835\udc4d\ud835\udc61 = \ud835\udc4d\ud835\udc61\u22121 \u222a{\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61 };\n24 Update GP posteriors:\n25 \ud835\udc66(0)\n\ud835\udc61 \u2190\ud835\udc66(0)\n\ud835\udc61\u22121 \u222a\ud835\udc62\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61 )(update cost posterior);\n26 \ud835\udc66(1)\n\ud835\udc61 \u2190\ud835\udc66(1)\n\ud835\udc61\u22121 \u222a\ud835\udf0c\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61 )(update accuracy posterior);\n27 \ud835\udc66(2)\n\ud835\udc61 \u2190\ud835\udc66(2)\n\ud835\udc61\u22121 \u222a\u210e\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61 )(update response time posterior);\n28 end\n\ud835\udc46\ud835\udc61 = \ud835\udc460 \u222a{\ud835\udc65 \u2208X| \ud835\udf07(1)\n\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)\u2212\ud835\udefd\ud835\udf0e(1)\n\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)\u2265 \ud835\udf0cmin\n\u2227\ud835\udf07(2)\n\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)+\ud835\udefd\ud835\udf0e(2)\n\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)\u2264 \u210emax}\n(7)", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "9372eccf-2134-4691-87a2-0a842ffacae0": {"text": "26 \ud835\udc66(1)\n\ud835\udc61 \u2190\ud835\udc66(1)\n\ud835\udc61\u22121 \u222a\ud835\udf0c\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61 )(update accuracy posterior);\n27 \ud835\udc66(2)\n\ud835\udc61 \u2190\ud835\udc66(2)\n\ud835\udc61\u22121 \u222a\u210e\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61 )(update response time posterior);\n28 end\n\ud835\udc46\ud835\udc61 = \ud835\udc460 \u222a{\ud835\udc65 \u2208X| \ud835\udf07(1)\n\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)\u2212\ud835\udefd\ud835\udf0e(1)\n\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)\u2265 \ud835\udf0cmin\n\u2227\ud835\udf07(2)\n\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)+\ud835\udefd\ud835\udf0e(2)\n\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)\u2264 \u210emax}\n(7)\nWithin the safe set, the algorithm selects the decision \ud835\udc65\ud835\udc61 that\nminimizes the expected total cost while considering the uncertainty\n(exploration) in the GP model. This is done by optimizing the fol-\nlowing acquisition function:\n\ud835\udc65\ud835\udc61 = arg min\n\ud835\udc65\u2208\ud835\udc46\ud835\udc61\n\ud835\udf07(0)\n\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)\u2212\n\u221a\ufe01\n\ud835\udefd\ud835\udc61\ud835\udf0e(0)\n\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65), (8)\nwhere \ud835\udf07(0)\n\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)and \ud835\udf0e(0)\n\ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)are the posterior mean and vari-\nance of the cost function at time \ud835\udc61, and \ud835\udefd\ud835\udc61 controls the trade-off\nbetween exploration and exploitation.", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "30e7b354-cbbd-4eb2-9ced-dd8f94384345": {"text": "EACO-RAG: Edge-Assisted and Collaborative RAG with\nAdaptive Knowledge Update\nBy iteratively updating the safe set and selecting decisions within\nit, the algorithm ensures optimal performance while respecting ac-\ncuracy and response time constraints. As more observations are\ngathered, the GP models are refined, allowing the system to contin-\nuously improve decision-making, optimizing both safety and utility\nover time.\n5 IMPLEMENTATION AND EVALUATION\n5.1 Prototype Implementation\nOur prototype implementation of the EACO-RAG system adopts\na dual-tier architecture that balances efficiency and performance\nby deploying models at both the edge and in the cloud. The system\nutilizes two distinct models, each tailored for different query types\nbased on complexity and resource requirements:\n\u2022 Edge 3B Model : A compact language model deployed on\nedge nodes, optimized for straightforward or moderately\ncomplex queries. This model delivers low-delay responses\nwith minimal computational overhead, making it ideal for\nscenarios where speed and resource efficiency are essential.\n\u2022 Cloud 72B Model : A high-capacity language model de-\nployed in the cloud, dedicated to handling more complex\nand cross-domain queries. Although it incurs higher com-\nputational costs and longer delay, this model ensures highly\naccurate responses for tasks requiring deep knowledge or\ncontextual understanding.\nThe architecture dynamically selects between these models based\non query complexity, processing simple requests at the edge to re-\nduce delay, while directing more demanding queries to the cloud.\nThis flexible design optimizes response times and resource alloca-\ntion across distributed environments.\nIn our experimental setup, the edge nodes are powered by NVIDIA\nRTX 4090 GPUs, enabling rapid inference and scalability at the lo-\ncal level. The cloud-based 72B model serves as a benchmark for\nhigh-accuracy, complex query processing, although it incurs higher\ndelays, highlighting the trade-off between edge efficiency and cloud-\nlevel precision.\nBy effectively distributing tasks between edge and cloud re-\nsources, the EACO-RAG system demonstrates significant improve-\nments in both query response times and resource utilization, un-\nderscoring the potential of EACO-RAG for practical applications.\n5.2 Convergence\nTo evaluate the convergence of the EACO-RAG system, we consider\nand measure a series of parameters, including different exploration\nrounds \ud835\udc470, varying delay cost weights \ud835\udeff2, and the impact on overall\ncost under specific accuracy and delay thresholds.", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "44b06616-b200-45e9-80fd-ce11b1955be7": {"text": "and measure a series of parameters, including different exploration\nrounds \ud835\udc470, varying delay cost weights \ud835\udeff2, and the impact on overall\ncost under specific accuracy and delay thresholds.\nTo specifically assess the convergence speed and the impact of\ud835\udc470\non the balance between exploration and exploitation, we first plot\nthe cost against the number of iterations for\ud835\udc470 = 5,10,20,50,100. In\nFigure 6, the cost variation across increasing iterations for different\n\ud835\udc470 values is illustrated. The results indicate that as \ud835\udc470 increases, the\nsystem\u2019s convergence rate initially accelerates but then slows down.\nThis suggests that, in our scenario, approximately 10 exploration\nrounds are sufficient for gathering the necessary initial information,\n0\n10Cost\nT0=5\n0\n10Cost\nT0=10\n0\n10Cost\nT0=20\n0\n10Cost\nT0=50\n0 100 200 300 400 500\nIterations\n0\n10Cost\nT0=100\nFigure 6: Converge evaluation with different exploration\nrounds.\nTable 2: TFLOP consumption per second of waiting for vari-\nous server-side NVIDIA GPUs.\nGPU Model FP64 (Double Precision)\nNVIDIA GeForce RTX 4090 1.29 TFLOPS\nNVIDIA Tesla P100 4.70 TFLOPS\nNVIDIA Tesla V100 7.80 TFLOPS\nNVIDIA A100 Tensor Core 9.70 TFLOPS\nNVIDIA H100 Tensor Core 60.00 TFLOPS\n0\n100Cost (TFLOPs)\n2\n4Accuracy\n0 100 200 300 400 500\nIterations\n2\n4Delay (s)\n200 4000.5\n1.0\n2=1.29\n 2=4.7\n 2=7 .8\n 2=9 .7\n 2=6 0\nFigure 7: Convergence evaluation under varying delay cost\nweights. The weights are based on the TFLOP per second\ngenerated by different GPU models. The cost is represented\nas total TFLOPs.\nwhereas excessive warm-up rounds may impede convergence. No-\ntably, regardless of the\ud835\udc470 value, the system eventually converges to\na similar cost level, demonstrating the robustness of the EACO-RAG\nalgorithm.\nFigure 7 presents the system\u2019s performance under different delay\ncost weights\ud835\udeff2, including total cost (measured in TFLOPs), accuracy,\nand delay (in seconds). We set the resource cost weight\ud835\udeff1 to 1, as", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "f4d546f2-c89b-4f23-91a4-5976c8240ce5": {"text": "Li et al.\n1.29 4.7 7.8 9.7 60\n2\n0\n20\n40\n60Cost (TFLOPs)\nmin = 0.9, hmax = 1\n min = 0.8, hmax = 1.5\n min = 0.7, hmax = 2\nFigure 8: Cost variation as a function of \ud835\udeff2 under different\naccuracy and delay thresholds.\nresource cost is directly measured in TFLOPs. Delay cost weights\ud835\udeff2\nare chosen based on the typical TFLOP performance of server GPUs\nin double-precision (FP64) computations, as detailed in Table 2.\nAfter convergence, we observe that as \ud835\udeff2 increases, the total cost\nrises, but delay decreases due to its increased weighting in the cost\nfunction. In practice, an optimal balance between cost, accuracy,\nand delay must be determined based on the specific application\nrequirements.\nFigure 8 shows how cost varies with \ud835\udeff2 under different accuracy\nand delay thresholds. In addition to the general trend of increasing\ncost with higher\ud835\udeff2 values, we note a faster rise in cost when stricter\naccuracy and delay thresholds are applied. The relationship between\ncost and \ud835\udeff2 is nonlinear, and the jump points are influenced by the\nthresholds, suggesting that certain GPUs may be more suitable for\nminimizing cost under specific thresholds.\nLLM-3b RAG-3b EACO-RAGKGRAG-3bKGRAG-72b\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Normalized Value\nCost\nDelay\nAccuracy\nFigure 9: Comparison of LLM, RAG, KGRAG, and our pro-\nposed EACO-RAG method, evaluated based on normalized\ncost, delay, and accuracy.\n5.3 Performance Comparison\nIn Figure 9, we compare the performance of LLM (LLM-3b), local\nRAG (RAG-3b), cloud KGRAG (KGRAG-3b and KGRAG-70b), and\nour proposed EACO-RAG method across three key metrics: cost,\ndelay, and accuracy. The values in the figure are normalized to\nenable a consistent comparison across these different metrics.\nCost and Delay: LLM-3B exhibits the lowest cost and delay, as it\ndoes not require additional retrieval operations. EACO-RAG\u2019s cost\nand delay are slightly higher than LLM-3B, but significantly lower\nthan other RAG methods, demonstrating its cost-effectiveness and\ndelay advantages. Compared to KGRAG-3B, EACO-RAG achieves\na 76.7% reduction in cost and a 74.2% reduction in delay, while\nonly sacrificing 11.5% in accuracy. KGRAG-72B, on the other hand,\nhas the highest cost and delay due to its use of a large model and\ncomplex knowledge graph retrieval process.\nAccuracy: LLM-3B has the lowest accuracy, reflecting the lim-\nitations of relying solely on a smaller model to handle complex\nqueries. EACO-RAG\u2019s accuracy is significantly higher than both\nLLM-3B and RAG-3B, approaching the performance of KGRAG-3B.", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "86f06f9e-5553-402c-b080-cd333eae9c2a": {"text": "Accuracy: LLM-3B has the lowest accuracy, reflecting the lim-\nitations of relying solely on a smaller model to handle complex\nqueries. EACO-RAG\u2019s accuracy is significantly higher than both\nLLM-3B and RAG-3B, approaching the performance of KGRAG-3B.\nCompared to RAG-3B, EACO-RAG achieves a 51% improvement\nin accuracy with comparable cost and delay, indicating that its\nadaptive knowledge updating mechanism effectively enhances the\nquality of responses.\nIn summary, EACO-RAG reduces cost and delay through its\nedge-assisted and collaborate design while improving accuracy\nby leveraging its adaptive knowledge update mechanism.\n6 RELATED WORK\nRetrieval-Augmented Generation Enhancements. Retrieval-\nAugmented Generation (RAG) improves language models (LMs) by\nintegrating relevant text from knowledge bases [ 19]. Extensions\nlike Adaptive RAG [16], Corrective RAG [20], and Self-RAG [21]\naddress retrieval strategy limitations and query complexity. These\nmethods adjust retrieval based on query difficulty to enhance per-\nformance. Mallen et al. [22] classify query complexity using entity\nfrequency, applying binary decisions on retrieval sufficiency. Qi\net al. [23] use fixed operations (retrieval, reading, reranking), re-\nquiring specialized LM training. Self-RAG [21] retrieves, critiques,\nand generates dynamically, though uniform query handling re-\nmains suboptimal. RAG systems are also being integrated with\n6G edge networks [ 24]. Traditional methods like Adaptive-RAG\nand Corrective-RAG refine retrieval based on query complexity\nbut often rely on centralized frameworks. EACO-RAG, by contrast,\nuses edge computing to distribute knowledge across multiple nodes,\nreducing delays and communication overhead while dynamically\nupdating local databases.\nCost-Effective Large Language Model Usage. Reducing LLM\ndeployment costs is an active research focus. Techniques like model\nquantization [25, 26] and pruning [27] cut costs but may reduce per-\nformance and require specialized hardware. LLM distillation, where\nsmaller models mimic larger ones, offers a solution. Caching LLM\nresponses for routine queries [28\u201331] or reusing key-value states\nduring inference [32, 33] are other strategies. FrugalGPT [34] uses\nprompt engineering and model multiplexing to select model size\nbased on query complexity, while model multiplexing dynamically\nchooses the appropriate model size [35, 36]. Techniques like model\nquantization and LLM distillation reduce computational costs but\nrequire specialized hardware. EACO-RAG offers a holistic solution,", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "0efd9a04-6be2-4908-b1af-a289f3ec01f1": {"text": "chooses the appropriate model size [35, 36]. Techniques like model\nquantization and LLM distillation reduce computational costs but\nrequire specialized hardware. EACO-RAG offers a holistic solution,\ncombining edge and cloud resources to optimize retrieval and gen-\neration, minimizing resource use and operational costs through a\nmulti-armed bandit framework.\nResource Allocation Strategies in Edge Computing. Opti-\nmizing resource allocation in edge computing focuses on balancing\ndelay, energy, and processing power [ 37]. Techniques like task\noffloading and resource scheduling aim to minimize latency and\nenergy consumption [38]. Edge-cloud collaboration dynamically\nallocates resources between edge nodes and cloud servers [ 39].\nRecent advancements in AI and reinforcement learning improve\nadaptability and scalability in heterogeneous edge environments,\nyielding significant efficiency and cost gains [ 40]. Conventional", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "bf1a2254-3513-42a4-9c03-e3485f1f29bb": {"text": "EACO-RAG: Edge-Assisted and Collaborative RAG with\nAdaptive Knowledge Update\nedge computing focuses on task offloading within local environ-\nments. EACO-RAG introduces inter-node collaboration, allowing\ndynamic knowledge sharing and optimizing resource allocation\nacross a network of distributed edge devices.\n7 CONCLUSION\nThis paper introduced EACO-RAG, an edge-assisted collaborative\nRAG system designed to reduce network delay and resource con-\nsumption in large-scale deployments while maintaining accuracy.\nBy leveraging edge computing for adaptive knowledge updates\nbased on user behavior, EACO-RAG optimizes performance using a\ncontextual multi-armed bandit framework and Safe Online Bayesian\nOptimization, balancing exploration and exploitation to achieve\noptimal results efficiently. Experimental results show that EACO-\nRAG outperforms traditional RAG systems in both response quality\nand resource efficiency, making it a scalable solution for real-time\napplications. Future work will focus on enhancing knowledge base\nmanagement and exploring further optimization techniques.\nREFERENCES\n[1] Y. Wu, A. Q. Jiang, W. Li, M. Rabe, C. Staats, M. Jamnik, and C. Szegedy, \u201cAut-\noformalization with large language models, \u201d Advances in Neural Information\nProcessing Systems , vol. 35, pp. 32 353\u201332 368, 2022.\n[2] H. Lyu, S. Jiang, H. Zeng, Y. Xia, Q. Wang, S. Zhang, R. Chen, C. Leung, J. Tang,\nand J. Luo, \u201cLlm-rec: Personalized recommendation via prompting large language\nmodels, \u201darXiv preprint arXiv:2307.15780 , 2023.\n[3] X. Ren, W. Wei, L. Xia, L. Su, S. Cheng, J. Wang, D. Yin, and C. Huang, \u201cRepresen-\ntation learning with large language models for recommendation, \u201d inProceedings\nof the ACM on Web Conference 2024 , 2024, pp. 3464\u20133475.\n[4] R. Yang, T. F. Tan, W. Lu, A. J. Thirunavukarasu, D. S. W. Ting, and N. Liu, \u201cLarge\nlanguage models in health care: Development, applications, and challenges, \u201d\nHealth Care Science , vol. 2, no. 4, pp. 255\u2013263, 2023.\n[5] S. Li, X. Puig, C. Paxton, Y. Du, C. Wang, L. Fan, T. Chen, D.-A. Huang, E. Aky\u00fcrek,\nA. Anandkumar et al. , \u201cPre-trained language models for interactive decision-\nmaking, \u201dAdvances in Neural Information Processing Systems , vol. 35, pp. 31 199\u2013\n31 212, 2022.\n[6] Grand View Research. (2024) Retrieval augmented generation market\nsize, share & trends analysis report. Accessed: 2024-10-11. [Online].\nAvailable: https://www.grandviewresearch.com/industry-analysis/retrieval-\naugmented-generation-rag-market-report", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "0826b4e6-4c9b-47d9-9e2b-dabc61f1d35d": {"text": "size, share & trends analysis report. Accessed: 2024-10-11. [Online].\nAvailable: https://www.grandviewresearch.com/industry-analysis/retrieval-\naugmented-generation-rag-market-report\n[7] J. Chen, H. Lin, X. Han, and L. Sun, \u201cBenchmarking large language models\nin retrieval-augmented generation, \u201d inProceedings of the AAAI Conference on\nArtificial Intelligence, vol. 38, no. 16, 2024, pp. 17 754\u201317 762.\n[8] S. Hofst\u00e4tter, J. Chen, K. Raman, and H. Zamani, \u201cFid-light: Efficient and effective\nretrieval-augmented text generation, \u201d inProceedings of the 46th International\nACM SIGIR Conference on Research and Development in Information Retrieval ,\n2023, pp. 1437\u20131447.\n[9] H. Yu, A. Gan, K. Zhang, S. Tong, Q. Liu, and Z. Liu, \u201cEvaluation of retrieval-\naugmented generation: A survey, \u201darXiv preprint arXiv:2405.07437 , 2024.\n[10] M. Zhang, J. Cao, X. Shen, and Z. Cui, \u201cEdgeshard: Efficient llm inference via\ncollaborative edge computing, \u201darXiv preprint arXiv:2405.14371 , 2024.\n[11] G. Qu, Q. Chen, W. Wei, Z. Lin, X. Chen, and K. Huang, \u201cMobile edge intel-\nligence for large language models: A contemporary survey, \u201d arXiv preprint\narXiv:2407.18921, 2024.\n[12] R. Qin, Z. Yan, D. Zeng, Z. Jia, D. Liu, J. Liu, Z. Zheng, N. Cao, K. Ni, J. Xiong\net al. , \u201cRobust implementation of retrieval-augmented generation on edge-based\ncomputing-in-memory architectures, \u201darXiv preprint arXiv:2405.04700 , 2024.\n[13] H. Lim, J. Ye, S. Abdu Jyothi, and D. Han, \u201cAccelerating model training in multi-\ncluster environments with consumer-grade gpus, \u201d in Proceedings of the ACM\nSIGCOMM 2024 Conference , 2024, pp. 707\u2013720.\n[14] Langchain-AI, \u201cAuto evaluator: An evaluation toolkit for language models, \u201d https:\n//github.com/langchain-ai/auto-evaluator, 2023.\n[15] D. Edge, H. Trinh, N. Cheng, J. Bradley, A. Chao, A. Mody, S. Truitt, and J. Larson,\n\u201cFrom local to global: A graph rag approach to query-focused summarization, \u201d\narXiv preprint arXiv:2404.16130 , 2024.\n[16] S. Jeong, J. Baek, S. Cho, S. J. Hwang, and J. C. Park, \u201cAdaptive-rag: Learning to\nadapt retrieval-augmented large language models through question complexity, \u201d\narXiv preprint arXiv:2403.14403 , 2024.\n[17] C. K. Williams and C. E. Rasmussen, Gaussian processes for machine learning .\nMIT press Cambridge, MA, 2006.\n[18] D. Duvenaud, \u201cAutomatic model construction with gaussian processes, \u201d Ph.D.\ndissertation, Apollo - University of Cambridge Repository, 2014.\n[19] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. K\u00fcttler,", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "4b7d91bc-af61-4e6d-841a-a2ff0deb569f": {"text": "[18] D. Duvenaud, \u201cAutomatic model construction with gaussian processes, \u201d Ph.D.\ndissertation, Apollo - University of Cambridge Repository, 2014.\n[19] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. K\u00fcttler,\nM. Lewis, W.-t. Yih, T. Rockt\u00e4schelet al. , \u201cRetrieval-augmented generation for\nknowledge-intensive nlp tasks, \u201dAdvances in Neural Information Processing Sys-\ntems, 2020.\n[20] S.-Q. Yan, J.-C. Gu, Y. Zhu, and Z.-H. Ling, \u201cCorrective retrieval augmented\ngeneration, \u201darXiv preprint arXiv:2401.15884 , 2024.\n[21] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi, \u201cSelf-rag: Learning to retrieve,\ngenerate, and critique through self-reflection, \u201darXiv preprint arXiv:2310.11511 ,\n2023.\n[22] A. Mallen, A. Asai, V. Zhong, R. Das, D. Khashabi, and H. Hajishirzi, \u201cWhen\nnot to trust language models: Investigating effectiveness of parametric and\nnon-parametric memories, \u201d in Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers) , A. Rogers,\nJ. Boyd-Graber, and N. Okazaki, Eds. Toronto, Canada: Association for\nComputational Linguistics, Jul. 2023, pp. 9802\u20139822. [Online]. Available:\nhttps://aclanthology.org/2023.acl-long.546\n[23] P. Qi, H. Lee, T. Sido, and C. Manning, \u201cAnswering open-domain questions\nof varying reasoning steps from text, \u201d in Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Processing , M.-F. Moens, X. Huang,\nL. Specia, and S. W.-t. Yih, Eds. Online and Punta Cana, Dominican Republic:\nAssociation for Computational Linguistics, Nov. 2021, pp. 3599\u20133614. [Online].\nAvailable: https://aclanthology.org/2021.emnlp-main.292\n[24] X. Huang, Y. Tang, J. Li, N. Zhang, and X. S. Shen, \u201cToward effective retrieval\naugmented generative services in 6g networks, \u201dIEEE Network , 2024.\n[25] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han, \u201cSmoothquant: Ac-\ncurate and efficient post-training quantization for large language models, \u201d in\nInternational Conference on Machine Learning . PMLR, 2023, pp. 38 087\u201338 099.\n[26] G. Park, M. Kim, S. Lee, J. Kim, B. Kwon, S. J. Kwon, B. Kim, Y. Lee, D. Lee\net al. , \u201cLut-gemm: Quantized matrix multiplication based on luts for efficient\ninference in large-scale generative language models, \u201d inThe Twelfth International\nConference on Learning Representations , 2023.\n[27] X. Ma, G. Fang, and X. Wang, \u201cLlm-pruner: On the structural pruning of large\nlanguage models, \u201dAdvances in neural information processing systems , vol. 36,\n2024.", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "fb13a5c3-480a-43c1-a12b-d9469f22a394": {"text": "Conference on Learning Representations , 2023.\n[27] X. Ma, G. Fang, and X. Wang, \u201cLlm-pruner: On the structural pruning of large\nlanguage models, \u201dAdvances in neural information processing systems , vol. 36,\n2024.\n[28] I. Stogiannidis, S. Vassos, P. Malakasiotis, and I. Androutsopoulos, \u201cCache me if\nyou can: An online cost-aware teacher-student framework to reduce the calls to\nlarge language models, \u201darXiv preprint arXiv:2310.13395 , 2023.\n[29] B. Zhu, Y. Sheng, L. Zheng, C. Barrett, M. I. Jordan, and J. Jiao, \u201cOn optimal\ncaching and model multiplexing for large model inference, \u201d arXiv preprint\narXiv:2306.02003, 2023.\n[30] W. Gill, M. Elidrisi, P. Kalapatapu, A. Anwar, and M. A. Gulzar, \u201cPrivacy-aware\nsemantic cache for large language models, \u201darXiv preprint arXiv:2403.02694 , 2024.\n[31] J. Li, C. Xu, F. Wang, I. M. von Riedemann, C. Zhang, and J. Liu, \u201cScalm: Towards\nsemantic caching for automated chat services with large language models, \u201darXiv\npreprint arXiv:2406.00025 , 2024.\n[32] Y. Liu, H. Li, Y. Cheng, S. Ray, Y. Huang, Q. Zhang, K. Du, J. Yao, S. Lu, G. Anantha-\nnarayanan et al. , \u201cCachegen: Kv cache compression and streaming for fast large\nlanguage model serving, \u201d inProceedings of the ACM SIGCOMM 2024 Conference ,\n2024, pp. 38\u201356.\n[33] J. Yao, H. Li, Y. Liu, S. Ray, Y. Cheng, Q. Zhang, K. Du, S. Lu, and J. Jiang,\n\u201cCacheblend: Fast large language model serving with cached knowledge fusion, \u201d\narXiv preprint arXiv:2405.16444 , 2024.\n[34] L. Chen, M. Zaharia, and J. Zou, \u201cFrugalgpt: How to use large language models\nwhile reducing cost and improving performance, \u201darXiv preprint arXiv:2305.05176 ,\n2023.\n[35] F. Bang, \u201cGptcache: An open-source semantic cache for llm applications enabling\nfaster answers and cost savings, \u201d inProceedings of the 3rd Workshop for Natural\nLanguage Processing Open Source Software (NLP-OSS 2023) , 2023.\n[36] S. Kim, K. Mangalam, J. Malik, M. W. Mahoney, A. Gholami, and K. Keutzer, \u201cBig\nlittle transformer decoder, \u201darXiv preprint arXiv:2302.07863 , 2023.\n[37] P. Wang, C. Yao, Z. Zheng, G. Sun, and L. Song, \u201cJoint task assignment, transmis-\nsion, and computing resource allocation in multilayer mobile edge computing\nsystems, \u201dIEEE Internet of Things Journal , vol. 6, no. 2, pp. 2872\u20132884, 2018.\n[38] A. Naouri, H. Wu, N. A. Nouri, S. Dhelim, and H. Ning, \u201cA novel framework for\nmobile-edge computing by optimizing task offloading, \u201dIEEE Internet of Things\nJournal, vol. 8, no. 16, pp. 13 065\u201313 076, 2021.\n[39] H. Gu, L. Zhao, Z. Han, G. Zheng, and S. Song, \u201cAi-enhanced cloud-edge-terminal", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "fa9d47b5-2038-4356-8a39-f7e192658eb7": {"text": "mobile-edge computing by optimizing task offloading, \u201dIEEE Internet of Things\nJournal, vol. 8, no. 16, pp. 13 065\u201313 076, 2021.\n[39] H. Gu, L. Zhao, Z. Han, G. Zheng, and S. Song, \u201cAi-enhanced cloud-edge-terminal\ncollaborative network: Survey, applications, and future directions, \u201dIEEE Com-\nmunications Surveys & Tutorials , 2023.\n[40] X. Xiong, K. Zheng, L. Lei, and L. Hou, \u201cResource allocation based on deep\nreinforcement learning in iot edge computing, \u201dIEEE Journal on Selected Areas in\nCommunications, vol. 38, no. 6, pp. 1133\u20131146, 2020.", "metadata": {"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2410.07176v1"}}, "e5885620-6125-42da-8f65-3b76b034d79f": {"text": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval\nAugmented Generation\nDaniel Fleischer Moshe Berchansky Moshe Wasserblat Peter Izsak\nIntel Labs\n{daniel.fleischer, moshe.berchansky, moshe.wasserblat, peter.izsak}@intel.com\nAbstract\nImplementing Retrieval-Augmented Genera-\ntion (RAG) systems is inherently complex,\nrequiring deep understanding of data, use\ncases, and intricate design decisions. Addi-\ntionally, evaluating these systems presents sig-\nnificant challenges, necessitating assessment of\nboth retrieval accuracy and generative quality\nthrough a multi-faceted approach. We intro-\nduce RAG F OUNDRY , an open-source frame-\nwork for augmenting large language models\nfor RAG use cases. RAG F OUNDRY inte-\ngrates data creation, training, inference and\nevaluation into a single workflow, facilitating\nthe creation of data-augmented datasets for\ntraining and evaluating large language mod-\nels in RAG settings. This integration en-\nables rapid prototyping and experimentation\nwith various RAG techniques, allowing users\nto easily generate datasets and train RAG\nmodels using internal or specialized knowl-\nedge sources. We demonstrate the frame-\nwork effectiveness by augmenting and fine-\ntuning Llama-3 and Phi-3 models with diverse\nRAG configurations, showcasing consistent im-\nprovements across three knowledge-intensive\ndatasets. Code is released as open-source in\nhttps://github.com/IntelLabs/RAGFoundry.\n1 Introduction\nLarge Language Models (LLMs) have emerged as\na transformative force in the field of AI, demon-\nstrating an impressive ability to perform a wide\nrange of tasks that traditionally required human in-\ntelligence (Brown et al., 2020; Kojima et al., 2022).\nDespite their impressive capabilities, LLMs have\ninherent limitations. These models can produce\nplausible-sounding but incorrect or nonsensical an-\nswers, struggle with factual accuracy, lack access\nto up-to-date information after their training cutoff\nand struggle in attending to relevant information in\nlarge contexts (Huang et al., 2023; Liu et al., 2023).\nData\n Training\nLoRA\n\uf085\nInference \uf11c\nLoaders\nAugmentation\nSelectors\nRetrievers\nSamplers\nPrompters\nCaching\nAPI\nEvaluation\nEM\n\uf00c\nF1\nFaithfulness\nRelevancy\nAnswer Processor\nROUGE\nFigure 1: An overview of the RAG F OUNDRY frame-\nwork: the Data Augmentationmodule persists RAG\ninteractions into a dedicated dataset, which is then used\nfor training, inference and evaluation.\nRetrieval-Augmented Generation (RAG) enhances\nLLMs performance by integrating external infor-\nmation using retrieval mechanisms. Combining re-", "metadata": {"title": "Seven Failure Points When Engineering a Retrieval Augmented Generation\n  System", "pdf_url": "http://arxiv.org/pdf/2401.05856v1"}}, "de8f5827-7789-46d8-9a79-d718e78940c9": {"text": "interactions into a dedicated dataset, which is then used\nfor training, inference and evaluation.\nRetrieval-Augmented Generation (RAG) enhances\nLLMs performance by integrating external infor-\nmation using retrieval mechanisms. Combining re-\ntrieval that leverages vast knowledge-bases outside\nthe knowledge of the model, effectively addresses\nknowledge limitations, can reduce hallucinations,\nimprove the relevance of generated content, pro-\nvide interpretability and could be vastly more cost-\nefficient (Lewis et al., 2021; Mallen et al., 2022;\nGao et al., 2023; Asai et al., 2023; Borgeaud et al.,\n2021; Peng et al., 2023; de Jong et al., 2023). Fur-\nthermore, recent research indicates that fine-tuning\nLLMs for RAG can achieve state-of-the-art perfor-\nmance, surpassing that of larger, proprietary mod-\nels (Yu et al., 2024b; Liu et al., 2024).\nHowever, the implementation of RAG systems\nis inherently complex and requires a series of\nintricate decisions that can significantly impact\nthe performance of the system. This process de-\narXiv:2408.02545v1  [cs.CL]  5 Aug 2024", "metadata": {"title": "Seven Failure Points When Engineering a Retrieval Augmented Generation\n  System", "pdf_url": "http://arxiv.org/pdf/2401.05856v1"}}, "5bffcb44-f7fc-4c99-abe6-b2b59d7cc769": {"text": "mands a thorough understanding of the data and\nuse case, and often, solutions do not generalize\nwell to other domains (Barnett et al., 2024; Bala-\nguer et al., 2024). Some key RAG design decisions\ninclude text embedding, indexing parameters, re-\ntrieval algorithms, query building, and prompt de-\nsign, among other considerations beyond the LLM\nconfiguration (Wang et al., 2024). Another issue is\nreproducibility: achieving consistent and compara-\nble results across runs, datasets and tasks. Varia-\ntions in training data, pre-processing steps, model\nconfigurations, and hardware can lead to discrep-\nancies in performance, making it challenging for\nresearchers and practitioners to replicate findings\nand build upon previous work. Additionally, evalu-\nating RAG systems presents a challenge due to the\ndual reliance on retrieval accuracy and generative\nquality. These systems require a sophisticated eval-\nuation suite that accounts for the interplay among\nthe retrieved information, the formalization of data,\nand the generated output (Chen et al., 2023; Yu\net al., 2024a; Es et al., 2024).\nWe introduce RAG F OUNDRY , an open-source\npython framework for developing sophisticated\nretrieval-augmented LLMs for RAG use-cases. The\nlibrary supports researchers and practitioners in the\nnuanced task of enhancing the capabilities of LLMs\nin RAG use cases. It is highly customizable, fa-\ncilitating rapid prototyping and experimentation\nacross all aspects of RAG, including data selec-\ntion, aggregation and filtering, retrieval, text pro-\ncessing, document ranking, few-shot generation,\nprompt design using templates, fine-tuning, infer-\nence, and evaluation. To cater to the specific needs\nof researchers, we designed the framework to func-\ntion as an end-to-end experimentation environment.\nThe backbone of the library consists of four dis-\ntinct modules: data creation, training, inference,\nand evaluation. Each module is encapsulated and\ncontrolled by a configuration file, ensuring compat-\nibility between the output of one module and the\ninput of the next. This modular approach allows\neach step to be isolated and independently experi-\nmented with, enabling the production of multiple\noutputs and the concurrent execution of numerous\nexperiments. Evaluation can be conducted on the\ngenerated outputs as well as on any feature within\nthe data, including retrieval, ranking, and reason-\ning.\nTo illustrate the utility of the framework, we\nconducted experiments involving retrieval, fine-\ntuning, chain-of-thought (CoT) reasoning (Wu", "metadata": {"title": "Seven Failure Points When Engineering a Retrieval Augmented Generation\n  System", "pdf_url": "http://arxiv.org/pdf/2401.05856v1"}}, "99deb00c-827d-45aa-bf1a-aa08427ece00": {"text": "generated outputs as well as on any feature within\nthe data, including retrieval, ranking, and reason-\ning.\nTo illustrate the utility of the framework, we\nconducted experiments involving retrieval, fine-\ntuning, chain-of-thought (CoT) reasoning (Wu\net al., 2023) and a negative distractor-documents\ntechnique (Zhang et al., 2024). We compared\ntwo widely accepted baseline models using vari-\nous enhancement methods across three knowledge-\nintensive question-answering tasks, demonstrating\nthe effectiveness of RAG FOUNDRY .\n2 Related Work\nThere are numerous open-source tools related to\nthe different aspects of RAG, namely inference,\ntraining and evaluation. LlamaIndex (Liu, 2022),\nLangChain (Chase, 2022) and Haystack (Pietsch\net al., 2019) are well known libraries for composing\nRAG pipelines; however they are not focused on\nevaluation and their training capability is under-\ndeveloped.\nHoshi et al. (2023) proposes a framework for\ndeveloping RAG-based LLMs; while our process-\ning may be similar in the sense of being comprised\nof custom individual steps, they do not introduce\nany form of training. Khattab et al. (2023, 2022)\npresents a different approach, where LLM prompt-\ning is represented as a programming language, to\nbe optimized and compiled; a rather unique and\ngeneral approach that could benefit RAG but has\na high level of complexity due to the abstractions\nintroduced. Saad-Falcon et al. (2024) focuses more\non the evaluation aspect, by creating synthetic data\nand training an LLM critic to evaluate the RAG sys-\ntem. Hsia et al. (2024) studies aspects of retrieval\non the performance of RAG; our RAG Foundry li-\nbrary is general and enables experimentation on all\naspects of RAG: retrieval, text-processing, prompt\ndesign, model selection, inference and evaluations.\nRecently, a concurrent work by Jin et al. (2024)\nproposes a RAG building framework, including\nsome RAG implementations and datasets; we fo-\ncus on extensibility, letting users define custom\ntypes of pipelines with custom components. Rau\net al. (2024) presents a framework, sharing a\nsimilar design-principle of extensibility-through-\nconfiguration as ours; their library imposes a spe-\ncific workflow structure (retriever, ranker, LLM)\nwhile our library is more general and does not im-\nposes any specific paradigm.\n3 RAG Foundry\nThe RAG FOUNDRY framework facilitates rapid\nprototyping and experimentation with various RAG\nsettings and configurations. The library is com-\nposed of four modules: dataset creation, training,", "metadata": {"title": "Seven Failure Points When Engineering a Retrieval Augmented Generation\n  System", "pdf_url": "http://arxiv.org/pdf/2401.05856v1"}}, "33523c77-be5d-4ada-b220-24df5866ede2": {"text": "name: my_pipeline\ncache: true\nsteps:\n- _target_: dataset_loaders.loaders.HFLoader\ninputs: main\ndataset_config:\npath: \"Tevatron/wikipedia-trivia\"\nsplit: train\n- _target_: dataset_loaders.loaders.LocalLoader\ninputs: fewshot-data\nfilename: prepared-fewshot-data.jsonl\n- _target_: global_steps.sampling.ShuffleSelect\ninputs: main\nshuffle: 42\nlimit: 10000\n- _target_:\nlocal_steps.retrievers.HaystackRetriever,\u2192\ninputs: main\npipeline_path: configs/qdrant.yaml\nquery_key: query\ndocs_key: positive_passages\n- _target_: global_steps.sampling.FewShot\ninputs: main\ninput_dataset: fewshot-data\nk: 3\noutput_key: fewshot_examples\n- _target_: local_steps.prompter.TextPrompter\ninputs: main\nprompt_file: prompts/basic.txt\noutput_key: my_prompt\nmapping:\nquestion: query\ncontext: positive_passages\nfewshot: fewshot_examples\nanswer: answers\n- _target_: global_steps.output.OutputData\ninputs: main\nfile_name: TQA_train_processed.jsonl\nListing 1: Example of a dataset creation configuration.\nThe example contains data loading, shuffling, sampling,\nretrieval, few-shot collection, prompt building and sav-\ning steps.\ninference, and evaluation. Below, we expand on\neach of the modules and provide example configu-\nrations for running them.\n3.1 Data Creation and Processing\nThe processing module facilitates the creation of\ncontext-enhanced datasets by persisting RAG in-\nteractions, which are essential for RAG-oriented\ntraining and inference (Berchansky et al., 2024; Liu\net al., 2024; Yu et al., 2024b). These interactions\nencompass dataset loading, column normalization,\ndata aggregation, information retrieval, template-\nbased prompt creation, and various other forms of\npre-processing. The processed data can be saved\nin a consistent, model-independent format, along\nwith all associated metadata, ensuring compatibil-\nity and reproducibility across different models and\nexperiments.\nThe processing module is comprised of an ab-\nstract pipeline with multiple steps, each defined by\nPython classes that implement specific data pro-\ncessing functionalities. These steps are categorized\ninto two types:\n\u2022 Global Steps: Can act on the dataset as a whole,\nmaking them useful for operations such as aggre-\ngations, group-by, examples filtering, join opera-\ntions, and more.\n\u2022 Local Steps: Operate on individual examples,\nmaking them suitable for tasks such as retrieval,\ntext processing, and field manipulation.\nThe modular design allows for building flexible\nand efficient data processes, tailored to the needs\nof RAG-oriented training and inference. Steps can", "metadata": {"title": "Seven Failure Points When Engineering a Retrieval Augmented Generation\n  System", "pdf_url": "http://arxiv.org/pdf/2401.05856v1"}}, "2421798d-ef66-4cd8-ab00-f306f6d2f7fb": {"text": "making them suitable for tasks such as retrieval,\ntext processing, and field manipulation.\nThe modular design allows for building flexible\nand efficient data processes, tailored to the needs\nof RAG-oriented training and inference. Steps can\nbe categorized into the following non-exclusive\ncategories:\n\u2022 Loaders: Load datasets from the Hugging Face1\nhub or from local sources.\n\u2022 Selectors: Filter examples, shuffle datasets, and\nselect subset datasets.\n\u2022 Retrievers: Integrate information from external\ndatabases, tools, libraries and pipelines.\n\u2022 Samplers: Collect random examples or features\nfrom any dataset to compile few-shot or negative\nexamples.\n\u2022 Prompters: Format prompts using custom tem-\nplates and keyword mappings.\nThe processing module supports the handling of\nmultiple datasetsat once, through global dataset\nsharing. This feature allows each step of the\npipeline to access any of the loaded datasets, en-\nhancing flexibility and allowing for complex pro-\ncessing procedures. Furthermore, the module in-\ncludes step caching, which caches each pipeline\nstep locally. This improves compute efficiency, and\nfacilitates easy reproduction of results.\n3.1.1 Example: Enhancing a Q&A Dataset\nTo showcase the effectiveness of the process-\ning module, we demonstrate how to enrich a\nquestion-answering dataset with external informa-\n1https://huggingface.co/", "metadata": {"title": "Seven Failure Points When Engineering a Retrieval Augmented Generation\n  System", "pdf_url": "http://arxiv.org/pdf/2401.05856v1"}}, "cd93c4cc-dc61-4d24-9522-46586cf9fd54": {"text": "model:\n_target_: ragfoundry.models.hf.HFTrain\nmodel_name_or_path:\n\"microsoft/Phi-3-mini-128k-instruct\",\u2192\nload_in_8bit: true\nlora:\npeft_type: \"LORA\"\nr: 16\ntarget_modules: [\"qkv_proj\"]\ncompletion_start: \"<|assistant|>\"\ntrain:\ngradient_accumulation_steps: 4\nlearning_rate: 2e-05\nlr_scheduler_type: \"cosine\"\nnum_train_epochs: 1\noptim: \"paged_adamw_8bit\"\ninstruction: prompts/prompt_instructions/qa.txt\ndata_file: TQA_train_processed.jsonl\nListing 2: Example of a training configuration. Model\nand training parameters are specified, in addition to an\ninstruction file containing the system prompt.\ntion fetched using a retrieval pipeline, prepare few-\nshot examples and combine everything together\nusing a prompt template. Listing 1 demonstrates\nhow such a processing pipeline is defined using a\nY AML configuration. The main structure of the file\nis a list of steps, each defined by a_target_ which\npoints to the step implementation. Each step has\ninputs, which is a name or list of dataset names\nto act upon. Other keys in a step relate to specific\nstep logic.\nThe first two steps in listing 1 load datasets from\nHugging Face hub and from a local path. The third\nstep shuffles and selects 10k examples from the\nmain dataset. The forth step runs a Haystack-based\n(Pietsch et al., 2019) retrieval pipeline to retrieve\nrelevant passages using questions from the loaded\ndataset as queries, storing them in docs_key. We\nnote that different retrieval processes or frame-\nworks (Liu, 2022; Chase, 2022; Lin et al., 2021)\ncan be used in retrieval steps. The fifth step selects\n3 few-shot examples from the secondary dataset,\nfollowing a prompt generator step that loads a\nprompt template and replaces all given informa-\ntion according to the defined mapping dictionary.\nLastly, the dataset is saved to a local path.\n3.2 Training\nWe provide a training module to fine-tune models\ngiven the datasets created by the previous process-\ning module. The training module relies on the\nwell established training framework TRL2 and sup-\n2https://github.com/huggingface/trl\nmodel:\n_target_: ragfoundry.models.hf.HFInference\nmodel_name_or_path:\n\"microsoft/Phi-3-mini-128k-instruct\",\u2192\nload_in_8bit: true\ninstruction: prompts/prompt_instructions/qa.txt\nlora_path: /path/to/adapter\ngeneration:\ndo_sample: false\nmax_new_tokens: 50\nreturn_full_text: false\ndata_file: my-processed-data.jsnol\ngenerated_file: model-predictions.jsonl\nListing 3: Example of an inference configuration. In ad-\ndition to model and generation options, a system prompt\ncan be defined.", "metadata": {"title": "Seven Failure Points When Engineering a Retrieval Augmented Generation\n  System", "pdf_url": "http://arxiv.org/pdf/2401.05856v1"}}, "7de4d590-6feb-45d0-8e3f-4f5b08df5c36": {"text": "max_new_tokens: 50\nreturn_full_text: false\ndata_file: my-processed-data.jsnol\ngenerated_file: model-predictions.jsonl\nListing 3: Example of an inference configuration. In ad-\ndition to model and generation options, a system prompt\ncan be defined.\nports advanced and efficient training techniques,\ne.g. LoRA (Hu et al., 2021). An example of a\ntraining configuration is presented in listing 2.\n3.3 Inference\nThe inference module generates predictions given\nthe processed datasets created by the processing\nmodule. Inference is conceptually separated from\nthe evaluation step, since it is more computation-\nally demanding than evaluation. Additionally, one\ncan run multiple evaluations on a single, prepared\ninference results file. An example configuration for\ngenerating predictions given a dataset is presented\nin listing 3.\n3.4 Evaluation\nThe goal of the framework is augmenting LLMs\nfor RAG. The evaluation module allows users to\nrun collections of metrics to evaluate RAG tech-\nniques and tuning processes. The evaluation mod-\nule loads the output of the inference module and\nruns a configurable list of metrics. Metrics are\nclasses implemented in the library. These classes\ncan be as simple as wrappers around other evalua-\ntion libraries, or can be implemented by the user.\nLocal metricscan be run on individual examples,\nlike Exact Match (EM), while Global metricsrun\non the entire dataset as a whole, e.g. Recall (for\nclassification-based metrics). Metrics can use any\nfield and metadata in the dataset, not just the input-\noutput pairs. Some of the metrics implemented\nin the library include: a wrapper for the Hugging\nFace evaluate library, EM, F1, classification met-\nrics, BERTScore (Zhang et al., 2019), Semantic\nSimilarity and a wrapper for DeepEval3 (for using\n3https://github.com/confident-ai/deepeval", "metadata": {"title": "Seven Failure Points When Engineering a Retrieval Augmented Generation\n  System", "pdf_url": "http://arxiv.org/pdf/2401.05856v1"}}, "2fb4747f-ccc2-49e6-8586-0a9d3c37b8f1": {"text": "answer_processor:\n_target_: ragfoundry.processing.RegexAnswer\ncapture_pattern: \"Answer: (.*)\"\nstopping_pattern:\nmetrics:\n- _target_: ragfoundry.evaluation.HFEvaluate\nmetric_names: [\"rouge\"]\n- _target_: ragfoundry.evaluation.EM\n- _target_: ragfoundry.evaluation.F1\n- _target_: ragfoundry.evaluation.BERTScore\nmodel: \"microsoft/deberta-large-mnli\"\n- _target_: ragfoundry.evaluation.Faithfulness\n- _target_: ragfoundry.evaluation.Relevancy\nembeddings: \"BAAI/bge-small-en-v1.5\"\nresults_file: my-evaluation.yaml\ngenerated_file: model-prediction.jsonl\ndata_file: my-processed-data.jsonl\nListing 4: Example of an evaluation configuration; it\ncontains an answer processor, as well as the list of met-\nrics, with optional parameters, to run.\nthe RAGAS metrics (Es et al., 2024)). After the\nevaluation is completed, a results file is written to\ndisk with the local and global metrics results.\nFurthermore, the evaluation module uses a pro-\ncessing step called an Answer Processor, which\ncan implement custom logic and serve many pur-\nposes, including cleaning and aligning outputs; for\nexample, using regex, one can isolate answers, re-\nmove stop words, chain-of-thought reasoning, de-\nfine a stopping criteria, process citations and attri-\nbutions and any other form of processing needed\nfor a given evaluation.\nSee listing 4 for a configuration example; it con-\ntains an answer processor that extracts an answer\nfrom an output, and a list of metrics to run.\n4 Experiments: RAG Tuning\nTo illustrate the usage and usefulness of the\nRAG F OUNDRY library, we experiment with sev-\neral possible RAG improvements to LLMs, and\nevaluate the results on three knowledge-intensive\ntasks.\n4.1 RAG Augmentation Techniques\nWe explore several techniques for RAG augmenta-\ntion, and use RAG F OUNDRY to easily implement\nand evaluate their benefit. As an initial step, we\nevaluate unmodified models; we set Baseline as a\nconfiguration that is defined by running unmodified\nmodels and without any external knowledge. We\ndefine a RAG setting that introduces top-relevant\ndocuments in a consistent prompt template format\nwith a system instruction, and aCoT scheme which\nguides the model to use the retrieved context, ex-\nplain the steps, quote relevant parts and produce\na final answer. Complementing that, we explore\nfine-tuning recipes. We fine-tune the model in the\nRAG setup and denote is as RAG-sft. To comple-\nment CoT, we implemented a fine-tuning recipe,\ndenoted as CoT-sft, introduced in (Zhang et al.,\n2024), where gold documents and purely distractor", "metadata": {"title": "Seven Failure Points When Engineering a Retrieval Augmented Generation\n  System", "pdf_url": "http://arxiv.org/pdf/2401.05856v1"}}, "76e455a7-d28f-46d3-96b2-12082d343903": {"text": "fine-tuning recipes. We fine-tune the model in the\nRAG setup and denote is as RAG-sft. To comple-\nment CoT, we implemented a fine-tuning recipe,\ndenoted as CoT-sft, introduced in (Zhang et al.,\n2024), where gold documents and purely distractor\ndocuments are used in the prompt, determined by\nprobability, in conjunction with a CoT prompt. All\nprompt templates are included in appendix A.1.\n4.2 Datasets\nWe evaluate our models onTriviaQA (Joshi et al.,\n2017), PubmedQA (Jin et al., 2019), and ASQA\n(Stelmakh et al., 2022) which are knowledge in-\ntensive question-answering datasets which ben-\nefit from external sources. The TriviaQA and\nPubmedQA datasets contain relevant context; for\nASQA, retrieval was done over a Wikipedia corpus\nusing a dense retriever4. Dataset sources and sizes\nare included in appendix A.2.\n4.3 Models\nWe experiment with two representative models:\nLlama-35 (Touvron et al., 2023; AI@Meta, 2024)\nand Phi-36 (Abdin et al., 2024) as they represent\nrobust capabilities and are ideal candidate models\nfor RAG use case deployments.\n4.4 Evaluation\nWe measure and report Exact Match (EM) for\nTriviaQA, STR-EM for ASQA, accuracy and F1\nfor PubmedQA. Additionally, we evaluate two\nRAGAS metrics (Es et al., 2024): Faithfulness and\nRelevancy. Faithfulness measures the relation be-\ntween the generated text and the context. Relevancy\nmeasures the relation between the generated text\nand the query. These two metrics use the context as\ninput for the LLM critic, so are only relevant in the\nRAG settings. The critic LLM used is GPT4-32k,\nversion 0613. An embedder 7 is required for the\nrelevancy evaluation.\n4.5 Results\nWe present a comparative study of RAG augmenta-\ntion techniques, on the TriviaQA, ASQA and Pub-\nmedQA datasets. Results are presented in table 1:\n4BAAI/llm-embedder\n5meta-llama/Meta-Llama-3-8B-Instruct.\n6microsoft/Phi-3-mini-128k-instruct.\n7BAAI/bge-small-en-v1.5.", "metadata": {"title": "Seven Failure Points When Engineering a Retrieval Augmented Generation\n  System", "pdf_url": "http://arxiv.org/pdf/2401.05856v1"}}, "99787138-c00f-436a-bd1d-b94881423416": {"text": "Model Method TriviaQA ASQA PubmedQA\nEM Faith. Rel. STR-EM Faith. Rel. Acc F1 Faith. Rel.\nPhi-3 3.8B\nBaseline 0.630 - - 0.109 - - 0.476 0.290 - -\nRAG 0.876 0.821 0.836 0.294 0.685 0.895 0.530 0.281 - -\nRAG-sft 0.878 0.777 0.750 0.252 0.717 0.833 0.720 0.491 - -\nCoT 0.923 0.555 0.741 0.367 0.263 0.826 0.574 0.439 0.477 0.705\nCoT-sft 0.795 0.793 0.749 0.386 0.749 0.839 0.620 0.458 0.631 0.853\nLlama-3 8B\nBaseline 0.722 - - 0.200 - - 0.560 0.366 - -\nRAG 0.828 0.783 0.746 0.285 0.610 0.861 0.556 0.398 - -\nRAG-sft 0.916 0.704 0.714 0.291 0.653 0.854 0.770 0.537 - -\nCoT 0.896 0.518 0.764 0.395 0.536 0.730 0.684 0.480 0.378 0.732\nCoT-sft 0.851 0.808 0.697 0.422 0.768 0.790 0.694 0.485 0.777 0.883\nTable 1: Evaluation results of baseline and different RAG settings, for the three datasets and two models tested. In\naddition to the main metrics for each dataset, faithfulness and relevancy are reported for the relevant configurations.\nIn bold are the best configurations per dataset, based on the main metrics.\nmain metrics for each dataset are displayed, as well\nas faithfulness and relevancy scores, as defined in\n(Es et al., 2024). For TriviaQA we observe the\nfollowing: retrieved context improves the results,\nfine-tuning the RAG setting improves the results,\nfine-tuning on CoT reasoning (which includes train-\ning on a combination of gold passages and distrac-\ntor passages) decreases performance. Best method\nis model dependent for this dataset. For ASQA,\nwe similarly observe every method improves upon\nthe baseline, CoT reasoning produces consistent\nimprovement in both models, as well as fine-tuning\nof the CoT configuration, which shows to perform\nbest. Finally, for PubmedQA, we observe that al-\nmost all methods improve upon the baseline (with\none exception); CoT reasoning improves upon the\nuntrained RAG setting, but upon fine-tuning, the\nRAG method appears to perform best in both mod-\nels.\nInspecting the faithfulness and relevancy scores,\nnotice that not all configurations are valid to be\nmeasured: these metrics require context, so are\nirrelevant for the baseline method. Additionally,\nin the PubmedQA dataset, the answers are binary\nYes/No; only in the CoT configurations the LLMs\nproduce a reasoning, which can be evaluated. Fi-\nnally, the faithfulness and relevancy scores often\ndo not correlate with the main metrics, neither with\neach other, possibly indicating they capture differ-\nent aspects of the retrieval and generated results,\nand represent a trade-off in performance.\nThe results demonstrate the usefulness of RAG", "metadata": {"title": "Seven Failure Points When Engineering a Retrieval Augmented Generation\n  System", "pdf_url": "http://arxiv.org/pdf/2401.05856v1"}}, "3fe191ef-dbf6-4dc0-a6dd-984488059275": {"text": "do not correlate with the main metrics, neither with\neach other, possibly indicating they capture differ-\nent aspects of the retrieval and generated results,\nand represent a trade-off in performance.\nThe results demonstrate the usefulness of RAG\ntechniques for improving performance, as well as\nthe need to carefully evaluate different aspects of a\nRAG system, on a diverse set of datasets, as effort\non developing generalized techniques is ongoing.\n5 Conclusion\nWe introduced RAG F OUNDRY , an open-source\nlibrary dedicated to the task of RAG-augmentation\nof LLMs, namely fine-tuning LLMs to become bet-\nter at RAG settings. The library is designed to serve\nas an end-to-end experimentation environment, en-\nabling users to quickly prototype and experiment\nwith different RAG techniques. We demonstrated\nthe usefulness of the library by augmenting two\nmodels with RAG configurations, evaluating on\nthree Q&A datasets and showing the benefit of\nRAG techniques, as well as of using multi-aspect\nmetrics relevant for RAG systems evaluation.\nLimitations and Future Plans\nOur hope is that the library will be useful to as\nmany people and use-cases as possible. However,\ndue to time and resource constraint, we were able to\ndemonstrate its usefulness on a subset of tasks and\ndatasets. Future work can expand the evaluation\nto other tasks, as well as implementing other RAG\ntechniques and evaluations.\nAlthough we designed the library to be general\nand customizable, there might be specific work-\nflows which will be difficult to run as-is and some\ncode changes may be required. The library proved\nuseful for our own research projects on a diverse\nset of datasets and tasks and extending it is easy\nand straightforward.\nFinally, despite our best efforts to offer detailed\ndocumentation in the library, there could be some\nmissing details regarding some functionality or spe-\ncific use-cases. The code repository will accept\nsuggestions, bug-fixes and pull requests.", "metadata": {"title": "Seven Failure Points When Engineering a Retrieval Augmented Generation\n  System", "pdf_url": "http://arxiv.org/pdf/2401.05856v1"}}, "1f9cd042-d24b-4004-9f1a-5f1a63d3dfd0": {"text": "Ethics Statement\nIn conducting our research we strive abiding to\nthe highest ethical standards, including integrity,\nfairness, and societal benefit of our work. We pri-\noritized data privacy and security throughout our\nresearch; any data used in our experiments was\npublicly available and did not contain any private\ninformation. We are committed to the principles of\ntransparency and reproducibility; the methodolo-\ngies, including data pre-processing, model training,\nand evaluation are documented in order to enable\nothers to replicate our findings. Code is made avail-\nable in an open repository. We advocate for the\nresponsible use of LLMs and RAG augmentation.\nIt is essential to exercise caution and verify the ac-\ncuracy and reliability of generated text produced by\nLLMs. Hallucinations can have negative implica-\ntions, and even when RAG methods can ameliorate\nsome of these aspects, verification and inspections\nare needed.\nReferences\nMarah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan,\nJyoti Aneja, Ahmed Awadallah, Hany Awadalla,\nNguyen Bach, Amit Bahree, Arash Bakhtiari, Jian-\nmin Bao, Harkirat Behl, Alon Benhaim, Misha\nBilenko, Johan Bjorck, S\u00e9bastien Bubeck, Qin Cai,\nMartin Cai, Caio C\u00e9sar Teodoro Mendes, Weizhu\nChen, Vishrav Chaudhary, Dong Chen, Dongdong\nChen, Yen-Chun Chen, Yi-Ling Chen, Parul Chopra,\nXiyang Dai, Allie Del Giorno, Gustavo de Rosa,\nMatthew Dixon, Ronen Eldan, Victor Fragoso, Dan\nIter, Mei Gao, Min Gao, Jianfeng Gao, Amit Garg,\nAbhishek Goswami, Suriya Gunasekar, Emman\nHaider, Junheng Hao, Russell J. Hewett, Jamie\nHuynh, Mojan Javaheripi, Xin Jin, Piero Kauff-\nmann, Nikos Karampatziakis, Dongwoo Kim, Ma-\nhoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat\nLee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Li-\nden, Ce Liu, Mengchen Liu, Weishung Liu, Eric Lin,\nZeqi Lin, Chong Luo, Piyush Madan, Matt Mazzola,\nArindam Mitra, Hardik Modi, Anh Nguyen, Brandon\nNorick, Barun Patra, Daniel Perez-Becker, Thomas\nPortet, Reid Pryzant, Heyang Qin, Marko Radmi-\nlac, Corby Rosset, Sambudha Roy, Olatunji Ruwase,\nOlli Saarikivi, Amin Saied, Adil Salim, Michael San-\ntacroce, Shital Shah, Ning Shang, Hiteshi Sharma,\nSwadheen Shukla, Xia Song, Masahiro Tanaka, An-\ndrea Tupini, Xin Wang, Lijuan Wang, Chunyu Wang,\nYu Wang, Rachel Ward, Guanhua Wang, Philipp\nWitte, Haiping Wu, Michael Wyatt, Bin Xiao, Can\nXu, Jiahang Xu, Weijian Xu, Sonali Yadav, Fan Yang,\nJianwei Yang, Ziyi Yang, Yifan Yang, Donghan Yu,\nLu Yuan, Chengruidong Zhang, Cyril Zhang, Jian-\nwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang,", "metadata": {"title": "Seven Failure Points When Engineering a Retrieval Augmented Generation\n  System", "pdf_url": "http://arxiv.org/pdf/2401.05856v1"}}, "5e5227bf-3c72-4793-be8c-0cddef83a306": {"text": "Witte, Haiping Wu, Michael Wyatt, Bin Xiao, Can\nXu, Jiahang Xu, Weijian Xu, Sonali Yadav, Fan Yang,\nJianwei Yang, Ziyi Yang, Yifan Yang, Donghan Yu,\nLu Yuan, Chengruidong Zhang, Cyril Zhang, Jian-\nwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang,\nYunan Zhang, and Xiren Zhou. 2024. Phi-3 technical\nreport: A highly capable language model locally on\nyour phone. Preprint, arXiv:2404.14219.\nAI@Meta. 2024. Llama 3 model card.\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. 2023. Self-rag: Learning to\nretrieve, generate, and critique through self-reflection.\nPreprint, arXiv:2310.11511.\nAngels Balaguer, Vinamra Benara, Renato Luiz de Fre-\nitas Cunha, Roberto de M. Estev\u00e3o Filho, Todd\nHendry, Daniel Holstein, Jennifer Marsman, Nick\nMecklenburg, Sara Malvar, Leonardo O. Nunes,\nRafael Padilha, Morris Sharp, Bruno Silva, Swati\nSharma, Vijay Aski, and Ranveer Chandra. 2024.\nRAG vs Fine-tuning: Pipelines, Tradeoffs, and a\nCase Study on Agriculture. arXiv preprint. ArXiv:\n2401.08406 [cs].\nScott Barnett, Stefanus Kurniawan, Srikanth Thudumu,\nZach Brannelly, and Mohamed Abdelrazek. 2024.\nSeven failure points when engineering a re-\ntrieval augmented generation system. Preprint,\narXiv:2401.05856.\nMoshe Berchansky, Daniel Fleischer, Moshe\nWasserblat, and Peter Izsak. 2024. Cotar: Chain-\nof-thought attribution reasoning with multi-level\ngranularity. Preprint, arXiv:2404.10513.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\nGuy, Jacob Menick, Roman Ring, T. W. Hennigan,\nSaffron Huang, Lorenzo Maggiore, Chris Jones, Al-\nbin Cassirer, Andy Brock, Michela Paganini, Geof-\nfrey Irving, Oriol Vinyals, Simon Osindero, Karen\nSimonyan, Jack W. Rae, Erich Elsen, and L. Sifre.\n2021. Improving language models by retrieving from\ntrillions of tokens. In International Conference on\nMachine Learning.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language Models are Few-Shot Learners.\narXiv preprint. ArXiv:2005.14165 [cs].", "metadata": {"title": "Seven Failure Points When Engineering a Retrieval Augmented Generation\n  System", "pdf_url": "http://arxiv.org/pdf/2401.05856v1"}}, "c114abfd-2ed2-47aa-910a-ceab86457d01": {"text": "Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language Models are Few-Shot Learners.\narXiv preprint. ArXiv:2005.14165 [cs].\nHarrison Chase. 2022. LangChain.\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\n2023. Benchmarking Large Language Models in\nRetrieval-Augmented Generation. arXiv.\nMichiel de Jong, Yury Zemlyanskiy, Nicholas FitzGer-\nald, Joshua Ainslie, Sumit Sanghai, Fei Sha, and\nWilliam Cohen. 2023. Pre-computed memory or", "metadata": {"title": "Seven Failure Points When Engineering a Retrieval Augmented Generation\n  System", "pdf_url": "http://arxiv.org/pdf/2401.05856v1"}}, "1a465845-0c96-4462-bce6-18936184463d": {"text": "on-the-fly encoding? A hybrid approach to retrieval\naugmentation makes the most of your compute. Pub-\nlisher: arXiv Version Number: 2.\nShahul Es, Jithin James, Luis Espinosa Anke, and\nSteven Schockaert. 2024. RAGAs: Automated evalu-\nation of retrieval augmented generation. In Proceed-\nings of the 18th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nSystem Demonstrations, pages 150\u2013158, St. Julians,\nMalta. Association for Computational Linguistics.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,\nJinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen\nWang. 2023. Retrieval-Augmented Generation for\nLarge Language Models: A Survey. arXiv preprint.\nArXiv:2312.10997 [cs].\nYasuto Hoshi, Daisuke Miyashita, Youyang Ng, Kento\nTatsuno, Yasuhiro Morioka, Osamu Torii, and Jun\nDeguchi. 2023. RaLLe: A Framework for Devel-\noping and Evaluating Retrieval-Augmented Large\nLanguage Models. arXiv preprint.\nJennifer Hsia, Afreen Shaikh, Zhiruo Wang, and Gra-\nham Neubig. 2024. RAGGED: Towards Informed\nDesign of Retrieval Augmented Generation Systems.\narXiv preprint. ArXiv:2403.09040 [cs].\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. LoRA: Low-Rank Adaptation\nof Large Language Models. arXiv preprint. ArXiv:\n2106.09685 [cs].\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,\nZhangyin Feng, Haotian Wang, Qianglong Chen,\nWeihua Peng, Xiaocheng Feng, Bing Qin, and\nTing Liu. 2023. A Survey on Hallucination in\nLarge Language Models: Principles, Taxonomy,\nChallenges, and Open Questions. arXiv preprint.\nArXiv:2311.05232 [cs].\nJiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang,\nand Zhicheng Dou. 2024. FlashRAG: A Modular\nToolkit for Efficient Retrieval-Augmented Genera-\ntion Research.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William W.\nCohen, and Xinghua Lu. 2019. PubMedQA: A\nDataset for Biomedical Research Question Answer-\ning. arXiv preprint. ArXiv: 1909.06146 [cs, q-bio].\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A Large Scale Dis-\ntantly Supervised Challenge Dataset for Reading\nComprehension. arXiv preprint. ArXiv:1705.03551\n[cs].\nOmar Khattab, Keshav Santhanam, Xiang Lisa\nLi, David Hall, Percy Liang, Christopher Potts,\nand Matei Zaharia. 2022. Demonstrate-search-\npredict: Composing retrieval and language mod-\nels for knowledge-intensive NLP. arXiv preprint\narXiv:2212.14024.\nOmar Khattab, Arnav Singhvi, Paridhi Maheshwari,\nZhiyuan Zhang, Keshav Santhanam, Sri Vard-", "metadata": {"title": "Seven Failure Points When Engineering a Retrieval Augmented Generation\n  System", "pdf_url": "http://arxiv.org/pdf/2401.05856v1"}}, "dadf97ee-929c-4f3b-8d33-b99a2d6077e8": {"text": "and Matei Zaharia. 2022. Demonstrate-search-\npredict: Composing retrieval and language mod-\nels for knowledge-intensive NLP. arXiv preprint\narXiv:2212.14024.\nOmar Khattab, Arnav Singhvi, Paridhi Maheshwari,\nZhiyuan Zhang, Keshav Santhanam, Sri Vard-\nhamanan, Saiful Haq, Ashutosh Sharma, Thomas T.\nJoshi, Hanna Moazam, Heather Miller, Matei Za-\nharia, and Christopher Potts. 2023. Dspy: Compiling\ndeclarative language model calls into self-improving\npipelines. arXiv preprint arXiv:2310.03714.\nTakeshi Kojima, S. Gu, Machel Reid, Yutaka Matsuo,\nand Yusuke Iwasawa. 2022. Large Language Models\nare Zero-Shot Reasoners. ArXiv.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rock-\nt\u00e4schel, Sebastian Riedel, and Douwe Kiela. 2021.\nRetrieval-Augmented Generation for Knowledge-\nIntensive NLP Tasks. arXiv preprint.\nJimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-\nHong Yang, Ronak Pradeep, and Rodrigo Nogueira.\n2021. Pyserini: A Python toolkit for reproducible\ninformation retrieval research with sparse and dense\nrepresentations. In Proceedings of the 44th Annual\nInternational ACM SIGIR Conference on Research\nand Development in Information Retrieval (SIGIR\n2021), pages 2356\u20132362.\nJerry Liu. 2022. LlamaIndex.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2023. Lost in the middle: How language mod-\nels use long contexts. Preprint, arXiv:2307.03172.\nZihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu\nLee, Mohammad Shoeybi, and Bryan Catanzaro.\n2024. ChatQA: Surpassing GPT-4 on Conversational\nQA and RAG. arXiv preprint. ArXiv: 2401.10225\n[cs].\nAlex Troy Mallen, Akari Asai, Victor Zhong, Rajarshi\nDas, Hannaneh Hajishirzi, and Daniel Khashabi.\n2022. When not to trust language models: Investigat-\ning effectiveness of parametric and non-parametric\nmemories. In Annual Meeting of the Association for\nComputational Linguistics.\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng,\nYujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou\nYu, Weizhu Chen, and Jianfeng Gao. 2023. Check\nYour Facts and Try Again: Improving Large Lan-\nguage Models with External Knowledge and Auto-\nmated Feedback. Publisher: arXiv Version Number:\n3.\nMalte Pietsch, Timo M\u00f6ller, Bogdan Kostic, Julian\nRisch, Massimiliano Pippi, Mayank Jobanputra, Sara\nZanzottera, Silvano Cerza, Vladimir Blagojevic,\nThomas Stadelmann, Tanay Soni, and Sebastian Lee.\n2019. Haystack: the end-to-end NLP framework for\npragmatic builders.", "metadata": {"title": "Seven Failure Points When Engineering a Retrieval Augmented Generation\n  System", "pdf_url": "http://arxiv.org/pdf/2401.05856v1"}}, "63f5f060-0c8c-4b32-af89-debaeb585575": {"text": "Risch, Massimiliano Pippi, Mayank Jobanputra, Sara\nZanzottera, Silvano Cerza, Vladimir Blagojevic,\nThomas Stadelmann, Tanay Soni, and Sebastian Lee.\n2019. Haystack: the end-to-end NLP framework for\npragmatic builders.\nDavid Rau, Herv\u2019e D\u2019ejean, Nadezhda Chirkova,\nThibault Formal, Shuai Wang, Vassilina Nikoulina,\nand S. Clinchant. 2024. BERGEN: A Benchmarking\nLibrary for Retrieval-Augmented Generation.", "metadata": {"title": "Seven Failure Points When Engineering a Retrieval Augmented Generation\n  System", "pdf_url": "http://arxiv.org/pdf/2401.05856v1"}}, "27ff2bbe-5ebf-42f8-b9e0-9bbfe8cf20b7": {"text": "Jon Saad-Falcon, Omar Khattab, Christopher Potts, and\nMatei Zaharia. 2024. ARES: An Automated Evalua-\ntion Framework for Retrieval-Augmented Generation\nSystems. arXiv preprint. ArXiv:2311.09476 [cs].\nIvan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-\nWei Chang. 2022. ASQA: Factoid Questions Meet\nLong-Form Answers. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 8273\u20138288, Abu Dhabi,\nUnited Arab Emirates. Association for Computa-\ntional Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. Preprint,\narXiv:2302.13971.\nXiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran\nZhang, Yixin Wu, Zhibo Xu, Tianyuan Shi,\nZhengyuan Wang, Shizheng Li, Qi Qian, Ruicheng\nYin, Changze Lv, Xiaoqing Zheng, and Xuanjing\nHuang. 2024. Searching for Best Practices in\nRetrieval-Augmented Generation. arXiv preprint.\nDingjun Wu, Jing Zhang, and Xinmei Huang. 2023.\nChain of thought prompting elicits knowledge aug-\nmentation. In Findings of the Association for Com-\nputational Linguistics: ACL 2023, pages 6519\u20136534,\nToronto, Canada. Association for Computational Lin-\nguistics.\nHao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu,\nand Zhaofeng Liu. 2024a. Evaluation of Retrieval-\nAugmented Generation: A Survey. arXiv preprint.\nArXiv:2405.07437 [cs].\nYue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You,\nChao Zhang, Mohammad Shoeybi, and Bryan Catan-\nzaro. 2024b. RankRAG: Unifying Context Rank-\ning with Retrieval-Augmented Generation in LLMs.\narXiv preprint. ArXiv:2407.02485 [cs].\nTianjun Zhang, Shishir G. Patil, Naman Jain, Sheng\nShen, Matei Zaharia, Ion Stoica, and Joseph E. Gon-\nzalez. 2024. Raft: Adapting language model to do-\nmain specific rag.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2019. BERTScore:\nEvaluating Text Generation with BERT. ArXiv.", "metadata": {"title": "Seven Failure Points When Engineering a Retrieval Augmented Generation\n  System", "pdf_url": "http://arxiv.org/pdf/2401.05856v1"}}, "ed93135e-501c-49ee-87f4-ba92a37fa969": {"text": "A Implementation Details\nA.1 Prompts\nYou are a helpful question answerer who can provide an answer given a question and relevant context.\nListing 5: System instruction used in the experiments.\nQuestion: {query}\nContext: {docs}\nListing 6: Template for inserting relevant documents as\ncontext.\nQuestion: {query}\nContext: {docs}\nAnswer this question using the information given in the context above. Here is things to pay attention to:\n- First provide step-by-step reasoning on how to answer the question.\n- In the reasoning, if you need to copy paste some sentences from the context, include them in\n##begin_quote## and ##end_quote##. This would mean that things outside of ##begin_quote## and\n##end_quote## are not directly copy paste from the context.\n- End your response with final answer in the form <ANSWER>: $answer, the answer should be succinct.\nListing 7: Template for Chain-of-Thought reasoning.\nA.2 Datasets\nDatasets used:\n\u2022 TriviaQA\n\u2022 ASQA\n\u2022 PubmedQA\nContext size was k = 5, unless indicated otherwise.\nDataset sizes are:\nDataset Training Evaluation\nTriviaQA 6000 1000\nASQA 4353 948\nPubmedQA 10000 500\nA.3 Training Details\nParameter Value\nLoRA r 16\nLoRA \u03b1 16\nLoRA Dropout 0.1\nLoRA Bias None\nLoRA Modules qkv_proj, Phi-3\nq/v_proj, Llama-3\nLR 1e-4\nLR Scheduler cosine\nWarmup Ratio 0.03\nWeight Decay 0.001\nBatch Size 1\nEpochs 1", "metadata": {"title": "Seven Failure Points When Engineering a Retrieval Augmented Generation\n  System", "pdf_url": "http://arxiv.org/pdf/2401.05856v1"}}, "75a42688-24f6-4e01-a30b-b09958ef5651": {"text": "In Defense of RAG in the Era of Long-Context Language Models\nTan Yu\nNVIDIA\nSanta Clara, California\nUnited States\ntayu@nvidia.com\nAnbang Xu\nNVIDIA\nSanta Clara, California\nUnited States\nanbangx@nvidia.com\nRama Akkiraju\nNVIDIA\nSanta Clara, California\nUnited States\nrakkiraju@nvidia.com\nAbstract\nOvercoming the limited context limitations in\nearly-generation LLMs, retrieval-augmented\ngeneration (RAG) has been a reliable solution\nfor context-based answer generation in the past.\nRecently, the emergence of long-context LLMs\nallows the models to incorporate much longer\ntext sequences, making RAG less attractive.\nRecent studies show that long-context LLMs\nsignificantly outperform RAG in long-context\napplications. Unlike the existing works favor-\ning the long-context LLM over RAG, we ar-\ngue that the extremely long context in LLMs\nsuffers from a diminished focus on relevant in-\nformation and leads to potential degradation in\nanswer quality. This paper revisits the RAG in\nlong-context answer generation. We propose\nan order-preserve retrieval-augmented genera-\ntion (OP-RAG) mechanism, which significantly\nimproves the performance of RAG for long-\ncontext question-answer applications. With\nOP-RAG, as the number of retrieved chunks\nincreases, the answer quality initially rises, and\nthen declines, forming an inverted U-shaped\ncurve. There exist sweet points where OP-RAG\ncould achieve higher answer quality with much\nless tokens than long-context LLM taking the\nwhole context as input. Extensive experiments\non public benchmark demonstrate the superior-\nity of our OP-RAG.\n1 Introduction\nDue to the limited context window length\n(eg, 4096) of early-generation large language\nmodels (LLMs), retrieval augmented generation\n(RAG) (Guu et al., 2020; Lewis et al., 2020) is an\nindispensable choice to handle a large-scale context\ncorpus. Since the answer quality is heavily depen-\ndent on the performance of the retrieval model, a\nlot of efforts are devoted to improving the retrieval\nrecall/precision when designing the RAG system.\nRecently, the state-of-art LLMs support much\nlonger context windows. For example, GPT-\n4O (OpenAI, 2023), Claudi-3.5 (Anthropic, 2024),\n(a) F1 score.\n(b) Input token count.\nFigure 1: Comparisons between the proposed order-\npreserve retrieval-augmented generation (OP-RAG) and\napproaches using long-context LLMs without RAG\non En.QA dataset of \u221eBench. Our OP-RAG uses\nLlama3.1-70B as generator, which significantly outper-\nforms its counterpart using Llama3.1-70B without RAG.\nLlama3.1 (Meta, 2024b), Phi-3 (Abdin et al., 2024),", "metadata": {"title": "Observations on Building RAG Systems for Technical Documents", "pdf_url": "http://arxiv.org/pdf/2404.00657v1"}}, "22e7b8d5-d4e3-43b8-b251-9eeabfeeb2ae": {"text": "on En.QA dataset of \u221eBench. Our OP-RAG uses\nLlama3.1-70B as generator, which significantly outper-\nforms its counterpart using Llama3.1-70B without RAG.\nLlama3.1 (Meta, 2024b), Phi-3 (Abdin et al., 2024),\nand Mistral-Large2 (AI, 2024) all support 128-K\ncontext. Gemini-1.5-pro even supports a 1M con-\ntext window. The recent emergence of long-context\nLLMs naturally leads to the question: is RAG nec-\nessary in the age of long-context LLMs? Li et al.\n(2024) recently systematically compares RAG with\nlong-context (LC) LLMs (w/o RAG) and demon-\nstrates that LC LLMs consistently outperform RAG\nin terms of answer quality.\nIn this work, we re-examine the effectiveness\nof RAG in long-context answer generation. We\nobserve that the order of retrieved chunks in the\narXiv:2409.01666v1  [cs.CL]  3 Sep 2024", "metadata": {"title": "Observations on Building RAG Systems for Technical Documents", "pdf_url": "http://arxiv.org/pdf/2404.00657v1"}}, "74a9b256-b2ed-451b-9202-6cc4caa4f6f8": {"text": "context of LLM is vital for the answer quality. Dif-\nferent from traditional RAG which places the re-\ntrieved chunks in a relevance-descending order, we\npropose to preserve the order of retrieved chunks\nin the original text. Our experiments show that the\nproposed order-preserving mechanism significantly\nimproves the answer quality of RAG.\nMeanwhile, using the proposed order-preserve\nRAG, as the number of retrieved chunks increases,\nthe answer quality initially rises and then declines.\nThis is because, with more retrieved chunks, the\nmodel has access to more potentially relevant in-\nformation, which improves the chances of retriev-\ning the correct context needed to generate a high-\nquality answer. However, as more chunks are re-\ntrieved, the likelihood of introducing irrelevant or\ndistracting information also increases. This excess\ninformation can confuse the model, leading to a\ndecline in answer quality. The trade-off, therefore,\nis between improving recall by retrieving more\ncontext and maintaining precision by limiting dis-\ntractions. The optimal point is where the balance\nbetween relevant and irrelevant information maxi-\nmizes the quality of the answer. Beyond this point,\nthe introduction of too much irrelevant information\ndegrades the model\u2019s performance. It explains the\ninferior performance of the approach taking the\nwhole long context as the input of LLM.\nDifferent from the conclusion from Li et al.\n(2024), with the proposed order-preserving mech-\nanism, RAG achieves higher answer quality com-\npared with its counterparts that rely solely on Long-\nContext LLMs. As shown in Figure 4a, On En.QA\ndataset of \u221eBench (Zhang et al., 2024), using only\n16K retrieved tokens, we achieve 44.43 F1 score\nwith Llama3.1-70B. In contrast, without RAG,\nLlama3.1-70B making full use of 128K context\nonly achieves 34.32 F1 score, GPT-4O achieves\nonly 32.36 F1 score and Gemini-1.5-Pro obtains\nonly 43.08 F1 score as evaluated by Li et al. (2024).\nThat is, RAG could achieve a higher F1 score even\nwith a significant reduction on input length.\n2 Related Work\nRetrieval-augmented generation. By incorporat-\ning the external knowledge as context, retrieval-\naugmented generation (RAG) (Guu et al., 2020;\nLewis et al., 2020; Mialon et al., 2023) allows lan-\nguage model to access up-to-date and specific in-\nformation, reducing hallucinations and improving\nfactual accuracy. Before the era of long-context\nFigure 2: Vanilla RAG versus the proposed order-\npreserve the RAG. As shown in the example, a long\ndocument is cropped into 13 chunks, {ci}13", "metadata": {"title": "Observations on Building RAG Systems for Technical Documents", "pdf_url": "http://arxiv.org/pdf/2404.00657v1"}}, "23118466-1ab6-4f9c-9408-19048a6ec359": {"text": "formation, reducing hallucinations and improving\nfactual accuracy. Before the era of long-context\nFigure 2: Vanilla RAG versus the proposed order-\npreserve the RAG. As shown in the example, a long\ndocument is cropped into 13 chunks, {ci}13\ni=1. The sim-\nilarity score is appended to each chunk. We retrieve\ntop 4 chunks with the highest similarity scores. Vanilla\nRAG places the chunks in a score-descending order,\nwhereas the proposed order-preserve RAG places the\nchunks based on the order in the original document.\nLLMs, RAG is a promising solution to overcoming\nthe limitation of short context window.\nLong-context LLM. To support the long sequence\nof language models, many efforts have been de-\nvoted to improving the computing efficiency of\nself-attention (Choromanski et al., 2020; Zaheer\net al., 2020; Tay et al., 2020; Dao et al., 2022; Dao,\n2024) and boosting extensibility of positional en-\ncoding (Press et al., 2021; Sun et al., 2022; Chen\net al., 2023). Recently, the flagship LLMs such as\nGPT-4O (OpenAI, 2023), Gemini-1.5-Pro (Reid\net al., 2024), Claudi-3.5 (Anthropic, 2024), Grok-\n2 (xAI, 2024), and Llama3.1 (Meta, 2024a) have\nsupported extremely large context. With the ex-\nistence of long-context LLMs, RAG is no longer\na indispensable module for long-context question-\nanswering task. Recently, Li et al. (2024) con-\ncludes that using long-context without RAG could\nsignificantly outperforms RAG. Different from the\nconclusion from (Li et al., 2024), in this work,\nwe demonstrate the proposed order-preserve RAG\ncould beat the long-context LLMs without RAG.\n3 Order-Preserve RAG\nLet us denote the long textual context, e.g., a long\ndocument, by d. We split d into N chunks sequen-\ntially and uniformly, {ci}N\ni=1. The index i implies\nthe sequential order of the chunk ci in d. That is,\nci\u22121 denotes the chunk before ci whereas ci+1 de-\nnotes the chunk right after ci. Given a query q, we\nobtain the relevance score of the chunk ci by com-\nputing cosine similarity between the embedding of\nq and that of ci:\nsi = cos(emb(q), emb(ci)), (1)", "metadata": {"title": "Observations on Building RAG Systems for Technical Documents", "pdf_url": "http://arxiv.org/pdf/2404.00657v1"}}, "8f4378b0-104e-4f1d-b524-602cf913d719": {"text": "(a) EN.QA\n (b) EN.MC\nFigure 3: The influence of context length on the performance of RAG. The evaluations are conducted on En.QA and\nEN.MC datasets of \u221eBench.\nwhere cos(\u00b7, \u00b7) denotes the cosine similarity func-\ntion and emb(\u00b7) denotes the embedding function.\nWe retrieve the top k chunks with the highest\nsimilarity scores with the query and denote the in-\ndices of top k chunks byJ = {ji}k\ni=1. We preserve\nthe order of chunks in the original long context d,\nthat is, we constrain\njl > jm \u21d0\u21d2 l > m. (2)\nFigure 2 visualizes the difference between the\nvanilla RAG and the proposed order-preserve RAG.\nDifferent from vanilla RAG placing the chunks in\nthe order of similarity descending, the proposed\norder-preserve RAG keep the order of chunks in\nthe original document.\n4 Experiments\n4.1 Datasets.\nWe conduct experiments on EN.QA and EN.MC\ndatasets of \u221eBench (Zhang et al., 2024) bench-\nmark, specially designed for long-context QA eval-\nuation. To be specific, En.QA consists of 351\nhuman-annotated question-answer pairs. On av-\nerage, the long context in En.QA contains 150,374\nwords. We use F1-score as metric for evaluation on\nEn.QA. EN.MC consists of 224 question-answer\npairs, which are annotated similarly to En.QA, but\neach question is provided with four answer choices.\nOn average, the long context in En.MC contains\n142,622 words. We use accuracy as metric for eval-\nuation on En.QA. We notice there is another bench-\nmark termed LongBench (Bai et al., 2023). Never-\ntheless, the average context length of LongBench\nis below 20K words, which is not long enough to\nevaluate the recent long-context LLMs supporting\n128K-token window size.\n4.2 Implementation details.\nWe set the chunk size as 128 tokens on all datasets.\nChunks are non-overlapped. We use BGE-large-en-\nv1.5 (Xiao et al., 2023) to extract the embedding\nof queries and chunks, by default.\n4.3 Ablation Study\nThe influence of context length. We evaluate\nthe influence of the context length on the perfor-\nmance of the proposed order-preserve RAG. Since\neach chunk contains 128 tokens, the context length\nis 128m, where m is the number of the retrieved\nchunks as the context for generating the answer. As\nshown in Figure 3, as the context length increases,\nthe performance initially increases. This is because\nmore context might have a greater chance of cover-\ning the relevant chunk. Nevertheless, as the context\nlength further increases, the answer quality drops\nsince more irrelevant chunks are used as distrac-\ntions. To be specific, Llama3.1-8B model achieves", "metadata": {"title": "Observations on Building RAG Systems for Technical Documents", "pdf_url": "http://arxiv.org/pdf/2404.00657v1"}}, "8c454368-50b0-4fd3-b5b5-b4d6819850cc": {"text": "ing the relevant chunk. Nevertheless, as the context\nlength further increases, the answer quality drops\nsince more irrelevant chunks are used as distrac-\ntions. To be specific, Llama3.1-8B model achieves\nthe performance peak when the context length is\n16K on both EN.QA dataset and EN.MC dataset,\nwhereas the best performance of Llama3.1-70B\nmodel is achieved at 48K on EN.QA and 32K on\nEN.MC. The fact that the peak point of Llama3.1-\n70B comes later than Llama3.1-8B model might\nbe because the larger-scale model has a stronger\ncapability to distinguish the relevant chunks from", "metadata": {"title": "Observations on Building RAG Systems for Technical Documents", "pdf_url": "http://arxiv.org/pdf/2404.00657v1"}}, "1d8cf616-93d1-4100-88bc-a9f09c73434e": {"text": "(a) EN.QA\n (b) EN.MC\nFigure 4: Comparisons between the proposed order-preserve RAG and vanilla RAG. The evaluations are conducted\non En.QA and EN.MC datasets of \u221eBench, using Llama3.1-70B model.\nirrelevant distractions.\nOrder-preserve RAG versus vanilla RAG. As\nshown in Figure 4, when the number of retrieved\nchunks are small (e.g, 8), the advantage of the pro-\nposed order-preserve RAG over vanilla RAG is not\nconsiderably. In contrast, when the number of re-\ntrieved chunks is large, our order-preserve RAG\nsignificantly outperforms vanilla RAG. To be spe-\ncific, on EN.QA dataset, when the number of re-\ntrieved chunk is 128, vanilla RAG only achieves\n38.40 F1-score whereas our order-preserve RAG\nachieves 44.43 F1-score. On EN.MC dataset, re-\ntrieving 192 chunks, vanialla RAG only achieves\n81.22 accuracy whereas our order-preserve RAG\nobtains 88.65 accuracy.\n4.4 Main Results\nWe compare the proposed order-preserve RAG with\ntwo types of baselines. The first category of ap-\nproaches uses the long-context LLM without RAG.\nAs shown in Table 1, without RAG, LLM takes a\nhuge number of tokens as input, which is inefficient\nand costly. In contrast, the proposed order-preserve\nRAG not only significantly reduces the number of\ntokens, but also significantly improves the answer\nquality. For instance, using Llama3.1-70B model,\nthe approach without RAG only achieves a 34.26\nF1 score on EN.QA with an average of 117K to-\nkens as input. In contrast, our OP-RAG with 48K\ntokens as input attains a 47.25 F1 score. The sec-\nond category of baselines takes the SELF-ROUTE\nmechanism (Li et al., 2024), which routes queries\nto RAG or long-context LLM based on the model\nself-reflection. As shown in Table 1, ours signifi-\nMethod EN.QA EN.MC\nF1 Score Tokens Acc. Tokens\nLong-context LLM w/o RAG\nLlama3.1-70B 34.26 117K 71.62 117K\nGPT-4O 32.36 117K 78.42 117K\nGemini-1.5-Pro 43.08 196K 85.57 188K\nSELF-ROUTE (Li et al., 2024)\nGPT-4O 34.95 85K 77.29 62K\nGemini-1.5-Pro 37.51 83K 76.86 62K\nLlama3.1-70B order-preserve RAG (ours)\nOP-RAG-16K 44.43 16K 84.72 16K\nOP-RAG-24K 45.45 24K 88.65 24K\nOP-RAG-48K 47.25 48K 85.59 48K\nTable 1: Comparisons among the long-context LLM\nwithout RAG, SELF-ROUTE mechanism (Li et al.,\n2024) and the proposed order-preserve (OP) RAG.\ncantly outperforms than using much fewer tokens\nin the input of LLMs.\n5 Conclusion\nIn this paper, we have revisited the role of retrieval-\naugmented generation (RAG) in the era of long-\ncontext language models (LLMs). While recent\ntrends have favored long-context LLMs over RAG", "metadata": {"title": "Observations on Building RAG Systems for Technical Documents", "pdf_url": "http://arxiv.org/pdf/2404.00657v1"}}, "2b254807-3d71-4b82-88be-29f33cd42162": {"text": "in the input of LLMs.\n5 Conclusion\nIn this paper, we have revisited the role of retrieval-\naugmented generation (RAG) in the era of long-\ncontext language models (LLMs). While recent\ntrends have favored long-context LLMs over RAG\nfor their ability to incorporate extensive text se-\nquences, our research challenges this perspective.\nWe argue that extremely long contexts in LLMs\ncan lead to a diminished focus on relevant infor-\nmation, potentially degrading answer quality in\nquestion-answering tasks. To address this issue, we\nproposed the order-preserve retrieval-augmented\ngeneration (OP-RAG) mechanism. Our extensive\nexperiments on public benchmarks have demon-\nstrated that OP-RAG significantly improves the", "metadata": {"title": "Observations on Building RAG Systems for Technical Documents", "pdf_url": "http://arxiv.org/pdf/2404.00657v1"}}, "c4cc04e2-b009-459b-abed-1b5c51e39a4f": {"text": "performance of RAG for long-context question-\nanswer applications. OP-RAG\u2019s superior perfor-\nmance suggests that efficient retrieval and focused\ncontext utilization can outperform the brute-force\napproach of processing extremely long contexts.\nReferences\nMarah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan,\nJyoti Aneja, Ahmed Awadallah, Hany Awadalla,\nNguyen Bach, Amit Bahree, Arash Bakhtiari, Harki-\nrat Behl, et al. 2024. Phi-3 technical report: A highly\ncapable language model locally on your phone.arXiv\npreprint arXiv:2404.14219.\nMistral AI. 2024. Mistral large 2.\nAnthropic. 2024. Claude 3.5 sonnet.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,\nJiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao\nLiu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,\nand Juanzi Li. 2023. Longbench: A bilingual, mul-\ntitask benchmark for long context understanding.\narXiv preprint arXiv:2308.14508.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and\nYuandong Tian. 2023. Extending context window of\nlarge language models via positional interpolation.\narXiv preprint arXiv:2306.15595.\nKrzysztof Choromanski, Valerii Likhosherstov, David\nDohan, Xingyou Song, Andreea Gane, Tamas Sar-\nlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nLukasz Kaiser, et al. 2020. Rethinking attention with\nperformers. arXiv preprint arXiv:2009.14794.\nTri Dao. 2024. FlashAttention-2: Faster attention with\nbetter parallelism and work partitioning. In Inter-\nnational Conference on Learning Representations\n(ICLR).\nTri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra,\nand Christopher R\u00e9. 2022. FlashAttention: Fast and\nmemory-efficient exact attention with IO-awareness.\nIn Advances in Neural Information Processing Sys-\ntems (NeurIPS).\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In International confer-\nence on machine learning, pages 3929\u20133938. PMLR.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rock-\nt\u00e4schel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459\u20139474.\nZhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu\nMei, and Michael Bendersky. 2024. Retrieval aug-\nmented generation or long-context llms? a compre-\nhensive study and hybrid approach. arXiv preprint\narXiv:2407.16833.\nMeta. 2024a. Introducing llama 3.1: Our most capable\nmodels to date.\nMeta. 2024b. Llama 3.1 models.", "metadata": {"title": "Observations on Building RAG Systems for Technical Documents", "pdf_url": "http://arxiv.org/pdf/2404.00657v1"}}, "49e0f66e-40e1-49c7-b8df-516a41b12f22": {"text": "mented generation or long-context llms? a compre-\nhensive study and hybrid approach. arXiv preprint\narXiv:2407.16833.\nMeta. 2024a. Introducing llama 3.1: Our most capable\nmodels to date.\nMeta. 2024b. Llama 3.1 models.\nGr\u00e9goire Mialon, Roberto Dess\u00ec, Maria Lomeli, Christo-\nforos Nalmpantis, Ram Pasunuru, Roberta Raileanu,\nBaptiste Rozi\u00e8re, Timo Schick, Jane Dwivedi-Yu,\nAsli Celikyilmaz, et al. 2023. Augmented language\nmodels: a survey. arXiv preprint arXiv:2302.07842.\nOpenAI. 2023. GPT-4 technical report. ArXiv,\n2303:08774.\nOfir Press, Noah A Smith, and Mike Lewis. 2021.\nTrain short, test long: Attention with linear biases\nenables input length extrapolation. arXiv preprint\narXiv:2108.12409.\nMachel Reid, Nikolay Savinov, Denis Teplyashin,\nDmitry Lepikhin, Timothy Lillicrap, Jean-baptiste\nAlayrac, Radu Soricut, Angeliki Lazaridou, Orhan Fi-\nrat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Un-\nlocking multimodal understanding across millions of\ntokens of context. arXiv preprint arXiv:2403.05530.\nYutao Sun, Li Dong, Barun Patra, Shuming Ma, Shao-\nhan Huang, Alon Benhaim, Vishrav Chaudhary, Xia\nSong, and Furu Wei. 2022. A length-extrapolatable\ntransformer. arXiv preprint arXiv:2212.10554.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-\nzler. 2020. Efficient transformers: A survey.(2020).\narXiv preprint cs.LG/2009.06732.\nxAI. 2024. Grok-2 beta release.\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\nMuennighoff. 2023. C-pack: Packaged resources\nto advance general chinese embedding. Preprint,\narXiv:2309.07597.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. Advances in neural information\nprocessing systems, 33:17283\u201317297.\nXinrong Zhang, Yingfa Chen, Shengding Hu, Zi-\nhang Xu, Junhao Chen, Moo Khai Hao, Xu Han,\nZhen Leng Thai, Shuo Wang, Zhiyuan Liu, and\nMaosong Sun. 2024. \u221ebench: Extending long\ncontext evaluation beyond 100k tokens. Preprint,\narXiv:2402.13718.", "metadata": {"title": "Observations on Building RAG Systems for Technical Documents", "pdf_url": "http://arxiv.org/pdf/2404.00657v1"}}, "18d96500-fc8d-497e-a044-1e27fe445e9a": {"text": "Published as a Tiny Paper at ICLR 2024\nOBSERVATIONS ON BUILDING RAG S YSTEMS FOR\nTECHNICAL DOCUMENTS\nSumit Soman and Sujoy Roychowdhury\u2217\n{sumit.soman, sujoy.roychowdhury}@ericsson.com\nABSTRACT\nRetrieval augmented generation (RAG) for technical documents creates chal-\nlenges as embeddings do not often capture domain information. We review prior\nart for important factors affecting RAG and perform experiments to highlight best\npractices and potential challenges to build RAG systems for technical documents.\n1 I NTRODUCTION\nLong form Question Answering (QA) involves generating paragraph-size responses from Large Lan-\nguage Models (LLMs). RAG for technical documents has several challenges Xu et al. (2023); Toro\net al. (2023). Factors affecting retrieval performance, including in-context documents, LLMs and\nmetrics, have been evaluated Chen et al. (2023a). To further build on this work, we conduct exper-\niments on technical documents with telecom and battery terminology to examine the influence of\nchunk length, keyword-based search and ranks (sequence) of retrieved results in the RAG pipeline.\n2 E XPERIMENTAL SETUP\nOur experiments are based on IEEE Wireless LAN Medium Access Control (MAC) and Physical\nLayer (PHY) specifications IEEE (2021) and IEEE Standard Glossary of Stationary Battery Ter-\nminology 1881-2016 (2016). We separately process the glossary of definitions and the full docu-\nment, as many expected questions are based on the definitions.We source questions based on domain\nknowledge and report experimental results on 42 representative queries across the documents. Mul-\ntiple embedding models can be used, Reimers & Gurevych (2019), we use MPNET Song et al.\n(2020) for the entire document - excluding tables and captions. For the glossary, we split the term\nand the definition and generate separate embeddings for them, as well as for the full paragraph hav-\ning the defined term and the definition. Soman & HG (2023) have reviewed other LLMs for telecom\ndomain, but we chose llama2-7b-chat model Touvron et al. (2023) as it is free and has a commercial-\nfriendly license. We evaluate on multiple questions and report on selected questions to substantiate\nour observations. For reference, the prompts used for the LLM are provided in Appendix A.\n3 O BSERVATIONS\nWe first observe that sentence embeddings become unreliable with increasing chunk size. Appendix\nB Fig. 1 shows the Kernel Density Estimate (KDE) plot of cosine similarity scores for various", "metadata": {"title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented\n  Generation", "pdf_url": "http://arxiv.org/pdf/2408.02545v1"}}, "424cd92f-8c17-4757-aa3a-1174be86d7c2": {"text": "3 O BSERVATIONS\nWe first observe that sentence embeddings become unreliable with increasing chunk size. Appendix\nB Fig. 1 shows the Kernel Density Estimate (KDE) plot of cosine similarity scores for various\nsentence lengths. We take 10,970 sentences and look at pairwise similarity for all the sentences. A\nhigh similarity is observed when the length of the sentences is relatively long. The higher similarity\ndistribution for larger lengths indicates spurious similarities which we manually validate for a few\nsamples. We find that when both the query and queried document are over 200 words, the similarity\ndistribution is bimodal. When either of them are over 200 words, there is a small but less perceptible\nlift at higher similarities.\nTable 1 summarizes our hypotheses and key observations - corresponding sample queries and their\nresults are provided in Appendix C. We hypothesize that splitting on definition and terms can help\nimprove results (H1), similarity scores being a good measure (H2), position of keywords influenc-\ning results (H3), sentence-based similarity resulting in a better retriever (H4) and generator (H5),\n\u2217Global AI Accelerator, Ericsson R&D, Bangalore, India. Both authors contributed equally. Git Repo Link.\n1\narXiv:2404.00657v1  [cs.LG]  31 Mar 2024", "metadata": {"title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented\n  Generation", "pdf_url": "http://arxiv.org/pdf/2408.02545v1"}}, "4705e314-7a60-49ca-a3fd-10e40baaa6a8": {"text": "Published as a Tiny Paper at ICLR 2024\nHyp Hypothesis Observation Support (Samples)\nH1 Splitting definition and de-\nfined words help in queries\nFor definitions, using the defined word\nand definition separately for retrieval\ngives better performance\n22 of 30\nqueries (ID\n2, 3)\nH2 Similarity scores should\nnot be used to compare re-\ntrieved results\nWe observe that similarity scores be-\ntween different approaches are not com-\nparable and absolute values are often\nvery small for correct answers\n24 of 30\nqueries (ID\n2, 3)\nH3 Position of keywords matterKeywords closer to the beginning of the\nsentence are retrieved with high accu-\nracy\n25 of 30 queries\nKeywords which occur later in the sen-\ntence are difficult to be retrieved\n(ID 1, 4, 5,\n6)\nH4 Sentence Based Similarity\nis better\nSimilarity based on sentence and distinct\nparagraphs retrieved gives much detailed\ncontext to generator\nID F1 - Ta-\nble 2 (8 of\n10 queries)\nH5 Generator for sentence\nbased similarity\nGenerated answer using sentence based\nsimilarity and paragraph based retrieval\ngives better results\n8 of 10\nqueries\n(App. Table\n3 - ID F1)\nH6 Definitions with acronyms\nor words having acronyms\ndon\u2019t perform well\nGenerated answers often expand or pro-\nvide abbreviations which is not helpful\n15 of 16\nqueries\n(App. Table\n3 - ID F2,\nF3)\nH7 Order of retrieved para-\ngraphs in generator results\nOrder of retrieved paragraphs do not af-\nfect generator results in our experiments\nNA\nTable 1: Summary of observations - details of individual queries in Appendix B\nanswers for definitions based on acronyms (H6) and effect of order of retrieved results on generator\nperformance (H7). Of these, H2 is a result of our experiments with distributions of similarity scores\nreferred earlier and H7 is based on Chen et al. (2023a). Others are derived from our experiments to\nimprove results. For each hypotheses, we provide the number of experiments that support the claim\nand those that are valid for the same in the last column, along with sample queries.\nWe find that retrieval by thresholding on similarity scores is not helpful. For queries 1, 2 and\n5, when the query phrase is present in the term or definition, top retrieved score is higher. For\nquery 3, the correct result is retrieved at the second position using definition embedding, but in\nother cases, result is not retrieved and similarity scores are close. For queries 4 and 6, we are\nunable to retrieve the correct result, though scores indicate otherwise. Thus, thresholding retriever", "metadata": {"title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented\n  Generation", "pdf_url": "http://arxiv.org/pdf/2408.02545v1"}}, "c7057eaf-1c61-42c4-9fe9-d5fa970cb3b4": {"text": "other cases, result is not retrieved and similarity scores are close. For queries 4 and 6, we are\nunable to retrieve the correct result, though scores indicate otherwise. Thus, thresholding retriever\nresults based on similarity scores can potentially result in sub-optimal generator augmentation. We\nevaluate generator performance on our queries based on the retrieved results. This is done using the\ntop k retrieved (a) definitions, and (b) terms and definitions. Better context gives better generated\nresponses. For acronyms and their expansions, the generator does not add any additional value.\nFor retrieval on the full document, we explore similarity search by sentence and paragraph sepa-\nrately. In the former, we retrieve the paragraph to which the sentence belongs and take top-k distinct\nparagraphs from top similar sentences. We observe that the results by sentence-based similarity\nsearch and paragraphs being used for generator provides better retriever and generator performance.\nAuthors in Chen et al. (2023a) mention order of presented information to be important, but we did\nnot observe different results on permuting the retrieved paragraphs. We observe generator responses\nto sometimes fail due to incorrect retrieval, hallucinated facts or incorrect synthesis as highlighted\nin Chen et al. (2023a). We recommend such approaches for definition QA and long form QA.\n4 C ONCLUSIONS AND FUTURE WORK\nWe show that chunk length affects retriever embeddings, and generator augmentation by threshold-\ning retriever results on similarity scores can be unreliable. However, use of abbreviations and a large\nnumber of related paragraphs for a topic make our observations particularly relevant for long form\nQA on technical documents. As future work, we would like to use RAG metrics Es et al. (2023);\nChen et al. (2023b) to choose retrieval strategies. Also, methods and evaluation metrics to answer\nfollow-up questions would be of interest.\n2", "metadata": {"title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented\n  Generation", "pdf_url": "http://arxiv.org/pdf/2408.02545v1"}}, "9ee0d30a-bf93-43f3-b4fc-af12779e2e90": {"text": "Published as a Tiny Paper at ICLR 2024\nURM S TATEMENT\nThe authors acknowledge that at least one key author of this work meets the URM criteria of ICLR\n2024 Tiny Papers Track.\nREFERENCES\nIEEE 1881-2016. IEEE standard glossary of stationary battery terminology. IEEE Std 1881-2016,\npp. 1\u201342, 2016. doi: 10.1109/IEEESTD.2016.7552407.\nHung-Ting Chen, Fangyuan Xu, Shane A Arora, and Eunsol Choi. Understanding retrieval augmen-\ntation for long-form question answering. arXiv preprint arXiv:2310.12150, 2023a.\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language models in\nretrieval-augmented generation. arXiv preprint arXiv:2309.01431, 2023b.\nShahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. Ragas: Automated evaluation\nof retrieval augmented generation. arXiv preprint arXiv:2309.15217, 2023.\nIEEE. IEEE standard for information technology\u2013telecommunications and information exchange\nbetween systems - local and Metropolitan Area Networks\u2013specific requirements - part 11: Wire-\nless LAN medium access control (MAC) and physical layer (PHY) specifications. IEEE Std\n802.11-2020 (Revision of IEEE Std 802.11-2016) , pp. 1\u20134379, 2021. doi: 10.1109/IEEESTD.\n2021.9363693.\nNils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using siamese bert-\nnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP). Association for Computational Linguistics, 2019.\nSumit Soman and Ranjani HG. Observations on LLMs for telecom domain: Capabilities and limi-\ntations (To appear in the proceedings of The Third International Conference on Artificial Intelli-\ngence and Machine Learning Systems). arXiv preprint arXiv:2305.13102, 2023.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. MPNET: Masked and permuted pre-\ntraining for language understanding. Advances in Neural Information Processing Systems , 33:\n16857\u201316867, 2020.\nSabrina Toro, Anna V Anagnostopoulos, Sue Bello, Kai Blumberg, Rhiannon Cameron, Leigh Car-\nmody, Alexander D Diehl, Damion Dooley, William Duncan, Petra Fey, et al. Dynamic retrieval\naugmented generation of ontologies using artificial intelligence (DRAGON-AI). arXiv preprint\narXiv:2312.10904, 2023.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-", "metadata": {"title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented\n  Generation", "pdf_url": "http://arxiv.org/pdf/2408.02545v1"}}, "a66fd603-b792-4dd3-9d5b-022463b1ab9d": {"text": "arXiv:2312.10904, 2023.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nBenfeng Xu, Chunxu Zhao, Wenbin Jiang, Pengfei Zhu, Songtai Dai, Chao Pang, Zhuo Sun, Shuo-\nhuan Wang, and Yu Sun. Retrieval-augmented domain adaptation of language models. In Pro-\nceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023), pp. 54\u201364,\n2023.\n3", "metadata": {"title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented\n  Generation", "pdf_url": "http://arxiv.org/pdf/2408.02545v1"}}, "ed69152d-4f20-473c-ad9a-2edda31146dd": {"text": "Published as a Tiny Paper at ICLR 2024\nA A PPENDIX A\nThe prompts used for the LLM in our experiments are as follows:\n\u2022 System Prompt: Answer the questions based on the paragraphs provided here. DO NOT\nuse any other information except that in the paragraphs. Keep the answers as short as\npossible. JUST GIVE THE ANSWER. NO PREAMBLE REQUIRED.\n\u2022 User Prompt: \u201cPARAGRAPHS : \u201d+context + \u201cQUESTIONS: \u201d + query\nB A PPENDIX B\nFigure 1: The distribution of similarities across 10974 documents of various sizes split by number\nof words in the document\nC A PPENDIX C - S UPPLEMENTARY MATERIAL\nWe provide an anonymized Git repository which contains\n\u2022 Anonymized source code\n\u2022 Experiment v/s hypothesis tabulation (for consolidated quantitative results)\n\u2022 Details of the experiments across 42 queries and 7 hypothesis\nIn addition, we provide details with respect to hypotheses in Table 1 by providing sample queries\nand the retrieved and generated results.\n4", "metadata": {"title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented\n  Generation", "pdf_url": "http://arxiv.org/pdf/2408.02545v1"}}, "5b5b8fdb-267b-457f-ac19-65f81e0453f0": {"text": "Table 1 \u2013 Retrieval from Glossary ID Query Expected answer Use full sentence Use defined word Use definition Observations 1. Explain EIRP effective isotropic radiated power (EIRP): The equivalent power of a transmitted signal in terms of an isotropic (omnidirectional) radiator. The EIRP equals the product of the transmitter power and the antenna gain (reduced by any coupling losses between the transmitter and antenna). \nEffective isotropic radiated power (EIRP): The equivalent power of a transmitted signal in terms of an isotropic (omnidirectional) radiator. The EIRP equals the product of the transmitter power and the antenna gain (reduced by any coupling losses between the transmitter and antenna). (0.614) \nEffective isotropic radiated power (EIRP): The equivalent power of a transmitted signal in terms of an isotropic (omnidirectional) radiator. The EIRP equals the product of the transmitter power and the antenna gain (reduced by any coupling losses between the transmitter and antenna (0.478)  \nEffective isotropic radiated power (EIRP): The equivalent power of a transmitted signal in terms of an isotropic (omnidirectional) radiator. The EIRP equals the product of the transmitter power and the antenna gain (reduced by any coupling losses between the transmitter and antenna (0.669)  \nThe retriever obtains the correct definition by all 3. Also keyword at beginning of sentence is picked up with high similarity in full sentence mode. \nEmergency alert system (EAS): A U.S. national public warning system. (0.372) Emergency alert system (EAS): A U.S. national public warning system. (0.337) IEEE 802.1X authentication',    ' Extensible Authentication Protocol (EAP) authentication transported by the IEEE 802.1X protocol. (0.360)  Received channel power indicator (RCPI): An indication of the total channel power (signal, noise, and interference) of a received frame measured on the channel and at the antenna connector used to receive the frame. (0.362) \nExtended service area (ESA): The area within which members of an extended service set (ESS) can communicate. An ESA is larger than or equal to a basic service area (BSA) and might involve several basic service sets (BSSs) in overlapping, disjointed, or both configurations. (0.322) \nMaster session key (MSK): Keying material that is derived between the Extensible Authentication Protocol (EAP) peer and exported by the EAP method to the Authentication Server (AS) (0.291)", "metadata": {"title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented\n  Generation", "pdf_url": "http://arxiv.org/pdf/2408.02545v1"}}, "f9402661-1c9c-430f-8053-f55c282c2d9a": {"text": "Master session key (MSK): Keying material that is derived between the Extensible Authentication Protocol (EAP) peer and exported by the EAP method to the Authentication Server (AS) (0.291) \n2 Why do we need a beam steering matrix beamforming steering matrix: A matrix determined using knowledge of the channel between a transmitter and an intended receiver that maps from space-time streams to transmit antennas with the goal of improving the signal power or signal-to-noise ratio (SNR) at the intended receiver \nbeamformee: A station (STA) that receives a physical layer (PHY) protocol data unit (PPDU) that was transmitted using a beamforming steering matrix. (0.626) \nbeamforming: A spatial filtering mechanism used at a transmitter to improve the received signal power or signal-to-noise ratio (SNR) at an intended receiver. Syn  beam steering. (0.745)  \nbeamformer: A station (STA) that transmits a physical layer (PHY) protocol data unit (PPDU) using a beamforming steering matrix. (0.426) the top-1 does not return the correct answer using the defined word. The definition does not return it in top-3. The full definition returns it in 3rd position. However top sim in definition > correct answer in defined word. Similarly wrong 2 answers in the full definition have higher sim than correct answer in full sentence \nbeamforming: A spatial filtering mechanism used at a transmitter to improve the received signal power or signal-to-noise ratio (SNR) at an intended receiver. Syn  beam steering. (0.458) \nbeamforming steering matrix: A matrix determined using knowledge of the channel between a transmitter and an intended receiver that maps from space-time streams to transmit antennas with the goal of improving the signal power or signal-to-noise ratio (SNR) at the intended receiver. (0.443) \nbeamforming: A spatial filtering mechanism used at a transmitter to improve the received signal power or signal-to-noise ratio (SNR) at an intended receiver. Syn  beam steering. (0.367)", "metadata": {"title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented\n  Generation", "pdf_url": "http://arxiv.org/pdf/2408.02545v1"}}, "d73dcabb-a71d-4ea0-9bbf-b486e6ec2431": {"text": "beamforming steering matrix: A matrix determined using knowledge of the channel between a transmitter and an intended receiver that maps from space-time streams to transmit antennas with the goal of improving the signal power or signal-to-noise ratio (SNR) at the intended receiver. (0.398)  \ndirected frame: See individually addressed. (0.309) unknown_definition_18:NOTE These uses include calculation of transmit steering, calculation of recommended modulation and coding scheme (MCS), and calculation of calibration parameters. (0.359) \n3 Which framework supports higher precision traffic under a condition of limited resources \nmulti-level precedence and preemption (MLPP): A framework used with admission control for the treatment of traffic streams based on precedence, which supports the preemption of an active traffic stream by a higher precedence traffic stream when resources are limited. Preemption is the act of forcibly removing a traffic stream in progress in order to free up resources for another higher precedence traffic stream. \ntraffic category (TC): A label for medium access control (MAC) service data units (MSDUs) that have a distinct user priority (UP), as viewed by higher layer entities, relative to other MSDUs provided for delivery over the same link. Traffic categories are meaningful only to MAC entities that support quality of service (QoS) within the MAC data service. These MAC entities determine the UP for MSDUs belonging to a particular traffic category using the priority value provided with those MSDUs at the MAC service access point (MAC SAP).  (0.456) \ntraffic classification (TCLAS): The specification of one of several types of matching filter to classify protocol data units (PDUs) or medium access control (MAC) service data units (MSDUs) as belonging to a particular traffic stream (TS). Depending on the type of classification, the filter is applied within the MAC sublayer management entity (MLME), above the MAC, or within the MAC itself. (0.535) \nadmission control: An algorithm intended to prevent the violation of parameterized service commitments made by the network to admitted flows by controlling the admittance of a new flow into a resource constrained network. (0.405) \nOnly the definition can extract the correct answer but similarity for the correct answer via definition is lesser than even the 3rd result from the other methods \ntraffic specification (TSPEC): The quality-of-service (QoS) characteristics of a data flow to and from a QoS station (STA). (0.437)", "metadata": {"title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented\n  Generation", "pdf_url": "http://arxiv.org/pdf/2408.02545v1"}}, "cde23353-1d3f-431f-9b76-e3436103582b": {"text": "traffic specification (TSPEC): The quality-of-service (QoS) characteristics of a data flow to and from a QoS station (STA). (0.437) \ntraffic specification (TSPEC): The quality-of-service (QoS) characteristics of a data flow to and from a QoS station (STA). (0.489)  \nmulti-level precedence and preemption (MLPP): A framework used with admission control for the treatment of traffic streams based on precedence, which supports the preemption of an active traffic stream by a higher precedence traffic stream when resources are limited. Preemption is the act of forcibly removing a traffic stream in progress in order to free up resources for another higher precedence traffic stream.(0.398) traffic stream (TS): A set of medium access control (MAC) service data units (MSDUs) to be delivered subject to the quality-of-service (QoS) parameter values provided to the MAC in a particular traffic specification (TSPEC). TSs are meaningful only to MAC entities that support QoS within the MAC data service. These MAC entities determine the TSPEC applicable for delivery of MSDUs belonging to a particular TS using the priority parameter provided with those MSDUs at the MAC service access point (MAC SAP). (0.411) \ntraffic stream (TS): A set of medium access control (MAC) service data units (MSDUs) to be delivered subject to the quality-of-service (QoS) parameter values provided to the MAC in a particular traffic specification (TSPEC). TSs are meaningful only to MAC entities that support QoS within the MAC data service. These MAC entities determine the TSPEC applicable for delivery of MSDUs belonging to a particular TS using the priority parameter provided with those MSDUs at the MAC service access point (MAC SAP). (0.461) \ntraffic specification (TSPEC): The quality-of-service (QoS) characteristics of a data flow to and from a QoS station (STA). (0.396) \n4. How is Ethertype protocol discrimination \nmedium access control (MAC) service tuple: The collection of a MAC service data unit (MSDU) along with the associated source address, \nunknown_definition_2: NOTE See IETF RFC 3610. (0.434) peer-to-peer traffic specification (PTP TSPEC): The quality-of-service (QoS) characteristics of a data flow between non-access point (non-AP) QoS stations (STAs). (0.413) \nframe: A unit of data exchanged between peer protocol entities. (0.418) Unable to identify this despite it being available as a keyword in the actual definition", "metadata": {"title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented\n  Generation", "pdf_url": "http://arxiv.org/pdf/2408.02545v1"}}, "8a214f91-7f7c-4839-91e6-3640010a4849": {"text": "(EPD) identified? destination addresses, priority, drop eligibility, service class, optional set of service_access_point_identifiers, and optional indication of whether the supplied MSDU is in Ethertype protocol discrimination (EPD) or logical link control (LLC) protocol discrimination (LPD) format, which are all passed as parameters across the MAC service access point (SAP) and are all except the service_access_point_identifiers delivered across the distribution system between access points (APs), mesh gates, and the portal of an extended service set (ESS). \nframe: A unit of data exchanged between peer protocol entities. (0.432) protocol instance: An execution of a particular protocol that consists of the state of the communicating parties as well as the messages exchanged. (0.380) \nunknown_definition_9: NOTE See IETF RFC 4282. (0.407) \nunknown_definition_9: NOTE See IETF RFC 4282. (0.404) traffic classification (TCLAS): The specification of one of several types of matching filter to classify protocol data units (PDUs) or medium access control (MAC) service data units (MSDUs) as belonging to a particular traffic stream (TS). Depending on the type of classification, the filter is applied within the MAC sublayer management entity (MLME), above the MAC, or within the MAC itself. (0.370) \ntraffic classification (TCLAS): The specification of one of several types of matching filter to classify protocol data units (PDUs) or medium access control (MAC) service data units (MSDUs) as belonging to a particular traffic stream (TS). Depending on the type of classification, the filter is applied within the MAC sublayer management entity (MLME), above the MAC, or within the MAC itself. (0.383) \n5 What does GAS stand for? registered location query protocol (RLQP): The query protocol for registered location information that is received and transported by generic advertisement service (GAS) Public Action frames. \nunknown_definition_1:NOTE See IETF RFC 2903 [B35]. (0.384) unknown_definition_8: NOTE IEEE Std 802.11 supports only downlink (DL) MU-MIMO. See downlink multi-user multiple input, multiple output (DL-MU-MIMO) (in 3.2). (0.343) \ndistribution system medium (DSM): The medium or set of media used by a distribution system (DS) for communications between access points (APs), mesh gates, and the portal of an extended service set (ESS). (0.357) \nUnable to identify this despite it being available as a keyword in the actual definition", "metadata": {"title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented\n  Generation", "pdf_url": "http://arxiv.org/pdf/2408.02545v1"}}, "f9a38bce-fe4e-4d69-8cd9-872ed2090615": {"text": "Unable to identify this despite it being available as a keyword in the actual definition \nunknown_definition_2:NOTE See IETF RFC 3610. (0.376) unknown_definition_13: NOTE For the purposes of this Standard, there is at most one portal in a given extended service set\u0000s (ESS\u0000s) infrastructure. In an implementation, a single logical portal function may be provided by multiple devices that provide integration services for the ESS. How such multiple devices coordinate to appear as a single logical portal is implementation dependent. (0.337) \nservice hash: A value used for representing a service. This value is formed from a hash of the service name. (0.322) \nunknown_definition_9:NOTE See IETF RFC 4282. (0.370) unknown_definition_18: NOTE These uses include calculation of transmit steering, calculation of recommended modulation and coding scheme (MCS), and calculation of calibration parameters. (0.334) \nsubscription service provider (SSP): An organization (operator) offering connection to network services, perhaps for a fee. (0.304) \n6 What is derived between EAP peer master session key (MSK): Keying material that is derived between the Extensible Authentication Protocol (EAP) peer and exported by the EAP method to the Authentication Server (AS). \nIEEE 802.1X authentication: Extensible Authentication Protocol (EAP) authentication transported by the IEEE 802.1X protocol. (0.491) \npeer mesh station (STA): A mesh STA to which a mesh peering has been established. (0.460) frame: A unit of data exchanged between peer protocol entities. (0.625) The keyword is relatively at the beginning of the sentence and is therefore picked up wherever available (it is not available in the defined word) \nmaster session key (MSK): Keying material that is derived between the Extensible Authentication Protocol (EAP) peer and exported by the EAP method to the Authentication Server (AS). (0.434) \npeer-to-peer link: A direct link within a quality-of-service (QoS) basic service set (BSS), a tunneled direct- link setup (TDLS) link, or a station-to-station (STA-to-STA) communication in an independent basic service set (IBSS). (0.401) \nIEEE 802.1X authentication: Extensible Authentication Protocol (EAP) authentication transported by the IEEE 802.1X protocol. (0.476) \npeer-to-peer traffic specification (PTP TSPEC): The quality-of-service (QoS) characteristics of a data flow between non-access point (non-AP) QoS stations (STAs). (0.410)", "metadata": {"title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented\n  Generation", "pdf_url": "http://arxiv.org/pdf/2408.02545v1"}}, "9190f5a0-9d19-45ef-934a-d02a4d21b9e7": {"text": "peer-to-peer traffic specification (PTP TSPEC): The quality-of-service (QoS) characteristics of a data flow between non-access point (non-AP) QoS stations (STAs). (0.410) \npeer-to-peer traffic specification (PTP TSPEC): The quality-of-service (QoS) characteristics of a data flow between non-access point (non-AP) QoS stations (STAs). (0.373) \nmaster session key (MSK): Keying material that is derived between the Extensible Authentication Protocol (EAP) peer and exported by the EAP method to the Authentication Server (AS). (0.451)", "metadata": {"title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented\n  Generation", "pdf_url": "http://arxiv.org/pdf/2408.02545v1"}}, "4169a02e-e588-499d-ab86-5436e55f7cc4": {"text": "Table 2 \u2013 Retrieval from full document  Query ID Query Expected answer Search for paragraph Search by sentence retrieve paragraph Observations F1 What do the values of RAW Group Indication subfield in RPS element indicate? \nThe RAW Group Indication subfield indicates whether the RAW Group subfield is present in the RAW Assignment subfield and is interpreted as follows: \u2014 When the RAW type is generic RAW, sounding RAW, or triggering frame RAW, the RAW Group Indication subfield indicates whether the RAW group defined in the current RAW assignment is the same RAW group as defined in the previous RAW assignment. When the RAW Group Indication subfield is equal to 0, the RAW group defined in the current RAW assignment is the same as the RAW group defined in the previous RAW assignment and the RAW Group subfield is not present in this RAW assignment. When the RAW Group Indication subfield is equal to 1, the RAW Group subfield is present in this RAW assignment. The RAW Group Indication subfield in the first RAW assignment is set to 0 to indicate the RAW group in the first RAW assignment is the same as the range of AIDs in all the TIM bitmaps in the S1G Beacon frame. \u2014 When the RAW is a non-TIM RAW, the RAW Group Indication subfield is set to 0 and the RAW Group subfield is not present. \u2014 When the RAW is an AP PM RAW, the RAW Group Indication subfield equal to 0 indicates that the RAW group does not include any of the non-AP STAs, and the RAW Group subfield is not present. When the RAW Group Indication subfield is equal to 1, the RAW Group subfield is present. \nThe format of the RAW Group subfield is shown in Figure 9-672 When the RAW is an AP PM RAW, the RAW Group Indication subfield equal to 0 indicates that the RAW group does not include any of the non-AP STAs, and the RAW Group subfield is not present. When the RAW Group Indication subfield is equal to 1, the RAW Group subfield is present. \nIt is clear that similarity search at sentence level and retrieval at paragraph level gives significantly better results. Since we retrieve 3 distinct paragraphs there is far more context available at the generator create good responses When the RAW is an AP PM RAW, the RAW Group Indication subfield equal to 0 indicates that the RAW group does not include any of the non-AP STAs, and the RAW Group subfield is not present. When the RAW Group Indication subfield is equal to 1, the RAW Group subfield is present.", "metadata": {"title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented\n  Generation", "pdf_url": "http://arxiv.org/pdf/2408.02545v1"}}, "3b675f2a-3361-4aea-98c5-0e36a2714979": {"text": "The RAW Group Indication subfield indicates whether the RAW Group subfield is present in the RAW Assignment subfield and is interpreted as follows: \nThe RAW Group Indication subfield indicates whether the RAW Group subfield is present in the RAW Assignment subfield and is interpreted as follows: \nWhen the RAW type is generic RAW, sounding RAW, or triggering frame RAW, the RAW Group Indication subfield indicates whether the RAW group defined in the current RAW assignment is the same RAW group as defined in the previous RAW assignment. When the RAW Group Indication subfield is equal to 0, the RAW group defined in the current RAW assignment is the same as the RAW group defined in the previous RAW assignment and the RAW Group subfield is not present in this RAW assignment. When the RAW Group Indication subfield is equal to 1, the RAW Group subfield is present in this RAW assignment. The RAW Group Indication subfield in the first RAW assignment is set to 0 to indicate the RAW group in the first RAW assignment is the same as the range of AIDs in all the TIM bitmaps in the S1G Beacon frame.", "metadata": {"title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented\n  Generation", "pdf_url": "http://arxiv.org/pdf/2408.02545v1"}}, "c028c706-7dfd-420d-88d9-d2f8afe96099": {"text": "Table 3 \u2013 Generator Performance  Query ID Query Expected answer Generated response \u2013 similarity by paragraph Generated Response \u2013 similarity by sentence, retrieve paragraph Observations F1 What do the values of RAW Group Indication subfield in RPS element indicate? \nThe RAW Group Indication subfield indicates whether the RAW Group subfield is present in the RAW Assignment subfield and is interpreted as follows: \u2014 When the RAW type is generic RAW, sounding RAW, or triggering frame RAW, the RAW Group Indication subfield indicates whether the RAW group defined in the current RAW assignment is the same RAW group as defined in the previous RAW assignment. When the RAW Group Indication subfield is equal to 0, the RAW group defined in the current RAW assignment is the same as the RAW group defined in the previous RAW assignment and the RAW Group subfield is not present in this RAW assignment. When the RAW Group Indication subfield is equal to 1, the RAW Group subfield is present in this RAW assignment. The RAW Group Indication subfield in the first RAW assignment is set to 0 to indicate the RAW group in the first RAW assignment is the same as the range of AIDs in all the TIM bitmaps in the S1G Beacon frame. \u2014 When the RAW is a non-TIM RAW, the RAW Group Indication subfield is set to 0 and the RAW Group subfield is not present. \u2014 When the RAW is an AP PM RAW, the RAW Group Indication subfield equal to 0 indicates that the RAW group does not include any of the non-AP STAs, and the RAW Group subfield is not present. When the RAW Group Indication subfield is equal to 1, the RAW Group subfield is present. \n'According to the given paragraphs, the values of the RAW Group Indication subfield in the RPS (Radio Parameter Set) element indicate whether the RAW Group subfield is present or not in the RAW Assignment subfield. \nAccording to the given paragraphs, the values of the RAW Group Indication subfield in the RPS (Radio Path Selection) element indicate whether the RAW Group subfield is present in the RAW Assignment subfield. If the value of RAW Group Indication subfield is equal to 0, it means that the RAW group defined in the current RAW assignment is the same as the RAW group defined in the previous RAW assignment, and the RAW Group subfield is not present in this RAW assignment. If the value of RAW Group Indication subfield is equal to 1, it means that the RAW Group subfield is present in this RAW assignment", "metadata": {"title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented\n  Generation", "pdf_url": "http://arxiv.org/pdf/2408.02545v1"}}, "c715a271-d56a-4012-aa70-28269b24344a": {"text": "A much broader context in general being available for the LLM because of multiple retrieved paragraphs helps it in create good long form question answering. \n  \nF2 What is IGTK integrity group temporal key (IGTK): A random value, assigned by the broadcast/multicast source station (STA), which is used to protect group addressed medium access control (MAC) management protocol data units (MMPDUs) from that source STA. \nSure! Based on the given paragraphs, the answer to your question is:\\n\\nIGTK stands for \"integrity group temporal key\" According to the given paragraphs, the answer to question 1 is: IGTK stands for \"integrity group temporal key\" Very often the generated response is a trivial answer giving the full form of an abbreviated technical term but not giving it\u2019s definition \nF3 What is idle power indicator idle power indicator (IPI): A physical layer (PHY) indication of the total channel power (noise and interference) as measured in the channel at the receiving antenna connector while the station (STA) is idle, i.e., neither transmitting nor receiving a frame. \nIPI - idle power indicator IPI - idle power indicator Very often a term is just abbreviated in a response. This can sometimes be reduced if a user a question like \u201cDefine Idle Power Indicator\u201d but since this is left to the user can create issues", "metadata": {"title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented\n  Generation", "pdf_url": "http://arxiv.org/pdf/2408.02545v1"}}}