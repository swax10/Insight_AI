{"d76189f2-3c59-488a-bf2a-d8f2105cd65b": [{"entity1": "Retrieval-augmented Generation (RAG)", "entity2": "Large Language Models (LLMs)", "relation": "Enhancement", "description": "RAG enhances the capabilities of LLMs in tackling knowledge-intensive tasks."}, {"entity1": "Modular RAG", "entity2": "LEGO-like Reconfigurable Frameworks", "relation": "Transformation", "description": "Modular RAG transforms RAG systems into LEGO-like reconfigurable frameworks."}, {"entity1": "Yunfan Gao", "entity2": "Modular RAG", "relation": "Author", "description": "Yunfan Gao is an author of the paper introducing Modular RAG."}, {"entity1": "Yun Xiong", "entity2": "Modular RAG", "relation": "Author", "description": "Yun Xiong is an author of the paper introducing Modular RAG."}, {"entity1": "Meng Wang", "entity2": "Modular RAG", "relation": "Author", "description": "Meng Wang is an author of the paper introducing Modular RAG."}, {"entity1": "Haofen Wang", "entity2": "Modular RAG", "relation": "Author", "description": "Haofen Wang is an author of the paper introducing Modular RAG."}, {"entity1": "RAG", "entity2": "Knowledge question answering", "relation": "Application", "description": "RAG is applied in knowledge question answering."}, {"entity1": "RAG", "entity2": "Recommendation systems", "relation": "Application", "description": "RAG is applied in recommendation systems."}, {"entity1": "RAG", "entity2": "Customer service", "relation": "Application", "description": "RAG is applied in customer service."}, {"entity1": "RAG", "entity2": "Personal assistants", "relation": "Application", "description": "RAG is applied in personal assistants."}, {"entity1": "Modular RAG", "entity2": "Routing", "relation": "Integration", "description": "Modular RAG integrates routing mechanisms."}, {"entity1": "Modular RAG", "entity2": "Scheduling", "relation": "Integration", "description": "Modular RAG integrates scheduling mechanisms."}, {"entity1": "Modular RAG", "entity2": "Fusion mechanisms", "relation": "Integration", "description": "Modular RAG integrates fusion mechanisms."}, {"entity1": "RAG", "entity2": "Information retrieval", "relation": "Enhancement", "description": "RAG enhances information retrieval capabilities."}, {"entity1": "Modular RAG", "entity2": "Linear architecture", "relation": "Transcendence", "description": "Modular RAG transcends traditional linear architecture."}, {"entity1": "RAG", "entity2": "Hallucination", "relation": "Challenge", "description": "RAG faces the challenge of hallucination."}, {"entity1": "RAG", "entity2": "Information updates", "relation": "Challenge", "description": "RAG faces the challenge of lag in information updates."}], "ed42e3cd-32ae-4bcc-ad2f-cc77d239fad7": [{"entity1": "Yunfan Gao", "entity2": "Shanghai Research Institute for Intelligent Autonomous Systems", "relation": "Affiliation", "description": "Yunfan Gao is affiliated with Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University."}, {"entity1": "Yun Xiong", "entity2": "Shanghai Key Laboratory of Data Science", "relation": "Affiliation", "description": "Yun Xiong is affiliated with Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University."}, {"entity1": "Meng Wang", "entity2": "College of Design and Innovation", "relation": "Affiliation", "description": "Meng Wang is affiliated with College of Design and Innovation, Tongji University."}, {"entity1": "Haofen Wang", "entity2": "College of Design and Innovation", "relation": "Affiliation", "description": "Haofen Wang is affiliated with College of Design and Innovation, Tongji University."}, {"entity1": "Naive RAG", "entity2": "RAG", "relation": "Theoretical Framework", "description": "Naive RAG is a core framework of RAG, constituted by indexing, retrieval, and generation."}, {"entity1": "Advanced RAG", "entity2": "RAG", "relation": "Theoretical Framework", "description": "Advanced RAG is an improved paradigm of RAG, focusing on optimizing the retrieval phase."}, {"entity1": "RAG", "entity2": "personal assistants", "relation": "Application", "description": "RAG technology is applied in personal assistants."}, {"entity1": "query rewriting", "entity2": "Advanced RAG", "relation": "Methodology", "description": "Query rewriting is a strategy used in Advanced RAG to optimize the retrieval phase."}, {"entity1": "reranking of retrieval results", "entity2": "Advanced RAG", "relation": "Methodology", "description": "Reranking of retrieval results is a strategy used in Advanced RAG to enhance the LLM's ability to identify and utilize key information."}, {"entity1": "LLMs", "entity2": "RAG", "relation": "Tool/Resource", "description": "LLMs are used in RAG to generate responses."}, {"entity1": "Figure 1", "entity2": "Naive RAG", "relation": "Result", "description": "Figure 1 depicts the limitations of Naive RAG."}, {"entity1": "Shanghai Research Institute for Intelligent Autonomous Systems", "entity2": "Tongji University", "relation": "Institution-Collaboration", "description": "Shanghai Research Institute for Intelligent Autonomous Systems is affiliated with Tongji University."}, {"entity1": "Shanghai Key Laboratory of Data Science", "entity2": "Fudan University", "relation": "Institution-Collaboration", "description": "Shanghai Key Laboratory of Data Science is affiliated with Fudan University."}, {"entity1": "College of Design and Innovation", "entity2": "Tongji University", "relation": "Institution-Collaboration", "description": "College of Design and Innovation is affiliated with Tongji University."}], "1d64a99e-70f1-4b5e-88ca-c2fa3633a9de": [{"entity1": "RAG technology", "entity2": "user expectations", "relation": "Influence", "description": "Advances in RAG technology lead to rising user expectations."}, {"entity1": "application demands", "entity2": "RAG technology", "relation": "Evolution Driver", "description": "Growth in application demands propels the evolution of RAG technology."}, {"entity1": "RAG systems", "entity2": "knowledge graphs", "relation": "Integration", "description": "Modern RAG systems integrate knowledge graphs for more refined index bases."}, {"entity1": "RAG systems", "entity2": "query construction methods", "relation": "Integration", "description": "RAG systems integrate query construction methods for structured data."}, {"entity1": "encoders", "entity2": "fine-tuning techniques", "relation": "Enhancement", "description": "Fine-tuning techniques enable encoders to better adapt to domain-specific documents."}, {"entity1": "RAG systems", "entity2": "linear retrieval-generation paradigm", "relation": "Supersession", "description": "RAG systems have surpassed the traditional linear retrieval-generation paradigm."}, {"entity1": "iterative retrieval", "entity2": "recursive retrieval", "relation": "Methodological Comparison", "description": "Iterative and recursive retrieval are used for richer context and complex queries, respectively."}, {"entity1": "adaptive retrieval", "entity2": "autonomy", "relation": "Enabler", "description": "Adaptive retrieval provides overall autonomy and flexibility in the process."}, {"entity1": "arXiv:2407.21059v1", "entity2": "cs.CL", "relation": "Classification", "description": "The publication arXiv:2407.21059v1 is classified under cs.CL."}, {"entity1": "arXiv:2407.21059v1", "entity2": "July 26, 2024", "relation": "Publication Date", "description": "The publication date of arXiv:2407.21059v1 is July 26, 2024."}], "a7187dbe-94ef-4942-b211-2d41101e8d60": [{"entity1": "Naive RAG", "entity2": "Advanced RAG", "relation": "Comparison", "description": "Both Naive RAG and Advanced RAG encounter limitations when faced with complex questions."}, {"entity1": "Advanced RAG", "entity2": "hierarchical indexing", "relation": "Methodology", "description": "Advanced RAG improves retrieval accuracy through hierarchical indexing."}, {"entity1": "RAG systems", "entity2": "semi-structured data", "relation": "Application", "description": "RAG systems have expanded to include various data types, such as semi-structured data like tables."}, {"entity1": "RAG systems", "entity2": "structured data", "relation": "Application", "description": "RAG systems have expanded to include various data types, such as structured data like knowledge graphs."}, {"entity1": "RAG systems", "entity2": "unstructured text data", "relation": "Application", "description": "RAG systems are no longer confined to a single type of unstructured text data source."}, {"entity1": "Modular RAG", "entity2": "system interpretability", "relation": "Objective", "description": "The system integrates diverse data and more functional components to improve system interpretability."}, {"entity1": "Modular RAG", "entity2": "controllability", "relation": "Objective", "description": "The system integrates diverse data and more functional components to improve controllability."}, {"entity1": "RAG systems", "entity2": "knowledge graphs", "relation": "Tool/Resource", "description": "RAG systems can access heterogeneous data from multiple sources, including knowledge graphs."}, {"entity1": "RAG systems", "entity2": "tables", "relation": "Tool/Resource", "description": "RAG systems can access heterogeneous data from multiple sources, including tables."}, {"entity1": "Advanced RAG", "entity2": "pre-retrieval", "relation": "Methodology", "description": "Advanced RAG improves retrieval accuracy through pre-retrieval processes."}, {"entity1": "Advanced RAG", "entity2": "post-retrieval", "relation": "Methodology", "description": "Advanced RAG improves retrieval accuracy through post-retrieval processes."}, {"entity1": "RAG systems", "entity2": "Fig. 1", "relation": "Publication Venue", "description": "Fig. 1 illustrates cases of Naive RAG and Advanced RAG."}, {"entity1": "RAG systems", "entity2": "Fig. 2", "relation": "Publication Venue", "description": "Fig. 2 illustrates the case of current Modular RAG."}], "887246af-a21a-48cd-a1cf-2fd07e624dae": [{"entity1": "Modular RAG architecture", "entity2": "scalability", "relation": "Improvement", "description": "Modular RAG architecture enhances the system's scalability."}, {"entity1": "Modular RAG architecture", "entity2": "maintainability", "relation": "Improvement", "description": "Modular RAG architecture enhances the system's maintainability."}, {"entity1": "component selection", "entity2": "optimization", "relation": "Requirement", "description": "Component selection requires optimization to meet specific task needs and resource configurations."}, {"entity1": "workflow orchestration", "entity2": "scheduling", "relation": "Dependency", "description": "Workflow orchestration depends on scheduling for efficient system execution."}, {"entity1": "Modular RAG", "entity2": "Advanced RAG paradigm", "relation": "Inheritance", "description": "Modular RAG inherits processes from the Advanced RAG paradigm."}, {"entity1": "Modular RAG", "entity2": "Naive RAG", "relation": "Evolution", "description": "Modular RAG evolves based on the previous development of RAG, including Naive RAG."}, {"entity1": "RAG system", "entity2": "neural networks", "relation": "Composition", "description": "The RAG system involves multiple neural networks."}, {"entity1": "Modular RAG", "entity2": "computational graphs", "relation": "Representation", "description": "RAG systems in Modular RAG can be represented as computational graphs."}, {"entity1": "nodes", "entity2": "operators", "relation": "Representation", "description": "Nodes in computational graphs represent specific operators."}, {"entity1": "LLM", "entity2": "workflow orchestration", "relation": "Judgment", "description": "The LLM judges components based on different outputs during workflow orchestration."}, {"entity1": "Modular RAG architecture", "entity2": "modularization", "relation": "Application", "description": "Modular RAG architecture applies modularization to enhance system efficiency."}], "845625db-e277-4071-b466-e1c1f0e46e34": [{"entity1": "Modular RAG", "entity2": "RAG", "relation": "Inheritance", "description": "Modular RAG evolves based on the previous development of RAG."}, {"entity1": "Advanced RAG", "entity2": "Modular RAG", "relation": "Special Case", "description": "Advanced RAG is a special case of Modular RAG."}, {"entity1": "Naive RAG", "entity2": "Advanced RAG", "relation": "Special Case", "description": "Naive RAG is a special case of Advanced RAG."}, {"entity1": "Modular RAG", "entity2": "RAG Flow", "relation": "Composition", "description": "The orchestration of modules and operators in Modular RAG forms the RAG Flow."}, {"entity1": "RAG Flow", "entity2": "RAG methods", "relation": "Expression", "description": "RAG Flow can flexibly express current RAG methods."}, {"entity1": "Modular RAG framework", "entity2": "New methods", "relation": "Adaptation", "description": "The Modular RAG framework offers exceptional flexibility and extensibility for adapting new methods."}, {"entity1": "Retro", "entity2": "Pre-trained language models", "relation": "Optimization", "description": "Retro optimized pre-trained autoregressive models through retrieval augmentation."}, {"entity1": "Atlas", "entity2": "Pre-trained language models", "relation": "Adaptation", "description": "Atlas utilized a retrieval-augmented few-shot fine-tuning method to adapt language models to diverse tasks."}, {"entity1": "IRCOT", "entity2": "Chain-of-thought", "relation": "Combination", "description": "IRCOT combined chain-of-thought and multi-step retrieval processes."}, {"entity1": "LLMs", "entity2": "Retrieval-augmented techniques", "relation": "Supplementation", "description": "Retrieval-augmented techniques serve as a means of supplementing additional knowledge to LLMs."}], "2f4f59a7-e12e-498c-83a0-b3899d47e2aa": [{"entity1": "Retrieval-Augmented Language Models (RALMs)", "entity2": "retrievers", "relation": "Component", "description": "Retrievers are a key component of RALMs."}, {"entity1": "Retrieval-Augmented Language Models (RALMs)", "entity2": "language models", "relation": "Component", "description": "Language models are a key component of RALMs."}, {"entity1": "Retrieval-Augmented Language Models (RALMs)", "entity2": "augmentations", "relation": "Component", "description": "Augmentations are a key component of RALMs."}, {"entity1": "Huang et al.", "entity2": "RAG methods", "relation": "Categorization", "description": "Huang et al. categorize RAG methods into four main classes: pre-retrieval, retrieval, post-retrieval, generation."}, {"entity1": "Gao et al.", "entity2": "RAG framework", "relation": "Development", "description": "Gao et al. subdivide RAG into enhancement during pre-training, inference, and fine-tuning stages."}, {"entity1": "Hu et al.", "entity2": "Retrieval-Augmented Language Models (RALMs)", "relation": "Analysis", "description": "Hu et al. discuss RALMs from three key components and their interactions."}, {"entity1": "LLMs", "entity2": "RAG technology", "relation": "Acceleration", "description": "The development of RAG technology has been accelerated by LLM technology."}, {"entity1": "RAG technology", "entity2": "graph neural networks", "relation": "Integration", "description": "RAG technology has been integrated with graph neural networks."}, {"entity1": "RAG technology", "entity2": "fine-tuning techniques", "relation": "Integration", "description": "RAG technology has been integrated with fine-tuning techniques."}, {"entity1": "RRR", "entity2": "rewriting phase", "relation": "Improvement", "description": "RRR improved the rewriting phase."}, {"entity1": "LLMlingua", "entity2": "retrieved document chunks", "relation": "Optimization", "description": "LLMlingua removed redundant tokens in retrieved document chunks."}], "0cc9b436-f2a3-4811-9da9-64442942e5e3": [{"entity1": "Ding et al.", "entity2": "RALMs", "relation": "Author-Expertise", "description": "Ding et al. provide a comprehensive review of RALMs from the perspectives of architecture, training strategies, and applications."}, {"entity1": "Zhao et al.", "entity2": "RAG technology", "relation": "Author-Expertise", "description": "Zhao et al. analyze the applications of RAG technology in various fields such as text generation, code generation, image generation, and video generation."}, {"entity1": "Modular RAG", "entity2": "previous paradigms", "relation": "Evolution", "description": "Modular RAG has evolved from previous paradigms and aligns with the current practical needs of RAG systems."}, {"entity1": "RAG systems", "entity2": "evaluation methods", "relation": "Methodology", "description": "They emphasize the importance of considering robustness, accuracy, and relevance when evaluating RALMs and propose several evaluation methods."}, {"entity1": "RALMs", "entity2": "training methods", "relation": "Methodology", "description": "Ding et al. specifically discuss four training methods of RALMs: training-free methods, independent training methods, sequence training methods, and joint training methods."}, {"entity1": "RAG technology", "entity2": "augmented intelligence", "relation": "Application", "description": "Zhao et al. analyze the applications of RAG technology in various fields from the perspective of augmented intelligence with generative capabilities."}, {"entity1": "RAG systems", "entity2": "process scheduling", "relation": "Limitation", "description": "The current collation of RAG systems primarily focuses on methods with a fixed process, and has not turned its attention to the new characteristics of process scheduling."}, {"entity1": "RAG systems", "entity2": "functional componentization", "relation": "Limitation", "description": "The current collation of RAG systems primarily focuses on methods with a fixed process, and has not turned its attention to the new characteristics of functional componentization."}, {"entity1": "Q", "entity2": "D", "relation": "Input-Output", "description": "For query Q, a typical RAG system mainly consists of three key components: Indexing, Retrieval, and Generation."}, {"entity1": "fe(\u00b7)", "entity2": "di", "relation": "Function-Input", "description": "Indexing is the process of converting di into vectors through an embedding model fe(\u00b7)."}, {"entity1": "R", "entity2": "q", "relation": "Function-Input", "description": "Retriever, find similar chunks from D based on q."}, {"entity1": "F", "entity2": "RAG Flow", "relation": "Component-System", "description": "F is a component of the RAG Flow pattern."}, {"entity1": "M", "entity2": "Modular RAG", "relation": "Component-System", "description": "M is a module in modular RAG."}], "00d00dce-8a37-4978-a172-0d5e1d4ad782": [{"entity1": "encoding model", "entity2": "vector similarity", "relation": "Tool/Resource", "description": "The encoding model is used to calculate vector similarity."}, {"entity1": "Sim(q, di)", "entity2": "Dq", "relation": "Result", "description": "Sim(q, di) is used to determine Dq, the relevant documents for question q."}, {"entity1": "Dq", "entity2": "LLM", "relation": "Input", "description": "Dq is inputted to the LLM for generation of the final answer."}, {"entity1": "q", "entity2": "LLM", "relation": "Input", "description": "The query q is inputted to the LLM for generation of the final answer."}, {"entity1": "dot product", "entity2": "cosine similarity", "relation": "Comparison", "description": "Dot product and cosine similarity are compared as possible similarity functions."}, {"entity1": "Sim(q, di)", "entity2": "dot product", "relation": "Methodology", "description": "Sim(q, di) can be calculated using the dot product."}, {"entity1": "Sim(q, di)", "entity2": "cosine similarity", "relation": "Methodology", "description": "Sim(q, di) can be calculated using cosine similarity."}, {"entity1": "LLM", "entity2": "y", "relation": "Result", "description": "The LLM generates the final answer y."}, {"entity1": "D", "entity2": "di", "relation": "Affiliation", "description": "di is a part of D, the set of documents."}, {"entity1": "R", "entity2": "top k document chunks", "relation": "Objective", "description": "R is used to filter out the top k document chunks."}, {"entity1": "Generation", "entity2": "LLM", "relation": "Methodology", "description": "Generation is the process of using the LLM to produce the final answer."}], "679e5264-bfbe-4df2-ab24-ddb3059f3b78": [{"entity1": "Modular RAG paradigm", "entity2": "L1 Module", "relation": "Composition", "description": "The Modular RAG paradigm includes the L1 Module as one of its levels."}, {"entity1": "L1 Module", "entity2": "Ms", "relation": "Composition", "description": "The L1 Module is composed of Ms (sub-modules)."}, {"entity1": "Ms", "entity2": "Op", "relation": "Composition", "description": "The sub-module Ms is composed of Op (operators)."}, {"entity1": "Op", "entity2": "f\u03b8i", "relation": "Implementation", "description": "The operator Op is implemented as f\u03b8i."}, {"entity1": "Modular RAG system", "entity2": "G", "relation": "Representation", "description": "The Modular RAG system can be represented as G = {q, D, M, {Ms}, {Op}}."}, {"entity1": "RAG Flow", "entity2": "M\u03d51", "relation": "Composition", "description": "The RAG Flow F is composed of M\u03d51 (and other module parameters)."}, {"entity1": "NaiveRAG", "entity2": "q", "relation": "Input", "description": "NaiveRAG takes q as input."}, {"entity1": "NaiveRAG", "entity2": "R(q,D)", "relation": "Process", "description": "NaiveRAG processes q and D to produce R(q,D)."}, {"entity1": "R(q,D)", "entity2": "Text\u2212Embedding", "relation": "Transformation", "description": "R(q,D) is transformed into Text\u2212Embedding."}, {"entity1": "Text\u2212Embedding", "entity2": "Dq LLM([q,Dq])", "relation": "Transformation", "description": "Text\u2212Embedding is transformed into Dq LLM([q,Dq])."}, {"entity1": "Dq LLM([q,Dq])", "entity2": "OpenAI/GPT \u22124", "relation": "Implementation", "description": "Dq LLM([q,Dq]) is implemented using OpenAI/GPT \u22124."}, {"entity1": "OpenAI/GPT \u22124", "entity2": "y", "relation": "Output", "description": "OpenAI/GPT \u22124 produces output y."}, {"entity1": "Indexing", "entity2": "Chunk Optimization", "relation": "Challenge", "description": "Indexing faces the challenge of Chunk Optimization."}, {"entity1": "Chunk Optimization", "entity2": "Li", "relation": "Measurement", "description": "Chunk Optimization is measured by Li (chunk size)."}, {"entity1": "Chunk Optimization", "entity2": "Lo", "relation": "Measurement", "description": "Chunk Optimization is measured by Lo (overlap between chunks)."}, {"entity1": "Sliding Window", "entity2": "Chunk Optimization", "relation": "Technique", "description": "Sliding Window is a technique used for Chunk Optimization."}], "aeda5635-9af2-4548-b4c8-8eaa19986a34": [{"entity1": "Sliding Window", "entity2": "chunks", "relation": "Methodology", "description": "Sliding Window uses overlapping chunks to enhance semantic transitions."}, {"entity1": "Metadata Attachment", "entity2": "chunks", "relation": "Enrichment", "description": "Chunks can be enriched with metadata like page number, file name, author, timestamp, summary, or relevant questions."}, {"entity1": "Small-to-Big", "entity2": "chunks", "relation": "Methodology", "description": "Small-to-Big separates chunks used for retrieval from those used for synthesis."}, {"entity1": "Structure Organization", "entity2": "documents", "relation": "Enhancement", "description": "Establishing a hierarchical structure for documents enhances information retrieval."}, {"entity1": "Hierarchical Index", "entity2": "documents", "relation": "Organization", "description": "Hierarchical Index organizes documents in a hierarchical structure with nodes arranged in parent-child relationships."}, {"entity1": "KG Index", "entity2": "Knowledge Graphs (KGs)", "relation": "Application", "description": "KG Index uses Knowledge Graphs (KGs) to structure documents and maintain consistency."}, {"entity1": "Knowledge Graphs (KGs)", "entity2": "language models", "relation": "Improvement", "description": "Knowledge Graphs (KGs) improve retrieval accuracy and enable contextually coherent responses for language models."}, {"entity1": "RAG system", "entity2": "Hierarchical Index", "relation": "Tool/Resource", "description": "RAG system uses Hierarchical Index to expedite retrieval and processing of pertinent data."}, {"entity1": "G = {V, E, X}", "entity2": "Knowledge Graphs (KGs)", "relation": "Representation", "description": "G = {V, E, X} represents a graph where node V = {vi}n i=1 represents document structures and edge E \u2282 V \u00d7 V represents connections between concepts and entities."}, {"entity1": "Latex", "entity2": "Content awareness", "relation": "Format", "description": "Latex is a format used for content awareness based on inherent structure in documents."}, {"entity1": "HTML", "entity2": "Content awareness", "relation": "Format", "description": "HTML is a format used for content awareness based on inherent structure in documents."}, {"entity1": "PDF", "entity2": "Content awareness", "relation": "Format", "description": "PDF is a format used for content awareness based on inherent structure in documents."}], "347eaaae-56b1-40e1-b2b0-18d30b2ef64c": [{"entity1": "RAG system", "entity2": "efficiency", "relation": "Enhancement", "description": "The RAG system's efficiency is enhanced by organizing a corpus in a graph format."}, {"entity1": "graph", "entity2": "G", "relation": "Representation", "description": "The graph is represented by the equation G = {V, E, X}."}, {"entity1": "node", "entity2": "V", "relation": "Composition", "description": "The node V represents document structures such as passage, pages, and table."}, {"entity1": "edge", "entity2": "E", "relation": "Representation", "description": "The edge E represents semantic or lexical similarity and belonging relations between nodes."}, {"entity1": "node features", "entity2": "X", "relation": "Representation", "description": "The node features X represent text or markdown content for passage."}, {"entity1": "Naive RAG", "entity2": "user's original query", "relation": "Dependency", "description": "Naive RAG relies directly on the user's original query for retrieval."}, {"entity1": "Poorly worded queries", "entity2": "retrieval effectiveness", "relation": "Negative Impact", "description": "Poorly worded queries result in subpar retrieval effectiveness."}, {"entity1": "Language complexity", "entity2": "language models", "relation": "Challenge", "description": "Language complexity and ambiguity pose challenges for language models."}, {"entity1": "LLM", "entity2": "Large Language Model", "relation": "Acronym", "description": "LLM is an acronym for Large Language Model."}, {"entity1": "LLM", "entity2": "Master of Laws", "relation": "Ambiguity", "description": "LLM can also refer to Master of Laws, causing ambiguity."}, {"entity1": "Query Expansion", "entity2": "retrieval effectiveness", "relation": "Improvement", "description": "Query Expansion improves retrieval effectiveness by enriching the content of the query."}], "d37ae40b-4086-4cbe-9fa1-bf5342c56b45": [{"entity1": "Multi-Query", "entity2": "prompt engineering", "relation": "Methodology", "description": "Multi-Query uses prompt engineering to expand queries via LLMs."}, {"entity1": "LLMs", "entity2": "prompt engineering", "relation": "Tool/Resource", "description": "LLMs are used in prompt engineering for query expansion."}, {"entity1": "least-to-most prompting", "entity2": "Sub-Query", "relation": "Methodology", "description": "least-to-most prompting is used to decompose complex problems into simpler sub-problems."}, {"entity1": "Chain-of-Verification (CoVe)", "entity2": "LLMs", "relation": "Methodology", "description": "Chain-of-Verification (CoVe) uses LLMs for validation to reduce hallucinations."}, {"entity1": "Query Transformation", "entity2": "fqt(q)", "relation": "Methodology", "description": "Query Transformation involves retrieving and generating based on a transformed query fqt(q) = q\u2032."}, {"entity1": "Rewrite", "entity2": "LLMs", "relation": "Methodology", "description": "LLMs can be prompted to rewrite original queries for better retrieval."}, {"entity1": "HyDE", "entity2": "LLMs", "relation": "Tool/Resource", "description": "HyDE constructs hypothetical documents and uses LLMs for retrieval."}, {"entity1": "Step-back Prompting", "entity2": "RAG system", "relation": "Methodology", "description": "Step-back Prompting is used in the RAG system to abstract the original query into a high-level concept question."}, {"entity1": "Query Construction", "entity2": "RAG systems", "relation": "Methodology", "description": "Query Construction is necessary for RAG systems to accommodate various data types."}, {"entity1": "Taobao", "entity2": "Rewrite", "relation": "Application", "description": "The implementation of the query rewrite method in Taobao has improved recall effectiveness."}, {"entity1": "GMV", "entity2": "Taobao", "relation": "Impact", "description": "The use of query rewrite in Taobao has led to an increase in GMV."}], "19b6fad8-0e68-432b-a8b6-a9de23bbbc59": [{"entity1": "RAG systems", "entity2": "text data", "relation": "Integration", "description": "RAG systems integrate text data along with structured data like tables and graph data."}, {"entity1": "RAG systems", "entity2": "tables", "relation": "Integration", "description": "RAG systems integrate tables as part of structured data."}, {"entity1": "RAG systems", "entity2": "graph data", "relation": "Integration", "description": "RAG systems integrate graph data as part of structured data."}, {"entity1": "Text-to-SQL", "entity2": "Text-to-Cypher", "relation": "Comparison", "description": "Both are methods for converting user queries into alternative query languages to access different data sources."}, {"entity1": "SQL", "entity2": "Cypher", "relation": "Comparison", "description": "Both are structured query languages used in conjunction with semantic information and metadata."}, {"entity1": "embedding models", "entity2": "latent spaces", "relation": "Application", "description": "Embedding models are used to represent queries and text in latent spaces for efficient retrieval."}, {"entity1": "RAG systems", "entity2": "retrieval process", "relation": "Dependency", "description": "The retrieval process is pivotal in RAG systems."}, {"entity1": "Sparse Retriever", "entity2": "Dense Retriever", "relation": "Comparison", "description": "Sparse Retriever uses statistical methods for efficiency, while Dense Retriever uses pre-trained language models for complex semantic representations."}, {"entity1": "TF-IDF", "entity2": "BM25", "relation": "Comparison", "description": "Both are statistical methods used by Sparse Retriever for converting queries and documents into sparse vectors."}, {"entity1": "BERT", "entity2": "ColBERT", "relation": "Innovation", "description": "ColBERT is a BERT structure PLM used for dense representations of queries and documents."}, {"entity1": "Hybrid Retriever", "entity2": "Sparse Retriever", "relation": "Combination", "description": "Hybrid Retriever combines Sparse Retriever with Dense Retriever for enhanced retrieval effectiveness."}, {"entity1": "Hybrid Retriever", "entity2": "Dense Retriever", "relation": "Combination", "description": "Hybrid Retriever combines Dense Retriever with Sparse Retriever for enhanced retrieval effectiveness."}, {"entity1": "BGE", "entity2": "GTE", "relation": "Comparison", "description": "Both are multi-task fine-tuned models used for dense representations of queries and documents."}, {"entity1": "RAG systems", "entity2": "embedding models", "relation": "Dependency", "description": "RAG systems leverage powerful embedding models for efficient retrieval."}, {"entity1": "semantic similarity", "entity2": "latent spaces", "relation": "Application", "description": "Semantic similarity is established between questions and documents in latent spaces."}, {"entity1": "Retriever Selection", "entity2": "RAG systems", "relation": "Methodology", "description": "Retriever Selection is a crucial step in RAG systems for choosing the appropriate retriever based on task scenarios."}], "bc847de4-5539-4153-80ca-2e04499a01b0": [{"entity1": "Sparse retriever", "entity2": "Dense models", "relation": "Complementary Relationship", "description": "Sparse retriever and dense models complement each other to enhance retrieval effectiveness."}, {"entity1": "Sparse models", "entity2": "Dense models", "relation": "Enhancement Relationship", "description": "Sparse models enhance the zero-shot retrieval capabilities of dense models, particularly in handling queries with rare entities."}, {"entity1": "Retriever Fine-tuning", "entity2": "Healthcare", "relation": "Application Relationship", "description": "Retriever Fine-tuning is particularly useful in highly specialized fields like healthcare."}, {"entity1": "Retriever Fine-tuning", "entity2": "Law", "relation": "Application Relationship", "description": "Retriever Fine-tuning is particularly useful in highly specialized fields like law."}, {"entity1": "Supervised Fine-Tuning (SFT)", "entity2": "Contrastive learning", "relation": "Methodology Relationship", "description": "Supervised Fine-Tuning (SFT) is typically done using contrastive learning."}, {"entity1": "L(DR)", "entity2": "T", "relation": "Mathematical Model Relationship", "description": "L(DR) is calculated using the formula L(DR) = \u2212 1/T * \u2211[log(e(sim(qi,d+ i )) / (e(sim(qi,d+ i )) + \u2211[e(sim(qi,d- i ))]))], where T is the total number of queries."}, {"entity1": "LM-supervised Retriever (LSR)", "entity2": "Fine-tuning dataset", "relation": "Alternative Relationship", "description": "LM-supervised Retriever (LSR) is an alternative approach to directly constructing a fine-tuning dataset from the dataset."}, {"entity1": "d+ i", "entity2": "qi", "relation": "Correspondence Relationship", "description": "d+ i is the positive sample document corresponding to the i-th query qi."}, {"entity1": "d- i", "entity2": "qi", "relation": "Correspondence Relationship", "description": "d- i is several negative sample corresponding to the i-th query qi."}], "388e52da-7068-4874-80cd-530ce0bec7c8": [{"entity1": "LM", "entity2": "RAG", "relation": "Methodology", "description": "LM-generated results are used as supervisory signals to fine-tune the embedding model during the RAG process."}, {"entity1": "PLSR", "entity2": "PLM", "relation": "Mathematical Model", "description": "PLSR is defined as ePLM(y|d,q)/\u03b2, where PLM is the LM probability of the ground truth output y given the input context d and query q."}, {"entity1": "Adapter", "entity2": "LLM", "relation": "Tool/Resource", "description": "An adapter module can be incorporated into LLMs like gte-Qwen to mitigate the cost of fine-tuning and achieve better alignment with specific downstream tasks."}, {"entity1": "Post-retrieval", "entity2": "LLM", "relation": "Methodology", "description": "Post-processing the retrieved chunks can aid in better leveraging the contextual information, and feeding all retrieved chunks directly into the LLM is not an optimal choice."}, {"entity1": "Rerank", "entity2": "Retrieved chunks", "relation": "Methodology", "description": "Rerank the retrieved chunks without altering their content or length to enhance the visibility of the more crucial document chunks."}, {"entity1": "Rule-base rerank", "entity2": "MMR", "relation": "Methodology", "description": "Rule-base rerank uses metrics like diversity, relevance, and MRR to rerank chunks according to certain rules and reduce redundancy."}, {"entity1": "Model-base rerank", "entity2": "Language model", "relation": "Methodology", "description": "Model-base rerank utilizes a language model to reorder the document chunks based on the relevance between the chunks and the query."}, {"entity1": "RAG", "entity2": "Rerank models", "relation": "Component", "description": "Rerank models have become an important component of RAG systems."}, {"entity1": "LM", "entity2": "Context Window", "relation": "Limitation", "description": "LLM tends to remember only the beginning or the end of long texts, while forgetting the middle portion, and has a limitation on the length of contextual information."}, {"entity1": "Retrieved chunks", "entity2": "Noise/anti-fact chunks", "relation": "Challenge", "description": "Retrieved noisy or factually contradictory documents can impact the final retrieval generation."}], "eac75492-f643-4a62-b00d-9e54ceee9374": [{"entity1": "RAG", "entity2": "LLM", "relation": "Methodology", "description": "RAG process involves retrieving relevant documents and concatenating them to form a retrieval prompt, which can introduce noise and diminish the LLM's perception of key information."}, {"entity1": "LLM", "entity2": "Compression", "relation": "Methodology", "description": "Compression is used to address the issue of excessive context in the RAG process, which can introduce noise and diminish the LLM's perception of key information."}, {"entity1": "GPT-2 Small", "entity2": "LLaMA-7B", "relation": "Comparison", "description": "Both GPT-2 Small and LLaMA-7B are small language models that can be used for prompt compression."}, {"entity1": "LLMLingua", "entity2": "GPT-2 Small", "relation": "Tool/Resource", "description": "LLMLingua utilizes aligned and trained small language models, such as GPT-2 Small, for prompt compression."}, {"entity1": "Dq", "entity2": "c", "relation": "Equation", "description": "c = fcomp(q, Dq), where |dqc| < |dq| \u2200dq \u2208 Dq, is an equation that represents the compression of retrieved content."}, {"entity1": "Dq", "entity2": "fsel", "relation": "Equation", "description": "Dq = fsel(Dq) = {di \u2208 D | \u00acP(di)} is an equation that represents the selection of relevant documents."}, {"entity1": "Selective Context", "entity2": "LLM", "relation": "Methodology", "description": "Selective Context is a method that refines the input context by removing redundant content, improving the language model's reasoning efficiency."}, {"entity1": "LLM-Critique", "entity2": "LLM", "relation": "Methodology", "description": "LLM-Critique is a method that involves having the LLM evaluate the retrieved content before generating the final answer, allowing it to filter out documents with poor relevance."}, {"entity1": "Chatlaw", "entity2": "LLM", "relation": "Tool/Resource", "description": "Chatlaw is a system that utilizes LLM-Critique to assess the relevance of referenced legal provisions."}, {"entity1": "LLM", "entity2": "User", "relation": "Objective", "description": "The objective of the LLM is to generate answers based on the user's input."}], "694c06a0-f608-4e6c-8d1f-a40412608693": [{"entity1": "LLM", "entity2": "legal provisions", "relation": "Citation", "description": "The LLM is prompted to self-suggestion on the referenced legal provisions to assess their relevance."}, {"entity1": "LLM", "entity2": "user's query", "relation": "Application", "description": "The LLM is utilized to generate answers based on the user's query and the retrieved contextual information."}, {"entity1": "Generator Fine-tuning", "entity2": "fine-tuning", "relation": "Methodology", "description": "Targeted fine-tuning based on the scenario and data characteristics can yield better results."}, {"entity1": "Instruct-Tuning", "entity2": "fine-tuning", "relation": "Methodology", "description": "Additional knowledge can be provided to the LLM through fine-tuning when it lacks data in a specific domain."}, {"entity1": "LLM", "entity2": "on-premise setup LLMs", "relation": "Affiliation", "description": "Using an on-premise setup LLMs allows for targeted fine-tuning based on the scenario and data characteristics."}, {"entity1": "Reinforcement learning", "entity2": "human", "relation": "Collaboration-Type", "description": "Reinforcement learning aligns LLM outputs with human preferences."}, {"entity1": "Reinforcement learning", "entity2": "retriever", "relation": "Collaboration-Type", "description": "Reinforcement learning aligns LLM outputs with retriever preferences."}, {"entity1": "fine-tuning", "entity2": "data formats", "relation": "Tool/Resource", "description": "Fine-tuning enables the LLM to adapt to specific data formats."}, {"entity1": "fine-tuning", "entity2": "style", "relation": "Tool/Resource", "description": "Fine-tuning enables the LLM to generate responses in a particular style."}, {"entity1": "LLM", "entity2": "privacy protection", "relation": "Objective", "description": "The LLM is used considering factors such as the need for fine-tuning, inference efficiency, and privacy protection."}], "b6edfd64-3793-47ce-9933-55e69f42e259": [{"entity1": "RA-DIT", "entity2": "KL divergence", "relation": "Utilization", "description": "RA-DIT utilizes KL divergence to align the scoring functions between retriever and generator."}, {"entity1": "PR(d|q)", "entity2": "e(sim(d,q))/\u03b3P", "relation": "Equivalence", "description": "PR(d|q) is calculated as e(sim(d,q))/\u03b3P."}, {"entity1": "PLM (y|d, q)", "entity2": "LM probability", "relation": "Definition", "description": "PLM (y|d, q) is defined as the LM probability of the ground truth output y given the input context d, question q."}, {"entity1": "\u03b3", "entity2": "hyperparameter", "relation": "Classification", "description": "\u03b3 is classified as a hyperparameter."}, {"entity1": "L", "entity2": "KL(PR(d|q)||PLSR(d|q, y|))", "relation": "Calculation", "description": "L is calculated as the average of KL(PR(d|q)||PLSR(d|q, y|)) over all samples in T."}, {"entity1": "RAG", "entity2": "LLMs", "relation": "Enhancement", "description": "RAG enhances the reliability of LLM-generated answers."}, {"entity1": "fverify (q, Dq, y)", "entity2": "yk", "relation": "Equivalence", "description": "yk is equivalent to fverify (q, Dq, y)."}, {"entity1": "Knowledge-base verification", "entity2": "Wikipedia", "relation": "Utilization", "description": "Knowledge-base verification utilizes Wikipedia as a verified knowledge base."}, {"entity1": "Model-based verification", "entity2": "small language model", "relation": "Utilization", "description": "Model-based verification utilizes a small language model to verify the responses generated by LLMs."}, {"entity1": "RAG", "entity2": "Orchestration", "relation": "Incorporation", "description": "RAG incorporates decision-making at pivotal points through orchestration."}], "01c057dd-19a7-4791-8c3d-6b282c25572f": [{"entity1": "Orchestration", "entity2": "RAG", "relation": "Control", "description": "Orchestration pertains to the control modules that govern the RAG process."}, {"entity1": "RAG", "entity2": "Naive RAG", "relation": "Comparison", "description": "Modular RAG is distinguished from the more simplistic Naive RAG paradigm."}, {"entity1": "RAG", "entity2": "Advance RAG", "relation": "Comparison", "description": "Modular RAG is distinguished from the more simplistic Advance RAG paradigm."}, {"entity1": "Routing", "entity2": "RAG", "relation": "Component", "description": "Routing is a feature essential for a versatile RAG architecture."}, {"entity1": "fr(\u00b7)", "entity2": "Routing", "relation": "Function", "description": "The routing mechanism is executed through a function, denoted as fr(\u00b7)."}, {"entity1": "Metadata routing", "entity2": "RAG", "relation": "Methodology", "description": "Metadata routing involves extracting key terms from the query to refine the routing parameters for RAG."}, {"entity1": "Semantic routing", "entity2": "RAG", "relation": "Methodology", "description": "Semantic routing routes to different modules based on the semantic information of the query for RAG."}, {"entity1": "Hybrid Routing", "entity2": "RAG", "relation": "Methodology", "description": "Hybrid Routing can be implemented to improve query routing by integrating both semantic analysis and metadata for RAG."}, {"entity1": "\u03b4(\u00b7)", "entity2": "Intent", "relation": "Mapping", "description": "The function \u03b4(\u00b7) serves as a mapping function that assigns an intent to a distinct RAG flow."}, {"entity1": "socresemantic(q, Fj)", "entity2": "RAG", "relation": "Scoring", "description": "The semantic score is determined by the function socresemantic(q, Fj) for RAG."}, {"entity1": "Fi(q)", "entity2": "RAG", "relation": "Selection", "description": "The most relevant flow for the query q is determined by the function Fi(q) for RAG."}], "d2c7d7d2-1ee7-4afd-9359-d2bd78b86917": [{"entity1": "\u03b4(\u00b7)", "entity2": "Fi", "relation": "Mapping", "description": "The function \u03b4(\u00b7) maps an intent to a distinct RAG flow Fi."}, {"entity1": "Hybrid Routing", "entity2": "query routing", "relation": "Improvement", "description": "Hybrid Routing improves query routing by integrating semantic analysis and metadata-based approaches."}, {"entity1": "\u03b1i", "entity2": "scorekey(q, Fj)", "relation": "Calculation", "description": "\u03b1i is calculated using the scorekey(q, Fj) and the semantic score."}, {"entity1": "\u03b1i", "entity2": "scoresemantic(q, Fj)", "relation": "Calculation", "description": "\u03b1i is calculated using the scoresemantic(q, Fj) and the key-based score."}, {"entity1": "a", "entity2": "\u03b1i", "relation": "Weighting", "description": "a is a weighting factor that balances the contribution of the key-based score and the semantic score in calculating \u03b1i."}, {"entity1": "RAG system", "entity2": "scheduling module", "relation": "Component", "description": "The scheduling module is a component of the RAG system."}, {"entity1": "scheduling module", "entity2": "processes", "relation": "Management", "description": "The scheduling module manages processes in the RAG system."}, {"entity1": "scheduling module", "entity2": "external data retrieval", "relation": "Trigger", "description": "The scheduling module identifies critical junctures that require external data retrieval."}, {"entity1": "scheduling module", "entity2": "responses", "relation": "Assessment", "description": "The scheduling module assesses the adequacy of the responses."}, {"entity1": "scheduling module", "entity2": "further investigation", "relation": "Decision", "description": "The scheduling module decides on the necessity for further investigation."}], "2c26d1fb-7c70-4a84-9d64-04329d52dddf": [{"entity1": "yt", "entity2": "\u02c6st", "relation": "Equivalence", "description": "yt is equivalent to \u02c6st if all tokens of \u02c6st have probabilities greater than or equal to \u03c4"}, {"entity1": "yt", "entity2": "st", "relation": "Alternative", "description": "yt is equivalent to st if the condition for \u02c6st is not met"}, {"entity1": "st", "entity2": "LM", "relation": "Output", "description": "st is the output from the language model LM"}, {"entity1": "LM", "entity2": "Dqt, x, y<t", "relation": "Input", "description": "LM takes Dqt, x, y<t as input"}, {"entity1": "\u03c4", "entity2": "tokens of \u02c6st", "relation": "Threshold", "description": "\u03c4 is the threshold for the probabilities of tokens in \u02c6st"}, {"entity1": "Rule judge", "entity2": "established rules", "relation": "Guided by", "description": "The Rule judge is guided by a set of established rules"}, {"entity1": "LLM judge", "entity2": "in-context learning capability", "relation": "Leverages", "description": "The LLM judge leverages the LLM's in-context learning capability"}, {"entity1": "LLM judge", "entity2": "prompt engineering", "relation": "Method", "description": "The LLM judge uses prompt engineering as a method"}, {"entity1": "Toolformer", "entity2": "fine-tuning", "relation": "Technique", "description": "The Toolformer is associated with a fine-tuning technique"}, {"entity1": "Self-RAG", "entity2": "Toolformer", "relation": "Integration", "description": "Self-RAG has integrated the Toolformer technique"}, {"entity1": "Knowledge-guide scheduling", "entity2": "knowledge graphs", "relation": "Harnesses", "description": "Knowledge-guide scheduling harnesses the power of knowledge graphs"}, {"entity1": "knowledge graphs", "entity2": "reasoning chain", "relation": "Constructs", "description": "Knowledge graphs are used to construct a reasoning chain"}, {"entity1": "reasoning chain", "entity2": "information retrieval and content generation", "relation": "Enables", "description": "The reasoning chain enables information retrieval and content generation"}], "6f8b62ab-fc43-4a8c-b0e9-f72e147854ce": [{"entity1": "LLM fusion", "entity2": "ensemble technique", "relation": "Methodology", "description": "LLM fusion is a type of ensemble technique used for multi-branch aggregation."}, {"entity1": "tokens", "entity2": "nodes", "relation": "Component", "description": "Tokens are components of the nodes in the reasoning chain."}, {"entity1": "pipelines", "entity2": "RAG process", "relation": "PartOf", "description": "Pipelines are part of the RAG process."}, {"entity1": "content generation", "entity2": "information retrieval", "relation": "RelatedTask", "description": "Content generation and information retrieval are related tasks in the problem-solving process."}, {"entity1": "\u03bb(d, q)", "entity2": "softmax function", "relation": "CalculationMethod", "description": "The weight \u03bb(d, q) is calculated using the softmax function."}, {"entity1": "RRF", "entity2": "ensemble technique", "relation": "TypeOf", "description": "RRF is a type of ensemble technique."}, {"entity1": "Weighted ensemble", "entity2": "tokens", "relation": "BasedOn", "description": "Weighted ensemble is based on the weighted values of different tokens."}, {"entity1": "fusion module", "entity2": "RAG process", "relation": "ComponentOf", "description": "The fusion module is a component of the RAG process."}, {"entity1": "LLMs", "entity2": "context window limitation", "relation": "Limitation", "description": "LLMs have a context window limitation."}, {"entity1": "information retrieval", "entity2": "content generation", "relation": "SeparateTask", "description": "Information retrieval and content generation can be performed separately."}, {"entity1": "reasoning chain", "entity2": "nodes", "relation": "ComposedOf", "description": "The reasoning chain is composed of nodes."}, {"entity1": "p(y|q, Dq)", "entity2": "\u03bb(d, q)", "relation": "Calculation", "description": "p(y|q, Dq) is calculated using \u03bb(d, q)."}, {"entity1": "RRF", "entity2": "weighted averaging approach", "relation": "Method", "description": "RRF employs a weighted averaging approach."}, {"entity1": "ensemble technique", "entity2": "pipelines", "relation": "AppliedTo", "description": "Ensemble techniques are applied to pipelines."}], "ff7530bb-2d46-4d2e-b669-92075d8b328c": [{"entity1": "RRF", "entity2": "model heterogeneity", "relation": "Effectiveness", "description": "RRF is especially potent in scenarios characterized by model heterogeneity, where it can markedly amplify the accuracy of predictions."}, {"entity1": "RRF", "entity2": "source heterogeneity", "relation": "Effectiveness", "description": "RRF is especially potent in scenarios characterized by source heterogeneity, where it can markedly amplify the accuracy of predictions."}, {"entity1": "RAG flow", "entity2": "operators", "relation": "Composition", "description": "The collaboration between operators forms the workflow of the module, which we refer to as RAG flow."}, {"entity1": "RAG flow", "entity2": "module parameters", "relation": "Characterization", "description": "A modular rag flow can be characterized by the set of module parameters, denoted as \u03d5."}, {"entity1": "RAG flow patterns", "entity2": "application domains", "relation": "Applicability", "description": "RAG flow patterns transcend various application domains and demonstrate a high level of consistency and reusability."}, {"entity1": "P", "entity2": "M\u03d51", "relation": "Composition", "description": "A RAG flow pattern can be defined as P = {M\u03d51 : {Op1} \u2192M\u03d52 : {Op2} \u2192. . .\u2192 M\u03d5n : {Opn}}"}, {"entity1": "Plinear", "entity2": "M1", "relation": "Composition", "description": "Plinear = {M1 \u2192 M2 \u2192 . . .\u2192 Mn}, which describes the linear organization of modules in the modular RAG system."}, {"entity1": "Algorithm 1", "entity2": "Linear Pattern", "relation": "Representation", "description": "The modules in the modular RAG system are organized in a linear way, and can be described as Algorithm 1, which represents the Linear Pattern."}, {"entity1": "RAG flow", "entity2": "graph", "relation": "Decomposition", "description": "A modular rag flow can be decomposed into a graph of sub-functions."}, {"entity1": "operators", "entity2": "control logic", "relation": "Execution", "description": "The operators can execute in a predetermined pipeline, while also performing conditional, branching or looping when necessary, through control logic."}], "72b198f4-966c-4d22-99b9-41cf51b1cfa4": [{"entity1": "Linear RAG flow pattern", "entity2": "Algorithm 1", "relation": "Methodology", "description": "Algorithm 1 implements the Linear RAG flow pattern."}, {"entity1": "Linear RAG flow pattern", "entity2": "Naive RAG paradigm", "relation": "Comparison", "description": "The Linear RAG flow pattern is compared to the Naive RAG paradigm."}, {"entity1": "RRR", "entity2": "Linear flow", "relation": "Affiliation", "description": "RRR is a type of linear flow."}, {"entity1": "RRR", "entity2": "T5-large", "relation": "Tool/Resource", "description": "RRR utilizes T5-large as a trainable language model."}, {"entity1": "RRR", "entity2": "BM25", "relation": "Tool/Resource", "description": "RRR utilizes BM25 as a sparse encoding model."}, {"entity1": "Markov decision process", "entity2": "RRR", "relation": "Methodology", "description": "RRR formalizes the optimization of the rewriter as a Markov decision process."}, {"entity1": "Algorithm 1", "entity2": "Fig. 4", "relation": "Illustration", "description": "Fig. 4 illustrates the Linear RAG flow pattern implemented in Algorithm 1."}, {"entity1": "Algorithm 2", "entity2": "Fig. 6", "relation": "Illustration", "description": "Fig. 6 illustrates the conditional flow pattern implemented in Algorithm 2."}, {"entity1": "Pre-retrieval branching flow pattern", "entity2": "Fig. 7", "relation": "Illustration", "description": "Fig. 7 illustrates the pre-retrieval branching flow pattern."}, {"entity1": "Linear RAG flow pattern", "entity2": "Plinearfull", "relation": "Definition", "description": "Plinearfull defines the full linear RAG flow pattern."}, {"entity1": "Plinearfull", "entity2": "Mindexing", "relation": "Component", "description": "Mindexing is a component of Plinearfull."}, {"entity1": "Plinearfull", "entity2": "Mpre-retrieval", "relation": "Component", "description": "Mpre-retrieval is a component of Plinearfull."}, {"entity1": "Plinearfull", "entity2": "Mretrieval", "relation": "Component", "description": "Mretrieval is a component of Plinearfull."}, {"entity1": "Plinearfull", "entity2": "Mpost-retrieval", "relation": "Component", "description": "Mpost-retrieval is a component of Plinearfull."}, {"entity1": "Plinearfull", "entity2": "Mgenerate", "relation": "Component", "description": "Mgenerate is a component of Plinearfull."}, {"entity1": "Algorithm 1", "entity2": "q", "relation": "Input", "description": "q is an input to Algorithm 1."}, {"entity1": "Algorithm 1", "entity2": "D", "relation": "Input", "description": "D is an input to Algorithm 1."}, {"entity1": "Algorithm 1", "entity2": "R", "relation": "Input", "description": "R is an input to Algorithm 1."}, {"entity1": "Algorithm 1", "entity2": "LLM", "relation": "Input", "description": "LLM is an input to Algorithm 1."}, {"entity1": "Algorithm 1", "entity2": "fpre", "relation": "Input", "description": "fpre is an input to Algorithm 1."}, {"entity1": "Algorithm 1", "entity2": "fpost", "relation": "Input", "description": "fpost is an input to Algorithm 1."}, {"entity1": "Algorithm 1", "entity2": "\u02c6y", "relation": "Output", "description": "\u02c6y is an output of Algorithm 1."}], "5d413234-47cc-44f1-8949-b51f8b15cfca": [{"entity1": "Pre-retrieval branching flow pattern", "entity2": "Fig. 7", "relation": "Illustration", "description": "Fig. 7 illustrates the pre-retrieval branching flow pattern."}, {"entity1": "Pconditional", "entity2": "fr(\u00b7)", "relation": "Definition", "description": "Pconditional is defined using the routing function fr(\u00b7)."}, {"entity1": "fr(\u00b7)", "entity2": "Mj", "relation": "Routing", "description": "The routing function fr(\u00b7) determines the flow to module Mj."}, {"entity1": "fr(\u00b7)", "entity2": "Mk", "relation": "Routing", "description": "The routing function fr(\u00b7) determines the flow to module Mk."}, {"entity1": "Algorithm 2", "entity2": "Conditional RAG Flow Pattern", "relation": "Implementation", "description": "Algorithm 2 implements the Conditional RAG Flow Pattern."}, {"entity1": "QueryTransform", "entity2": "q", "relation": "Transformation", "description": "QueryTransform is used to pre-process the initial query q."}, {"entity1": "R", "entity2": "D", "relation": "Retrieval", "description": "R retrieves or updates documents related to the query from D."}, {"entity1": "fr", "entity2": "q\u2032", "relation": "Input", "description": "The routing function fr takes q\u2032 as input."}, {"entity1": "fr", "entity2": "D\u2032", "relation": "Input", "description": "The routing function fr takes D\u2032 as input."}, {"entity1": "Mj", "entity2": "q\u2032", "relation": "Execution", "description": "Module Mj is executed with q\u2032 as input."}, {"entity1": "Mj", "entity2": "D\u2032", "relation": "Execution", "description": "Module Mj is executed with D\u2032 as input."}, {"entity1": "Mk", "entity2": "q\u2032", "relation": "Execution", "description": "Module Mk is executed with q\u2032 as input."}, {"entity1": "Mk", "entity2": "D\u2032", "relation": "Execution", "description": "Module Mk is executed with D\u2032 as input."}, {"entity1": "Pipeline selection", "entity2": "question", "relation": "Dependence", "description": "Pipeline selection depends on the nature of the question."}, {"entity1": "LLMs", "entity2": "tolerance", "relation": "Variation", "description": "The tolerance for responses generated by LLMs varies across questions."}], "ef49aff6-6e98-4382-87f0-3e927d5c89a4": [{"entity1": "RAG flow system", "entity2": "Branching", "relation": "Structural Component", "description": "The RAG flow system may have multiple parallel running branches to increase the diversity of generated results."}, {"entity1": "Msplit", "entity2": "bi", "relation": "Module Output", "description": "Msplit generates multiple branches bi."}, {"entity1": "bi", "entity2": "Mik", "relation": "Input-Output", "description": "Each branch bi is processed through multiple modules {M1, M2, ..., Mk} to obtain branch output result pi."}, {"entity1": "pi", "entity2": "Mmerge", "relation": "Input-Output", "description": "The results of multiple branches pi are aggregated using Mmerge to obtain intermediate output results \u02c6O."}, {"entity1": "Mmerge", "entity2": "Mjn", "relation": "Sequential Processing", "description": "The output of Mmerge \u02c6O can continue to connect to other modules Mjn for further processing."}, {"entity1": "Algorithm 3", "entity2": "Pre-retrieval Branching Flow Pattern", "relation": "Implementation", "description": "Algorithm 3 implements the Pre-retrieval Branching Flow Pattern."}, {"entity1": "Mexpand", "entity2": "q", "relation": "Input-Output", "description": "Mexpand expands the original query q to multiple sub-queries Q\u2032."}, {"entity1": "Mretrieve", "entity2": "q\u2032i", "relation": "Input-Output", "description": "Mretrieve retrieves documents for each sub-query q\u2032i."}, {"entity1": "LLM", "entity2": "yij", "relation": "Generation", "description": "LLM generates results yij for each document of the sub-query."}, {"entity1": "Mmerge", "entity2": "Oi", "relation": "Input-Output", "description": "Mmerge merges generated results Oi of the sub-query into the final result \u02c6y."}, {"entity1": "Figure 7", "entity2": "Figure 8", "relation": "Illustration", "description": "Figures 7 and 8 depict the two types of branching structures in the RAG flow system."}, {"entity1": "Pre-Retrieval Branching", "entity2": "Multi-Query", "relation": "Characterization", "description": "Pre-Retrieval Branching is characterized by a multi-query, parallel retrieval process."}], "4ed70df3-3cc2-43a6-88e3-a0dac88a4712": [{"entity1": "Algorithm 3", "entity2": "Mexpand", "relation": "Methodology", "description": "Algorithm 3 involves expanding a query through Mexpand to generate multiple sub-queries."}, {"entity1": "Mexpand", "entity2": "Mretrieve", "relation": "Methodology", "description": "Mexpand generates sub-queries that are used by Mretrieve to retrieve relevant documents."}, {"entity1": "Mretrieve", "entity2": "Mgenerate", "relation": "Methodology", "description": "Mretrieve retrieves documents that are then used by Mgenerate to produce answers."}, {"entity1": "Mgenerate", "entity2": "Mmerge", "relation": "Methodology", "description": "Mgenerate produces answers that are combined by Mmerge to form the final result."}, {"entity1": "Algorithm 4", "entity2": "Mretrieve", "relation": "Methodology", "description": "Algorithm 4 involves retrieving documents through Mretrieve based on a pre-processed query."}, {"entity1": "Mretrieve", "entity2": "Mgenerate", "relation": "Methodology", "description": "Mretrieve retrieves documents that are then independently processed by Mgenerate to produce results."}, {"entity1": "Mgenerate", "entity2": "Mmerge", "relation": "Methodology", "description": "Mgenerate produces results that are merged by Mmerge to form the final output."}, {"entity1": "REPLUG", "entity2": "Post-Retrieval Branching", "relation": "Application", "description": "REPLUG embodies a classic post-retrieval branching structure."}, {"entity1": "Post-Retrieval Branching", "entity2": "Algorithm 4", "relation": "Methodology", "description": "Post-Retrieval Branching is implemented in Algorithm 4."}, {"entity1": "Pbranchpre", "entity2": "Mmerge", "relation": "Methodology", "description": "Pbranchpre involves merging generated answers using Mmerge."}, {"entity1": "Pbranchpost", "entity2": "Mmerge", "relation": "Methodology", "description": "Pbranchpost involves merging generated results using Mmerge."}, {"entity1": "q", "entity2": "Mexpand", "relation": "Input-Output", "description": "The query q is expanded through Mexpand to generate sub-queries."}, {"entity1": "q", "entity2": "Mretrieve", "relation": "Input-Output", "description": "The query q is used to retrieve documents through Mretrieve."}, {"entity1": "Dq", "entity2": "Mgenerate", "relation": "Input-Output", "description": "The document set Dq is used by Mgenerate to produce results."}, {"entity1": "LLM", "entity2": "Mgenerate", "relation": "Tool/Resource", "description": "LLM is used by Mgenerate to generate results."}, {"entity1": "Mmerge", "entity2": "y", "relation": "Output", "description": "Mmerge produces the final output y."}], "84da14d6-b883-46da-aa4e-a063cb1bd93d": [{"entity1": "RAG", "entity2": "REPLUG", "relation": "Affiliation", "description": "RAG flow is used in REPLUG"}, {"entity1": "Contriever", "entity2": "RAG", "relation": "Tool/Resource", "description": "Contriever is used for fine-tuning the retriever in RAG"}, {"entity1": "Modular RAG", "entity2": "G = (V, E)", "relation": "Theoretical Framework", "description": "Modular RAG can be abstracted as a directed graph G = (V, E)"}, {"entity1": "Mi", "entity2": "Mj", "relation": "Institution-Collaboration", "description": "Mi and Mj are successor and predecessor modules in the Modular RAG system"}, {"entity1": "Judge", "entity2": "Mi", "relation": "Author-Role", "description": "Judge module decides whether to return to Mj or a previous module Mk"}, {"entity1": "ITER-RETGEN", "entity2": "RAG", "relation": "Application", "description": "ITER-RETGEN is an exemplary case of iterative retrieval in RAG"}, {"entity1": "Algorithm 5", "entity2": "RAG", "relation": "Methodology", "description": "Algorithm 5 is used for iterative retrieval in RAG"}, {"entity1": "yt", "entity2": "y<t", "relation": "Result", "description": "yt is generated based on y<t and qt"}, {"entity1": "qt", "entity2": "yt", "relation": "Input-Output", "description": "qt is used to generate yt"}, {"entity1": "Dt\u22121", "entity2": "yt\u22121", "relation": "Dataset-Origin", "description": "Dt\u22121 is retrieved using yt\u22121 and qt"}, {"entity1": "Fig. 11", "entity2": "ITER-RETGEN", "relation": "Publication Venue", "description": "Fig. 11 illustrates ITER-RETGEN"}, {"entity1": "Fig. 9", "entity2": "RAG", "relation": "Publication Venue", "description": "Fig. 9 illustrates the RAG flow in REPLUG"}], "ea82ba34-3b21-417c-be6a-e526df072a5f": [{"entity1": "RETGEN", "entity2": "ITER-RETGEN", "relation": "Innovation", "description": "RETGEN is an iterative retrieval-augmented generation model, and ITER-RETGEN is an extension of it, indicating an innovation in the approach to retrieval and generation."}, {"entity1": "Algorithm 5", "entity2": "Iterative RAG Flow Pattern", "relation": "Methodology", "description": "Algorithm 5 describes the Iterative RAG Flow Pattern, outlining the steps involved in the iterative retrieval and generation process."}, {"entity1": "LLM", "entity2": "yt", "relation": "Result", "description": "The language model LLM generates the output yt based on the input [y<t\u22121, qt, Dt]."}, {"entity1": "RAG system", "entity2": "Fig. 10", "relation": "Illustration", "description": "Fig. 10 illustrates the loop flow pattern of a RAG system, which performs multiple rounds of retrieval and generation."}, {"entity1": "yt\u22121||qt", "entity2": "R", "relation": "Input", "description": "The retriever R takes yt\u22121||qt as input to retrieve or update documents related to the current query."}, {"entity1": "Judge(yt, q)", "entity2": "ITER-RETGEN", "relation": "Termination Condition", "description": "The Judge(yt, q) function determines whether the iteration should stop, and if it returns false, the ITER-RETGEN process terminates."}, {"entity1": "y<t\u22121", "entity2": "qt\u22121", "relation": "Previous State", "description": "y<t\u22121 and qt\u22121 represent the previous output and query, respectively, which are used to generate the next query and output."}, {"entity1": "QueryTransform(y<t\u22121, qt\u22121)", "entity2": "qt", "relation": "Transformation", "description": "The QueryTransform function transforms the previous output y<t\u22121 and query qt\u22121 into a new query qt."}, {"entity1": "yt", "entity2": "yfinal", "relation": "Synthesis", "description": "The final output yfinal is synthesized from the list of outputs yt using the synthesizeOutput function."}, {"entity1": "T", "entity2": "ITER-RETGEN", "relation": "Parameter", "description": "T represents the maximum number of iterations for the ITER-RETGEN process."}, {"entity1": "D", "entity2": "R", "relation": "Input", "description": "The retriever R takes the documents D as input to retrieve or update documents related to the current query."}, {"entity1": "LLM", "entity2": "D", "relation": "Input", "description": "The language model LLM takes the documents D as input to generate the output yt."}, {"entity1": "yt\u22121", "entity2": "LLM", "relation": "Input", "description": "The language model LLM takes the previous output yt\u22121 as input to generate the next output yt."}, {"entity1": "qt", "entity2": "LLM", "relation": "Input", "description": "The language model LLM takes the query qt as input to generate the output yt."}], "1ab61aea-3b7a-49bb-ac56-b21db3812968": [{"entity1": "Recursive RAG Flow Pattern", "entity2": "Algorithm 6", "relation": "Implementation", "description": "Algorithm 6 implements the Recursive RAG Flow Pattern."}, {"entity1": "Recursive RAG Flow Pattern", "entity2": "RAG systems", "relation": "PartOf", "description": "Recursive RAG Flow Pattern is a part of RAG systems."}, {"entity1": "Algorithm 6", "entity2": "q", "relation": "Input", "description": "Algorithm 6 takes q as an input."}, {"entity1": "Algorithm 6", "entity2": "D", "relation": "Input", "description": "Algorithm 6 takes D as an input."}, {"entity1": "Algorithm 6", "entity2": "R", "relation": "Input", "description": "Algorithm 6 takes R as an input."}, {"entity1": "Algorithm 6", "entity2": "LM", "relation": "Input", "description": "Algorithm 6 takes LM as an input."}, {"entity1": "Algorithm 6", "entity2": "Kmax", "relation": "Input", "description": "Algorithm 6 takes Kmax as an input."}, {"entity1": "Algorithm 6", "entity2": "\u02c6y", "relation": "Output", "description": "Algorithm 6 produces \u02c6y as an output."}, {"entity1": "ToC", "entity2": "RAC", "relation": "Implementation", "description": "ToC implements RAC."}, {"entity1": "ToC", "entity2": "Fig. 12", "relation": "Illustration", "description": "Fig. 12 illustrates the RAG flow of ToC."}, {"entity1": "RAG systems", "entity2": "Recursive retrieval", "relation": "Methodology", "description": "RAG systems use recursive retrieval as a methodology."}, {"entity1": "Recursive retrieval", "entity2": "Tree-like structure", "relation": "Characteristics", "description": "Recursive retrieval has a tree-like structure."}, {"entity1": "ITER-RETGEN", "entity2": "Fig. 11", "relation": "Illustration", "description": "Fig. 11 illustrates the ITER-RETGEN."}, {"entity1": "ITER-RETGEN", "entity2": "RAG systems", "relation": "PartOf", "description": "ITER-RETGEN is a part of RAG systems."}, {"entity1": "Q", "entity2": "Q\u2032", "relation": "Transformation", "description": "Q is transformed into Q\u2032."}, {"entity1": "Q\u2032", "entity2": "Q\u2032\u2032", "relation": "Transformation", "description": "Q\u2032 is transformed into Q\u2032\u2032."}, {"entity1": "q", "entity2": "q\u2032", "relation": "Transformation", "description": "q is transformed into q\u2032."}, {"entity1": "D", "entity2": "Dq", "relation": "Update", "description": "D is updated to Dq."}, {"entity1": "Y", "entity2": "\u02c6y", "relation": "Synthesis", "description": "Y is synthesized into \u02c6y."}], "c94b1d06-f4d8-4ae6-a479-3a5765acdf99": [{"entity1": "RAG", "entity2": "Adaptive retrieval", "relation": "Methodology", "description": "RAG systems enable adaptive retrieval, also known as active retrieval, which is a gradual shift beyond passive retrieval."}, {"entity1": "LLM", "entity2": "RAG", "relation": "Tool/Resource", "description": "RAG systems utilize the powerful capabilities of LLM."}, {"entity1": "Algorithm 7", "entity2": "Active RAG Flow Pattern", "relation": "Methodology", "description": "Algorithm 7 describes the Active RAG Flow Pattern, which is a method for adaptive retrieval."}, {"entity1": "Prompt Engineering", "entity2": "LLM", "relation": "Methodology", "description": "Prompt Engineering is used to control the flow and direct LLM in the prompt-base approach."}, {"entity1": "Tuning-base approaches", "entity2": "Prompt-base approach", "relation": "Comparison", "description": "The prompt-base approach is compared to Tuning-base approaches as two different methods for adaptive retrieval."}, {"entity1": "ToC", "entity2": "Clarification tree", "relation": "Methodology", "description": "ToC constructs a clarification tree and generates a comprehensive long-text answer to address AQ."}, {"entity1": "RAG", "entity2": "LLM Agent", "relation": "Comparison", "description": "RAG systems share a core concept with LLM Agent."}, {"entity1": "Active retrieval", "entity2": "Passive retrieval", "relation": "Comparison", "description": "Active retrieval is a gradual shift beyond passive retrieval, enabled by the advancement of RAG."}, {"entity1": "QueryTransform", "entity2": "Qt", "relation": "Methodology", "description": "QueryTransform is used to derive a new query from previous output and query."}, {"entity1": "Evaluate", "entity2": "Qt", "relation": "Methodology", "description": "Evaluate is used to assess the new query Qt."}, {"entity1": "R", "entity2": "Dt", "relation": "Methodology", "description": "R is used to retrieve documents based on the new query."}, {"entity1": "LLM", "entity2": "yt", "relation": "Methodology", "description": "LLM is used to generate output using the language model."}, {"entity1": "isOutputAcceptable", "entity2": "yt", "relation": "Methodology", "description": "isOutputAcceptable is used to assess the acceptability of the output yt."}, {"entity1": "synthesizeOutput", "entity2": "y\u2264t", "relation": "Methodology", "description": "synthesizeOutput is used to synthesize the final output from the list of outputs."}], "1ac13e92-99ec-422c-b1ac-f3d695177f5b": [{"entity1": "FLARE", "entity2": "RAG", "relation": "Implementation", "description": "FLARE is a typical implementation example of RAG."}, {"entity1": "SELF-RAG", "entity2": "RAG", "relation": "Variant", "description": "SELF-RAG is a variant of RAG."}, {"entity1": "GPT-4", "entity2": "SELF-RAG", "relation": "Tool/Resource", "description": "GPT-4 is used in SELF-RAG for fine-tuning."}, {"entity1": "LLM", "entity2": "RAG", "relation": "Component", "description": "LLM is a component of RAG systems."}, {"entity1": "Retriever fine-tuning", "entity2": "RAG", "relation": "Methodology", "description": "Retriever fine-tuning is a methodology used in RAG."}, {"entity1": "Generator fine-tuning", "entity2": "RAG", "relation": "Methodology", "description": "Generator fine-tuning is a methodology used in RAG."}, {"entity1": "Dual fine-tuning", "entity2": "RAG", "relation": "Methodology", "description": "Dual fine-tuning is a methodology used in RAG."}, {"entity1": "Toolformer", "entity2": "Tuning-based approach", "relation": "Origin", "description": "The tuning-based approach concept can be traced back to Toolformer."}, {"entity1": "FLARE", "entity2": "Confidence assessment", "relation": "Component", "description": "Confidence assessment is a component of FLARE."}, {"entity1": "RAG", "entity2": "Modular RAG", "relation": "Variant", "description": "Modular RAG is a variant of RAG."}, {"entity1": "LLM Reward RL", "entity2": "Retriever fine-tuning", "relation": "Methodology", "description": "LLM Reward RL is a methodology used in retriever fine-tuning."}, {"entity1": "Direct SFT", "entity2": "Retriever fine-tuning", "relation": "Methodology", "description": "Direct SFT is a methodology used in retriever fine-tuning."}, {"entity1": "LM-supervised retrieval", "entity2": "Retriever fine-tuning", "relation": "Methodology", "description": "LM-supervised retrieval is a methodology used in retriever fine-tuning."}], "276d51f0-bda2-4710-aed6-78434b701d00": [{"entity1": "Retriever FT", "entity2": "Figure 15", "relation": "Illustration", "description": "Figure 15 illustrates common methods for fine-tuning the retriever in the RAG flow."}, {"entity1": "Retriever FT", "entity2": "Direct supervised fine-tuning", "relation": "Methodology", "description": "Direct supervised fine-tuning is a method for fine-tuning the retriever."}, {"entity1": "Retriever FT", "entity2": "Trainable adapter modules", "relation": "Methodology", "description": "Adding trainable adapter modules is a method for fine-tuning the retriever."}, {"entity1": "Retriever FT", "entity2": "LM-supervised Retrieval (LSR)", "relation": "Methodology", "description": "LM-supervised Retrieval (LSR) is a method for fine-tuning the retriever based on LLM results."}, {"entity1": "Retriever FT", "entity2": "LLM Reward RL", "relation": "Methodology", "description": "LLM Reward RL is a method for fine-tuning the retriever using reinforcement learning and LLM output results."}, {"entity1": "Generator FT", "entity2": "Figure 16", "relation": "Illustration", "description": "Figure 16 illustrates primary methods for fine-tuning a generator in the RAG flow."}, {"entity1": "Generator FT", "entity2": "Direct supervised fine-tuning", "relation": "Methodology", "description": "Direct supervised fine-tuning is a method for fine-tuning the generator."}, {"entity1": "Generator FT", "entity2": "Distillation", "relation": "Methodology", "description": "Distillation is a method for fine-tuning the generator using GPT-4 to enhance open-source model capabilities."}, {"entity1": "Generator FT", "entity2": "RL from LLM/human feedback", "relation": "Methodology", "description": "RL from LLM/human feedback is a method for fine-tuning the generator using reinforcement learning and feedback from LLM or humans."}, {"entity1": "Dual FT", "entity2": "RAG system", "relation": "Methodology", "description": "Dual FT is a unique feature of the RAG system, fine-tuning both the retriever and the generator simultaneously."}, {"entity1": "RA-DIT", "entity2": "Dual FT", "relation": "Implementation", "description": "RA-DIT is an exemplary implementation of Dual FT, fine-tuning both the LLM and the retriever."}, {"entity1": "LM-ft", "entity2": "LLM", "relation": "Component", "description": "LM-ft is a component that updates the LLM to maximize its capabilities."}, {"entity1": "OpenAI Ada-002", "entity2": "Trainable adapter modules", "relation": "Application", "description": "OpenAI Ada-002 is an example of a model that can use trainable adapter modules for fine-tuning."}, {"entity1": "Cohere", "entity2": "Trainable adapter modules", "relation": "Application", "description": "Cohere is an example of a model that can use trainable adapter modules for fine-tuning."}, {"entity1": "GPT-4", "entity2": "Distillation", "relation": "Tool/Resource", "description": "GPT-4 is used as a tool for distillation to enhance open-source model capabilities."}, {"entity1": "PRCA", "entity2": "Trainable adapter modules", "relation": "Application", "description": "PRCA is an example of a task-specific application that can use trainable adapter modules for fine-tuning."}, {"entity1": "AAR", "entity2": "Trainable adapter modules", "relation": "Application", "description": "AAR is an example of a general-purpose application that can use trainable adapter modules for fine-tuning."}], "713068a1-920e-41eb-adad-d6cd954fdf69": [{"entity1": "Modular RAG", "entity2": "RAG technology", "relation": "Research Area", "description": "Modular RAG is a paradigm within the RAG technology field, focusing on its scalability and innovation."}, {"entity1": "Modular RAG", "entity2": "Scalability", "relation": "Research Area", "description": "Modular RAG is highly scalable, enabling researchers to introduce innovative modules and operators."}, {"entity1": "Modular RAG", "entity2": "Innovation", "relation": "Innovation", "description": "Modular RAG fosters a fertile ground for model innovation and seamless adaptation to dynamic application requirements."}, {"entity1": "RAG systems", "entity2": "Modularity", "relation": "Tool/Resource", "description": "The modularity of RAG systems simplifies their design and implementation, enhancing adaptability to diverse requirements."}, {"entity1": "Modular RAG paradigm", "entity2": "New methods", "relation": "Compatibility", "description": "Modular RAG paradigm demonstrates exceptional compatibility with new developments and provides robust support for RAG technology innovation."}, {"entity1": "DR-RAG", "entity2": "Modular RAG", "relation": "Application", "description": "DR-RAG employs a two-stage retrieval strategy, showcasing the compatibility of Modular RAG with new methods and applications."}, {"entity1": "R-ft", "entity2": "Retriever", "relation": "Methodology", "description": "R-ft updates the retriever to minimize the KL-Divergence between the retriever score distribution and the LLM preference."}, {"entity1": "SFT", "entity2": "Generator fine-tuning", "relation": "Methodology", "description": "SFT is a method for generator fine-tuning, as shown in Fig. 16."}, {"entity1": "Distillation", "entity2": "RL from LLM/human feedback", "relation": "Methodology", "description": "Distillation is a method used in conjunction with RL from LLM/human feedback for fine-tuning."}, {"entity1": "Modular RAG", "entity2": "RAG-related work", "relation": "Research Area", "description": "Modular RAG provides a fresh and comprehensive perspective on existing RAG-related work."}, {"entity1": "Modular RAG", "entity2": "Theoretical dimensions", "relation": "Theoretical Framework", "description": "Modular RAG enables the exploration of new theoretical dimensions in the field of RAG technology."}, {"entity1": "Modular RAG", "entity2": "Practical dimensions", "relation": "Application", "description": "Modular RAG allows for the exploration of new practical dimensions in the field of RAG technology, enhancing its adaptability and functionality."}], "9d1e8ccb-57a8-4309-b324-adfd4be130ac": [{"entity1": "DR-RAG", "entity2": "classifier selection mechanism", "relation": "Incorporation", "description": "DR-RAG employs a classifier selection mechanism as part of its two-stage retrieval strategy."}, {"entity1": "DR-RAG", "entity2": "branching retrieval structure", "relation": "Employment", "description": "DR-RAG incorporates a branching retrieval structure in its retrieval strategy."}, {"entity1": "DR-RAG", "entity2": "query", "relation": "Improvement", "description": "DR-RAG significantly enhances the accuracy and efficiency of answers to the query."}, {"entity1": "DR-RAG", "entity2": "RAG", "relation": "Enhancement", "description": "DR-RAG improves RAG's performance in multi-hop question-answering scenarios."}, {"entity1": "PlanRAG", "entity2": "preliminary planning stage", "relation": "Introduction", "description": "PlanRAG introduces a preliminary planning stage before retrieval and generation."}, {"entity1": "PlanRAG", "entity2": "judge module", "relation": "Employment", "description": "PlanRAG employs a judge module to assess whether a new plan is needed."}, {"entity1": "PlanRAG", "entity2": "query expansion module", "relation": "Usage", "description": "PlanRAG uses a query expansion module to extend and refine the query."}, {"entity1": "PlanRAG", "entity2": "targeted retrieval", "relation": "Conductance", "description": "PlanRAG conducts targeted retrieval for each derived sub-query."}, {"entity1": "Multi-Head RAG", "entity2": "novel flow design", "relation": "Introduction", "description": "Multi-Head RAG introduces a novel flow design through new operators."}, {"entity1": "classifier", "entity2": "dynamic documents", "relation": "Filtering", "description": "The classifier filters out the most relevant dynamic documents."}, {"entity1": "query", "entity2": "chunks", "relation": "Retrieval", "description": "The query retrieves chunks relevant to it in the first stage of DR-RAG."}, {"entity1": "retrieval strategy", "entity2": "branching retrieval structure", "relation": "Incorporation", "description": "The retrieval strategy incorporates a branching retrieval structure in DR-RAG."}, {"entity1": "RAG", "entity2": "multi-hop question-answering scenarios", "relation": "Performance", "description": "RAG's performance in multi-hop question-answering scenarios is improved by DR-RAG."}], "6173b65a-34c0-490b-b24d-95b979fd7238": [{"entity1": "Multi-Head RAG", "entity2": "RAG solutions", "relation": "Innovation", "description": "Multi-Head RAG introduces a novel flow design compared to existing RAG solutions."}, {"entity1": "Multi-Head RAG", "entity2": "complex queries", "relation": "Application", "description": "Multi-Head RAG is designed to handle complex queries that require retrieving multiple documents with significantly different content."}, {"entity1": "embeddings", "entity2": "embedding space", "relation": "Dataset-Origin", "description": "Embeddings of documents may be far apart in the embedding space."}, {"entity1": "multi-head attention layers", "entity2": "Transformer", "relation": "Methodology", "description": "The multi-head attention layers of the Transformer are used to generate keys for retrieving documents."}, {"entity1": "activation results", "entity2": "embeddings", "relation": "Result", "description": "Activation results are used to generate embeddings that represent different aspects of the data items and the query."}, {"entity1": "Multi-Head RAG", "entity2": "retrieval accuracy", "relation": "Impact", "description": "Multi-Head RAG enhances the retrieval accuracy for complex queries."}, {"entity1": "queries", "entity2": "documents", "relation": "Comparison", "description": "Queries may require retrieving multiple documents with significantly different content."}, {"entity1": "decoder layers", "entity2": "Transformer", "relation": "Methodology", "description": "Unlike Multi-Head RAG, existing solutions use the decoder layers of the Transformer for retrieving documents."}], "8047fd18-6b39-45f5-8aa3-3922c92b4851": [{"entity1": "RAG", "entity2": "LLM", "relation": "Application", "description": "RAG is emerging as a pivotal technology for LLM applications."}, {"entity1": "Modular RAG", "entity2": "Lego bricks", "relation": "Comparison", "description": "The entire system is composed of modules and operators, akin to Lego bricks."}, {"entity1": "Modular RAG", "entity2": "RAG systems", "relation": "Methodology", "description": "Modular RAG systematically disassembles the complex architecture of RAG systems into well-defined, discrete functional modules."}, {"entity1": "Modular RAG", "entity2": "RAG design patterns", "relation": "Research Area", "description": "The paper distills common RAG design patterns and scrutinizes key case studies to illustrate these patterns in practice."}, {"entity1": "Modular RAG", "entity2": "Customization", "relation": "Innovation", "description": "Modular RAG enables a scenario-based customization of RAG systems."}, {"entity1": "Modular RAG", "entity2": "Maintainability", "relation": "Impact", "description": "The modularity inherent in this design facilitates ease of tracking and debugging, significantly enhancing the maintainability and scalability of RAG systems."}, {"entity1": "Y. Zhang", "entity2": "arXiv preprint arXiv:2309.01219", "relation": "Author-Role", "description": "Y. Zhang is an author of the arXiv preprint arXiv:2309.01219."}, {"entity1": "Y. Gao", "entity2": "arXiv preprint arXiv:2312.10997", "relation": "Author-Role", "description": "Y. Gao is an author of the arXiv preprint arXiv:2312.10997."}, {"entity1": "Z. Xu", "entity2": "Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval", "relation": "Author-Role", "description": "Z. Xu is an author of a paper in the Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval."}, {"entity1": "C. Zhang", "entity2": "Companion Proceedings of the ACM on Web Conference 2024", "relation": "Author-Role", "description": "C. Zhang is an author of a paper in the Companion Proceedings of the ACM on Web Conference 2024."}, {"entity1": "R. Anantha", "entity2": "Companion Proceedings of the ACM on Web Conference 2024", "relation": "Author-Role", "description": "R. Anantha is an author of a paper in the Companion Proceedings of the ACM on Web Conference 2024."}, {"entity1": "arXiv preprint arXiv:2309.01219", "entity2": "arXiv preprint arXiv:2312.10997", "relation": "Citation", "description": "The paper cites arXiv preprint arXiv:2309.01219 and arXiv preprint arXiv:2312.10997 as references."}, {"entity1": "Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval", "entity2": "Companion Proceedings of the ACM on Web Conference 2024", "relation": "Publication Venue", "description": "Both are publication venues for papers related to RAG and LLM."}], "6744a07f-5577-409d-acb3-25e5106270d7": [{"entity1": "Notellm", "entity2": "Large language model", "relation": "Tool/Resource", "description": "Notellm is a retrievable large language model for note recommendation."}, {"entity1": "R. Anantha", "entity2": "Context tuning", "relation": "Author-Expertise", "description": "R. Anantha is an author of the paper on context tuning for retrieval augmented generation."}, {"entity1": "Y. Gao", "entity2": "Chat-rec", "relation": "Author-Expertise", "description": "Y. Gao is an author of the paper on Chat-rec, an interactive and explainable llms-augmented recommender system."}, {"entity1": "J. Liu", "entity2": "RAG applications", "relation": "Author-Expertise", "description": "J. Liu is an author of the paper on building production-ready RAG applications."}, {"entity1": "D. S. Asudani", "entity2": "Word embedding models", "relation": "Author-Expertise", "description": "D. S. Asudani is an author of the paper on the impact of word embedding models on text analytics in deep learning environment."}, {"entity1": "F. Cuconasu", "entity2": "RAG systems", "relation": "Author-Expertise", "description": "F. Cuconasu is an author of the paper on redefining retrieval for RAG systems."}, {"entity1": "W. Peng", "entity2": "Large language model", "relation": "Author-Expertise", "description": "W. Peng is an author of the paper on large language model based long-tail query rewriting in Taobao search."}, {"entity1": "Y. Xi", "entity2": "Reranking", "relation": "Author-Expertise", "description": "Y. Xi is an author of the paper on reranking from list level to page level."}, {"entity1": "Z. Feng", "entity2": "Retrieval-generation synergy", "relation": "Author-Expertise", "description": "Z. Feng is an author of the paper on retrieval-generation synergy augmented large language models."}, {"entity1": "G. Kim", "entity2": "Tree of clarifications", "relation": "Author-Expertise", "description": "G. Kim is an author of the paper on tree of clarifications for answering ambiguous questions with retrieval-augmented large language models."}, {"entity1": "Z. Jiang", "entity2": "Active retrieval", "relation": "Author-Expertise", "description": "Z. Jiang is an author of the paper on active retrieval augmented generation."}, {"entity1": "D. Edge", "entity2": "Graph RAG", "relation": "Author-Expertise", "description": "D. Edge is an author of the paper on graph RAG approach to query-focused summarization."}, {"entity1": "Q. Leng", "entity2": "LLM evaluation", "relation": "Author-Expertise", "description": "Q. Leng is an author of the paper on best practices for LLM evaluation of RAG applications."}, {"entity1": "Notellm", "entity2": "ACM", "relation": "Publication Venue", "description": "Notellm is published in the Companion Proceedings of the ACM on Web Conference 2024."}, {"entity1": "R. Anantha", "entity2": "arXiv", "relation": "Publication Venue", "description": "R. Anantha's paper on context tuning is published on arXiv."}, {"entity1": "Y. Gao", "entity2": "arXiv", "relation": "Publication Venue", "description": "Y. Gao's paper on Chat-rec is published on arXiv."}, {"entity1": "J. Liu", "entity2": "AI Engineer", "relation": "Publication Venue", "description": "J. Liu's paper on building production-ready RAG applications is published on AI Engineer."}, {"entity1": "D. S. Asudani", "entity2": "Artificial intelligence review", "relation": "Publication Venue", "description": "D. S. Asudani's paper on word embedding models is published in Artificial intelligence review."}, {"entity1": "F. Cuconasu", "entity2": "arXiv", "relation": "Publication Venue", "description": "F. Cuconasu's paper on redefining retrieval for RAG systems is published on arXiv."}, {"entity1": "W. Peng", "entity2": "arXiv", "relation": "Publication Venue", "description": "W. Peng's paper on large language model based long-tail query rewriting is published on arXiv."}, {"entity1": "Y. Xi", "entity2": "ACM", "relation": "Publication Venue", "description": "Y. Xi's paper on reranking is published in the Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining."}, {"entity1": "Z. Feng", "entity2": "arXiv", "relation": "Publication Venue", "description": "Z. Feng's paper on retrieval-generation synergy augmented large language models is published on arXiv."}, {"entity1": "G. Kim", "entity2": "arXiv", "relation": "Publication Venue", "description": "G. Kim's paper on tree of clarifications is published on arXiv."}, {"entity1": "Z. Jiang", "entity2": "arXiv", "relation": "Publication Venue", "description": "Z. Jiang's paper on active retrieval augmented generation is published on arXiv."}, {"entity1": "D. Edge", "entity2": "arXiv", "relation": "Publication Venue", "description": "D. Edge's paper on graph RAG approach is published on arXiv."}, {"entity1": "Q. Leng", "entity2": "Databricks", "relation": "Publication Venue", "description": "Q. Leng's paper on best practices for LLM evaluation is published on Databricks."}, {"entity1": "Notellm", "entity2": "Note recommendation", "relation": "Application", "description": "Notellm is applied to note recommendation."}, {"entity1": "R. Anantha", "entity2": "Retrieval-augmented generation", "relation": "Research Area", "description": "R. Anantha's research area is retrieval-augmented generation."}, {"entity1": "Y. Gao", "entity2": "LLMs-augmented recommender system", "relation": "Research Area", "description": "Y. Gao's research area is LLMS-augmented recommender system."}, {"entity1": "J. Liu", "entity2": "RAG applications", "relation": "Research Area", "description": "J. Liu's research area is RAG applications."}, {"entity1": "D. S. Asudani", "entity2": "Text analytics", "relation": "Research Area", "description": "D. S. Asudani's research area is text analytics."}, {"entity1": "F. Cuconasu", "entity2": "RAG systems", "relation": "Research Area", "description": "F. Cuconasu's research area is RAG systems."}, {"entity1": "W. Peng", "entity2": "Long-tail query rewriting", "relation": "Research Area", "description": "W. Peng's research area is long-tail query rewriting."}, {"entity1": "Y. Xi", "entity2": "Reranking", "relation": "Research Area", "description": "Y. Xi's research area is reranking."}, {"entity1": "Z. Feng", "entity2": "Retrieval-generation synergy", "relation": "Research Area", "description": "Z. Feng's research area is retrieval-generation synergy."}, {"entity1": "G. Kim", "entity2": "Tree of clarifications", "relation": "Research Area", "description": "G. Kim's research area is tree of clarifications."}, {"entity1": "Z. Jiang", "entity2": "Active retrieval", "relation": "Research Area", "description": "Z. Jiang's research area is active retrieval."}, {"entity1": "D. Edge", "entity2": "Graph RAG", "relation": "Research Area", "description": "D. Edge's research area is graph RAG."}, {"entity1": "Q. Leng", "entity2": "LLM evaluation", "relation": "Research Area", "description": "Q. Leng's research area is LLM evaluation."}], "d5e5f4da-fed7-4d84-8058-eaeb1e61e635": [{"entity1": "L. Luo", "entity2": "Y.-F. Li", "relation": "Co-author", "description": "L. Luo and Y.-F. Li are co-authors of the paper \"Reasoning on graphs: Faithful and interpretable large language model reasoning\"."}, {"entity1": "X. V. Lin", "entity2": "X. Chen", "relation": "Co-author", "description": "X. V. Lin and X. Chen are co-authors of the paper \"Ra-dit: Retrieval-augmented dual instruction tuning\"."}, {"entity1": "A. Asai", "entity2": "Z. Wu", "relation": "Co-author", "description": "A. Asai and Z. Wu are co-authors of the paper \"Self-rag: Learning to retrieve, generate, and critique through self-reflection\"."}, {"entity1": "Y. Huang", "entity2": "J. Huang", "relation": "Co-author", "description": "Y. Huang and J. Huang are co-authors of the paper \"A survey on retrieval-augmented text generation for large language models\"."}, {"entity1": "Y. Hu", "entity2": "Y. Lu", "relation": "Co-author", "description": "Y. Hu and Y. Lu are co-authors of the paper \"Rag and rau: A survey on retrieval-augmented language model in natural language processing\"."}, {"entity1": "Y. Ding", "entity2": "W. Fan", "relation": "Co-author", "description": "Y. Ding and W. Fan are co-authors of the paper \"A survey on rag meets llms: Towards retrieval-augmented large language models\"."}, {"entity1": "P. Zhao", "entity2": "H. Zhang", "relation": "Co-author", "description": "P. Zhao and H. Zhang are co-authors of the paper \"Retrieval-augmented generation for ai-generated content: A survey\"."}, {"entity1": "arXiv:2310.01061", "entity2": "L. Luo", "relation": "Author-Role", "description": "L. Luo is an author of the paper with arXiv ID arXiv:2310.01061."}, {"entity1": "arXiv:2310.01352", "entity2": "X. V. Lin", "relation": "Author-Role", "description": "X. V. Lin is an author of the paper with arXiv ID arXiv:2310.01352."}, {"entity1": "arXiv:2310.11511", "entity2": "A. Asai", "relation": "Author-Role", "description": "A. Asai is an author of the paper with arXiv ID arXiv:2310.11511."}, {"entity1": "arXiv:2404.10981", "entity2": "Y. Huang", "relation": "Author-Role", "description": "Y. Huang is an author of the paper with arXiv ID arXiv:2404.10981."}, {"entity1": "arXiv:2404.19543", "entity2": "Y. Hu", "relation": "Author-Role", "description": "Y. Hu is an author of the paper with arXiv ID arXiv:2404.19543."}, {"entity1": "arXiv:2405.06211", "entity2": "Y. Ding", "relation": "Author-Role", "description": "Y. Ding is an author of the paper with arXiv ID arXiv:2405.06211."}, {"entity1": "arXiv:2402.19473", "entity2": "P. Zhao", "relation": "Author-Role", "description": "P. Zhao is an author of the paper with arXiv ID arXiv:2402.19473."}, {"entity1": "https://towardsdatascience.com/advanced-rag-01-small-to-big-retrieval-172181b396d4", "entity2": "S. Yang", "relation": "Author-Role", "description": "S. Yang is the author of the blog post with the given URL."}, {"entity1": "https://aclanthology.org/2022.amta-upg.14", "entity2": "Translation in the Americas", "relation": "Publication Venue", "description": "The paper \"Translation in the Americas\" is published at the venue with the given URL."}], "6dbe68f0-083e-4fea-baa5-a0d0588a4d53": [{"entity1": "Y. Wang", "entity2": "N. Lipka", "relation": "Co-author", "description": "Y. Wang and N. Lipka are co-authors of the paper 'Knowledge graph prompting for multi-document question answering'."}, {"entity1": "N. Lipka", "entity2": "arXiv", "relation": "Publication Venue", "description": "N. Lipka's paper 'Knowledge graph prompting for multi-document question answering' is published on arXiv."}, {"entity1": "D. Zhou", "entity2": "N. Sch\u00e4rli", "relation": "Co-author", "description": "D. Zhou and N. Sch\u00e4rli are co-authors of the paper 'Least-to-most prompting enables complex reasoning in large language models'."}, {"entity1": "S. Dhuliawala", "entity2": "M. Komeili", "relation": "Co-author", "description": "S. Dhuliawala and M. Komeili are co-authors of the paper 'Chain-of-verification reduces hallucination in large language models'."}, {"entity1": "L. Gao", "entity2": "X. Ma", "relation": "Co-author", "description": "L. Gao and X. Ma are co-authors of the paper 'Precise zero-shot dense retrieval without relevance labels'."}, {"entity1": "H. S. Zheng", "entity2": "S. Mishra", "relation": "Co-author", "description": "H. S. Zheng and S. Mishra are co-authors of the paper 'Take a step back: Evoking reasoning via abstraction in large language models'."}, {"entity1": "H. Cao", "entity2": "arXiv", "relation": "Publication Venue", "description": "H. Cao's paper 'Recent advances in text embedding: A comprehensive review of top-performing methods on the mteb benchmark' is published on arXiv."}, {"entity1": "BAAI", "entity2": "Flagembedding", "relation": "Tool/Resource", "description": "BAAI is associated with the tool/resource Flagembedding."}, {"entity1": "Z. Li", "entity2": "X. Zhang", "relation": "Co-author", "description": "Z. Li and X. Zhang are co-authors of the paper 'Towards general text embeddings with multi-stage contrastive learning'."}, {"entity1": "H. Yang", "entity2": "Z. Li", "relation": "Co-author", "description": "H. Yang and Z. Li are co-authors of the paper 'Prca: Fitting black-box large language models for retrieval question answering via pluggable reward-driven contextual adapter'."}, {"entity1": "N. F. Liu", "entity2": "K. Lin", "relation": "Co-author", "description": "N. F. Liu and K. Lin are co-authors of the paper 'Lost in the middle: How language models use long contexts'."}, {"entity1": "Y. Lyu", "entity2": "Z. Li", "relation": "Co-author", "description": "Y. Lyu and Z. Li are co-authors of the paper 'Crud-rag: A comprehensive chinese benchmark for retrieval-augmented generation of large language models'."}, {"entity1": "L. Xia", "entity2": "J. Xu", "relation": "Co-author", "description": "L. Xia and J. Xu are co-authors of the paper 'Learning maximal marginal relevance model via directly optimizing diversity evaluation measures'."}, {"entity1": "Cohere", "entity2": "Cohere rerank", "relation": "Tool/Resource", "description": "Cohere is associated with the tool/resource Cohere rerank."}], "ab38c414-38e7-4475-a73c-969d8c30fbf3": [{"entity1": "Cohere", "entity2": "Cohere rerank", "relation": "Tool/Resource", "description": "Cohere introduced Cohere rerank, a tool for improving search results."}, {"entity1": "H. Jiang", "entity2": "Longllmlingua", "relation": "Author-Role", "description": "H. Jiang is an author of the Longllmlingua paper."}, {"entity1": "R. Litman", "entity2": "Scatter", "relation": "Author-Role", "description": "R. Litman is an author of the Scatter paper."}, {"entity1": "J. Cui", "entity2": "Chatlaw", "relation": "Author-Role", "description": "J. Cui is an author of the Chatlaw paper."}, {"entity1": "T. Schick", "entity2": "Toolformer", "relation": "Author-Role", "description": "T. Schick is an author of the Toolformer paper."}, {"entity1": "L. Ouyang", "entity2": "Training language models to follow instructions with human feedback", "relation": "Author-Role", "description": "L. Ouyang is an author of the paper on training language models with human feedback."}, {"entity1": "S. J. Semnani", "entity2": "Wikichat", "relation": "Author-Role", "description": "S. J. Semnani is an author of the Wikichat paper."}, {"entity1": "J. Baek", "entity2": "Knowledge-augmented language model verification", "relation": "Author-Role", "description": "J. Baek is an author of the paper on knowledge-augmented language model verification."}, {"entity1": "G. V. Cormack", "entity2": "Reciprocal rank fusion outperforms condorcet and individual rank learning methods", "relation": "Author-Role", "description": "G. V. Cormack is an author of the paper on reciprocal rank fusion."}, {"entity1": "W. Shi", "entity2": "Replug", "relation": "Author-Role", "description": "W. Shi is an author of the Replug paper."}, {"entity1": "Z. Shao", "entity2": "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy", "relation": "Author-Role", "description": "Z. Shao is an author of the paper on enhancing retrieval-augmented large language models."}, {"entity1": "S. Hong", "entity2": "Metagpt", "relation": "Author-Role", "description": "S. Hong is an author of the Metagpt paper."}, {"entity1": "Longllmlingua", "entity2": "arXiv", "relation": "Publication Venue", "description": "The Longllmlingua paper was published on arXiv."}, {"entity1": "Scatter", "entity2": "IEEE/CVF conference on computer vision and pattern recognition", "relation": "Publication Venue", "description": "The Scatter paper was published in the IEEE/CVF conference on computer vision and pattern recognition."}, {"entity1": "Chatlaw", "entity2": "arXiv", "relation": "Publication Venue", "description": "The Chatlaw paper was published on arXiv."}, {"entity1": "Toolformer", "entity2": "arXiv", "relation": "Publication Venue", "description": "The Toolformer paper was published on arXiv."}, {"entity1": "Training language models to follow instructions with human feedback", "entity2": "Advances in neural information processing systems", "relation": "Publication Venue", "description": "The paper on training language models with human feedback was published in Advances in neural information processing systems."}, {"entity1": "Wikichat", "entity2": "arXiv", "relation": "Publication Venue", "description": "The Wikichat paper was published on arXiv."}, {"entity1": "Knowledge-augmented language model verification", "entity2": "arXiv", "relation": "Publication Venue", "description": "The paper on knowledge-augmented language model verification was published on arXiv."}, {"entity1": "Reciprocal rank fusion outperforms condorcet and individual rank learning methods", "entity2": "Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval", "relation": "Publication Venue", "description": "The paper on reciprocal rank fusion was published in the Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval."}, {"entity1": "Replug", "entity2": "arXiv", "relation": "Publication Venue", "description": "The Replug paper was published on arXiv."}, {"entity1": "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy", "entity2": "arXiv", "relation": "Publication Venue", "description": "The paper on enhancing retrieval-augmented large language models was published on arXiv."}, {"entity1": "Metagpt", "entity2": "arXiv", "relation": "Publication Venue", "description": "The Metagpt paper was published on arXiv."}], "18a25bbc-a6a1-4b5b-9086-9d5178e2e4ee": [{"entity1": "S. Hong", "entity2": "Metagpt", "relation": "Author-Role", "description": "S. Hong is an author of the Metagpt paper."}, {"entity1": "X. Zheng", "entity2": "Metagpt", "relation": "Author-Role", "description": "X. Zheng is an author of the Metagpt paper."}, {"entity1": "J. Chen", "entity2": "Metagpt", "relation": "Author-Role", "description": "J. Chen is an author of the Metagpt paper."}, {"entity1": "Y. Cheng", "entity2": "Metagpt", "relation": "Author-Role", "description": "Y. Cheng is an author of the Metagpt paper."}, {"entity1": "J. Wang", "entity2": "Metagpt", "relation": "Author-Role", "description": "J. Wang is an author of the Metagpt paper."}, {"entity1": "C. Zhang", "entity2": "Metagpt", "relation": "Author-Role", "description": "C. Zhang is an author of the Metagpt paper."}, {"entity1": "Z. Wang", "entity2": "Metagpt", "relation": "Author-Role", "description": "Z. Wang is an author of the Metagpt paper."}, {"entity1": "S. K. S. Yau", "entity2": "Metagpt", "relation": "Author-Role", "description": "S. K. S. Yau is an author of the Metagpt paper."}, {"entity1": "Z. Lin", "entity2": "Metagpt", "relation": "Author-Role", "description": "Z. Lin is an author of the Metagpt paper."}, {"entity1": "L. Zhou", "entity2": "Metagpt", "relation": "Author-Role", "description": "L. Zhou is an author of the Metagpt paper."}, {"entity1": "Z. Yu", "entity2": "Augmentation-adapted retriever", "relation": "Author-Role", "description": "Z. Yu is an author of the Augmentation-adapted retriever paper."}, {"entity1": "C. Xiong", "entity2": "Augmentation-adapted retriever", "relation": "Author-Role", "description": "C. Xiong is an author of the Augmentation-adapted retriever paper."}, {"entity1": "S. Yu", "entity2": "Augmentation-adapted retriever", "relation": "Author-Role", "description": "S. Yu is an author of the Augmentation-adapted retriever paper."}, {"entity1": "Z. Liu", "entity2": "Augmentation-adapted retriever", "relation": "Author-Role", "description": "Z. Liu is an author of the Augmentation-adapted retriever paper."}, {"entity1": "Z. Hei", "entity2": "Dr-rag", "relation": "Author-Role", "description": "Z. Hei is an author of the Dr-rag paper."}, {"entity1": "W. Wei", "entity2": "Dr-rag", "relation": "Author-Role", "description": "W. Wei is an author of the Dr-rag paper."}, {"entity1": "W. Ou", "entity2": "Dr-rag", "relation": "Author-Role", "description": "W. Ou is an author of the Dr-rag paper."}, {"entity1": "J. Qiao", "entity2": "Dr-rag", "relation": "Author-Role", "description": "J. Qiao is an author of the Dr-rag paper."}, {"entity1": "J. Jiao", "entity2": "Dr-rag", "relation": "Author-Role", "description": "J. Jiao is an author of the Dr-rag paper."}, {"entity1": "Z. Zhu", "entity2": "Dr-rag", "relation": "Author-Role", "description": "Z. Zhu is an author of the Dr-rag paper."}, {"entity1": "G. Song", "entity2": "Dr-rag", "relation": "Author-Role", "description": "G. Song is an author of the Dr-rag paper."}, {"entity1": "M. Besta", "entity2": "Multi-head rag", "relation": "Author-Role", "description": "M. Besta is an author of the Multi-head rag paper."}, {"entity1": "A. Kubicek", "entity2": "Multi-head rag", "relation": "Author-Role", "description": "A. Kubicek is an author of the Multi-head rag paper."}, {"entity1": "R. Niggli", "entity2": "Multi-head rag", "relation": "Author-Role", "description": "R. Niggli is an author of the Multi-head rag paper."}, {"entity1": "R. Gerstenberger", "entity2": "Multi-head rag", "relation": "Author-Role", "description": "R. Gerstenberger is an author of the Multi-head rag paper."}, {"entity1": "L. Weitzen-dorf", "entity2": "Multi-head rag", "relation": "Author-Role", "description": "L. Weitzen-dorf is an author of the Multi-head rag paper."}, {"entity1": "M. Chi", "entity2": "Multi-head rag", "relation": "Author-Role", "description": "M. Chi is an author of the Multi-head rag paper."}, {"entity1": "P. Iff", "entity2": "Multi-head rag", "relation": "Author-Role", "description": "P. Iff is an author of the Multi-head rag paper."}, {"entity1": "J. Gajda", "entity2": "Multi-head rag", "relation": "Author-Role", "description": "J. Gajda is an author of the Multi-head rag paper."}, {"entity1": "P. Nyczyk", "entity2": "Multi-head rag", "relation": "Author-Role", "description": "P. Nyczyk is an author of the Multi-head rag paper."}, {"entity1": "J. M \u00a8uller", "entity2": "Multi-head rag", "relation": "Author-Role", "description": "J. M \u00a8uller is an author of the Multi-head rag paper."}, {"entity1": "Metagpt", "entity2": "arXiv:2308.00352", "relation": "Publication Venue", "description": "Metagpt is published on arXiv:2308.00352."}, {"entity1": "Augmentation-adapted retriever", "entity2": "arXiv:2305.17331", "relation": "Publication Venue", "description": "Augmentation-adapted retriever is published on arXiv:2305.17331."}, {"entity1": "Dr-rag", "entity2": "arXiv:2406.07348", "relation": "Publication Venue", "description": "Dr-rag is published on arXiv:2406.07348."}, {"entity1": "Multi-head rag", "entity2": "arXiv:2406.05085", "relation": "Publication Venue", "description": "Multi-head rag is published on arXiv:2406.05085."}], "6260ba66-47fd-44af-98b4-82722a1e6a73": [{"entity1": "Scott Barnett", "entity2": "Applied Artificial Intelligence Institute", "relation": "Affiliation", "description": "Scott Barnett is affiliated with the Applied Artificial Intelligence Institute."}, {"entity1": "Stefanus Kurniawan", "entity2": "Applied Artificial Intelligence Institute", "relation": "Affiliation", "description": "Stefanus Kurniawan is affiliated with the Applied Artificial Intelligence Institute."}, {"entity1": "Srikanth Thudumu", "entity2": "Applied Artificial Intelligence Institute", "relation": "Affiliation", "description": "Srikanth Thudumu is affiliated with the Applied Artificial Intelligence Institute."}, {"entity1": "Zach Brannelly", "entity2": "Applied Artificial Intelligence Institute", "relation": "Affiliation", "description": "Zach Brannelly is affiliated with the Applied Artificial Intelligence Institute."}, {"entity1": "Mohamed Abdelrazek", "entity2": "Applied Artificial Intelligence Institute", "relation": "Affiliation", "description": "Mohamed Abdelrazek is affiliated with the Applied Artificial Intelligence Institute."}, {"entity1": "Retrieval Augmented Generation", "entity2": "Large Language Model", "relation": "Methodology", "description": "Retrieval Augmented Generation involves using a Large Language Model to extract answers from documents."}, {"entity1": "Retrieval Augmented Generation", "entity2": "ChatGPT", "relation": "Tool/Resource", "description": "Retrieval Augmented Generation can use ChatGPT as a Large Language Model."}, {"entity1": "Seven Failure Points When Engineering a Retrieval Augmented Generation System", "entity2": "CAIN 2024", "relation": "Publication Venue", "description": "The paper 'Seven Failure Points When Engineering a Retrieval Augmented Generation System' was published at CAIN 2024."}, {"entity1": "Scott Barnett", "entity2": "Stefanus Kurniawan", "relation": "Co-author", "description": "Scott Barnett and Stefanus Kurniawan are co-authors of the paper."}, {"entity1": "Scott Barnett", "entity2": "Srikanth Thudumu", "relation": "Co-author", "description": "Scott Barnett and Srikanth Thudumu are co-authors of the paper."}, {"entity1": "Scott Barnett", "entity2": "Zach Brannelly", "relation": "Co-author", "description": "Scott Barnett and Zach Brannelly are co-authors of the paper."}, {"entity1": "Scott Barnett", "entity2": "Mohamed Abdelrazek", "relation": "Co-author", "description": "Scott Barnett and Mohamed Abdelrazek are co-authors of the paper."}, {"entity1": "Stefanus Kurniawan", "entity2": "Srikanth Thudumu", "relation": "Co-author", "description": "Stefanus Kurniawan and Srikanth Thudumu are co-authors of the paper."}, {"entity1": "Stefanus Kurniawan", "entity2": "Zach Brannelly", "relation": "Co-author", "description": "Stefanus Kurniawan and Zach Brannelly are co-authors of the paper."}, {"entity1": "Stefanus Kurniawan", "entity2": "Mohamed Abdelrazek", "relation": "Co-author", "description": "Stefanus Kurniawan and Mohamed Abdelrazek are co-authors of the paper."}, {"entity1": "Srikanth Thudumu", "entity2": "Zach Brannelly", "relation": "Co-author", "description": "Srikanth Thudumu and Zach Brannelly are co-authors of the paper."}, {"entity1": "Srikanth Thudumu", "entity2": "Mohamed Abdelrazek", "relation": "Co-author", "description": "Srikanth Thudumu and Mohamed Abdelrazek are co-authors of the paper."}, {"entity1": "Zach Brannelly", "entity2": "Mohamed Abdelrazek", "relation": "Co-author", "description": "Zach Brannelly and Mohamed Abdelrazek are co-authors of the paper."}, {"entity1": "Applied Artificial Intelligence Institute", "entity2": "Geelong", "relation": "Affiliation", "description": "The Applied Artificial Intelligence Institute is located in Geelong, Australia."}, {"entity1": "CAIN 2024", "entity2": "ACM", "relation": "Publication Venue", "description": "CAIN 2024 is a conference organized by ACM."}, {"entity1": "Seven Failure Points When Engineering a Retrieval Augmented Generation System", "entity2": "ACM", "relation": "Publication Venue", "description": "The paper 'Seven Failure Points When Engineering a Retrieval Augmented Generation System' was published by ACM."}, {"entity1": "Retrieval Augmented Generation", "entity2": "SE4AI", "relation": "Research Area", "description": "Retrieval Augmented Generation is a research area related to SE4AI."}, {"entity1": "Scott Barnett", "entity2": "SE4AI", "relation": "Research Area", "description": "Scott Barnett's research area includes SE4AI."}, {"entity1": "Stefanus Kurniawan", "entity2": "SE4AI", "relation": "Research Area", "description": "Stefanus Kurniawan's research area includes SE4AI."}, {"entity1": "Srikanth Thudumu", "entity2": "SE4AI", "relation": "Research Area", "description": "Srikanth Thudumu's research area includes SE4AI."}, {"entity1": "Zach Brannelly", "entity2": "SE4AI", "relation": "Research Area", "description": "Zach Brannelly's research area includes SE4AI."}, {"entity1": "Mohamed Abdelrazek", "entity2": "SE4AI", "relation": "Research Area", "description": "Mohamed Abdelrazek's research area includes SE4AI."}], "da762418-8d3f-4e29-b574-05ab7df795cb": [{"entity1": "Association for Computing Machinery", "entity2": "permissions@acm.org", "relation": "Affiliation", "description": "The Association for Computing Machinery is affiliated with the contact email permissions@acm.org for requesting permissions."}, {"entity1": "Retrieval-Augmented Generation (RAG) systems", "entity2": "LLMs", "relation": "Methodology", "description": "RAG systems rely on LLMs for the generation of answers using existing knowledge artifacts."}, {"entity1": "CAIN 2024", "entity2": "Lisbon, Portugal", "relation": "Publication Venue", "description": "CAIN 2024 is held in Lisbon, Portugal."}, {"entity1": "ACM", "entity2": "https://doi.org/10.1145/nnnnnnn.nnnnnnn", "relation": "Publication", "description": "ACM publishes content available at the specified DOI."}, {"entity1": "Software engineers", "entity2": "RAG systems", "relation": "Author-Role", "description": "Software engineers are involved in building RAG systems."}, {"entity1": "Finetuning LLMs", "entity2": "LLMs", "relation": "Methodology", "description": "Finetuning is a method applied to LLMs to continue their training with domain-specific artifacts."}, {"entity1": "Retrieval-Augmented Generation (RAG) Systems", "entity2": "query-artifact matching", "relation": "Methodology", "description": "RAG systems involve query-artifact matching as part of their functionality."}, {"entity1": "vector database", "entity2": "RAG systems", "relation": "Tool/Resource", "description": "A vector database is used in RAG systems to store processed information."}, {"entity1": "Association for Computing Machinery", "entity2": "CAIN 2024", "relation": "Institution-Collaboration", "description": "The Association for Computing Machinery is involved with CAIN 2024, indicating a form of collaboration or sponsorship."}, {"entity1": "LLMs", "entity2": "HCI", "relation": "Application", "description": "LLMs can be applied to build new HCI solutions."}, {"entity1": "RAG systems", "entity2": "HCI", "relation": "Application", "description": "RAG systems can be used to build new HCI solutions by providing up-to-date and domain-specific knowledge."}, {"entity1": "April 2024", "entity2": "CAIN 2024", "relation": "Publication Date", "description": "CAIN 2024 is scheduled for April 2024."}], "5b59df86-9ca8-40cf-8d6a-c560447d1f71": [{"entity1": "RAG systems", "entity2": "LLMs", "relation": "Integration", "description": "RAG systems integrate LLMs API for user queries and context documents."}, {"entity1": "vector data-base", "entity2": "processed information", "relation": "Storage", "description": "Processed information is stored in a vector data-base."}, {"entity1": "query-artifact matching strategy", "entity2": "RAG systems", "relation": "Implementation", "description": "The right query-artifact matching strategy is implemented or integrated in RAG systems."}, {"entity1": "RAG systems", "entity2": "robustness", "relation": "Objective", "description": "The objective of RAG systems is to achieve robustness."}, {"entity1": "software engineering community", "entity2": "LLMs", "relation": "Application", "description": "The software engineering community applies LLMs to realise robust systems."}, {"entity1": "BioASQ data set", "entity2": "empirical experiment", "relation": "Dataset-Origin", "description": "The BioASQ data set is used as the origin for an empirical experiment."}, {"entity1": "empirical experiment", "entity2": "15,000 documents", "relation": "Experiment-Design", "description": "The empirical experiment involves 15,000 documents."}, {"entity1": "empirical experiment", "entity2": "1000 question", "relation": "Experiment-Design", "description": "The empirical experiment involves 1000 questions."}, {"entity1": "RAG systems", "entity2": "research road map", "relation": "Theoretical Framework", "description": "A research road map is presented for RAG systems."}, {"entity1": "arXiv:2401.05856v1", "entity2": "cs.SE", "relation": "Publication Venue", "description": "The paper arXiv:2401.05856v1 is published in cs.SE."}, {"entity1": "arXiv:2401.05856v1", "entity2": "January 11, 2024", "relation": "Publication Date", "description": "The paper arXiv:2401.05856v1 was published on January 11, 2024."}], "89e26c92-3080-4c67-96b5-9f5a1efe4048": [{"entity1": "Scott Barnett", "entity2": "Stefanus Kurniawan", "relation": "Co-author", "description": "Scott Barnett and Stefanus Kurniawan are co-authors of the paper presented at CAIN 2024."}, {"entity1": "Srikanth Thudumu", "entity2": "Zach Brannelly", "relation": "Co-author", "description": "Srikanth Thudumu and Zach Brannelly are co-authors of the paper presented at CAIN 2024."}, {"entity1": "Mohamed Abdelrazek", "entity2": "CAIN 2024", "relation": "Author-Role", "description": "Mohamed Abdelrazek is an author of a paper presented at CAIN 2024."}, {"entity1": "GPT-4", "entity2": "OpenAI", "relation": "Tool/Resource", "description": "GPT-4 is a tool/resource provided by OpenAI used for generating responses."}, {"entity1": "Deakin University", "entity2": "RAG system", "relation": "Institution-Collaboration", "description": "Deakin University is involved in the implementation of a RAG system."}, {"entity1": "Retrieval augmented generation", "entity2": "large language models", "relation": "Methodology", "description": "Retrieval augmented generation is a methodology that uses large language models."}, {"entity1": "CAIN 2024", "entity2": "Lisbon", "relation": "Publication Venue", "description": "CAIN 2024 is a publication venue located in Lisbon, Portugal."}, {"entity1": "RAG system", "entity2": "software engineering", "relation": "Application", "description": "RAG system has an application in software engineering."}, {"entity1": "query rewriting", "entity2": "document re-ranking", "relation": "Comparison", "description": "Query rewriting and document re-ranking are compared as challenges in RAG systems."}, {"entity1": "content summarisation", "entity2": "information extraction", "relation": "Challenge", "description": "Content summarisation and information extraction are challenges in RAG systems."}, {"entity1": "April 2024", "entity2": "CAIN 2024", "relation": "Publication Date", "description": "April 2024 is the publication date of CAIN 2024."}], "f651ad4d-d280-4f80-8750-75fe1a470454": [{"entity1": "RAG system", "entity2": "information retrieval approach", "relation": "Application", "description": "A RAG system is an information retrieval approach designed to overcome the limitations of using a LLM directly."}, {"entity1": "large language models", "entity2": "hallucinations", "relation": "Limitation", "description": "Large language models can produce hallucinations, which are responses that look right but are incorrect."}, {"entity1": "RAG system", "entity2": "large language model", "relation": "Methodology", "description": "A RAG system works by taking a natural language query, converting it into an embedding, and using it to semantically search a set of documents, which are then passed to a large language model to generate an answer."}, {"entity1": "embedding model", "entity2": "document", "relation": "Tool/Resource", "description": "An embedding model is used to convert chunks of a document into an embedding, which provides a compressed semantic representation of the document."}, {"entity1": "software engineers", "entity2": "design decisions", "relation": "Author-Role", "description": "Software engineers face design decisions around how best to chunk the document and how large a chunk should be."}, {"entity1": "chunking", "entity2": "document", "relation": "Methodology", "description": "Chunking is a process of splitting a document into smaller chunks that are converted into an embedding using an embedding model."}, {"entity1": "video content", "entity2": "transcription pipeline", "relation": "Data Collection", "description": "Video content requires a transcription pipeline to extract the audio and convert to text prior to encoding."}, {"entity1": "embedding strategy", "entity2": "re-indexing", "relation": "Limitation", "description": "Changing the embedding strategy requires re-indexing all chunks, which can be a limitation."}, {"entity1": "RAG system", "entity2": "Figure 1", "relation": "Publication Venue", "description": "An overview of a RAG system is shown in Figure 1 as two separate processes, Index and Query."}, {"entity1": "ChatGPT2", "entity2": "Bard", "relation": "Comparison", "description": "ChatGPT2, Claude, and Bard are large language model services that have been explored for use as question and answering systems."}, {"entity1": "information retrieval systems", "entity2": "content summarisation", "relation": "Research Area", "description": "Other information retrieval systems have been explored for query rewriting, document re-ranking, and effective content summarisation."}, {"entity1": "large language models", "entity2": "factual accuracy", "relation": "Evaluation", "description": "The unique aspects of large language models include evaluating factual accuracy."}], "c82d686c-12ef-47d6-8da1-3369473e3b9e": [{"entity1": "Query Process", "entity2": "natural language", "relation": "Input", "description": "The Query Process takes a question expressed in natural language as input."}, {"entity1": "Query Process", "entity2": "general query", "relation": "Transformation", "description": "The Query Process transforms a natural language question into a general query."}, {"entity1": "general query", "entity2": "large language model", "relation": "Utilization", "description": "A large language model is used to generalize the query."}, {"entity1": "large language model", "entity2": "previous chat history", "relation": "Incorporation", "description": "The large language model incorporates previous chat history to generalize the query."}, {"entity1": "general query", "entity2": "embedding", "relation": "Calculation", "description": "An embedding is calculated from the generalized query."}, {"entity1": "embedding", "entity2": "vector databases", "relation": "Utilization", "description": "The embedding is used to locate relevant documents from vector databases."}, {"entity1": "vector databases", "entity2": "inverted indexes", "relation": "Technique", "description": "Vector databases use techniques such as inverted indexes to speed up retrieval time."}, {"entity1": "Query Process", "entity2": "cosine similarity", "relation": "Method", "description": "The Query Process uses cosine similarity to retrieve top-k similar documents."}, {"entity1": "retrieved documents", "entity2": "Consolidator", "relation": "Input", "description": "The retrieved documents are processed by the Consolidator."}, {"entity1": "Consolidator", "entity2": "chunks", "relation": "Processing", "description": "The Consolidator processes the chunks to overcome limitations of large language models."}, {"entity1": "large language models", "entity2": "token limit", "relation": "Limitation", "description": "Large language models have a token limit that restricts the amount of text to include in a prompt."}, {"entity1": "large language models", "entity2": "rate limit", "relation": "Limitation", "description": "Large language models have a rate limit that restricts the number of tokens to use within a time frame."}, {"entity1": "OpenAI", "entity2": "rate limit", "relation": "Restriction", "description": "OpenAI restricts the number of tokens to use within a time frame."}, {"entity1": "Software engineers", "entity2": "RAG system", "relation": "Design", "description": "Software engineers need to consider tradeoffs when designing a RAG system."}, {"entity1": "RAG system", "entity2": "large language models", "relation": "Utilization", "description": "A RAG system utilizes large language models."}], "9fc2e929-773f-4f6d-a24f-fc5ffff91c53": [{"entity1": "CAIN 2024", "entity2": "April 2024", "relation": "Publication Date", "description": "CAIN 2024 was published in April 2024"}, {"entity1": "CAIN 2024", "entity2": "Lisbon, Portugal", "relation": "Publication Venue", "description": "CAIN 2024 was held in Lisbon, Portugal"}, {"entity1": "Retrieval Augmented Generation System", "entity2": "Indexing", "relation": "Methodology", "description": "Indexing is a process required for creating a Retrieval Augmented Generation System"}, {"entity1": "Retrieval Augmented Generation System", "entity2": "Query", "relation": "Methodology", "description": "Query is a process required for creating a Retrieval Augmented Generation System"}, {"entity1": "RAG", "entity2": "Development time", "relation": "Experiment-Design", "description": "The indexing process in RAG is typically done at development time"}, {"entity1": "RAG", "entity2": "Runtime", "relation": "Experiment-Design", "description": "The query process in RAG is typically done at runtime"}, {"entity1": "Cognitive Reviewer", "entity2": "Deakin University", "relation": "Institution-Collaboration", "description": "Cognitive Reviewer is used by PhD students from Deakin University"}, {"entity1": "Cognitive Reviewer", "entity2": "PhD students", "relation": "Author-Role", "description": "PhD students use Cognitive Reviewer to support their literature reviews"}, {"entity1": "BioASQ", "entity2": "Synthetic data generation", "relation": "Research Area", "description": "BioASQ is a case study that uses synthetic data generation"}, {"entity1": "BioASQ", "entity2": "Piloting", "relation": "Research Area", "description": "BioASQ is a case study that uses piloting"}, {"entity1": "Large language models", "entity2": "Retrieval Augmented Generation System", "relation": "Tool/Resource", "description": "Large language models are used in Retrieval Augmented Generation Systems"}, {"entity1": "Table 1", "entity2": "CAIN 2024", "relation": "Publication Venue", "description": "Table 1 is part of the publication CAIN 2024"}, {"entity1": "Figure 1", "entity2": "CAIN 2024", "relation": "Publication Venue", "description": "Figure 1 is part of the publication CAIN 2024"}, {"entity1": "Figure 1", "entity2": "Indexing", "relation": "Methodology", "description": "Figure 1 shows the indexing process"}, {"entity1": "Figure 1", "entity2": "Query", "relation": "Methodology", "description": "Figure 1 shows the query process"}, {"entity1": "AI Tutor", "entity2": "Retrieval Augmented Generation System", "relation": "Tool/Resource", "description": "AI Tutor is a type of Retrieval Augmented Generation System"}, {"entity1": "Cognitive Reviewer", "entity2": "Retrieval Augmented Generation System", "relation": "Tool/Resource", "description": "Cognitive Reviewer is a type of Retrieval Augmented Generation System"}], "e05c3406-8518-4bbc-99c9-b2e763e59b0e": [{"entity1": "AI Tutor", "entity2": "Deakin", "relation": "Institution-Collaboration", "description": "The AI Tutor was integrated into Deakin's learning management system."}, {"entity1": "AI Tutor", "entity2": "RAG", "relation": "Tool/Resource", "description": "The AI Tutor is a RAG system."}, {"entity1": "Whisper", "entity2": "AI Tutor", "relation": "Tool/Resource", "description": "Whisper is used by the AI Tutor for video transcription."}, {"entity1": "BioASQ dataset", "entity2": "RAG system", "relation": "Dataset-Origin", "description": "The BioASQ dataset was used to create a RAG system."}, {"entity1": "BioASQ dataset", "entity2": "Biomedical experts", "relation": "Author-Expertise", "description": "The BioASQ dataset was prepared by biomedical experts."}, {"entity1": "AI Tutor", "entity2": "Chat interface", "relation": "Application", "description": "The AI Tutor uses a chat interface for user interaction."}, {"entity1": "Rewriter", "entity2": "AI Tutor", "relation": "Tool/Resource", "description": "The AI Tutor includes a rewriter to generalise queries."}, {"entity1": "August 2023", "entity2": "November 2023", "relation": "Publication Date", "description": "The AI Tutor was developed between August 2023 and November 2023."}, {"entity1": "Deakin", "entity2": "200 students", "relation": "Research Area", "description": "The AI Tutor pilot was conducted with 200 students at Deakin."}, {"entity1": "BioASQ dataset", "entity2": "1000 questions", "relation": "Dataset-Origin", "description": "The BioASQ dataset contains 1000 questions."}, {"entity1": "AI Tutor", "entity2": "PDF", "relation": "Tool/Resource", "description": "The AI Tutor indexes PDF documents as part of its content."}], "9e0fecd8-0c7c-4824-97ca-c0ebef448e73": [{"entity1": "CAIN 2024", "entity2": "April 2024", "relation": "Publication Date", "description": "CAIN 2024 was published in April 2024"}, {"entity1": "CAIN 2024", "entity2": "Lisbon", "relation": "Publication Venue", "description": "CAIN 2024 was held in Lisbon"}, {"entity1": "CAIN 2024", "entity2": "Portugal", "relation": "Publication Venue", "description": "CAIN 2024 was held in Portugal"}, {"entity1": "Scott Barnett", "entity2": "CAIN 2024", "relation": "Author-Role", "description": "Scott Barnett is an author of CAIN 2024"}, {"entity1": "Stefanus Kurniawan", "entity2": "CAIN 2024", "relation": "Author-Role", "description": "Stefanus Kurniawan is an author of CAIN 2024"}, {"entity1": "Srikanth Thudumu", "entity2": "CAIN 2024", "relation": "Author-Role", "description": "Srikanth Thudumu is an author of CAIN 2024"}, {"entity1": "Zach Brannelly", "entity2": "CAIN 2024", "relation": "Author-Role", "description": "Zach Brannelly is an author of CAIN 2024"}, {"entity1": "Mohamed Abdelrazek", "entity2": "CAIN 2024", "relation": "Author-Role", "description": "Mohamed Abdelrazek is an author of CAIN 2024"}, {"entity1": "Cognitive Reviewer", "entity2": "RAG", "relation": "Tool/Resource", "description": "Cognitive Reviewer is a case study of RAG"}, {"entity1": "AI Tutor", "entity2": "RAG", "relation": "Tool/Resource", "description": "AI Tutor is a case study of RAG"}, {"entity1": "BioASQ", "entity2": "RAG", "relation": "Tool/Resource", "description": "BioASQ is a case study of RAG"}, {"entity1": "FP1", "entity2": "RAG", "relation": "Failure Point", "description": "FP1 is a failure point of RAG systems"}, {"entity1": "FP2", "entity2": "RAG", "relation": "Failure Point", "description": "FP2 is a failure point of RAG systems"}, {"entity1": "FP3", "entity2": "RAG", "relation": "Failure Point", "description": "FP3 is a failure point of RAG systems"}, {"entity1": "FP4", "entity2": "RAG", "relation": "Failure Point", "description": "FP4 is a failure point of RAG systems"}, {"entity1": "OpenEvals", "entity2": "OpenAI", "relation": "Affiliation", "description": "OpenEvals is a technique implemented by OpenAI"}, {"entity1": "Table 1", "entity2": "CAIN 2024", "relation": "Publication Venue", "description": "Table 1 is a part of CAIN 2024"}], "2de6b514-8607-4d64-84eb-f6796bca57fa": [{"entity1": "Chunking", "entity2": "Embeddings", "relation": "Methodology", "description": "Chunking affects the quality of embeddings, which in turn affects similarity matching and retrieval accuracy."}, {"entity1": "RAG system", "entity2": "Chunking", "relation": "Methodology", "description": "Chunking is a crucial step in engineering a RAG system, affecting the retrieval process and embeddings."}, {"entity1": "Heuristics based chunking", "entity2": "Semantic chunking", "relation": "Comparison", "description": "Heuristics based chunking and semantic chunking are two different methods of chunking, with tradeoffs to be explored."}, {"entity1": "Embeddings", "entity2": "Multimedia chunks", "relation": "Application", "description": "Embeddings can be generated for multimedia and multimodal chunks, such as tables, figures, and formulas."}, {"entity1": "RAG system", "entity2": "Query relevance", "relation": "Objective", "description": "The RAG system aims to improve query relevance and retrieval accuracy through effective chunking and embeddings."}, {"entity1": "Table 2", "entity2": "LESSONS AND FUTURE RESEARCH DIRECTIONS", "relation": "Publication Venue", "description": "Table 2 presents the lessons learned from the three case studies, which are discussed in the LESSONS AND FUTURE RESEARCH DIRECTIONS section."}, {"entity1": "GitHub", "entity2": "Openai/evals", "relation": "Affiliation", "description": "The openai/evals repository is hosted on GitHub."}, {"entity1": "Teachers", "entity2": "Students", "relation": "Author-Role", "description": "Teachers and students are the target audience for the educational content provided by the RAG system."}, {"entity1": "FP4", "entity2": "RAG system", "relation": "Limitation", "description": "FP4 highlights the limitation of the RAG system in extracting the correct answer due to noise or contradicting information in the context."}, {"entity1": "FP5", "entity2": "RAG system", "relation": "Limitation", "description": "FP5 highlights the limitation of the RAG system in ignoring the instruction to extract information in a certain format."}, {"entity1": "FP6", "entity2": "RAG system", "relation": "Limitation", "description": "FP6 highlights the limitation of the RAG system in providing answers that are not specific enough or are too specific to address the user's need."}, {"entity1": "FP7", "entity2": "RAG system", "relation": "Limitation", "description": "FP7 highlights the limitation of the RAG system in providing incomplete answers that miss some of the information available in the context."}], "7a880614-0175-464d-bd42-ffce54f2887c": [{"entity1": "Embeddings", "entity2": "multimedia", "relation": "Research Area", "description": "Embeddings represent an active research area for generating embeddings for multimedia and multimodal chunks."}, {"entity1": "Chunk embeddings", "entity2": "system development", "relation": "Creation Context", "description": "Chunk embeddings are typically created once during system development or when a new document is indexed."}, {"entity1": "Query preprocessing", "entity2": "RAG system", "relation": "Performance Impact", "description": "Query preprocessing significantly impacts a RAG system's performance, particularly handling negative or ambiguous queries."}, {"entity1": "Architectural patterns", "entity2": "embeddings", "relation": "Research Need", "description": "Further research is needed on architectural patterns and approaches to address the inherent limitations with embeddings."}, {"entity1": "LLMs", "entity2": "Finetuning", "relation": "Customisation Pathway", "description": "Finetuning and RAG offer two potential customisation pathways for LLMs, each with distinct tradeoffs."}, {"entity1": "RAG system", "entity2": "Finetuning", "relation": "Comparison", "description": "RAG and Finetuning are compared as two different approaches for customising LLMs, with RAG offering an alternative to finetuning."}, {"entity1": "Domain", "entity2": "embeddings", "relation": "Limitation", "description": "The quality of a match for embeddings is domain-specific, highlighting a limitation of using embeddings in different domains."}, {"entity1": "Multimodal chunks", "entity2": "tables", "relation": "Example", "description": "Multimodal chunks include examples such as tables, figures, and formulas."}, {"entity1": "RAG", "entity2": "domain", "relation": "Customisation", "description": "RAG offers a customisation pathway that can be tailored to a specific domain, addressing the limitations of general-purpose LLMs."}], "6f4ff19e-fde4-406a-a6fb-48711fb96203": [{"entity1": "FP1", "entity2": "Semantic caching", "relation": "Methodology", "description": "FP1 describes semantic caching as a method to drive cost and latency down in RAG systems."}, {"entity1": "FP4", "entity2": "Larger context", "relation": "Result", "description": "FP4 shows that a larger context enables more accurate responses."}, {"entity1": "GPT-3.5", "entity2": "Prior work", "relation": "Citation", "description": "GPT-3.5 is cited as prior work in the context of FP4."}, {"entity1": "AI Tutor", "entity2": "FP1", "relation": "Author-Role", "description": "AI Tutor is associated with FP1, providing lessons on semantic caching."}, {"entity1": "FP5-7", "entity2": "Safety training", "relation": "Challenge", "description": "FP5-7 highlights the challenge of jailbreaks bypassing the RAG system and hitting the safety training."}, {"entity1": "Fine-tuning", "entity2": "LLMs", "relation": "Methodology", "description": "Fine-tuning is a methodology used to improve LLMs, but it can reverse safety training."}, {"entity1": "FP2", "entity2": "Meta-data", "relation": "Result", "description": "FP2 shows that adding meta-data improves retrieval in RAG systems."}, {"entity1": "Open source embedding models", "entity2": "Small text", "relation": "Application", "description": "Open source embedding models perform better for small text."}, {"entity1": "BioASQ", "entity2": "RAG systems", "relation": "Tool/Resource", "description": "BioASQ is a tool/resource used in the context of RAG systems."}, {"entity1": "FP2-7", "entity2": "Continuous calibration", "relation": "Methodology", "description": "FP2-7 emphasizes the need for continuous calibration in RAG systems."}, {"entity1": "Cognitive Reviewer", "entity2": "RAG pipeline", "relation": "Author-Expertise", "description": "Cognitive Reviewer is associated with the implementation of a RAG pipeline."}, {"entity1": "End-to-end training", "entity2": "Domain adaptation", "relation": "Methodology", "description": "End-to-end training enhances domain adaptation in RAG systems."}, {"entity1": "G-Evals", "entity2": "Offline evaluation", "relation": "Tool/Resource", "description": "G-Evals is a tool/resource used for offline evaluation of RAG systems."}, {"entity1": "CAIN 2024", "entity2": "Lisbon, Portugal", "relation": "Publication Venue", "description": "CAIN 2024 is a publication venue located in Lisbon, Portugal."}, {"entity1": "April 2024", "entity2": "CAIN 2024", "relation": "Publication Date", "description": "CAIN 2024 was published in April 2024."}, {"entity1": "Table 2", "entity2": "Case studies", "relation": "Conclusion", "description": "Table 2 summarizes the lessons learned from the case studies."}, {"entity1": "RAG systems", "entity2": "Security/privacy", "relation": "Challenge", "description": "RAG systems face the challenge of sorting out security/privacy issues."}, {"entity1": "Fine-tuning", "entity2": "Foundation model", "relation": "Methodology", "description": "Fine-tuning is a methodology used to update the foundation model."}], "73454bc9-4f2f-4546-8cb5-a8c692739b7a": [{"entity1": "RAG systems", "entity2": "LLMs", "relation": "Leverages", "description": "RAG systems leverage LLMs for information retrieval."}, {"entity1": "Software engineers", "entity2": "RAG systems", "relation": "Interaction", "description": "Software engineers interact with RAG systems through implementing semantic search or new code-dependent tasks."}, {"entity1": "Chunking", "entity2": "Embeddings", "relation": "Research Area", "description": "Optimal strategies for chunk embedding, retrieval, and contextual fusion remain an active research area."}, {"entity1": "Finetuning", "entity2": "RAG paradigms", "relation": "Comparison", "description": "Further work should systematically compare finetuning and RAG paradigms across factors including accuracy, latency, operating costs, and robustness."}, {"entity1": "Software testing", "entity2": "Test case generation", "relation": "Methodology", "description": "Software testing and test case generation are areas for refinement in RAG systems."}, {"entity1": "LLMs", "entity2": "Question generation", "relation": "Application", "description": "LLMs can be used for generating questions from multiple documents."}, {"entity1": "Quality metrics", "entity2": "RAG systems", "relation": "Evaluation", "description": "Quality metrics are required to assist engineers in making quality tradeoffs in RAG systems."}, {"entity1": "Self-adaptive systems", "entity2": "RAG systems", "relation": "Incorporation", "description": "Incorporating ideas from self-adaptive systems can support monitoring and adapting RAG systems."}, {"entity1": "Machine learning systems", "entity2": "RAG systems", "relation": "Similarity", "description": "RAG systems share similarities with machine learning systems in terms of performance characteristics and required adaptations."}, {"entity1": "Case studies", "entity2": "Empirical investigation", "relation": "Research Methodology", "description": "The paper presents lessons learned from 3 case studies, including an empirical investigation involving 15,000 documents and 1000 questions."}, {"entity1": "RAG systems", "entity2": "Software engineering", "relation": "Research Area", "description": "This paper presents the first investigation into RAG systems from a software engineering perspective."}], "74c1536b-8720-492b-bbf9-5a16a060a2ec": [{"entity1": "CAIN 2024", "entity2": "April 2024", "relation": "Publication Date", "description": "CAIN 2024 was held in April 2024"}, {"entity1": "CAIN 2024", "entity2": "Lisbon, Portugal", "relation": "Location", "description": "CAIN 2024 was held in Lisbon, Portugal"}, {"entity1": "Scott Barnett", "entity2": "Stefanus Kurniawan", "relation": "Co-author", "description": "Scott Barnett and Stefanus Kurniawan are co-authors"}, {"entity1": "Fu Bang", "entity2": "GPTCache", "relation": "Author-Role", "description": "Fu Bang is the author of GPTCache"}, {"entity1": "Maria Casimiro", "entity2": "Self-adaptive Machine Learning Systems: Research Challenges and Opportunities", "relation": "Author-Role", "description": "Maria Casimiro is the author of the paper 'Self-adaptive Machine Learning Systems: Research Challenges and Opportunities'"}, {"entity1": "Jiawei Chen", "entity2": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "relation": "Author-Role", "description": "Jiawei Chen is the author of the paper 'Benchmarking Large Language Models in Retrieval-Augmented Generation'"}, {"entity1": "Mingda Chen", "entity2": "Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis", "relation": "Author-Role", "description": "Mingda Chen is the author of the paper 'Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis'"}, {"entity1": "Alex Cummaudo", "entity2": "Threshy", "relation": "Author-Role", "description": "Alex Cummaudo is the author of Threshy"}, {"entity1": "Kelvin Guu", "entity2": "Retrieval augmented language model pre-training", "relation": "Author-Role", "description": "Kelvin Guu is the author of the paper 'Retrieval augmented language model pre-training'"}, {"entity1": "Sebastian Hofst\u00e4tter", "entity2": "Fid-light", "relation": "Author-Role", "description": "Sebastian Hofst\u00e4tter is the author of Fid-light"}, {"entity1": "Gautier Izacard", "entity2": "Leveraging passage retrieval with generative models for open domain question answering", "relation": "Author-Role", "description": "Gautier Izacard is the author of the paper 'Leveraging passage retrieval with generative models for open domain question answering'"}, {"entity1": "Anastasia Krithara", "entity2": "BioASQ-QA", "relation": "Author-Role", "description": "Anastasia Krithara is the author of BioASQ-QA"}, {"entity1": "Simon Lermen", "entity2": "LoRA Fine-tuning", "relation": "Author-Role", "description": "Simon Lermen is the author of LoRA Fine-tuning"}, {"entity1": "3rd Workshop for Natural Language Processing Open Source Software", "entity2": "Fu Bang", "relation": "Publication Venue", "description": "Fu Bang published in the 3rd Workshop for Natural Language Processing Open Source Software"}, {"entity1": "28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering", "entity2": "Alex Cummaudo", "relation": "Publication Venue", "description": "Alex Cummaudo published in the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering"}, {"entity1": "46th International ACM SIGIR Conference on Research and Development in Information Retrieval", "entity2": "Sebastian Hofst\u00e4tter", "relation": "Publication Venue", "description": "Sebastian Hofst\u00e4tter published in the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval"}], "44cc90ba-14d1-4467-a551-25baf1a410c0": [{"entity1": "Fei Wang", "entity2": "Google Cloud AI Research", "relation": "Affiliation", "description": "Fei Wang is affiliated with Google Cloud AI Research."}, {"entity1": "Fei Wang", "entity2": "University of Southern California", "relation": "Affiliation", "description": "Fei Wang is also affiliated with the University of Southern California."}, {"entity1": "Xingchen Wan", "entity2": "Google Cloud AI Research", "relation": "Affiliation", "description": "Xingchen Wan is affiliated with Google Cloud AI Research."}, {"entity1": "Ruoxi Sun", "entity2": "Google Cloud AI Research", "relation": "Affiliation", "description": "Ruoxi Sun is affiliated with Google Cloud AI Research."}, {"entity1": "Jiefeng Chen", "entity2": "Google Cloud AI Research", "relation": "Affiliation", "description": "Jiefeng Chen is affiliated with Google Cloud AI Research."}, {"entity1": "Sercan \u00d6. Ar\u0131k", "entity2": "Google Cloud AI Research", "relation": "Affiliation", "description": "Sercan \u00d6. Ar\u0131k is affiliated with Google Cloud AI Research."}, {"entity1": "Astute RAG", "entity2": "Retrieval-Augmented Generation (RAG)", "relation": "Methodology", "description": "Astute RAG is a novel approach to Retrieval-Augmented Generation (RAG)."}, {"entity1": "Astute RAG", "entity2": "Large Language Models (LLMs)", "relation": "Application", "description": "Astute RAG is designed to improve the performance of Large Language Models (LLMs)."}, {"entity1": "Guu et al.", "entity2": "Retrieval-Augmented Generation (RAG)", "relation": "Citation", "description": "Guu et al. is cited as a reference for Retrieval-Augmented Generation (RAG)."}, {"entity1": "Lewis et al.", "entity2": "Retrieval-Augmented Generation (RAG)", "relation": "Citation", "description": "Lewis et al. is cited as a reference for Retrieval-Augmented Generation (RAG)."}, {"entity1": "Chen et al.", "entity2": "Retrieval-Augmented Generation (RAG)", "relation": "Citation", "description": "Chen et al. is cited as a reference for Retrieval-Augmented Generation (RAG)."}, {"entity1": "Xiang et al.", "entity2": "Retrieval-Augmented Generation (RAG)", "relation": "Citation", "description": "Xiang et al. is cited as a reference for Retrieval-Augmented Generation (RAG)."}, {"entity1": "Zou et al.", "entity2": "Retrieval-Augmented Generation (RAG)", "relation": "Citation", "description": "Zou et al. is cited as a reference for Retrieval-Augmented Generation (RAG)."}, {"entity1": "Gemini", "entity2": "Astute RAG", "relation": "Tool/Resource", "description": "Gemini is used as a tool/resource to evaluate the performance of Astute RAG."}, {"entity1": "Claude", "entity2": "Astute RAG", "relation": "Tool/Resource", "description": "Claude is used as a tool/resource to evaluate the performance of Astute RAG."}, {"entity1": "Fei Wang", "entity2": "Astute RAG", "relation": "Author-Role", "description": "Fei Wang is an author of the Astute RAG approach."}, {"entity1": "Xingchen Wan", "entity2": "Astute RAG", "relation": "Author-Role", "description": "Xingchen Wan is an author of the Astute RAG approach."}, {"entity1": "Ruoxi Sun", "entity2": "Astute RAG", "relation": "Author-Role", "description": "Ruoxi Sun is an author of the Astute RAG approach."}, {"entity1": "Jiefeng Chen", "entity2": "Astute RAG", "relation": "Author-Role", "description": "Jiefeng Chen is an author of the Astute RAG approach."}, {"entity1": "Sercan \u00d6. Ar\u0131k", "entity2": "Astute RAG", "relation": "Author-Role", "description": "Sercan \u00d6. Ar\u0131k is an author of the Astute RAG approach."}], "1894123d-771d-4b5c-acf8-6d0055f4bc19": [{"entity1": "Chen et al.", "entity2": "LLMs", "relation": "Research Area", "description": "Chen et al. researched the impact of malicious information on LLM responses."}, {"entity1": "Xiang et al.", "entity2": "LLMs", "relation": "Research Area", "description": "Xiang et al. researched the impact of malicious information on LLM responses."}, {"entity1": "Zou et al.", "entity2": "LLMs", "relation": "Research Area", "description": "Zou et al. researched the impact of malicious information on LLM responses."}, {"entity1": "Shao et al.", "entity2": "corpus quality limitations", "relation": "Research Finding", "description": "Shao et al. found that corpus quality limitations drive imperfect retrieval augmentation."}, {"entity1": "Dai et al.", "entity2": "reliability of retrievers", "relation": "Research Finding", "description": "Dai et al. found that the reliability of retrievers drives imperfect retrieval augmentation."}, {"entity1": "Su et al.", "entity2": "complexity of queries", "relation": "Research Finding", "description": "Su et al. found that the complexity of queries drives imperfect retrieval augmentation."}, {"entity1": "Mallen et al.", "entity2": "information retrieval and RAG", "relation": "Research Area", "description": "Mallen et al. analyzed information retrieval and RAG in the context of LLMs."}, {"entity1": "Su et al.", "entity2": "information retrieval and RAG", "relation": "Research Area", "description": "Su et al. analyzed information retrieval and RAG in the context of LLMs."}, {"entity1": "Longpre et al.", "entity2": "knowledge conflicts", "relation": "Research Finding", "description": "Longpre et al. found that information retrieval errors lead to knowledge conflicts between LLMs and context."}, {"entity1": "Wang et al.", "entity2": "knowledge conflicts", "relation": "Research Finding", "description": "Wang et al. found that information retrieval errors lead to knowledge conflicts between LLMs and context."}, {"entity1": "Xue et al.", "entity2": "knowledge conflicts", "relation": "Research Finding", "description": "Xue et al. found that information retrieval errors lead to knowledge conflicts between LLMs and context."}, {"entity1": "Fei", "entity2": "Google Cloud AI Research", "relation": "Affiliation", "description": "Fei was a research intern at Google Cloud AI Research."}, {"entity1": "fwang598@usc.edu", "entity2": "Corresponding author", "relation": "Author-Role", "description": "fwang598@usc.edu is a corresponding author."}, {"entity1": "soarik@google.com", "entity2": "Corresponding author", "relation": "Author-Role", "description": "soarik@google.com is a corresponding author."}, {"entity1": "https://www.bbc.com/news/articles/cd11gzejgz4o", "entity2": "satirical news source", "relation": "Tool/Resource", "description": "The URL is an example of a satirical news source."}, {"entity1": "arXiv:2410.07176v1", "entity2": "Publication Venue", "relation": "Publication Venue", "description": "The paper is published on arXiv with the identifier 2410.07176v1."}, {"entity1": "RAG", "entity2": "LLMs", "relation": "Tool/Resource", "description": "RAG is a tool/resource used in the context of LLMs."}], "af8a79c3-6c87-4563-9652-95a08bcce7f9": [{"entity1": "Astute RAG", "entity2": "LLMs", "relation": "Methodology", "description": "Astute RAG is a novel RAG approach designed to leverage LLMs' internal knowledge to recover from RAG failures."}, {"entity1": "Jin et al.", "entity2": "Astute RAG", "relation": "Citation", "description": "Jin et al. is cited as a reference for the study on LLM-internal and external knowledge."}, {"entity1": "NQ", "entity2": "Astute RAG", "relation": "Dataset-Origin", "description": "NQ is one of the datasets used to conduct controlled experiments for Astute RAG."}, {"entity1": "TriviaQA", "entity2": "Astute RAG", "relation": "Dataset-Origin", "description": "TriviaQA is one of the datasets used to conduct controlled experiments for Astute RAG."}, {"entity1": "BioASQ", "entity2": "Astute RAG", "relation": "Dataset-Origin", "description": "BioASQ is one of the datasets used to conduct controlled experiments for Astute RAG."}, {"entity1": "PopQA", "entity2": "Astute RAG", "relation": "Dataset-Origin", "description": "PopQA is one of the datasets used to conduct controlled experiments for Astute RAG."}, {"entity1": "Kwiatkowski et al.", "entity2": "NQ", "relation": "Author-Role", "description": "Kwiatkowski et al. is the author of the NQ dataset."}, {"entity1": "Joshi et al.", "entity2": "TriviaQA", "relation": "Author-Role", "description": "Joshi et al. is the author of the TriviaQA dataset."}, {"entity1": "Tsatsaronis et al.", "entity2": "BioASQ", "relation": "Author-Role", "description": "Tsatsaronis et al. is the author of the BioASQ dataset."}, {"entity1": "Mallen et al.", "entity2": "PopQA", "relation": "Author-Role", "description": "Mallen et al. is the author of the PopQA dataset."}, {"entity1": "Google Search", "entity2": "Web", "relation": "Tool/Resource", "description": "Google Search is a tool that uses the Web as its corpus."}, {"entity1": "Astute RAG", "entity2": "Section 3", "relation": "Publication Venue", "description": "Astute RAG is proposed in Section 3."}, {"entity1": "Astute RAG", "entity2": "Figure 1", "relation": "Result", "description": "Figure 1 shows the knowledge conflicts between LLMs' internal knowledge and retrieved knowledge from external sources for Astute RAG."}, {"entity1": "LLMs", "entity2": "RAG", "relation": "Methodology", "description": "LLMs use RAG to enhance their performance."}, {"entity1": "Astute RAG", "entity2": "RAG", "relation": "Innovation", "description": "Astute RAG is a novel approach to RAG that overcomes imperfect retrieval augmentation and knowledge conflicts."}, {"entity1": "Claude", "entity2": "Astute RAG", "relation": "Experiment-Outcome", "description": "The overall results with Claude are reported under the setting in Section 4.1 for Astute RAG."}, {"entity1": "Tan et al.", "entity2": "Astute RAG", "relation": "Citation", "description": "Tan et al. is cited as a reference for the study on LLM-internal and external knowledge."}, {"entity1": "Xie et al.", "entity2": "Astute RAG", "relation": "Citation", "description": "Xie et al. is cited as a reference for the study on LLM-internal and external knowledge."}], "ebe8b836-8a76-4fbc-8d75-4df0cd2f8702": [{"entity1": "Astute RAG", "entity2": "LLMs", "relation": "Methodology", "description": "Astute RAG elicits information from LLMs' internal knowledge to complement passages retrieved from external sources."}, {"entity1": "Astute RAG", "entity2": "external sources", "relation": "Data Collection", "description": "Astute RAG conducts source-aware knowledge consolidation of information from various internal and external sources."}, {"entity1": "Astute RAG", "entity2": "Gemini", "relation": "Comparison", "description": "Experiments demonstrate the superior performance of Astute RAG compared to previous RAG approaches, including Gemini."}, {"entity1": "Astute RAG", "entity2": "Claude3", "relation": "Comparison", "description": "Experiments demonstrate the superior performance of Astute RAG compared to previous RAG approaches, including Claude3."}, {"entity1": "Astute RAG", "entity2": "RAG approaches", "relation": "Innovation", "description": "Astute RAG proposes an innovative approach to combining consistent information, identifying conflicting information, and filtering out irrelevant information."}, {"entity1": "Anthropic", "entity2": "Claude", "relation": "Affiliation", "description": "Claude is affiliated with Anthropic, as indicated by the URL https://www.anthropic.com/claude."}, {"entity1": "Astute RAG", "entity2": "Section 4", "relation": "Publication Venue", "description": "Experiments involving Astute RAG are discussed in Section 4."}], "975618d0-0a28-46f6-bc23-ce3cb3135f62": [{"entity1": "Astute RAG", "entity2": "Imperfect Retrieval", "relation": "Methodology", "description": "Astute RAG is designed to overcome imperfect retrieval augmentation and knowledge conflicts for Large Language Models."}, {"entity1": "Astute RAG", "entity2": "Knowledge Conflicts", "relation": "Resolution", "description": "Astute RAG explicitly addresses conflicts between LLM-internal and external knowledge, thereby recovering from RAG failures."}, {"entity1": "RAG", "entity2": "Imperfect Retrieval", "relation": "Challenge", "description": "Imperfect retrieval is a significant contributor to RAG failures."}, {"entity1": "Astute RAG", "entity2": "LLMs", "relation": "Comparison", "description": "Astute RAG achieves performance comparable to or even surpassing conventional use of LLMs under the worst-case scenario."}, {"entity1": "Retrieval Precision", "entity2": "Imperfect Retrieval", "relation": "Measurement", "description": "Retrieval precision is defined as the ratio of passages containing the correct answer for each instance, used to measure imperfect retrieval."}, {"entity1": "Google Search", "entity2": "Web", "relation": "Tool/Resource", "description": "Google Search is used as the retriever and the Web as the corpus for analyzing retrieval quality and knowledge conflicts."}, {"entity1": "NQ", "entity2": "TriviaQA", "relation": "Dataset-Origin", "description": "NQ, TriviaQA, BioASQ, and PopQA are datasets used for evaluating retrieval quality, end-to-end RAG performance, and knowledge conflicts."}, {"entity1": "Kwiatkowski et al.", "entity2": "NQ", "relation": "Author-Role", "description": "Kwiatkowski et al. are authors of the NQ dataset."}, {"entity1": "Joshi et al.", "entity2": "TriviaQA", "relation": "Author-Role", "description": "Joshi et al. are authors of the TriviaQA dataset."}, {"entity1": "Tsatsaronis et al.", "entity2": "BioASQ", "relation": "Author-Role", "description": "Tsatsaronis et al. are authors of the BioASQ dataset."}, {"entity1": "Mallen et al.", "entity2": "PopQA", "relation": "Author-Role", "description": "Mallen et al. are authors of the PopQA dataset."}, {"entity1": "Astute RAG", "entity2": "RAG", "relation": "Innovation", "description": "Astute RAG is an improved version of RAG, designed to overcome imperfect retrieval augmentation and knowledge conflicts."}], "2aca8fdc-8544-42da-b2d7-aa55bfb42e21": [{"entity1": "Thakur et al., 2024", "entity2": "information retrieval", "relation": "Citation", "description": "Thakur et al., 2024 is cited in relation to information retrieval."}, {"entity1": "NQ", "entity2": "Retrieval Precision", "relation": "Result", "description": "NQ dataset shows 34% of instances with no mentions of the correct answer."}, {"entity1": "TriviaQA", "entity2": "Retrieval Precision", "relation": "Result", "description": "TriviaQA dataset shows 18% of instances with no mentions of the correct answer."}, {"entity1": "BioASQ", "entity2": "Retrieval Precision", "relation": "Result", "description": "BioASQ dataset shows 24% of instances with no mentions of the correct answer."}, {"entity1": "PopQA", "entity2": "Retrieval Precision", "relation": "Result", "description": "PopQA dataset shows 50% of instances with no mentions of the correct answer."}, {"entity1": "Figure 2", "entity2": "Retrieval Precision", "relation": "Illustration", "description": "Figure 2 illustrates the retrieval precision across different datasets."}, {"entity1": "https://developers.google.com/custom-search/v1/overview", "entity2": "information retrieval", "relation": "Tool/Resource", "description": "The provided URL is a resource related to information retrieval."}, {"entity1": "Thakur et al., 2024", "entity2": "information retrieval", "relation": "Research Area", "description": "Thakur et al., 2024 is related to the research area of information retrieval."}], "60716359-477f-434d-bb83-d0b84e323b9c": [{"entity1": "Astute RAG", "entity2": "Large Language Models", "relation": "Methodology", "description": "Astute RAG is designed to improve the performance of Large Language Models by addressing information conflicts and combining internal and external knowledge."}, {"entity1": "Astute RAG", "entity2": "consolidation mechanism", "relation": "Component", "description": "Astute RAG employs a consolidation mechanism to address information conflicts between internal and external knowledge."}, {"entity1": "Large Language Models", "entity2": "knowledge bases", "relation": "Data Source", "description": "Large Language Models can utilize knowledge bases as external sources of information."}, {"entity1": "Astute RAG", "entity2": "web", "relation": "Data Source", "description": "Astute RAG can retrieve information from the web as an external source."}, {"entity1": "Astute RAG", "entity2": "domain-specific corpora", "relation": "Data Source", "description": "Astute RAG can retrieve information from domain-specific corpora as an external source."}, {"entity1": "Claude 3.5 Sonnet", "entity2": "Astute RAG", "relation": "Experiment-Design", "description": "Claude 3.5 Sonnet is used as a Large Language Model in the experiment to evaluate the performance of Astute RAG."}, {"entity1": "Jin et al.", "entity2": "Astute RAG", "relation": "Citation", "description": "Jin et al. is cited as previous work related to the limitations of Large Language Models and the importance of addressing knowledge conflicts."}, {"entity1": "Tan et al.", "entity2": "Astute RAG", "relation": "Citation", "description": "Tan et al. is cited as previous work related to the limitations of Large Language Models and the importance of addressing knowledge conflicts."}, {"entity1": "Xie et al.", "entity2": "Astute RAG", "relation": "Citation", "description": "Xie et al. is cited as previous work related to the limitations of Large Language Models and the importance of addressing knowledge conflicts."}, {"entity1": "Yu et al.", "entity2": "Astute RAG", "relation": "Citation", "description": "Yu et al. is cited as previous work related to the limitations of retrieval augmentation and the importance of addressing imperfect retrieval."}, {"entity1": "Astute RAG", "entity2": "Figure 3", "relation": "Illustration", "description": "Figure 3 illustrates the overview of the proposed Astute RAG framework."}, {"entity1": "Astute RAG", "entity2": "Figure 4", "relation": "Illustration", "description": "Figure 4 illustrates the performance of Claude 3.5 Sonnet with and without RAG, reported by retrieval precision."}, {"entity1": "Astute RAG", "entity2": "source-aware knowledge consolidation", "relation": "Component", "description": "Astute RAG includes source-aware knowledge consolidation as one of its major steps."}, {"entity1": "Astute RAG", "entity2": "adaptive generation of internal knowledge", "relation": "Component", "description": "Astute RAG includes adaptive generation of internal knowledge as one of its major steps."}, {"entity1": "Astute RAG", "entity2": "answer finalization", "relation": "Component", "description": "Astute RAG includes answer finalization as one of its major steps."}], "1fa223a6-5245-4c41-a1d3-fd82530948d2": [{"entity1": "Astute RAG", "entity2": "Large Language ModelM", "relation": "Methodology", "description": "Astute RAG utilizes Large Language ModelM for adaptive generation of internal knowledge and consolidation of knowledge."}, {"entity1": "Query\ud835\udc5e", "entity2": "Retrieved Passages\ud835\udc38", "relation": "Input", "description": "Query\ud835\udc5e and Retrieved Passages\ud835\udc38 are inputs to the Astute RAG algorithm."}, {"entity1": "Astute RAG", "entity2": "Algorithm 1", "relation": "Implementation", "description": "Astute RAG is implemented through Algorithm 1, which outlines the steps for adaptive generation and consolidation of knowledge."}, {"entity1": "Large Language ModelM", "entity2": "Prompt Templates\ud835\udc5d\ud835\udc54\ud835\udc52\ud835\udc5b,\ud835\udc5d\ud835\udc50\ud835\udc5c\ud835\udc5b,\ud835\udc5d\ud835\udc4e\ud835\udc5b\ud835\udc60", "relation": "Tool/Resource", "description": "Large Language ModelM uses Prompt Templates\ud835\udc5d\ud835\udc54\ud835\udc52\ud835\udc5b,\ud835\udc5d\ud835\udc50\ud835\udc5c\ud835\udc5b,\ud835\udc5d\ud835\udc4e\ud835\udc5b\ud835\udc60 for generating passages and consolidating knowledge."}, {"entity1": "Section 3.3", "entity2": "Adaptive Generation of Internal Knowledge", "relation": "Research Area", "description": "Section 3.3 discusses the adaptive generation of internal knowledge, a key research area in the Astute RAG framework."}, {"entity1": "Astute RAG", "entity2": "Figure 3", "relation": "Illustration", "description": "Figure 3 illustrates the Astute RAG framework, providing a visual representation of the algorithm's steps."}, {"entity1": "LLMs", "entity2": "Internal Knowledge", "relation": "Author-Expertise", "description": "LLMs have internal knowledge that can be leveraged for more reliable responses, demonstrating the expertise of LLMs in knowledge consolidation."}, {"entity1": "Astute RAG", "entity2": "Knowledge Conflicts", "relation": "Challenge", "description": "Astute RAG aims to resolve knowledge conflicts between internal and external knowledge sources, addressing a key challenge in large language models."}, {"entity1": "Retrieved Passages\ud835\udc38", "entity2": "External Sources", "relation": "Dataset-Origin", "description": "Retrieved Passages\ud835\udc38 originate from external sources, such as custom or public corpora and knowledge bases."}, {"entity1": "Astute RAG", "entity2": "Reliable Responses", "relation": "Objective", "description": "The primary objective of Astute RAG is to produce more accurate and reliable responses from large language models."}], "cc3a1915-1057-4c23-b893-e6935838737a": [{"entity1": "Bai et al.", "entity2": "Constitutional AI", "relation": "Author-Role", "description": "Bai et al. are the authors of Constitutional AI."}, {"entity1": "Constitutional AI", "entity2": "LLM", "relation": "Tool/Resource", "description": "Constitutional AI is used to enhance the reliability and trustworthiness of LLM-generated passages."}, {"entity1": "Yu et al.", "entity2": "LLM", "relation": "Methodology", "description": "Yu et al.'s method is used to generate diverse internal passages, which is built upon by the current work to emphasize reliability and trustworthiness."}, {"entity1": "\ud835\udc5e", "entity2": "LLM", "relation": "Input", "description": "The given question \ud835\udc5e is used as input to prompt LLMs to generate passages."}, {"entity1": "Bai et al.", "entity2": "Yu et al.", "relation": "Citation", "description": "Bai et al.'s work is cited by the current work, which builds upon Yu et al.'s method."}, {"entity1": "Constitutional AI", "entity2": "\ud835\udc5e", "relation": "Application", "description": "Constitutional AI is applied to generate passages based on the given question \ud835\udc5e."}], "cae8b54a-b0ce-405f-a225-11605322ac9f": [{"entity1": "Astute RAG", "entity2": "Large Language Models", "relation": "Application", "description": "Astute RAG is applied to Large Language Models to overcome imperfect retrieval augmentation and knowledge conflicts."}, {"entity1": "\ud835\udc5d\ud835\udc54\ud835\udc52\ud835\udc5b", "entity2": "Appendix A", "relation": "Reference", "description": "\ud835\udc5d\ud835\udc54\ud835\udc52\ud835\udc5b is described in detail in Appendix A."}, {"entity1": "Large Language Models", "entity2": "Internal Knowledge", "relation": "Methodology", "description": "Large Language Models use internal knowledge to generate passages."}, {"entity1": "\ud835\udc5a", "entity2": "\u02c6\ud835\udc5a", "relation": "Comparison", "description": "The number of passages generated (\ud835\udc5a) is compared to the maximum number of passages allowed (\u02c6\ud835\udc5a)."}, {"entity1": "\ud835\udc3c", "entity2": "M", "relation": "Result", "description": "\ud835\udc3c is the result of the function M, which generates passages based on internal knowledge."}, {"entity1": "\ud835\udc370", "entity2": "\ud835\udc38", "relation": "Combination", "description": "\ud835\udc370 is the combination of internal and external knowledge sources (\ud835\udc38 and \ud835\udc3c)."}, {"entity1": "\ud835\udc460", "entity2": "\ud835\udc370", "relation": "Source Attribution", "description": "\ud835\udc460 provides source information for each passage in \ud835\udc370."}, {"entity1": "\ud835\udc5d\ud835\udc50\ud835\udc5c\ud835\udc5b", "entity2": "Appendix A", "relation": "Reference", "description": "\ud835\udc5d\ud835\udc50\ud835\udc5c\ud835\udc5b is described in detail in Appendix A."}, {"entity1": "\u27e8\ud835\udc37\ud835\udc57+1,\ud835\udc46\ud835\udc57+1\u27e9", "entity2": "M", "relation": "Result", "description": "\u27e8\ud835\udc37\ud835\udc57+1,\ud835\udc46\ud835\udc57+1\u27e9 is the result of the function M, which consolidates knowledge."}, {"entity1": "\ud835\udc61", "entity2": "Knowledge Consolidation", "relation": "Iteration", "description": "The knowledge consolidation process can run iteratively for \ud835\udc61 times."}], "247251f5-c337-437f-a3e7-4cfaf7128827": [{"entity1": "LLM", "entity2": "\ud835\udc5d\ud835\udc4e\ud835\udc5b\ud835\udc60", "relation": "Input-Data", "description": "The LLM uses \ud835\udc5d\ud835\udc4e\ud835\udc5b\ud835\udc60 as input data to generate answers."}, {"entity1": "LLM", "entity2": "\u27e8\ud835\udc37\ud835\udc61,\ud835\udc46\ud835\udc61\u27e9", "relation": "Input-Data", "description": "The LLM uses \u27e8\ud835\udc37\ud835\udc61,\ud835\udc46\ud835\udc61\u27e9 as input data to generate answers."}, {"entity1": "LLM", "entity2": "\ud835\udc5e", "relation": "Input-Query", "description": "The LLM uses \ud835\udc5e as a query to generate answers."}, {"entity1": "LLM", "entity2": "Appendix A", "relation": "Reference", "description": "The LLM uses information from Appendix A to generate answers."}, {"entity1": "Astute RAG", "entity2": "Experiments", "relation": "Evaluation-Method", "description": "Astute RAG is evaluated through experiments to assess its effectiveness."}, {"entity1": "\ud835\udc61", "entity2": "Iterations", "relation": "Control-Parameter", "description": "\ud835\udc61 controls the number of iterations to improve context usefulness."}, {"entity1": "M", "entity2": "\ud835\udc5d\ud835\udc4e\ud835\udc5b\ud835\udc60", "relation": "Function-Input", "description": "M is a function that takes \ud835\udc5d\ud835\udc4e\ud835\udc5b\ud835\udc60 as input."}, {"entity1": "M", "entity2": "\ud835\udc5e", "relation": "Function-Input", "description": "M is a function that takes \ud835\udc5e as input."}, {"entity1": "M", "entity2": "\u27e8\ud835\udc370,\ud835\udc460\u27e9", "relation": "Function-Input", "description": "M is a function that takes \u27e8\ud835\udc370,\ud835\udc460\u27e9 as input."}, {"entity1": "M", "entity2": "\u27e8\ud835\udc37\ud835\udc61,\ud835\udc46\ud835\udc61\u27e9", "relation": "Function-Input", "description": "M is a function that takes \u27e8\ud835\udc37\ud835\udc61,\ud835\udc46\ud835\udc61\u27e9 as input."}, {"entity1": "API", "entity2": "Inference", "relation": "Tool-Usage", "description": "API is used for inference, with the number of calls affecting complexity."}, {"entity1": "\ud835\udc4e", "entity2": "M", "relation": "Output-Function", "description": "\ud835\udc4e is the output of the function M."}], "88cb8b96-ffaf-4822-bcd4-4f24f386e516": [{"entity1": "Astute RAG", "entity2": "Claude", "relation": "Comparison", "description": "Astute RAG is compared to Claude in terms of accuracy and prediction complexity."}, {"entity1": "Astute RAG", "entity2": "USC", "relation": "Comparison", "description": "Astute RAG is compared to USC (Chen et al., 2024b) in terms of accuracy and prediction complexity."}, {"entity1": "Astute RAG", "entity2": "GenRead", "relation": "Comparison", "description": "Astute RAG is compared to GenRead (Yu et al., 2023a) in terms of accuracy and prediction complexity."}, {"entity1": "Astute RAG", "entity2": "RobustRAG", "relation": "Comparison", "description": "Astute RAG is compared to RobustRAG (Xiang et al., 2024) in terms of accuracy and prediction complexity."}, {"entity1": "Astute RAG", "entity2": "InstructRAG", "relation": "Comparison", "description": "Astute RAG is compared to InstructRAG (Wei et al., 2024) in terms of accuracy and prediction complexity."}, {"entity1": "Astute RAG", "entity2": "Self-Route", "relation": "Comparison", "description": "Astute RAG is compared to Self-Route (Xu et al., 2024a) in terms of accuracy and prediction complexity."}, {"entity1": "Astute RAG", "entity2": "NQ", "relation": "Dataset-Origin", "description": "Astute RAG uses data from NQ for experiments."}, {"entity1": "Astute RAG", "entity2": "TriviaQA", "relation": "Dataset-Origin", "description": "Astute RAG uses data from TriviaQA for experiments."}, {"entity1": "Astute RAG", "entity2": "BioASQ", "relation": "Dataset-Origin", "description": "Astute RAG uses data from BioASQ for experiments."}, {"entity1": "Astute RAG", "entity2": "PopQA", "relation": "Dataset-Origin", "description": "Astute RAG uses data from PopQA for experiments."}, {"entity1": "Astute RAG", "entity2": "Google Search", "relation": "Tool/Resource", "description": "Astute RAG uses Google Search for retrieving passages."}, {"entity1": "Chen et al.", "entity2": "USC", "relation": "Author-Affiliation", "description": "Chen et al. are affiliated with USC."}, {"entity1": "Yu et al.", "entity2": "GenRead", "relation": "Author-Role", "description": "Yu et al. are authors of GenRead."}, {"entity1": "Xiang et al.", "entity2": "RobustRAG", "relation": "Author-Role", "description": "Xiang et al. are authors of RobustRAG."}, {"entity1": "Wei et al.", "entity2": "InstructRAG", "relation": "Author-Role", "description": "Wei et al. are authors of InstructRAG."}, {"entity1": "Xu et al.", "entity2": "Self-Route", "relation": "Author-Role", "description": "Xu et al. are authors of Self-Route."}, {"entity1": "Section 4.1", "entity2": "Astute RAG", "relation": "Publication Venue", "description": "Section 4.1 is a publication venue for Astute RAG."}, {"entity1": "Section 4.2", "entity2": "Astute RAG", "relation": "Publication Venue", "description": "Section 4.2 is a publication venue for Astute RAG."}, {"entity1": "Section 4.3", "entity2": "Astute RAG", "relation": "Publication Venue", "description": "Section 4.3 is a publication venue for Astute RAG."}], "60d984b7-beac-4859-be3c-4b867b12b8e9": [{"entity1": "Gemini 1.5 Pro", "entity2": "DeepMind", "relation": "Affiliation", "description": "Gemini 1.5 Pro is a technology developed by DeepMind."}, {"entity1": "Claude 3.5 Sonnet", "entity2": "Anthropic", "relation": "Affiliation", "description": "Claude 3.5 Sonnet is a technology developed by Anthropic."}, {"entity1": "Chen et al.", "entity2": "RGB", "relation": "Author-Role", "description": "Chen et al. are the authors of the RGB benchmark."}, {"entity1": "RGB", "entity2": "RAG", "relation": "Tool/Resource", "description": "RGB is a diagnostic benchmark for evaluating RAG abilities."}, {"entity1": "Mallen et al.", "entity2": "Wei et al.", "relation": "Co-author", "description": "Mallen et al. and Wei et al. are co-authors of previous work."}, {"entity1": "Xiang et al.", "entity2": "Yang et al.", "relation": "Co-author", "description": "Xiang et al. and Yang et al. are co-authors of previous work."}, {"entity1": "Gemini 1.5 Pro", "entity2": "LLMs", "relation": "Application", "description": "Gemini 1.5 Pro is an application of LLMs."}, {"entity1": "Claude 3.5 Sonnet", "entity2": "LLMs", "relation": "Application", "description": "Claude 3.5 Sonnet is an application of LLMs."}, {"entity1": "Google", "entity2": "DeepMind", "relation": "Affiliation", "description": "DeepMind is affiliated with Google."}, {"entity1": "https://deepmind.google/technologies/gemini/pro/", "entity2": "Gemini 1.5 Pro", "relation": "Publication Venue", "description": "The URL is a publication venue for Gemini 1.5 Pro."}, {"entity1": "https://www.anthropic.com/news/claude-3-5-sonnet", "entity2": "Claude 3.5 Sonnet", "relation": "Publication Venue", "description": "The URL is a publication venue for Claude 3.5 Sonnet."}], "2719a11a-ee73-4d8f-9cd9-b4cfc2e3c271": [{"entity1": "Astute RAG", "entity2": "Gemini 1.5 Pro (002)", "relation": "Comparison", "description": "Astute RAG is compared with Gemini 1.5 Pro (002) in terms of accuracy and API calls."}, {"entity1": "Astute RAG", "entity2": "USC (Chen et al., 2024b)", "relation": "Comparison", "description": "Astute RAG is compared with USC (Chen et al., 2024b) in terms of accuracy and API calls."}, {"entity1": "Astute RAG", "entity2": "GenRead (Yu et al., 2023a)", "relation": "Comparison", "description": "Astute RAG is compared with GenRead (Yu et al., 2023a) in terms of accuracy and API calls."}, {"entity1": "Astute RAG", "entity2": "RobustRAG (Xiang et al., 2024)", "relation": "Comparison", "description": "Astute RAG is compared with RobustRAG (Xiang et al., 2024) in terms of accuracy and API calls."}, {"entity1": "Astute RAG", "entity2": "InstructRAG (Wei et al., 2024)", "relation": "Comparison", "description": "Astute RAG is compared with InstructRAG (Wei et al., 2024) in terms of accuracy and API calls."}, {"entity1": "Astute RAG", "entity2": "Self-Route (Xu et al., 2024a)", "relation": "Comparison", "description": "Astute RAG is compared with Self-Route (Xu et al., 2024a) in terms of accuracy and API calls."}, {"entity1": "Astute RAG", "entity2": "NQ", "relation": "Experiment-Outcome", "description": "Astute RAG achieves an accuracy of 50.17 on NQ with t=1."}, {"entity1": "Astute RAG", "entity2": "TriviaQA", "relation": "Experiment-Outcome", "description": "Astute RAG achieves an accuracy of 81.63 on TriviaQA with t=1."}, {"entity1": "Astute RAG", "entity2": "BioASQ", "relation": "Experiment-Outcome", "description": "Astute RAG achieves an accuracy of 58.04 on BioASQ with t=1."}, {"entity1": "Astute RAG", "entity2": "PopQA", "relation": "Experiment-Outcome", "description": "Astute RAG achieves an accuracy of 40.45 on PopQA with t=1."}, {"entity1": "Astute RAG", "entity2": "API calls", "relation": "Methodology", "description": "Astute RAG uses 2 API calls per query by default."}, {"entity1": "Astute RAG", "entity2": "\ud835\udc61", "relation": "Methodology", "description": "Astute RAG uses \ud835\udc61 to merge the prompt for queries."}, {"entity1": "USC (Chen et al., 2024b)", "entity2": "LLM", "relation": "Methodology", "description": "USC (Chen et al., 2024b) samples multiple LLM responses given the same context and aggregates the answers."}, {"entity1": "GenRead (Yu et al., 2023a)", "entity2": "LLM", "relation": "Methodology", "description": "GenRead (Yu et al., 2023a) augments retrieved passages with LLM-generated passages."}, {"entity1": "RobustRAG (Xiang et al., 2024)", "entity2": "LLM", "relation": "Methodology", "description": "RobustRAG (Xiang et al., 2024) aggregates answers from each independent passage to provide certifiable robustness."}, {"entity1": "InstructRAG (Wei et al., 2024)", "entity2": "LLM", "relation": "Methodology", "description": "InstructRAG (Wei et al., 2024) instructs the LLM to provide a rationale connecting the answer with information in passages."}, {"entity1": "Self-Route (Xu et al., 2024a)", "entity2": "LLM", "relation": "Methodology", "description": "Self-Route (Xu et al., 2024a) adaptively switches between LLMs with and without RAG."}, {"entity1": "Astute RAG", "entity2": "Table 2", "relation": "Publication Venue", "description": "Astute RAG is presented in Table 2, which shows the main results on Gemini under zero-shot setting."}], "c4c79ac3-8e79-43d4-a1f3-55cb8538c485": [{"entity1": "LLMs", "entity2": "internal knowledge", "relation": "Knowledge-Origin", "description": "LLMs have internal knowledge"}, {"entity1": "LLMs", "entity2": "external knowledge", "relation": "Knowledge-Origin", "description": "LLMs have external knowledge"}, {"entity1": "Astute RAG", "entity2": "Appendix A", "relation": "Publication Venue", "description": "Astute RAG prompt templates are found in Appendix A"}, {"entity1": "API", "entity2": "Astute RAG", "relation": "Tool/Resource", "description": "API is used by Astute RAG"}, {"entity1": "\ud835\udc61", "entity2": "Astute RAG", "relation": "Methodology", "description": "\ud835\udc61 is set to 1 for Astute RAG"}, {"entity1": "Table 1", "entity2": "Table 2", "relation": "Comparison", "description": "Results are compared between Table 1 and Table 2"}, {"entity1": "RAG", "entity2": "No RAG", "relation": "Comparison", "description": "RAG performance is compared to No RAG"}, {"entity1": "NQ", "entity2": "TriviaQA", "relation": "Dataset-Origin", "description": "NQ and TriviaQA are datasets"}, {"entity1": "Self-Route", "entity2": "RobustRAG", "relation": "Innovation", "description": "Self-Route and RobustRAG are related innovations"}, {"entity1": "LLMs", "entity2": "RAG", "relation": "Methodology", "description": "LLMs use RAG for knowledge consolidation"}], "192fe976-abc6-4ea1-8511-d57a07470e0e": [{"entity1": "Astute RAG", "entity2": "LLM", "relation": "Improvement", "description": "Astute RAG improves LLM performance, especially on domain-specific and long-tail questions."}, {"entity1": "RAG", "entity2": "BioASQ", "relation": "Performance Improvement", "description": "RAG significantly improves LLM performance on BioASQ."}, {"entity1": "RAG", "entity2": "PopQA", "relation": "Performance Improvement", "description": "RAG significantly improves LLM performance on PopQA."}, {"entity1": "InstructRAG", "entity2": "TriviaQA", "relation": "Best Performance", "description": "InstructRAG achieves the best performance among all baselines on TriviaQA with both Claude and Gemini."}, {"entity1": "Self-Route", "entity2": "NQ", "relation": "Better Performance", "description": "Self-Route performs better than InstructRAG on NQ."}, {"entity1": "Self-Route", "entity2": "BioASQ", "relation": "Better Performance", "description": "Self-Route performs better than InstructRAG on BioASQ."}, {"entity1": "RobustRAG", "entity2": "Gemini", "relation": "High Refusal Rate", "description": "RobustRAG with Gemini exhibits a high refusal rate in responses."}, {"entity1": "RobustRAG", "entity2": "Claude", "relation": "Different Performance", "description": "RobustRAG achieves very different performance when applied to Gemini and Claude."}, {"entity1": "Astute RAG", "entity2": "Baselines", "relation": "Outperformance", "description": "Astute RAG consistently outperforms baselines across all datasets."}, {"entity1": "Astute RAG", "entity2": "Claude", "relation": "Relative Improvement", "description": "Astute RAG achieves a relative improvement of 6.85% on Claude compared to the best baseline."}, {"entity1": "Astute RAG", "entity2": "Gemini", "relation": "Relative Improvement", "description": "Astute RAG achieves a relative improvement of 4.13% on Gemini compared to the best baseline."}, {"entity1": "Astute RAG", "entity2": "API", "relation": "No Correlation", "description": "Increasing the number of API calls does not necessarily correlate with improved performance for Astute RAG."}, {"entity1": "Astute RAG", "entity2": "\ud835\udc61", "relation": "Improvement Margin", "description": "The improvement margin of Astute RAG becomes lower when \ud835\udc61 becomes larger."}, {"entity1": "Astute RAG", "entity2": "BioASQ", "relation": "Benefit from Iterative Knowledge Consolidation", "description": "Increasing \ud835\udc61 primarily benefits BioASQ, which relies heavily on external knowledge."}, {"entity1": "Astute RAG", "entity2": "PopQA", "relation": "Benefit from Iterative Knowledge Consolidation", "description": "Increasing \ud835\udc61 primarily benefits PopQA, which relies heavily on external knowledge."}, {"entity1": "Astute RAG", "entity2": "NQ", "relation": "No Further Improvement", "description": "Performance on NQ does not improve further when \ud835\udc61 reaches 3."}, {"entity1": "Astute RAG", "entity2": "TriviaQA", "relation": "No Further Improvement", "description": "Performance on TriviaQA does not improve further when \ud835\udc61 reaches 3."}], "b0eead99-110a-4a00-9577-c09ec66bfa39": [{"entity1": "Astute RAG", "entity2": "TriviaQA", "relation": "Performance Comparison", "description": "Astute RAG's performance does not improve further when t reaches 3 on TriviaQA, due to the less critical role of external knowledge."}, {"entity1": "Astute RAG", "entity2": "internal knowledge", "relation": "Influence Limitation", "description": "The influence of internal knowledge is limited by setting the parameter \u02c6m to a smaller value."}, {"entity1": "Astute RAG", "entity2": "Claude", "relation": "Performance Comparison", "description": "Astute RAG achieves consistently better performance than all baselines, including Claude, across different retrieval precision."}, {"entity1": "Astute RAG", "entity2": "retrieval precision", "relation": "Performance Improvement", "description": "Astute RAG improves performance across different retrieval precision, indicating its effectiveness in improving RAG trustworthiness."}, {"entity1": "Astute RAG", "entity2": "knowledge conflicts", "relation": "Conflict Resolution", "description": "Astute RAG successfully chooses the correct answer in approximately 80% of cases where internal and external knowledge conflict."}, {"entity1": "Astute RAG", "entity2": "LLM-internal knowledge", "relation": "Information Combination", "description": "Astute RAG can effectively combine partially-correct information from LLM-internal and external knowledge to improve performance."}, {"entity1": "Astute RAG", "entity2": "external knowledge", "relation": "Performance Improvement", "description": "Astute RAG brings performance improvement even when neither internal nor external knowledge alone leads to the correct answer."}, {"entity1": "Figure 4", "entity2": "Astute RAG", "relation": "Performance Visualization", "description": "Figure 4 shows the performance of Astute RAG across different retrieval precision and its effectiveness in addressing knowledge conflicts."}, {"entity1": "RGB", "entity2": "Astute RAG", "relation": "Performance Comparison", "description": "The observation on RGB verifies the effectiveness of Astute RAG in overcoming imperfect retrieval augmentation."}], "d8a05b43-5cc9-497e-ad08-13ab63bdf998": [{"entity1": "Astute RAG", "entity2": "Imperfect Retrieval Augmentation", "relation": "Overcoming", "description": "Astute RAG is designed to overcome imperfect retrieval augmentation and knowledge conflicts for Large Language Models."}, {"entity1": "Astute RAG", "entity2": "Knowledge Conflicts", "relation": "Overcoming", "description": "Astute RAG is designed to overcome knowledge conflicts for Large Language Models."}, {"entity1": "Astute RAG", "entity2": "Large Language Models (LLMs)", "relation": "Improvement", "description": "Astute RAG improves the performance of Large Language Models by addressing imperfect retrieval augmentation and knowledge conflicts."}, {"entity1": "Retrieval Augmented Generation (RAG)", "entity2": "Large Language Models (LLMs)", "relation": "Enhancement", "description": "RAG seeks to address the inherent knowledge limitation of LLMs with passages retrieved from external sources of information."}, {"entity1": "Astute RAG", "entity2": "No RAG", "relation": "Comparison", "description": "Astute RAG reaches a performance close to No RAG under the worst-case scenario, outperforming other RAG systems."}, {"entity1": "Tan et al.", "entity2": "Confirmation Bias", "relation": "Research", "description": "Tan et al. researched confirmation bias, which is relevant to the effectiveness of Astute RAG in avoiding incorrect information."}, {"entity1": "Guu et al.", "entity2": "Retrieval Augmented Generation (RAG)", "relation": "Research", "description": "Guu et al. researched RAG, which is a relevant concept to the development of Astute RAG."}, {"entity1": "Borgeaud et al.", "entity2": "Retrieval Augmented Generation (RAG)", "relation": "Research", "description": "Borgeaud et al. researched RAG, which is a relevant concept to the development of Astute RAG."}, {"entity1": "Lewis et al.", "entity2": "Retrieval Augmented Generation (RAG)", "relation": "Research", "description": "Lewis et al. researched RAG, which is a relevant concept to the development of Astute RAG."}, {"entity1": "Cuconasu", "entity2": "Imperfect Retrieval Augmentation", "relation": "Research", "description": "Cuconasu researched the negative impact of noisy information within retrieved passages, which is relevant to the development of Astute RAG."}, {"entity1": "Figure 4", "entity2": "Astute RAG", "relation": "Illustration", "description": "Figure 4 presents the results under the worst-case setting on RGB, demonstrating the noise robustness of Astute RAG."}, {"entity1": "Figure 5", "entity2": "Astute RAG", "relation": "Illustration", "description": "Figure 5 presents two representative examples showing the intermediate outputs of Astute RAG."}, {"entity1": "RGB", "entity2": "Astute RAG", "relation": "Experiment", "description": "Astute RAG was tested under the worst-case scenario on RGB, where all retrieved documents are negative."}, {"entity1": "Claude", "entity2": "Astute RAG", "relation": "Comparison", "description": "Claude's performance is compared to Astute RAG in terms of retrieval precision and knowledge conflicts."}], "2e7e4e91-9ac0-4dce-8a17-f21e73a7ee8a": [{"entity1": "RAG", "entity2": "risk-sensitive domains", "relation": "Application", "description": "RAG is adopted in various real-world applications, including risk-sensitive domains."}, {"entity1": "Cuconasu et al.", "entity2": "noisy information", "relation": "Research Area", "description": "Cuconasu et al. have researched the negative impact of noisy information within retrieved passages."}, {"entity1": "Fang et al.", "entity2": "LLMs", "relation": "Methodology", "description": "Fang et al. have trained LLMs with noisy context to enhance robustness."}, {"entity1": "Pan et al.", "entity2": "LLMs", "relation": "Methodology", "description": "Pan et al. have trained LLMs with noisy context to enhance robustness."}, {"entity1": "Yoran et al.", "entity2": "LLMs", "relation": "Methodology", "description": "Yoran et al. have trained LLMs with noisy context to enhance robustness."}, {"entity1": "Yu et al.", "entity2": "LLMs", "relation": "Methodology", "description": "Yu et al. have trained LLMs with noisy context to enhance robustness."}, {"entity1": "Wang et al.", "entity2": "passage reranking", "relation": "Methodology", "description": "Wang et al. have used small models to filter out irrelevant passages."}, {"entity1": "Xu et al.", "entity2": "passage reranking", "relation": "Methodology", "description": "Xu et al. have used small models to filter out irrelevant passages."}, {"entity1": "Glass et al.", "entity2": "passage reranking", "relation": "Methodology", "description": "Glass et al. have researched passage reranking."}, {"entity1": "Yu et al.", "entity2": "passage reranking", "relation": "Methodology", "description": "Yu et al. have researched passage reranking."}, {"entity1": "Asai et al.", "entity2": "dynamic and iterative retrieval", "relation": "Methodology", "description": "Asai et al. have researched dynamic and iterative retrieval."}, {"entity1": "Jiang et al.", "entity2": "dynamic and iterative retrieval", "relation": "Methodology", "description": "Jiang et al. have researched dynamic and iterative retrieval."}, {"entity1": "Yan et al.", "entity2": "dynamic and iterative retrieval", "relation": "Methodology", "description": "Yan et al. have researched dynamic and iterative retrieval."}, {"entity1": "Ma et al.", "entity2": "query rewriting", "relation": "Methodology", "description": "Ma et al. have researched query rewriting."}], "0ee6405c-07e9-46ff-bee4-c2606b3b03c3": [{"entity1": "Astute RAG", "entity2": "Large Language Models", "relation": "Application", "description": "Astute RAG is designed to overcome imperfect retrieval augmentation and knowledge conflicts for Large Language Models."}, {"entity1": "Astute RAG", "entity2": "RAG systems", "relation": "Innovation", "description": "Astute RAG enhances RAG robustness at the post-retrieval stage."}, {"entity1": "RobustRAG", "entity2": "Astute RAG", "relation": "Comparison", "description": "RobustRAG aggregates answers from each independent passage, whereas Astute RAG incorporates internal knowledge to recover from RAG failures."}, {"entity1": "InstructRAG", "entity2": "Astute RAG", "relation": "Comparison", "description": "InstructRAG instructs the LLM to provide a rationale, whereas Astute RAG detects the correct answer from noisy retrieved information."}, {"entity1": "MADRA", "entity2": "Astute RAG", "relation": "Comparison", "description": "MADRA applies multi-agent debate, whereas Astute RAG uses internal knowledge to select helpful evidence."}, {"entity1": "Xiang et al.", "entity2": "RobustRAG", "relation": "Author-Role", "description": "Xiang et al. are the authors of RobustRAG."}, {"entity1": "Wei et al.", "entity2": "InstructRAG", "relation": "Author-Role", "description": "Wei et al. are the authors of InstructRAG."}, {"entity1": "Wang et al.", "entity2": "MADRA", "relation": "Author-Role", "description": "Wang et al. are the authors of MADRA."}, {"entity1": "Yu et al.", "entity2": "LLM-generated passage", "relation": "Author-Expertise", "description": "Yu et al. have explored using LLM-generated passage as context."}, {"entity1": "Zhang et al.", "entity2": "LLM-generated passage", "relation": "Author-Expertise", "description": "Zhang et al. have explored using LLM-generated passage as context."}, {"entity1": "Jeong et al.", "entity2": "Adaptive switching", "relation": "Author-Expertise", "description": "Jeong et al. have explored adaptively switching between LLMs with and without RAG."}, {"entity1": "Mallen et al.", "entity2": "Adaptive switching", "relation": "Author-Expertise", "description": "Mallen et al. have explored adaptively switching between LLMs with and without RAG."}, {"entity1": "Xu et al.", "entity2": "Adaptive switching", "relation": "Author-Expertise", "description": "Xu et al. have explored adaptively switching between LLMs with and without RAG."}, {"entity1": "Figure 5", "entity2": "Astute RAG", "relation": "Result", "description": "Figure 5 shows qualitative examples of Astute RAG's performance."}, {"entity1": "2023", "entity2": "Wang et al.", "relation": "Publication Date", "description": "Wang et al.'s work was published in 2023."}, {"entity1": "2024", "entity2": "Xiang et al.", "relation": "Publication Date", "description": "Xiang et al.'s work was published in 2024."}, {"entity1": "2024", "entity2": "Wei et al.", "relation": "Publication Date", "description": "Wei et al.'s work was published in 2024."}, {"entity1": "2024", "entity2": "Wang et al.", "relation": "Publication Date", "description": "Wang et al.'s work was published in 2024."}], "20b9cf33-a4f3-49fe-967a-74d67a505355": [{"entity1": "Astute RAG", "entity2": "contrastive decoding", "relation": "Methodology", "description": "Astute RAG utilizes contrastive decoding to overcome imperfect retrieval augmentation and knowledge conflicts for Large Language Models."}, {"entity1": "Jin et al.", "entity2": "contrastive decoding", "relation": "Author-Expertise", "description": "Jin et al. are experts in contrastive decoding, as referenced in the context of Astute RAG."}, {"entity1": "Astute RAG", "entity2": "Large Language Models", "relation": "Application", "description": "Astute RAG is applied to Large Language Models to improve their performance by addressing knowledge conflicts."}, {"entity1": "Google Cloud AI Research", "entity2": "Astute RAG", "relation": "Affiliation", "description": "The research on Astute RAG is affiliated with Google Cloud AI Research."}, {"entity1": "A. Asai", "entity2": "Google Cloud AI Research", "relation": "Affiliation", "description": "A. Asai is affiliated with Google Cloud AI Research, contributing to the development of Astute RAG."}, {"entity1": "The Twelfth International Conference on Learning Representations", "entity2": "A. Asai", "relation": "Publication Venue", "description": "A. Asai's work was published at The Twelfth International Conference on Learning Representations."}, {"entity1": "Constitutional AI", "entity2": "Y. Bai", "relation": "Author-Role", "description": "Y. Bai is an author of the work on Constitutional AI."}, {"entity1": "Astute RAG", "entity2": "black-box setting", "relation": "Experiment-Design", "description": "Astute RAG is designed to operate in a black-box setting, requiring no further training."}, {"entity1": "Jinsung Yoon", "entity2": "Astute RAG", "relation": "Author-Expertise", "description": "Jinsung Yoon provided valuable discussions and insights that helped improve the paper on Astute RAG."}, {"entity1": "Zhao et al.", "entity2": "contrastive decoding", "relation": "Author-Expertise", "description": "Zhao et al. are experts in contrastive decoding, as referenced in the context of Astute RAG."}, {"entity1": "Astute RAG", "entity2": "internal knowledge", "relation": "Methodology", "description": "Astute RAG leverages the internal knowledge of Large Language Models to refine generated responses."}, {"entity1": "Astute RAG", "entity2": "external knowledge", "relation": "Methodology", "description": "Astute RAG consolidates internal and external knowledge to achieve more reliable answers."}, {"entity1": "V. Balachandran", "entity2": "Astute RAG", "relation": "Author-Expertise", "description": "V. Balachandran's work is referenced in the context of Astute RAG, indicating expertise in related areas."}, {"entity1": "arXiv", "entity2": "Y. Bai", "relation": "Publication Venue", "description": "Y. Bai's work on Constitutional AI was published on arXiv."}], "b864796c-61f2-4dc3-bea3-e9153675b20d": [{"entity1": "innon", "entity2": "Constitutional ai", "relation": "Author-Role", "description": "innon is an author of the paper Constitutional ai: Harmlessness from ai feedback"}, {"entity1": "V. Balachandran", "entity2": "Eureka", "relation": "Author-Role", "description": "V. Balachandran is an author of the paper Eureka: Evaluating and understanding large foundation models"}, {"entity1": "S. Borgeaud", "entity2": "Improving language models by retrieving from trillions of tokens", "relation": "Author-Role", "description": "S. Borgeaud is an author of the paper Improving language models by retrieving from trillions of tokens"}, {"entity1": "J. Chen", "entity2": "Benchmarking large language models in retrieval-augmented generation", "relation": "Author-Role", "description": "J. Chen is an author of the paper Benchmarking large language models in retrieval-augmented generation"}, {"entity1": "arXiv:2212.08073", "entity2": "Constitutional ai: Harmlessness from ai feedback", "relation": "Publication Venue", "description": "The paper Constitutional ai: Harmlessness from ai feedback is published on arXiv with the identifier arXiv:2212.08073"}, {"entity1": "arXiv:2409.10566", "entity2": "Eureka: Evaluating and understanding large foundation models", "relation": "Publication Venue", "description": "The paper Eureka: Evaluating and understanding large foundation models is published on arXiv with the identifier arXiv:2409.10566"}, {"entity1": "International conference on machine learning", "entity2": "Improving language models by retrieving from trillions of tokens", "relation": "Publication Venue", "description": "The paper Improving language models by retrieving from trillions of tokens is published in the International conference on machine learning"}, {"entity1": "PMLR", "entity2": "Improving language models by retrieving from trillions of tokens", "relation": "Publication Venue", "description": "The paper Improving language models by retrieving from trillions of tokens is published by PMLR"}, {"entity1": "AAAI Conference on Artificial Intelligence", "entity2": "Benchmarking large language models in retrieval-augmented generation", "relation": "Publication Venue", "description": "The paper Benchmarking large language models in retrieval-augmented generation is published in the AAAI Conference on Artificial Intelligence"}, {"entity1": "V. Balachandran", "entity2": "J. Chen", "relation": "Co-author", "description": "V. Balachandran and J. Chen are co-authors of the paper Eureka: Evaluating and understanding large foundation models"}, {"entity1": "S. Borgeaud", "entity2": "A. Mensch", "relation": "Co-author", "description": "S. Borgeaud and A. Mensch are co-authors of the paper Improving language models by retrieving from trillions of tokens"}, {"entity1": "J. Chen", "entity2": "H. Lin", "relation": "Co-author", "description": "J. Chen and H. Lin are co-authors of the paper Benchmarking large language models in retrieval-augmented generation"}], "c5ad6145-e62e-486d-a5c4-ae540a5767e8": [{"entity1": "Astute RAG", "entity2": "Large Language Models", "relation": "Methodology", "description": "Astute RAG is a method used to overcome imperfect retrieval augmentation and knowledge conflicts for large language models."}, {"entity1": "X. Chen", "entity2": "R. Aksitov", "relation": "Co-author", "description": "X. Chen and R. Aksitov are co-authors of the paper 'Universal self-consistency for large language models'."}, {"entity1": "ICML2024 Workshop on In-Context Learning", "entity2": "Universal self-consistency for large language models", "relation": "Publication Venue", "description": "The paper 'Universal self-consistency for large language models' was published at the ICML2024 Workshop on In-Context Learning."}, {"entity1": "F. Cuconasu", "entity2": "G. Trappolini", "relation": "Co-author", "description": "F. Cuconasu and G. Trappolini are co-authors of the paper 'The power of noise: Redefining retrieval for rag systems'."}, {"entity1": "International ACM SIGIR Conference on Research and Development in Information Retrieval", "entity2": "The power of noise: Redefining retrieval for rag systems", "relation": "Publication Venue", "description": "The paper 'The power of noise: Redefining retrieval for rag systems' was published at the International ACM SIGIR Conference on Research and Development in Information Retrieval."}, {"entity1": "S. Dai", "entity2": "C. Xu", "relation": "Co-author", "description": "S. Dai and C. Xu are co-authors of the paper 'Unifying bias and unfairness in information retrieval: A survey of challenges and opportunities with large language models'."}, {"entity1": "arXiv", "entity2": "Unifying bias and unfairness in information retrieval: A survey of challenges and opportunities with large language models", "relation": "Publication Venue", "description": "The paper 'Unifying bias and unfairness in information retrieval: A survey of challenges and opportunities with large language models' was published on arXiv."}, {"entity1": "F. Fang", "entity2": "Y. Bai", "relation": "Co-author", "description": "F. Fang and Y. Bai are co-authors of the paper 'Enhancing noise robustness of retrieval-augmented language models with adaptive adversarial training'."}, {"entity1": "arXiv", "entity2": "Enhancing noise robustness of retrieval-augmented language models with adaptive adversarial training", "relation": "Publication Venue", "description": "The paper 'Enhancing noise robustness of retrieval-augmented language models with adaptive adversarial training' was published on arXiv."}, {"entity1": "M. Glass", "entity2": "G. Rossiello", "relation": "Co-author", "description": "M. Glass and G. Rossiello are co-authors of the paper 'Re2g: Retrieve, rerank, generate'."}, {"entity1": "2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "entity2": "Re2g: Retrieve, rerank, generate", "relation": "Publication Venue", "description": "The paper 'Re2g: Retrieve, rerank, generate' was published at the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies."}, {"entity1": "K. Guu", "entity2": "K. Lee", "relation": "Co-author", "description": "K. Guu and K. Lee are co-authors of the paper 'Retrieval augmented language model pre-training'."}, {"entity1": "International Conference on Machine Learning", "entity2": "Retrieval augmented language model pre-training", "relation": "Publication Venue", "description": "The paper 'Retrieval augmented language model pre-training' was published at the International Conference on Machine Learning."}, {"entity1": "S. Jeong", "entity2": "J. Baek", "relation": "Co-author", "description": "S. Jeong and J. Baek are co-authors of the paper 'Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity'."}, {"entity1": "2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "entity2": "Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity", "relation": "Publication Venue", "description": "The paper 'Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity' was published at the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies."}, {"entity1": "Z. Jiang", "entity2": "F. F. Xu", "relation": "Co-author", "description": "Z. Jiang and F. F. Xu are co-authors of the paper 'Active retrieval augmented generation'."}, {"entity1": "2023 Conference on Empirical Methods in Natural Language Processing", "entity2": "Active retrieval augmented generation", "relation": "Publication Venue", "description": "The paper 'Active retrieval augmented generation' was published at the 2023 Conference on Empirical Methods in Natural Language Processing."}, {"entity1": "Z. Jin", "entity2": "P. Cao", "relation": "Co-author", "description": "Z. Jin and P. Cao are co-authors of the paper 'Tug-of-war between knowledge: Exploring and resolving knowledge conflicts in retrieval-augmented language models'."}, {"entity1": "2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)", "entity2": "Tug-of-war between knowledge: Exploring and resolving knowledge conflicts in retrieval-augmented language models", "relation": "Publication Venue", "description": "The paper 'Tug-of-war between knowledge: Exploring and resolving knowledge conflicts in retrieval-augmented language models' was published at the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)."}, {"entity1": "M. Joshi", "entity2": "E. Choi", "relation": "Co-author", "description": "M. Joshi and E. Choi are co-authors of the paper 'Triviaqa: A large scale distantly supervised challenge'."}], "003bb1d0-8d14-4ddd-b589-fcfaf508410a": [{"entity1": "M. Joshi", "entity2": "Triviaqa", "relation": "Author-Role", "description": "M. Joshi is an author of the Triviaqa dataset."}, {"entity1": "T. Kwiatkowski", "entity2": "Naturalquestions", "relation": "Author-Role", "description": "T. Kwiatkowski is an author of the Naturalquestions benchmark."}, {"entity1": "P. Lewis", "entity2": "Retrieval-augmented generation", "relation": "Author-Role", "description": "P. Lewis is an author of the Retrieval-augmented generation paper."}, {"entity1": "S. Longpre", "entity2": "Entity-based knowledge conflicts", "relation": "Author-Role", "description": "S. Longpre is an author of the Entity-based knowledge conflicts paper."}, {"entity1": "Triviaqa", "entity2": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics", "relation": "Publication Venue", "description": "Triviaqa was published in the Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics."}, {"entity1": "Naturalquestions", "entity2": "Transactions of the Association for Computational Linguistics", "relation": "Publication Venue", "description": "Naturalquestions was published in the Transactions of the Association for Computational Linguistics."}, {"entity1": "Retrieval-augmented generation", "entity2": "Advances in Neural Information Processing Systems", "relation": "Publication Venue", "description": "Retrieval-augmented generation was published in the Advances in Neural Information Processing Systems."}, {"entity1": "Entity-based knowledge conflicts", "entity2": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing", "relation": "Publication Venue", "description": "Entity-based knowledge conflicts was published in the Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing."}, {"entity1": "M. Joshi", "entity2": "E. Choi", "relation": "Co-author", "description": "M. Joshi and E. Choi are co-authors of the Triviaqa paper."}, {"entity1": "T. Kwiatkowski", "entity2": "J. Palomaki", "relation": "Co-author", "description": "T. Kwiatkowski and J. Palomaki are co-authors of the Naturalquestions paper."}, {"entity1": "P. Lewis", "entity2": "E. Perez", "relation": "Co-author", "description": "P. Lewis and E. Perez are co-authors of the Retrieval-augmented generation paper."}, {"entity1": "S. Longpre", "entity2": "K. Perisetla", "relation": "Co-author", "description": "S. Longpre and K. Perisetla are co-authors of the Entity-based knowledge conflicts paper."}, {"entity1": "Association for Computational Linguistics", "entity2": "Transactions of the Association for Computational Linguistics", "relation": "Affiliation", "description": "The Association for Computational Linguistics is affiliated with the Transactions of the Association for Computational Linguistics."}, {"entity1": "Association for Computational Linguistics", "entity2": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics", "relation": "Affiliation", "description": "The Association for Computational Linguistics is affiliated with the Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics."}], "42ef21ec-3f6d-4085-ae7a-76959c0dbcb4": [{"entity1": "Astute RAG", "entity2": "Large Language Models", "relation": "Methodology", "description": "Astute RAG is designed to overcome imperfect retrieval augmentation and knowledge conflicts for large language models."}, {"entity1": "X. Ma", "entity2": "Y. Gong", "relation": "Co-author", "description": "X. Ma and Y. Gong co-authored a paper on query rewriting in retrieval-augmented large language models."}, {"entity1": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing", "entity2": "X. Ma", "relation": "Publication Venue", "description": "X. Ma published a paper in the Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing."}, {"entity1": "A. Mallen", "entity2": "A. Asai", "relation": "Co-author", "description": "A. Mallen and A. Asai co-authored a paper on investigating the effectiveness of parametric and non-parametric memories."}, {"entity1": "R. Pan", "entity2": "B. Cao", "relation": "Co-author", "description": "R. Pan and B. Cao co-authored a paper on teaching large language models credibility-aware generation."}, {"entity1": "R. Shao", "entity2": "J. He", "relation": "Co-author", "description": "R. Shao and J. He co-authored a paper on scaling retrieval-based language models with a trillion-token datastore."}, {"entity1": "H. Su", "entity2": "H. Yen", "relation": "Co-author", "description": "H. Su and H. Yen co-authored a paper on a realistic and challenging benchmark for reasoning-intensive retrieval."}, {"entity1": "H. Tan", "entity2": "F. Sun", "relation": "Co-author", "description": "H. Tan and F. Sun co-authored a paper on how language models merge generated and retrieved contexts for open-domain QA."}, {"entity1": "N. Thakur", "entity2": "N. Reimers", "relation": "Co-author", "description": "N. Thakur and N. Reimers co-authored a paper on a heterogeneous benchmark for zero-shot evaluation of information retrieval models."}, {"entity1": "G. Tsatsaronis", "entity2": "G. Balikas", "relation": "Co-author", "description": "G. Tsatsaronis and G. Balikas co-authored a paper on an overview of the bioasq large-scale biomedical semantic indexing and question answering competition."}, {"entity1": "F. Wang", "entity2": "W. Mo", "relation": "Co-author", "description": "F. Wang and W. Mo co-authored a paper on a causal view of entity bias in large language models."}, {"entity1": "H. Wang", "entity2": "X. Du", "relation": "Co-author", "description": "H. Wang and X. Du co-authored a paper on retrieval-augmented reasoning in multi-agent debates."}, {"entity1": "Z. Wang", "entity2": "J. Araki", "relation": "Co-author", "description": "Z. Wang and J. Araki co-authored a paper on learning to filter context for retrieval-augmented generation."}, {"entity1": "Astute RAG", "entity2": "Retrieval Augmentation", "relation": "Methodology", "description": "Astute RAG uses retrieval augmentation to improve large language models."}, {"entity1": "Large Language Models", "entity2": "Retrieval Augmentation", "relation": "Methodology", "description": "Large language models use retrieval augmentation to improve their performance."}, {"entity1": "X. Ma", "entity2": "Retrieval Augmentation", "relation": "Author-Expertise", "description": "X. Ma has expertise in retrieval augmentation for large language models."}, {"entity1": "A. Asai", "entity2": "Parametric Memories", "relation": "Author-Expertise", "description": "A. Asai has expertise in parametric memories for language models."}, {"entity1": "R. Pan", "entity2": "Credibility-Aware Generation", "relation": "Author-Expertise", "description": "R. Pan has expertise in credibility-aware generation for large language models."}, {"entity1": "H. Su", "entity2": "Reasoning-Intensive Retrieval", "relation": "Author-Expertise", "description": "H. Su has expertise in reasoning-intensive retrieval for language models."}, {"entity1": "N. Thakur", "entity2": "Zero-Shot Evaluation", "relation": "Author-Expertise", "description": "N. Thakur has expertise in zero-shot evaluation of information retrieval models."}, {"entity1": "G. Tsatsaronis", "entity2": "Biomedical Semantic Indexing", "relation": "Author-Expertise", "description": "G. Tsatsaronis has expertise in biomedical semantic indexing and question answering."}, {"entity1": "F. Wang", "entity2": "Entity Bias", "relation": "Author-Expertise", "description": "F. Wang has expertise in entity bias in large language models."}, {"entity1": "H. Wang", "entity2": "Multi-Agent Debates", "relation": "Author-Expertise", "description": "H. Wang has expertise in multi-agent debates with retrieval-augmented reasoning."}, {"entity1": "Z. Wang", "entity2": "Context Filtering", "relation": "Author-Expertise", "description": "Z. Wang has expertise in context filtering for retrieval-augmented generation."}], "6c1519f9-b171-4bf1-96bf-36a1ebed1dfa": [{"entity1": "Z. Wang", "entity2": "arXiv:2312.04854", "relation": "Author-Publication", "description": "Z. Wang is an author of the publication arXiv:2312.04854"}, {"entity1": "Z. Wang", "entity2": "J. Araki", "relation": "Co-author", "description": "Z. Wang and J. Araki are co-authors of the publication arXiv:2312.04854"}, {"entity1": "arXiv:2312.04854", "entity2": "augmented reasoning", "relation": "Publication-Research Area", "description": "The publication arXiv:2312.04854 is related to the research area of augmented reasoning"}, {"entity1": "Z. Wang", "entity2": "G. Neubig", "relation": "Co-author", "description": "Z. Wang and G. Neubig are co-authors of the publication arXiv:2311.08377"}, {"entity1": "arXiv:2311.08377", "entity2": "Learning to filter context for retrieval-augmented generation", "relation": "Publication-Title", "description": "The publication arXiv:2311.08377 has the title 'Learning to filter context for retrieval-augmented generation'"}, {"entity1": "Speculative rag", "entity2": "arXiv:2407.08223", "relation": "Tool/Resource-Publication", "description": "Speculative rag is a tool/resource mentioned in the publication arXiv:2407.08223"}, {"entity1": "Z. Wang", "entity2": "Speculative rag", "relation": "Author-Tool/Resource", "description": "Z. Wang is an author of the publication that mentions the tool/resource Speculative rag"}, {"entity1": "Instructrag", "entity2": "arXiv:2406.13629", "relation": "Tool/Resource-Publication", "description": "Instructrag is a tool/resource mentioned in the publication arXiv:2406.13629"}, {"entity1": "J. Xie", "entity2": "The Twelfth International Conference on Learning Representations", "relation": "Author-Publication Venue", "description": "J. Xie is an author who published at The Twelfth International Conference on Learning Representations"}, {"entity1": "K. Zhang", "entity2": "J. Xie", "relation": "Co-author", "description": "K. Zhang and J. Xie are co-authors of the publication presented at The Twelfth International Conference on Learning Representations"}, {"entity1": "large language models", "entity2": "knowledge conflicts", "relation": "Research Area-Research Area", "description": "Large language models and knowledge conflicts are related research areas"}, {"entity1": "J. Xie", "entity2": "Adaptive chameleon or stubborn sloth", "relation": "Author-Publication", "description": "J. Xie is an author of the publication 'Adaptive chameleon or stubborn sloth'"}, {"entity1": "R. Lou", "entity2": "Y. Su", "relation": "Co-author", "description": "R. Lou and Y. Su are co-authors of the publication presented at The Twelfth International Conference on Learning Representations"}, {"entity1": "G. Neubig", "entity2": "M. R. Parvez", "relation": "Co-author", "description": "G. Neubig and M. R. Parvez are co-authors of the publication arXiv:2311.08377"}], "481a0c55-c8e1-41b3-bed8-dbab5be5c5bb": [{"entity1": "arXiv:2410.03659", "entity2": "vision-language models", "relation": "Publication Venue", "description": "The paper arXiv:2410.03659 is a publication venue for research on vision-language models."}, {"entity1": "W. Zou", "entity2": "Poisonedrag", "relation": "Author-Role", "description": "W. Zou is an author of the paper Poisonedrag."}, {"entity1": "R. Geng", "entity2": "Poisonedrag", "relation": "Author-Role", "description": "R. Geng is an author of the paper Poisonedrag."}, {"entity1": "B. Wang", "entity2": "Poisonedrag", "relation": "Author-Role", "description": "B. Wang is an author of the paper Poisonedrag."}, {"entity1": "J. Jia", "entity2": "Poisonedrag", "relation": "Author-Role", "description": "J. Jia is an author of the paper Poisonedrag."}, {"entity1": "arXiv:2402.07867", "entity2": "Poisonedrag", "relation": "Publication Venue", "description": "The paper arXiv:2402.07867 is a publication venue for the research Poisonedrag."}, {"entity1": "large language models", "entity2": "Poisonedrag", "relation": "Research Area", "description": "Poisonedrag is related to the research area of large language models."}, {"entity1": "vision-language models", "entity2": "large language models", "relation": "Comparison", "description": "The text compares or relates vision-language models to large language models."}], "845be6a1-0fa2-41a3-81e8-c1a43f419320": [{"entity1": "Astute RAG", "entity2": "Large Language Models", "relation": "Application", "description": "Astute RAG is applied to overcome imperfect retrieval augmentation and knowledge conflicts in Large Language Models."}, {"entity1": "Prompt Template", "entity2": "Astute RAG", "relation": "Tool/Resource", "description": "A Prompt Template is used for Astute RAG."}, {"entity1": "Adaptive Passage Generation", "entity2": "\ud835\udc5d\ud835\udc54\ud835\udc52\ud835\udc5b", "relation": "Methodology", "description": "Adaptive Passage Generation is related to \ud835\udc5d\ud835\udc54\ud835\udc52\ud835\udc5b for generating documents."}, {"entity1": "Iterative Knowledge Consolidation", "entity2": "\ud835\udc5d\ud835\udc50\ud835\udc5c\ud835\udc5b", "relation": "Methodology", "description": "Iterative Knowledge Consolidation is related to \ud835\udc5d\ud835\udc50\ud835\udc5c\ud835\udc5b for consolidating information."}, {"entity1": "memory", "entity2": "information", "relation": "Storage", "description": "Memory is used to store information."}, {"entity1": "retrieval", "entity2": "information", "relation": "Access", "description": "Retrieval is used to access information."}, {"entity1": "document", "entity2": "information", "relation": "Container", "description": "A document contains information."}, {"entity1": "transparency", "entity2": "document", "relation": "Property", "description": "Transparency is a property of a document, indicating the source and original document numbers."}, {"entity1": "question", "entity2": "document", "relation": "Input-Output", "description": "A question is used as input to generate a document as output."}, {"entity1": "context", "entity2": "question", "relation": "Input-Context", "description": "Context is provided as input to answer a question."}, {"entity1": "knowledge", "entity2": "information", "relation": "Type", "description": "Knowledge is a type of information."}, {"entity1": "Large Language Models", "entity2": "knowledge", "relation": "Application", "description": "Large Language Models are applied to knowledge."}], "f17da396-4eea-4699-800f-34496a61f6fb": [{"entity1": "Astute RAG", "entity2": "Large Language Models", "relation": "Methodology", "description": "Astute RAG is designed to overcome imperfect retrieval augmentation and knowledge conflicts for Large Language Models."}, {"entity1": "Prompt for Knowledge Consolidation and Answer Finalization", "entity2": "Astute RAG", "relation": "Tool/Resource", "description": "The prompt is utilized in the process of Astute RAG for knowledge consolidation and answer finalization."}, {"entity1": "Google Search", "entity2": "Web", "relation": "Dataset-Origin", "description": "Google Search is used as the retriever with the Web as the corpus for the benchmark."}, {"entity1": "Chen et al.", "entity2": "Yang et al.", "relation": "Comparison", "description": "The benchmark is compared to previous ones by Chen et al. and Yang et al. in terms of handling imperfect retrieval."}, {"entity1": "NQ", "entity2": "TriviaQA", "relation": "Dataset-Origin", "description": "NQ and TriviaQA are two widely-studied question-answering datasets in general domains."}, {"entity1": "BioASQ", "entity2": "Tsatsaronis et al.", "relation": "Author-Role", "description": "BioASQ is a dataset from the biomedical domain, introduced by Tsatsaronis et al."}, {"entity1": "Kwiatkowski et al.", "entity2": "NQ", "relation": "Author-Role", "description": "Kwiatkowski et al. are the authors of the NQ dataset."}, {"entity1": "Josh et al.", "entity2": "TriviaQA", "relation": "Author-Role", "description": "Josh et al. are the authors of the TriviaQA dataset."}, {"entity1": "Astute RAG", "entity2": "RAG", "relation": "Innovation", "description": "Astute RAG is an improvement over traditional RAG, addressing imperfect retrieval augmentation and knowledge conflicts."}, {"entity1": "Large Language Models", "entity2": "LLMs", "relation": "Synonym", "description": "Large Language Models are also referred to as LLMs."}], "cf8a178a-1f19-4961-9326-ab291b93eeb2": [{"entity1": "BioASQ", "entity2": "biomedical domain", "relation": "Affiliation", "description": "BioASQ is a QA dataset from the biomedical domain."}, {"entity1": "Tsatsaronis et al.", "entity2": "BioASQ", "relation": "Author-Role", "description": "Tsatsaronis et al. are authors of BioASQ."}, {"entity1": "RAG", "entity2": "BioASQ", "relation": "Tool/Resource", "description": "RAG has demonstrated significant benefits for BioASQ when general-purpose LLMs are considered."}, {"entity1": "PopQA", "entity2": "long-tail knowledge", "relation": "Research Area", "description": "PopQA focuses on long-tail knowledge."}, {"entity1": "Mallen et al.", "entity2": "PopQA", "relation": "Author-Role", "description": "Mallen et al. are authors of PopQA."}, {"entity1": "LLMs", "entity2": "PopQA", "relation": "Challenge", "description": "PopQA has been shown to be challenging for even advanced LLMs to solve without external knowledge."}, {"entity1": "Custom Search API", "entity2": "Google", "relation": "Affiliation", "description": "Custom Search API is a tool provided by Google."}, {"entity1": "BioASQ", "entity2": "QA datasets", "relation": "Publication Venue", "description": "BioASQ is a type of QA dataset."}, {"entity1": "PopQA", "entity2": "QA datasets", "relation": "Publication Venue", "description": "PopQA is a type of QA dataset."}], "21bc5716-cc44-4f1d-bdf8-28f82afc63bd": [{"entity1": "Astute RAG", "entity2": "Imperfect Retrieval Augmentation", "relation": "Overcoming", "description": "Astute RAG is designed to overcome Imperfect Retrieval Augmentation."}, {"entity1": "Astute RAG", "entity2": "Knowledge Conflicts", "relation": "Overcoming", "description": "Astute RAG is designed to overcome Knowledge Conflicts for Large Language Models."}, {"entity1": "Google Search", "entity2": "retrieval process", "relation": "Tool/Resource", "description": "Google Search is used as a tool in the retrieval process."}, {"entity1": "information retrieval systems", "entity2": "query rewriting", "relation": "Incorporation", "description": "Commercial information retrieval systems typically incorporate query rewriting enhancements."}, {"entity1": "retrieval process", "entity2": "benchmark", "relation": "Evaluation", "description": "The retrieval process is evaluated using a benchmark for each question."}, {"entity1": "Large Language Models", "entity2": "Astute RAG", "relation": "Application", "description": "Astute RAG is applied to Large Language Models to overcome Imperfect Retrieval Augmentation and Knowledge Conflicts."}], "f76c5700-555b-4085-9efa-714200109f60": [{"entity1": "Yixuan Tang", "entity2": "Yi Yang", "relation": "Co-author", "description": "Yixuan Tang and Yi Yang are co-authors of the paper introducing MultiHop-RAG."}, {"entity1": "Yixuan Tang", "entity2": "Hong Kong University of Science and Technology", "relation": "Affiliation", "description": "Yixuan Tang is affiliated with Hong Kong University of Science and Technology."}, {"entity1": "Yi Yang", "entity2": "Hong Kong University of Science and Technology", "relation": "Affiliation", "description": "Yi Yang is affiliated with Hong Kong University of Science and Technology."}, {"entity1": "MultiHop-RAG", "entity2": "Retrieval-Augmented Generation", "relation": "Tool/Resource", "description": "MultiHop-RAG is a tool/resource for Retrieval-Augmented Generation."}, {"entity1": "English news article dataset", "entity2": "MultiHop-RAG", "relation": "Dataset-Origin", "description": "The English news article dataset is used as the underlying knowledge base for MultiHop-RAG."}, {"entity1": "GPT-4", "entity2": "MultiHop-RAG", "relation": "Experiment-Outcome", "description": "GPT-4 is one of the LLMs examined in the experiments using MultiHop-RAG."}, {"entity1": "PaLM", "entity2": "MultiHop-RAG", "relation": "Experiment-Outcome", "description": "PaLM is one of the LLMs examined in the experiments using MultiHop-RAG."}, {"entity1": "Llama2-70B", "entity2": "MultiHop-RAG", "relation": "Experiment-Outcome", "description": "Llama2-70B is one of the LLMs examined in the experiments using MultiHop-RAG."}, {"entity1": "Asai et al.", "entity2": "Retrieval-Augmented Generation", "relation": "Citation", "description": "Asai et al. are cited for their work on Retrieval-Augmented Generation."}, {"entity1": "Borgeaud et al.", "entity2": "Retrieval-Augmented Generation", "relation": "Citation", "description": "Borgeaud et al. are cited for their work on improving LLM response using RAG."}, {"entity1": "Gao et al.", "entity2": "Retrieval-Augmented Generation", "relation": "Citation", "description": "Gao et al. are cited for their work on mitigating LLM hallucinations using RAG."}, {"entity1": "OpenAI", "entity2": "ChatGPT", "relation": "Affiliation", "description": "OpenAI is affiliated with ChatGPT, a large language model."}, {"entity1": "Yixuan Tang", "entity2": "https://github.com/yixuantt/MultiHop-RAG/", "relation": "Author-Role", "description": "Yixuan Tang is the author of the MultiHop-RAG repository on GitHub."}, {"entity1": "MultiHop-RAG", "entity2": "https://github.com/yixuantt/MultiHop-RAG/", "relation": "Tool/Resource", "description": "The MultiHop-RAG repository on GitHub provides access to the tool/resource."}], "bb90b2f0-9fcd-4008-8a73-ef04b4023a6c": [{"entity1": "RAG", "entity2": "LLM", "relation": "Improvement", "description": "RAG improves LLM's response and mitigates hallucinations."}, {"entity1": "Borgeaud et al.", "entity2": "RAG", "relation": "Research Contribution", "description": "Borgeaud et al. contributed to the improvement of LLM's response using RAG."}, {"entity1": "Gao et al.", "entity2": "RAG", "relation": "Research Contribution", "description": "Gao et al. contributed to the enhancement of models' credibility using RAG."}, {"entity1": "LlamaIndex", "entity2": "RAG", "relation": "Application", "description": "LlamaIndex is a framework that supports RAG pipelines."}, {"entity1": "LangChain", "entity2": "RAG", "relation": "Application", "description": "LangChain is a framework that supports RAG pipelines."}, {"entity1": "Liu", "entity2": "LlamaIndex", "relation": "Author-Role", "description": "Liu is the author of LlamaIndex."}, {"entity1": "Chase", "entity2": "LangChain", "relation": "Author-Role", "description": "Chase is the author of LangChain."}, {"entity1": "RAG", "entity2": "multi-hop query", "relation": "Methodology", "description": "RAG involves retrieving and reasoning over evidence from multiple documents, known as multi-hop query."}, {"entity1": "financial analysis", "entity2": "RAG", "relation": "Application", "description": "RAG is used in financial analysis to retrieve and reason over evidence from multiple documents."}, {"entity1": "Google", "entity2": "financial reports", "relation": "Data Source", "description": "Google's financial reports are used as a data source for financial analysis."}, {"entity1": "Apple", "entity2": "financial reports", "relation": "Data Source", "description": "Apple's financial reports are used as a data source for financial analysis."}, {"entity1": "Nvidia", "entity2": "financial reports", "relation": "Data Source", "description": "Nvidia's financial reports are used as a data source for financial analysis."}, {"entity1": "cosine similarity", "entity2": "RAG", "relation": "Comparison", "description": "Cosine similarity is compared to RAG in the context of financial analysis."}, {"entity1": "arXiv:2401.15391v1", "entity2": "2024", "relation": "Publication Date", "description": "The publication arXiv:2401.15391v1 was published in 2024."}], "8ba1f2ee-0971-474c-81aa-a691e5b54f21": [{"entity1": "Federal Reserve", "entity2": "interest rates", "relation": "Funding", "description": "The Federal Reserve makes decisions on interest rates based on incoming economic data."}, {"entity1": "Federal Reserve", "entity2": "inflation", "relation": "Research Area", "description": "The Federal Reserve aims to combat inflation by adjusting interest rates."}, {"entity1": "home prices", "entity2": "inflation", "relation": "Comparison", "description": "The article compares home prices and inflation, stating that home prices had boomed before Fed officials hiked interest rates to fight inflation."}, {"entity1": "Fortune Magazine", "entity2": "The Sydney Morning Herald", "relation": "Publication Venue", "description": "Both Fortune Magazine and The Sydney Morning Herald are news sources mentioned in the article."}, {"entity1": "Liu et al.", "entity2": "RECALL", "relation": "Author-Role", "description": "Liu et al. are the authors of the RECALL benchmark."}, {"entity1": "Chen et al.", "entity2": "RGB", "relation": "Author-Role", "description": "Chen et al. are the authors of the RGB benchmark."}, {"entity1": "MultiHop-RAG", "entity2": "RAG", "relation": "Tool/Resource", "description": "MultiHop-RAG is a RAG dataset focusing on multi-hop queries."}, {"entity1": "Inference query", "entity2": "MultiHop-RAG", "relation": "Application", "description": "Inference query is one of the types of multi-hop queries in the MultiHop-RAG dataset."}, {"entity1": "Comparison query", "entity2": "MultiHop-RAG", "relation": "Application", "description": "Comparison query is one of the types of multi-hop queries in the MultiHop-RAG dataset."}, {"entity1": "Temporal query", "entity2": "MultiHop-RAG", "relation": "Application", "description": "Temporal query is one of the types of multi-hop queries in the MultiHop-RAG dataset."}, {"entity1": "Null query", "entity2": "MultiHop-RAG", "relation": "Application", "description": "Null query is one of the types of multi-hop queries in the MultiHop-RAG dataset."}, {"entity1": "LLMs", "entity2": "MultiHop-RAG", "relation": "Tool/Resource", "description": "MultiHop-RAG is used to evaluate the retrieval and reasoning capability of LLMs for complex multi-hop queries."}, {"entity1": "Federal Reserve", "entity2": "Fed officials", "relation": "Affiliation", "description": "Fed officials are affiliated with the Federal Reserve."}, {"entity1": "interest rates", "entity2": "home prices", "relation": "Impact", "description": "The article suggests that interest rate hikes are a response to past conditions, such as booming home prices."}, {"entity1": "Figure 1", "entity2": "MultiHop-RAG", "relation": "Publication Venue", "description": "Figure 1 demonstrates the multi-hop retrieval process in the MultiHop-RAG dataset."}, {"entity1": "Table 1", "entity2": "MultiHop-RAG", "relation": "Publication Venue", "description": "Table 1 provides an example of a multi-hop query in the MultiHop-RAG dataset."}], "3965646b-8363-4ad0-bcb7-a8e38afc295c": [{"entity1": "Null query", "entity2": "knowledge base", "relation": "Comparison", "description": "The Null query is compared to the knowledge base to assess whether an LLM might hallucinate an answer to a multi-hop query when the retrieved text lacks relevance."}, {"entity1": "RAG knowledge base", "entity2": "news articles", "relation": "Dataset-Origin", "description": "The RAG knowledge base is constructed using a collection of news articles."}, {"entity1": "GPT-4", "entity2": "data generator", "relation": "Tool/Resource", "description": "GPT-4 is used as a data generator to construct a diverse set of multi-hop queries."}, {"entity1": "Table 1", "entity2": "query construction", "relation": "Example", "description": "Table 1 shows an example of query construction."}, {"entity1": "Federal Reserve officials", "entity2": "interest rates", "relation": "Experiment-Outcome", "description": "Federal Reserve officials were forced to aggressively hike interest rates to combat inflation after years of booming home prices."}, {"entity1": "GPT-4", "entity2": "evidence", "relation": "Input-Output", "description": "GPT-4 takes evidence as input and rephrases it into a claim."}, {"entity1": "LlamaIndex", "entity2": "Liu", "relation": "Author-Role", "description": "Liu is the author of LlamaIndex."}, {"entity1": "MultiHop-RAG", "entity2": "benchmarking", "relation": "Application", "description": "MultiHop-RAG is used for benchmarking the capabilities of RAG systems."}, {"entity1": "GPT-4", "entity2": "GPT-3.5", "relation": "Comparison", "description": "GPT-4 and GPT-3.5 are compared in terms of their reasoning and answering abilities for multi-hop queries."}, {"entity1": "PaLM", "entity2": "Mixtral-8x7B", "relation": "Comparison", "description": "PaLM and Mixtral-8x7B are compared in terms of their reasoning and answering abilities for multi-hop queries."}, {"entity1": "Llama2-70B", "entity2": "Claude-2", "relation": "Comparison", "description": "Llama2-70B and Claude-2 are compared in terms of their reasoning and answering abilities for multi-hop queries."}, {"entity1": "RAG system", "entity2": "LlamaIndex", "relation": "Implementation", "description": "The RAG system is implemented with LlamaIndex."}, {"entity1": "inflation", "entity2": "home prices", "relation": "Cause-Effect", "description": "Inflation is caused by booming home prices."}, {"entity1": "Federal Reserve officials", "entity2": "inflation", "relation": "Experiment-Outcome", "description": "Federal Reserve officials were forced to aggressively hike interest rates to combat inflation."}], "a3f053ab-a08b-4e95-ab8d-9b06593e6658": [{"entity1": "RAG systems", "entity2": "generative AI", "relation": "Application", "description": "RAG systems utilize generative AI in practice to unleash its great potential."}, {"entity1": "MultiHop-RAG dataset", "entity2": "RAG systems", "relation": "Benchmarking", "description": "The MultiHop-RAG dataset is used for developing and benchmarking RAG systems."}, {"entity1": "D", "entity2": "embedding database", "relation": "Tool/Resource", "description": "The external corpus D is transformed into an embedding database using an embedding model."}, {"entity1": "di", "entity2": "D", "relation": "PartOf", "description": "Each document di is part of the corpus D."}, {"entity1": "embedding model", "entity2": "embedding database", "relation": "Methodology", "description": "The embedding model is used to transform chunks into vector representations stored in the embedding database."}, {"entity1": "LLM", "entity2": "query q", "relation": "Input", "description": "The LLM takes the query q, retrieval set Rq, and an optional prompt as input to generate a final answer."}, {"entity1": "Rq", "entity2": "query q", "relation": "Result", "description": "The retrieval set Rq is the result of retrieving the top-K chunks that best match the query q."}, {"entity1": "multi-hop query", "entity2": "single-hop query", "relation": "Comparison", "description": "A multi-hop query requires retrieving and reasoning over multiple pieces of evidence, unlike a single-hop query."}, {"entity1": "Google", "entity2": "Apple", "relation": "Comparison", "description": "The query compares the profit margins of Google and Apple in their third-quarter reports for 2023."}, {"entity1": "Nvidia", "entity2": "Google", "relation": "Comparison", "description": "The query compares the profit margins of Nvidia and Google in their third-quarter reports for 2023."}, {"entity1": "Netflix", "entity2": "Google", "relation": "Comparison", "description": "The query compares the profit margins of Netflix and Google."}, {"entity1": "inference query", "entity2": "retrieval set Rq", "relation": "Input", "description": "The answer to an inference query is deduced through reasoning from the retrieval set Rq."}, {"entity1": "comparison query", "entity2": "retrieval set Rq", "relation": "Input", "description": "The answer to a comparison query requires a comparison of evidence within the retrieval set Rq."}, {"entity1": "2019 annual report", "entity2": "2020 annual report", "relation": "Comparison", "description": "The query compares the supply chain risk of Apple discussed in the 2019 and 2020 annual reports."}], "88723282-75ea-4e09-930e-8c6876c1af42": [{"entity1": "Netflix", "entity2": "Google", "relation": "Comparison", "description": "A comparison query asks which company reported higher revenue for the year 2023."}, {"entity1": "Apple", "entity2": "AirTag", "relation": "Product Introduction", "description": "A temporal query asks if Apple introduced the AirTag before or after the launch of the 5th generation iPad Pro."}, {"entity1": "Apple", "entity2": "iPad Pro", "relation": "Product Launch", "description": "A temporal query compares the introduction of AirTag with the launch of the 5th generation iPad Pro."}, {"entity1": "ABCD", "entity2": "Null query", "relation": "Query Type", "description": "ABCD is used as an example for a null query, assessing the generation quality of an LLM."}, {"entity1": "RAG system", "entity2": "Evaluation Metrics", "relation": "Assessment", "description": "A RAG system is assessed using retrieval and generation evaluation metrics."}, {"entity1": "Retrieval Evaluation", "entity2": "Generation Evaluation", "relation": "Evaluation Aspect", "description": "A RAG system is evaluated from two key aspects: retrieval and generation."}, {"entity1": "Mean Average Precision at K", "entity2": "Retrieval Evaluation", "relation": "Metric", "description": "Mean Average Precision at K is a metric used for retrieval evaluation."}, {"entity1": "Mean Reciprocal Rank at K", "entity2": "Retrieval Evaluation", "relation": "Metric", "description": "Mean Reciprocal Rank at K is a metric used for retrieval evaluation."}, {"entity1": "Hit Rate at K", "entity2": "Retrieval Evaluation", "relation": "Metric", "description": "Hit Rate at K is a metric used for retrieval evaluation."}, {"entity1": "LLM", "entity2": "Response Evaluation", "relation": "Assessment", "description": "The reasoning capability of an LLM is evaluated by comparing its response with the ground truth answer."}, {"entity1": "Netflix", "entity2": "2023", "relation": "Revenue Report", "description": "A comparison query asks about the revenue of Netflix for the year 2023."}, {"entity1": "Google", "entity2": "2023", "relation": "Revenue Report", "description": "A comparison query asks about the revenue of Google for the year 2023."}, {"entity1": "ABCD", "entity2": "2022", "relation": "Non-Existent Report", "description": "ABCD is a non-existent company, and thus has no annual report for 2022."}, {"entity1": "ABCD", "entity2": "2023", "relation": "Non-Existent Report", "description": "ABCD is a non-existent company, and thus has no annual report for 2023."}, {"entity1": "Apple", "entity2": "2023", "relation": "Product Introduction", "description": "A temporal query asks about the introduction of AirTag in relation to the year 2023."}, {"entity1": "iPad Pro", "entity2": "2023", "relation": "Product Launch", "description": "A temporal query compares the launch of the 5th generation iPad Pro with the introduction of AirTag."}], "9f32c421-7ef5-4178-9c19-039c528c16c5": [{"entity1": "MultiHop-RAG", "entity2": "mediastack API", "relation": "Tool/Resource", "description": "MultiHop-RAG uses the mediastack API to download a news dataset."}, {"entity1": "MultiHop-RAG", "entity2": "news articles", "relation": "Dataset-Origin", "description": "MultiHop-RAG dataset is constructed from a collection of news articles."}, {"entity1": "news articles", "entity2": "English-language websites", "relation": "Source", "description": "News articles are collected from various English-language websites."}, {"entity1": "news articles", "entity2": "entertainment, business, sports, technology, health, and science", "relation": "Category", "description": "News articles cover a range of categories including entertainment, business, sports, technology, health, and science."}, {"entity1": "MultiHop-RAG", "entity2": "GPT-4", "relation": "Tool/Resource", "description": "MultiHop-RAG uses GPT-4 to automatically generate high-quality multi-hop queries."}, {"entity1": "GPT-4", "entity2": "UniEval", "relation": "Methodology", "description": "GPT-4 is used in conjunction with UniEval for fact-checking and ensuring consistency between generated claims and evidence."}, {"entity1": "MultiHop-RAG", "entity2": "Zhong et al.", "relation": "Research Area", "description": "MultiHop-RAG is related to the research area of Zhong et al., specifically the UniEval framework."}, {"entity1": "Chat-GPT", "entity2": "LLaMA", "relation": "Comparison", "description": "Chat-GPT and LLaMA are compared in terms of their knowledge cutoff and potential exposure to news articles used in MultiHop-RAG."}, {"entity1": "MultiHop-RAG", "entity2": "Figure 2", "relation": "Publication Venue", "description": "Figure 2 is a part of the publication that presents the MultiHop-RAG construction pipeline."}, {"entity1": "news articles", "entity2": "September 26, 2023, to December 26, 2023", "relation": "Publication Date", "description": "News articles used in MultiHop-RAG were published between September 26, 2023, and December 26, 2023."}], "1ecd0724-837c-4dd9-8118-f4f828e0629e": [{"entity1": "UniEval", "entity2": "Zhong et al.", "relation": "Author-Expertise", "description": "UniEval framework was developed by Zhong et al. in 2022"}, {"entity1": "GPT-4", "entity2": "Appendix A", "relation": "Tool/Resource", "description": "GPT-4 is used with the prompt presented in Appendix A for claim generation"}, {"entity1": "Google", "entity2": "financial performance", "relation": "Research Area", "description": "Google's financial performance is a research area of interest"}, {"entity1": "profit margin", "entity2": "Google", "relation": "Bridge-Topic", "description": "Profit margin is a bridge-topic linking different pieces of evidence related to Google"}, {"entity1": "Google", "entity2": "bridge-entity", "relation": "Bridge-Entity", "description": "Google is a bridge-entity linking different pieces of evidence"}, {"entity1": "huggingface.co", "entity2": "lighteternal/fact-or-opinion-xlmr-el", "relation": "Tool/Resource", "description": "huggingface.co hosts the lighteternal/fact-or-opinion-xlmr-el model"}, {"entity1": "GPT-4", "entity2": "Query and Answer Generation", "relation": "Application", "description": "GPT-4 is used for query and answer generation"}, {"entity1": "claims", "entity2": "evidence", "relation": "Result", "description": "Claims are generated based on the original evidence and its context"}, {"entity1": "UniEval", "entity2": "fact-checking", "relation": "Methodology", "description": "UniEval framework is used for fact-checking to verify the alignment between evidence and claim"}, {"entity1": "2023", "entity2": "Google", "relation": "Publication Date", "description": "Google's third-quarter results for 2023 are reported"}, {"entity1": "revenue growth", "entity2": "financial performance", "relation": "Research Area", "description": "Revenue growth is a aspect of financial performance"}, {"entity1": "GPT-4", "entity2": "Step 4", "relation": "Experiment-Design", "description": "GPT-4 is used in Step 4 for query and answer generation"}], "495d371c-0f91-47b3-a504-987b4480c651": [{"entity1": "Inference Query", "entity2": "GPT-4", "relation": "Tool/Resource", "description": "GPT-4 is used to generate queries for Inference Query type."}, {"entity1": "Comparison Query", "entity2": "GPT-4", "relation": "Tool/Resource", "description": "GPT-4 is used to generate queries for Comparison Query type."}, {"entity1": "Temporal Query", "entity2": "GPT-4", "relation": "Tool/Resource", "description": "GPT-4 is used to generate queries for Temporal Query type."}, {"entity1": "Null Query", "entity2": "GPT-4", "relation": "Tool/Resource", "description": "GPT-4 is used to generate queries for Null Query type."}, {"entity1": "GPT-4", "entity2": "Quality Assurance", "relation": "Tool/Resource", "description": "GPT-4 is utilized for Quality Assurance to assess dataset quality."}, {"entity1": "technology", "entity2": "Category Avg. Tokens Entry Count", "relation": "Research Area", "description": "Technology is one of the research areas with an average of 2262.3 tokens and 172 entries."}, {"entity1": "entertainment", "entity2": "Category Avg. Tokens Entry Count", "relation": "Research Area", "description": "Entertainment is one of the research areas with an average of 2084.3 tokens and 114 entries."}, {"entity1": "sports", "entity2": "Category Avg. Tokens Entry Count", "relation": "Research Area", "description": "Sports is one of the research areas with an average of 2030.6 tokens and 211 entries."}, {"entity1": "Inference Query", "entity2": "bridge-entity", "relation": "Methodology", "description": "Inference Query synthesizes characterizations of the bridge-entity across multiple claims."}, {"entity1": "Comparison Query", "entity2": "bridge-entity", "relation": "Methodology", "description": "Comparison Query compares similarities and differences related to the bridge entity or topic."}, {"entity1": "Temporal Query", "entity2": "bridge-entity", "relation": "Methodology", "description": "Temporal Query explores the temporal ordering of events across different points in time related to the bridge entity."}, {"entity1": "Null Query", "entity2": "bridge-entity", "relation": "Methodology", "description": "Null Query is generated using entities that do not exist in the existing bridge-entities."}, {"entity1": "GPT-4", "entity2": "Step 5", "relation": "Tool/Resource", "description": "GPT-4 is used in Step 5 for Quality Assurance."}], "94fd41f7-8326-44c0-9891-89df92324677": [{"entity1": "MultiHop-RAG dataset", "entity2": "news articles", "relation": "Dataset-Origin", "description": "The MultiHop-RAG dataset contains six different types of news articles."}, {"entity1": "MultiHop-RAG", "entity2": "knowledge base", "relation": "Tool/Resource", "description": "The MultiHop-RAG dataset is used with a knowledge base for querying."}, {"entity1": "Inference Query", "entity2": "MultiHop-RAG", "relation": "Publication Venue", "description": "Inference Queries are part of the MultiHop-RAG dataset."}, {"entity1": "Comparison Query", "entity2": "MultiHop-RAG", "relation": "Publication Venue", "description": "Comparison Queries are part of the MultiHop-RAG dataset."}, {"entity1": "Temporal Query", "entity2": "MultiHop-RAG", "relation": "Publication Venue", "description": "Temporal Queries are part of the MultiHop-RAG dataset."}, {"entity1": "Null Query", "entity2": "MultiHop-RAG", "relation": "Publication Venue", "description": "Null Queries are part of the MultiHop-RAG dataset."}, {"entity1": "Table 2", "entity2": "news article knowledge base", "relation": "Affiliation", "description": "Table 2 provides descriptive statistics of the news article knowledge base in MultiHop-RAG."}, {"entity1": "Table 3", "entity2": "query types", "relation": "Affiliation", "description": "Table 3 shows the distribution of query types in MultiHop-RAG."}, {"entity1": "MultiHop-RAG", "entity2": "RAG system", "relation": "Tool/Resource", "description": "MultiHop-RAG can be used as a benchmark for various RAG-related tasks."}, {"entity1": "tokens", "entity2": "news articles", "relation": "Dataset-Origin", "description": "The news articles in the MultiHop-RAG dataset have an average of 2,046 tokens."}, {"entity1": "technology", "entity2": "news articles", "relation": "Research Area", "description": "Technology is one of the categories of news articles in the MultiHop-RAG dataset."}, {"entity1": "entertainment", "entity2": "news articles", "relation": "Research Area", "description": "Entertainment is one of the categories of news articles in the MultiHop-RAG dataset."}, {"entity1": "sports", "entity2": "news articles", "relation": "Research Area", "description": "Sports is one of the categories of news articles in the MultiHop-RAG dataset."}, {"entity1": "science", "entity2": "news articles", "relation": "Research Area", "description": "Science is one of the categories of news articles in the MultiHop-RAG dataset."}, {"entity1": "business", "entity2": "news articles", "relation": "Research Area", "description": "Business is one of the categories of news articles in the MultiHop-RAG dataset."}, {"entity1": "health", "entity2": "news articles", "relation": "Research Area", "description": "Health is one of the categories of news articles in the MultiHop-RAG dataset."}, {"entity1": "MultiHop-RAG dataset", "entity2": "evidence", "relation": "Dataset-Origin", "description": "The MultiHop-RAG dataset contains queries that require varying numbers of evidence to answer."}], "4676f727-48b3-49d5-95ac-c0c809502a9d": [{"entity1": "MultiHop-RAG", "entity2": "retrieval-related tasks", "relation": "Application", "description": "MultiHop-RAG can be employed for retrieval-related tasks."}, {"entity1": "MultiHop-RAG", "entity2": "generation-related tasks", "relation": "Application", "description": "MultiHop-RAG can be employed for generation-related tasks."}, {"entity1": "RAG system", "entity2": "embedding model", "relation": "Methodology", "description": "An important design choice in an RAG system is the selection of the embedding model."}, {"entity1": "LlamaIndex", "entity2": "RAG system", "relation": "Tool/Resource", "description": "We implement an RAG system using the LlamaIndex framework."}, {"entity1": "Liu", "entity2": "LlamaIndex", "relation": "Author-Role", "description": "Liu is the author of the LlamaIndex framework."}, {"entity1": "embedding model", "entity2": "cosine similarity", "relation": "Methodology", "description": "We retrieve the top-K most relevant chunks that have the highest cosine similarity with the query embedding."}, {"entity1": "text-embedding-ada-002", "entity2": "OpenAI", "relation": "Affiliation", "description": "text-embedding-ada-002 is an embedding model by OpenAI."}, {"entity1": "voyage-02", "entity2": "embedding model", "relation": "Tool/Resource", "description": "voyage-02 is an embedding model."}, {"entity1": "llm-embedder", "entity2": "Zhang", "relation": "Author-Role", "description": "Zhang is the author of the llm-embedder."}, {"entity1": "bge-large-en-v1.5", "entity2": "Xiao", "relation": "Author-Role", "description": "Xiao is the author of the bge-large-en-v1.5."}, {"entity1": "jina-embeddings-v2-base-en", "entity2": "G\u00fcnther", "relation": "Author-Role", "description": "G\u00fcnther is the author of the jina-embeddings-v2-base-en."}, {"entity1": "e5-base-v2", "entity2": "Wang", "relation": "Author-Role", "description": "Wang is the author of the e5-base-v2."}, {"entity1": "instructor-large", "entity2": "Su", "relation": "Author-Role", "description": "Su is the author of the instructor-large."}, {"entity1": "Reranker module", "entity2": "bge-reranker-large", "relation": "Tool/Resource", "description": "We use a Reranker module with bge-reranker-large."}, {"entity1": "Hits@10", "entity2": "Reranker technique", "relation": "Experiment-Outcome", "description": "The highest Hits@10 is only 0.7467 when the Reranker technique is used."}], "f22d23a1-083f-4812-b634-1b6f1a0c6d9d": [{"entity1": "Rerank", "entity2": "retrieval relevance", "relation": "Improvement", "description": "Rerank can effectively improve retrieval relevance."}, {"entity1": "RAG systems", "entity2": "context window limit", "relation": "Limitation", "description": "The underlying LLM in RAG systems often has a context window limit, restricting the number of retrieved chunks."}, {"entity1": "direct similarity matching", "entity2": "multi-hop queries", "relation": "Challenge", "description": "Direct similarity matching between the multi-hop query and text chunks highlights the challenges in retrieving relevant pieces of evidence for multi-hop queries."}, {"entity1": "voyage-02", "entity2": "bge-reranker-large", "relation": "Combination", "description": "The best-performing retrieval model is voyage-02 with bge-reranker-large, as indicated in Table 5."}, {"entity1": "LLM", "entity2": "response generation", "relation": "Role", "description": "The underlying LLMs play a crucial role in generating responses in an RAG system."}, {"entity1": "GPT-4", "entity2": "OpenAI", "relation": "Affiliation", "description": "GPT-4 is a commercial model from OpenAI."}, {"entity1": "GPT-3.5", "entity2": "OpenAI", "relation": "Affiliation", "description": "GPT-3.5 is a commercial model from OpenAI."}, {"entity1": "Claude-2", "entity2": "Anthropic", "relation": "Affiliation", "description": "Claude-2 is a commercial model from Anthropic."}, {"entity1": "Google-PaLM", "entity2": "Google", "relation": "Affiliation", "description": "Google-PaLM is a commercial model from Google."}, {"entity1": "Mixtral-8x7b-instruct", "entity2": "Jiang et al.", "relation": "Authorship", "description": "Mixtral-8x7b-instruct is an open-source model from Jiang et al."}, {"entity1": "Llama-2-70b-chat-hf", "entity2": "Touvron et al.", "relation": "Authorship", "description": "Llama-2-70b-chat-hf is an open-source model from Touvron et al."}, {"entity1": "Table 5", "entity2": "voyage-02 with bge-reranker-large", "relation": "Reference", "description": "Table 5 indicates that voyage-02 with bge-reranker-large is the best-performing retrieval model."}, {"entity1": "Table 6", "entity2": "response accuracy of different LLMs", "relation": "Reference", "description": "Table 6 shows the response accuracy of different LLMs."}], "9c523e3b-051c-4193-8f59-da13ee9729c0": [{"entity1": "Mixtral-8x7B", "entity2": "temporal queries", "relation": "Limitation", "description": "Mixtral-8x7B often fails to correctly identify the chronological order of events, which is crucial for answering temporal queries."}, {"entity1": "LLMs", "entity2": "reasoning capabilities", "relation": "Limitation", "description": "There is still room for improvement in the reasoning capabilities of LLMs, particularly those that are open-source, for multi-hop queries."}, {"entity1": "query decomposition", "entity2": "RAG frameworks", "relation": "Methodology", "description": "Query decomposition is a widely utilized technique in RAG frameworks, such as LLamaIndex."}, {"entity1": "LLamaIndex", "entity2": "query decomposition", "relation": "Tool/Resource", "description": "LLamaIndex is an example of a RAG framework that utilizes query decomposition."}, {"entity1": "AutoGPT", "entity2": "Gravitas", "relation": "Affiliation", "description": "AutoGPT is mentioned in the context of Gravitas (2023)."}, {"entity1": "hybrid retrieval approach", "entity2": "keyword matching", "relation": "Methodology", "description": "The hybrid retrieval approach combines keyword and embedding matching techniques."}, {"entity1": "hybrid retrieval approach", "entity2": "embedding matching", "relation": "Methodology", "description": "The hybrid retrieval approach combines keyword and embedding matching techniques."}, {"entity1": "2023", "entity2": "Gravitas", "relation": "Publication Date", "description": "Gravitas is mentioned in the context of the year 2023."}, {"entity1": "Figure 3", "entity2": "Generation accuracy", "relation": "Result", "description": "Figure 3 shows the generation accuracy for different query types."}], "abe9a26e-f501-4454-a53f-fa6eb9bb7faf": [{"entity1": "RAG", "entity2": "MultiHop-RAG", "relation": "Dataset-Origin", "description": "MultiHop-RAG is a curated dataset for enhancing RAG's performance on multi-hop queries."}, {"entity1": "RAG", "entity2": "RGB", "relation": "Evaluation", "description": "RGB evaluates the performance of LLMs in generating responses for RAG systems."}, {"entity1": "RAG", "entity2": "RECALL", "relation": "Evaluation", "description": "RECALL evaluates the performance of LLMs in generating responses for RAG systems."}, {"entity1": "ARES", "entity2": "RAG", "relation": "Tool/Resource", "description": "ARES is an automated RAG evaluation tool that utilizes LLMs to assess the quality of RAG generation."}, {"entity1": "RAGAS", "entity2": "RAG", "relation": "Tool/Resource", "description": "RAGAS is an automated RAG evaluation tool that utilizes LLMs to assess the quality of RAG generation."}, {"entity1": "FEVER", "entity2": "RAG", "relation": "Comparison", "description": "FEVER is a benchmarking dataset for information retrieval evaluation, but it contains single-hop statements, unlike RAG's multi-hop queries."}, {"entity1": "SciFact", "entity2": "RAG", "relation": "Comparison", "description": "SciFact is a benchmarking dataset for information retrieval evaluation, but it contains single-hop statements, unlike RAG's multi-hop queries."}, {"entity1": "HoVer", "entity2": "RAG", "relation": "Comparison", "description": "HoVer is a dataset that involves claims that require extracting and reasoning from multiple Wikipedia articles, but it focuses solely on classifying claims without evaluating an LLM generation step."}, {"entity1": "Kamalloo et al.", "entity2": "RAG", "relation": "Comparison", "description": "Kamalloo et al. evaluates a range of commercial embedding APIs for information retrieval, but this evaluation is separate from RAG."}, {"entity1": "Liu et al.", "entity2": "RECALL", "relation": "Author-Role", "description": "Liu et al. are the authors of the RECALL dataset."}, {"entity1": "Chen et al.", "entity2": "RGB", "relation": "Author-Role", "description": "Chen et al. are the authors of the RGB dataset."}, {"entity1": "Saad-Falcon et al.", "entity2": "ARES", "relation": "Author-Role", "description": "Saad-Falcon et al. are the authors of the ARES tool."}, {"entity1": "Es et al.", "entity2": "RAGAS", "relation": "Author-Role", "description": "Es et al. are the authors of the RAGAS tool."}, {"entity1": "Thorne et al.", "entity2": "FEVER", "relation": "Author-Role", "description": "Thorne et al. are the authors of the FEVER dataset."}, {"entity1": "Wadden et al.", "entity2": "SciFact", "relation": "Author-Role", "description": "Wadden et al. are the authors of the SciFact dataset."}, {"entity1": "Jiang et al.", "entity2": "HoVer", "relation": "Author-Role", "description": "Jiang et al. are the authors of the HoVer dataset."}, {"entity1": "Kamalloo et al.", "entity2": "Information Retrieval", "relation": "Research Area", "description": "Kamalloo et al. evaluates a range of commercial embedding APIs for information retrieval."}, {"entity1": "RAG", "entity2": "LLMs", "relation": "Tool/Resource", "description": "RAG systems utilize LLMs to generate responses."}, {"entity1": "Wikipedia", "entity2": "RAG", "relation": "Dataset-Origin", "description": "Wikipedia is a knowledge base used in RAG systems."}], "b77ef1e6-5f84-498b-92c3-48288980ef8f": [{"entity1": "Khashabi et al.", "entity2": "MultiRC", "relation": "Author-Publication", "description": "Khashabi et al. are the authors of the MultiRC dataset."}, {"entity1": "Ho et al.", "entity2": "2WikiMultiHopQA", "relation": "Author-Publication", "description": "Ho et al. are the authors of the 2WikiMultiHopQA dataset."}, {"entity1": "Yang et al.", "entity2": "HotpotQA", "relation": "Author-Publication", "description": "Yang et al. are the authors of the HotpotQA dataset."}, {"entity1": "Kamalloo et al.", "entity2": "Commercial embedding APIs", "relation": "Author-Research", "description": "Kamalloo et al. evaluated a range of commercial embedding APIs for information retrieval."}, {"entity1": "MultiHop-RAG", "entity2": "RAG systems", "relation": "Tool/Resource", "description": "MultiHop-RAG is a dataset designed for benchmarking RAG systems."}, {"entity1": "MultiHop-RAG", "entity2": "GPT-4", "relation": "Tool/Resource", "description": "MultiHop-RAG was created using a hybrid approach that integrates human effort with GPT-4."}, {"entity1": "Wikipedia", "entity2": "LLMs", "relation": "Dataset-Origin", "description": "Wikipedia is a primary data source that significantly overlaps with the training data of most existing LLMs."}, {"entity1": "MultiRC", "entity2": "HotpotQA", "relation": "Comparison", "description": "MultiRC and HotpotQA are compared as multi-document QA datasets."}, {"entity1": "2WikiMultiHopQA", "entity2": "MultiHop-RAG", "relation": "Comparison", "description": "2WikiMultiHopQA is compared to MultiHop-RAG as a multi-document QA dataset."}, {"entity1": "RAG systems", "entity2": "NLP", "relation": "Research Area", "description": "RAG systems are a research area within NLP."}, {"entity1": "MultiHop-RAG", "entity2": "NLP", "relation": "Application", "description": "MultiHop-RAG is a dataset designed for NLP tasks, specifically multi-hop query RAG tasks."}], "a7624dc8-ece8-4fec-9254-8233e816c0c3": [{"entity1": "Akari Asai", "entity2": "Danqi Chen", "relation": "Co-author", "description": "Co-authored the paper 'Retrieval-based language models and applications'"}, {"entity1": "Sebastian Borgeaud", "entity2": "Arthur Mensch", "relation": "Co-author", "description": "Co-authored the paper 'Improving language models by retrieving from trillions of tokens'"}, {"entity1": "Anthropic", "entity2": "Claude 2.1", "relation": "Publication Venue", "description": "Published Claude 2.1 in May 2023"}, {"entity1": "Google", "entity2": "PaLM 2", "relation": "Publication Venue", "description": "Published PaLM 2 in May 2023"}, {"entity1": "Shahul Es", "entity2": "Jithin James", "relation": "Co-author", "description": "Co-authored the paper 'Ragas: Automated evaluation of retrieval augmented generation'"}, {"entity1": "Tianyu Gao", "entity2": "Danqi Chen", "relation": "Co-author", "description": "Co-authored the paper 'Enabling large language models to generate text with citations'"}, {"entity1": "Harrison Chase", "entity2": "LangChain", "relation": "Author-Role", "description": "Creator of LangChain"}, {"entity1": "Significant Gravitas", "entity2": "Autogpt", "relation": "Author-Role", "description": "Creator of Autogpt"}, {"entity1": "Michael G\u00fcnther", "entity2": "Jina embeddings 2", "relation": "Author-Role", "description": "Contributor to Jina embeddings 2"}, {"entity1": "Association for Computational Linguistics", "entity2": "Proceedings of the 61st Annual Meeting", "relation": "Publication Venue", "description": "Published the proceedings of the 61st Annual Meeting"}, {"entity1": "RAG framework", "entity2": "LlamaIndex", "relation": "Tool/Resource", "description": "RAG framework uses LlamaIndex"}, {"entity1": "Sebastian Borgeaud", "entity2": "RAG framework", "relation": "Author-Expertise", "description": "Expertise in RAG framework"}, {"entity1": "Danqi Chen", "entity2": "Retrieval-based language models", "relation": "Research Area", "description": "Research area of expertise"}, {"entity1": "Google", "entity2": "Generative language models", "relation": "Research Area", "description": "Research area of expertise"}, {"entity1": "Anthropic", "entity2": "Claude 2.1", "relation": "Innovation", "description": "Innovative language model"}, {"entity1": "Harrison Chase", "entity2": "LangChain", "relation": "Innovation", "description": "Innovative framework for large language models"}], "7805b16e-498c-4a3b-b0cf-f9a42cbf40bd": [{"entity1": "Michael G\u00fcnther", "entity2": "Jina embeddings 2", "relation": "Author-Role", "description": "Michael G\u00fcnther is an author of the paper introducing Jina embeddings 2."}, {"entity1": "Xanh Ho", "entity2": "Proceedings of the 28th International Conference on Computational Linguistics", "relation": "Publication Venue", "description": "Xanh Ho published a paper in the Proceedings of the 28th International Conference on Computational Linguistics."}, {"entity1": "Albert Q. Jiang", "entity2": "Mixtral of experts", "relation": "Author-Role", "description": "Albert Q. Jiang is an author of the paper introducing Mixtral of experts."}, {"entity1": "Yichen Jiang", "entity2": "HoVer", "relation": "Author-Role", "description": "Yichen Jiang is an author of the paper introducing HoVer."}, {"entity1": "Ehsan Kamalloo", "entity2": "arXiv preprint arXiv:2305.06300", "relation": "Author-Role", "description": "Ehsan Kamalloo is an author of the arXiv preprint arXiv:2305.06300."}, {"entity1": "Daniel Khashabi", "entity2": "Proc. of the Annual Conference of the North American Chapter of the Association for Computational Linguistics", "relation": "Publication Venue", "description": "Daniel Khashabi published a paper in the Proc. of the Annual Conference of the North American Chapter of the Association for Computational Linguistics."}, {"entity1": "Jerry Liu", "entity2": "LlamaIndex", "relation": "Author-Role", "description": "Jerry Liu is the creator of LlamaIndex."}, {"entity1": "Yi Liu", "entity2": "Recall", "relation": "Author-Role", "description": "Yi Liu is an author of the paper introducing Recall."}, {"entity1": "OpenAI", "entity2": "GPT4", "relation": "Author-Role", "description": "OpenAI is the developer of GPT4."}, {"entity1": "Jon Saad-Falcon", "entity2": "Ares", "relation": "Author-Role", "description": "Jon Saad-Falcon is an author of the paper introducing Ares."}, {"entity1": "Jina embeddings 2", "entity2": "Michael G\u00fcnther", "relation": "Author-Role", "description": "Jina embeddings 2 was introduced by Michael G\u00fcnther and others."}, {"entity1": "Proceedings of the 28th International Conference on Computational Linguistics", "entity2": "Xanh Ho", "relation": "Publication Venue", "description": "Xanh Ho published a paper in the Proceedings of the 28th International Conference on Computational Linguistics."}, {"entity1": "Mixtral of experts", "entity2": "Albert Q. Jiang", "relation": "Author-Role", "description": "Mixtral of experts was introduced by Albert Q. Jiang and others."}, {"entity1": "HoVer", "entity2": "Yichen Jiang", "relation": "Author-Role", "description": "HoVer was introduced by Yichen Jiang and others."}, {"entity1": "arXiv preprint arXiv:2305.06300", "entity2": "Ehsan Kamalloo", "relation": "Author-Role", "description": "Ehsan Kamalloo is an author of the arXiv preprint arXiv:2305.06300."}, {"entity1": "Proc. of the Annual Conference of the North American Chapter of the Association for Computational Linguistics", "entity2": "Daniel Khashabi", "relation": "Publication Venue", "description": "Daniel Khashabi published a paper in the Proc. of the Annual Conference of the North American Chapter of the Association for Computational Linguistics."}, {"entity1": "LlamaIndex", "entity2": "Jerry Liu", "relation": "Author-Role", "description": "LlamaIndex was created by Jerry Liu."}, {"entity1": "Recall", "entity2": "Yi Liu", "relation": "Author-Role", "description": "Recall was introduced by Yi Liu and others."}, {"entity1": "GPT4", "entity2": "OpenAI", "relation": "Author-Role", "description": "GPT4 was developed by OpenAI."}, {"entity1": "Ares", "entity2": "Jon Saad-Falcon", "relation": "Author-Role", "description": "Ares was introduced by Jon Saad-Falcon and others."}], "7332e89c-e5cb-4c65-b5e1-1432f0567d08": [{"entity1": "Hongjin Su", "entity2": "One embedder, any task: Instruction-finetuned text embeddings", "relation": "Author-Role", "description": "Hongjin Su is an author of the paper 'One embedder, any task: Instruction-finetuned text embeddings'"}, {"entity1": "James Thorne", "entity2": "Fever", "relation": "Author-Role", "description": "James Thorne is an author of the paper 'Fever: a large-scale dataset for fact extraction and verification'"}, {"entity1": "Hugo Touvron", "entity2": "Llama 2: Open foundation and fine-tuned chat models", "relation": "Author-Role", "description": "Hugo Touvron is an author of the paper 'Llama 2: Open foundation and fine-tuned chat models'"}, {"entity1": "David Wadden", "entity2": "Fact or fiction: Verifying scientific claims", "relation": "Author-Role", "description": "David Wadden is an author of the paper 'Fact or fiction: Verifying scientific claims'"}, {"entity1": "Liang Wang", "entity2": "Text embeddings by weakly-supervised contrastive pre-training", "relation": "Author-Role", "description": "Liang Wang is an author of the paper 'Text embeddings by weakly-supervised contrastive pre-training'"}, {"entity1": "Shitao Xiao", "entity2": "C-pack: Packaged resources to advance general chinese embedding", "relation": "Author-Role", "description": "Shitao Xiao is an author of the paper 'C-pack: Packaged resources to advance general chinese embedding'"}, {"entity1": "Zhilin Yang", "entity2": "HotpotQA", "relation": "Author-Role", "description": "Zhilin Yang is an author of the paper 'HotpotQA: A dataset for diverse, explainable multi-hop question answering'"}, {"entity1": "Peitian Zhang", "entity2": "Retrieve anything to augment large language models", "relation": "Author-Role", "description": "Peitian Zhang is an author of the paper 'Retrieve anything to augment large language models'"}, {"entity1": "Hongjin Su", "entity2": "Weijia Shi", "relation": "Co-author", "description": "Hongjin Su and Weijia Shi are co-authors of the paper 'One embedder, any task: Instruction-finetuned text embeddings'"}, {"entity1": "James Thorne", "entity2": "Andreas Vlachos", "relation": "Co-author", "description": "James Thorne and Andreas Vlachos are co-authors of the paper 'Fever: a large-scale dataset for fact extraction and verification'"}, {"entity1": "Hugo Touvron", "entity2": "Louis Martin", "relation": "Co-author", "description": "Hugo Touvron and Louis Martin are co-authors of the paper 'Llama 2: Open foundation and fine-tuned chat models'"}, {"entity1": "David Wadden", "entity2": "Shanchuan Lin", "relation": "Co-author", "description": "David Wadden and Shanchuan Lin are co-authors of the paper 'Fact or fiction: Verifying scientific claims'"}, {"entity1": "Liang Wang", "entity2": "Nan Yang", "relation": "Co-author", "description": "Liang Wang and Nan Yang are co-authors of the paper 'Text embeddings by weakly-supervised contrastive pre-training'"}, {"entity1": "Shitao Xiao", "entity2": "Zheng Liu", "relation": "Co-author", "description": "Shitao Xiao and Zheng Liu are co-authors of the paper 'C-pack: Packaged resources to advance general chinese embedding'"}, {"entity1": "Zhilin Yang", "entity2": "Peng Qi", "relation": "Co-author", "description": "Zhilin Yang and Peng Qi are co-authors of the paper 'HotpotQA: A dataset for diverse, explainable multi-hop question answering'"}, {"entity1": "Peitian Zhang", "entity2": "Shitao Xiao", "relation": "Co-author", "description": "Peitian Zhang and Shitao Xiao are co-authors of the paper 'Retrieve anything to augment large language models'"}, {"entity1": "One embedder, any task: Instruction-finetuned text embeddings", "entity2": "arXiv", "relation": "Publication Venue", "description": "The paper 'One embedder, any task: Instruction-finetuned text embeddings' is published on arXiv"}, {"entity1": "Fever", "entity2": "EMNLP", "relation": "Publication Venue", "description": "The paper 'Fever: a large-scale dataset for fact extraction and verification' is published in EMNLP"}, {"entity1": "Llama 2: Open foundation and fine-tuned chat models", "entity2": "arXiv", "relation": "Publication Venue", "description": "The paper 'Llama 2: Open foundation and fine-tuned chat models' is published on arXiv"}, {"entity1": "Fact or fiction: Verifying scientific claims", "entity2": "EMNLP", "relation": "Publication Venue", "description": "The paper 'Fact or fiction: Verifying scientific claims' is published in EMNLP"}, {"entity1": "Text embeddings by weakly-supervised contrastive pre-training", "entity2": "arXiv", "relation": "Publication Venue", "description": "The paper 'Text embeddings by weakly-supervised contrastive pre-training' is published on arXiv"}, {"entity1": "C-pack: Packaged resources to advance general chinese embedding", "entity2": "arXiv", "relation": "Publication Venue", "description": "The paper 'C-pack: Packaged resources to advance general chinese embedding' is published on arXiv"}, {"entity1": "HotpotQA", "entity2": "EMNLP", "relation": "Publication Venue", "description": "The paper 'HotpotQA: A dataset for diverse, explainable multi-hop question answering' is published in EMNLP"}, {"entity1": "Retrieve anything to augment large language models", "entity2": "arXiv", "relation": "Publication Venue", "description": "The paper 'Retrieve anything to augment large language models' is published on arXiv"}], "8cb69154-4b36-4839-af61-f78da3642a40": [{"entity1": "Peitian Zhang", "entity2": "Retrieve anything to augment large language models", "relation": "Author-Publication", "description": "Peitian Zhang is an author of the publication 'Retrieve anything to augment large language models'."}, {"entity1": "Ming Zhong", "entity2": "Towards a unified multi-dimensional evaluator for text generation", "relation": "Author-Publication", "description": "Ming Zhong is an author of the publication 'Towards a unified multi-dimensional evaluator for text generation'."}, {"entity1": "GPT-4", "entity2": "Appendix A", "relation": "Tool/Resource", "description": "GPT-4 is used as a tool/resource in Appendix A for data generation."}, {"entity1": "Table 7", "entity2": "claim generation", "relation": "Methodology", "description": "Table 7 shows the prompt used for claim generation."}, {"entity1": "Table 8", "entity2": "Inference Queries", "relation": "Methodology", "description": "Table 8 shows the prompts used for generating multi-hop queries of the inference type."}, {"entity1": "Table 9", "entity2": "Comparison Queries", "relation": "Methodology", "description": "Table 9 shows the prompts used for generating multi-hop queries of the comparison type."}, {"entity1": "Table 10", "entity2": "Temporal Queries", "relation": "Methodology", "description": "Table 10 shows the prompts used for generating multi-hop queries of the temporal type."}, {"entity1": "MultiHop-RAG dataset", "entity2": "Appendix B", "relation": "Dataset-Origin", "description": "The MultiHop-RAG dataset is presented in Appendix B with examples of each type of multi-hop query."}, {"entity1": "Table 12", "entity2": "Inference Queries", "relation": "Methodology", "description": "Table 12 illustrates an example of Inference Queries in the MultiHop-RAG dataset."}, {"entity1": "Table 13", "entity2": "Comparison Queries", "relation": "Methodology", "description": "Table 13 illustrates an example of Comparison Queries in the MultiHop-RAG dataset."}, {"entity1": "Table 14", "entity2": "Temporal Queries", "relation": "Methodology", "description": "Table 14 illustrates an example of Temporal Queries in the MultiHop-RAG dataset."}, {"entity1": "Table 15", "entity2": "Null Queries", "relation": "Methodology", "description": "Table 15 illustrates an example of Null Queries in the MultiHop-RAG dataset."}, {"entity1": "EMNLP", "entity2": "Retrieve anything to augment large language models", "relation": "Publication Venue", "description": "The publication 'Retrieve anything to augment large language models' was presented at EMNLP."}, {"entity1": "EMNLP", "entity2": "Towards a unified multi-dimensional evaluator for text generation", "relation": "Publication Venue", "description": "The publication 'Towards a unified multi-dimensional evaluator for text generation' was presented at EMNLP."}], "8ccc4b4d-96cc-45d3-b105-22e5413b2eaf": [{"entity1": "claim", "entity2": "statement", "relation": "Assertion", "description": "A claim is a statement or assertion made within a text expressing a belief, opinion, or fact."}, {"entity1": "claim", "entity2": "evidence", "relation": "Evidence-Based", "description": "Given evidence from the original context, please extract one claim and its associated topics."}, {"entity1": "claim", "entity2": "target", "relation": "Directed-Towards", "description": "The target of the claim is the specific individual, group, or organization that the statement or assertion within a text is directed towards or about which it is making a case."}, {"entity1": "claim", "entity2": "topic", "relation": "Central-Argument", "description": "The topic of the claim should be a simple phrase representing the claim\u2019s central argument concept."}, {"entity1": "multi-hop question", "entity2": "query", "relation": "Inference-Question", "description": "A multi-hop question is a query requiring multiple inferential leaps or accessing several pieces of information from different locations or sources to arrive at an answer."}, {"entity1": "multi-hop question", "entity2": "answer", "relation": "Requires-Understanding", "description": "The answer is the target, which requires understanding and linking the information from all of the sources."}, {"entity1": "news articles", "entity2": "metadata", "relation": "Source-Of", "description": "The claims from the article are related to a similar target and come from the articles\u2019 metadata."}, {"entity1": "claim", "entity2": "news articles", "relation": "Origin", "description": "The claims come from the news articles."}, {"entity1": "target", "entity2": "claim", "relation": "Subject-Of", "description": "The target is the subject of the claim."}, {"entity1": "evidence", "entity2": "context", "relation": "Part-Of", "description": "The evidence is part of the original context."}, {"entity1": "inference", "entity2": "question", "relation": "Requires-Inference", "description": "The question requires inference and linking of information from all sources."}], "8f805959-0b18-4313-830c-3f043afb4fe9": [{"entity1": "news articles", "entity2": "metadata", "relation": "Affiliation", "description": "The metadata comes from the news articles."}, {"entity1": "claims", "entity2": "target", "relation": "Author-Role", "description": "All claims are related to a similar target."}, {"entity1": "Comparison Query Generation Prompting", "entity2": "keywords", "relation": "Tool/Resource", "description": "The Comparison Query Generation Prompting uses the provided keywords."}, {"entity1": "time-sensitive comparison question", "entity2": "metadata", "relation": "Methodology", "description": "The time-sensitive comparison question is generated using metadata and excerpts from multiple news articles."}, {"entity1": "Temporal Query Generation Prompting", "entity2": "time frame", "relation": "Application", "description": "The Temporal Query Generation Prompting utilizes the time frame to compare the consistency or sequence of reports."}, {"entity1": "multi-hop question", "entity2": "news sources", "relation": "Institution-Collaboration", "description": "The multi-hop question incorporates all the news sources."}, {"entity1": "Null Query Generation Prompting", "entity2": "entity", "relation": "Research Area", "description": "The Null Query Generation Prompting is related to the entity."}, {"entity1": "news articles", "entity2": "claims", "relation": "Publication Venue", "description": "The claims come from the news articles."}, {"entity1": "Comparison Query Generation Prompting", "entity2": "Table 9", "relation": "Dataset-Origin", "description": "The Comparison Query Generation Prompting is related to Table 9."}, {"entity1": "Temporal Query Generation Prompting", "entity2": "Table 10", "relation": "Dataset-Origin", "description": "The Temporal Query Generation Prompting is related to Table 10."}, {"entity1": "Null Query Generation Prompting", "entity2": "Table 11", "relation": "Dataset-Origin", "description": "The Null Query Generation Prompting is related to Table 11."}, {"entity1": "news source", "entity2": "time frame", "relation": "Collaboration-Type", "description": "The news source and time frame are used to compare the consistency or sequence of reports."}, {"entity1": "keywords", "entity2": "key set", "relation": "Tool/Resource", "description": "The keywords are provided in the key set."}, {"entity1": "comparison question", "entity2": "factual elements", "relation": "Research Area", "description": "The comparison question compares factual elements of the claims."}, {"entity1": "inferential leaps", "entity2": "multi-hop question", "relation": "Methodology", "description": "The multi-hop question requires multiple inferential leaps."}], "29cf7460-e590-4422-a3ae-c70df52134b7": [{"entity1": "YouTube", "entity2": "Music Business Worldwide", "relation": "Publication Venue", "description": "YouTube is discussed in an article from Music Business Worldwide regarding the policing of AI-driven voice replication."}, {"entity1": "YouTube", "entity2": "Polygon", "relation": "Publication Venue", "description": "YouTube is discussed in an article from Polygon concerning the debate over 'reaction' content."}, {"entity1": "YouTube", "entity2": "FOX News - Health", "relation": "Publication Venue", "description": "YouTube is mentioned in an article from FOX News - Health as the most used app overnight by young people."}, {"entity1": "Sony Music", "entity2": "YouTube", "relation": "Affiliation", "description": "Sony Music's artists are involved in YouTube's new voice-cloning AI experiment."}, {"entity1": "SSSniperwolf", "entity2": "YouTube", "relation": "Author-Role", "description": "SSSniperwolf is a content creator on YouTube who was demonetized after doxxing accusations."}, {"entity1": "Nike", "entity2": "CNBC | World Business News Leader", "relation": "Publication Venue", "description": "Nike's net income is reported in an article from CNBC | World Business News Leader."}, {"entity1": "10-year Treasury yield", "entity2": "The Age", "relation": "Publication Venue", "description": "The 10-year Treasury yield is discussed in an article from The Age."}, {"entity1": "CNBC | World Business News Leader", "entity2": "The Age", "relation": "Comparison", "description": "Both CNBC | World Business News Leader and The Age report on financial metrics, with Nike's net income and the 10-year Treasury yield both showing a decrease."}], "be18bd49-9777-4f22-9f14-85979824489b": [{"entity1": "Chicago Bears", "entity2": "Yardbarker", "relation": "Reported By", "description": "Yardbarker reported on the improvement of the Chicago Bears' defense."}, {"entity1": "Sporting News", "entity2": "Chicago Bears", "relation": "Reported On", "description": "Sporting News reported on the Chicago Bears' game, highlighting a sack by the Bears' defense."}, {"entity1": "Joshua Dobbs", "entity2": "Chicago Bears", "relation": "Sacked By", "description": "Joshua Dobbs was sacked by the Chicago Bears' defense, specifically by Sweat and Brisker."}, {"entity1": "Sweat", "entity2": "Brisker", "relation": "Co-Sacker", "description": "Sweat and Brisker were involved in sacking Joshua Dobbs together."}, {"entity1": "Yardbarker", "entity2": "Sporting News", "relation": "Information Source", "description": "Yardbarker's report on the Chicago Bears' defense improvement follows information from Sporting News about the Bears' game."}, {"entity1": "Chicago Bears", "entity2": "NFL", "relation": "Part Of", "description": "The Chicago Bears are part of the NFL."}, {"entity1": "Monday Night Football", "entity2": "NFL", "relation": "Event Of", "description": "Monday Night Football is an event of the NFL."}, {"entity1": "Bloomberg", "entity2": "TomTom", "relation": "Reported On", "description": "Bloomberg reported on TomTom in a news article."}, {"entity1": "Reuters", "entity2": "TomTom", "relation": "Reported On", "description": "Reuters reported on TomTom in a news article, mentioning the location of the company's headquarters."}], "447f2df0-6fc5-4c6b-8cb3-32b18b2d7fab": [{"entity1": "Wanlong Liu", "entity2": "Junying Chen", "relation": "Co-author", "description": "Wanlong Liu and Junying Chen are co-authors of the RAG-Instruct paper."}, {"entity1": "Wanlong Liu", "entity2": "Ke Ji", "relation": "Co-author", "description": "Wanlong Liu and Ke Ji are co-authors of the RAG-Instruct paper."}, {"entity1": "Wanlong Liu", "entity2": "Li Zhou", "relation": "Co-author", "description": "Wanlong Liu and Li Zhou are co-authors of the RAG-Instruct paper."}, {"entity1": "Wanlong Liu", "entity2": "Wenyu Chen", "relation": "Co-author", "description": "Wanlong Liu and Wenyu Chen are co-authors of the RAG-Instruct paper."}, {"entity1": "Wanlong Liu", "entity2": "Benyou Wang", "relation": "Co-author", "description": "Wanlong Liu and Benyou Wang are co-authors of the RAG-Instruct paper."}, {"entity1": "Junying Chen", "entity2": "The Chinese University of Hong Kong, Shenzhen", "relation": "Affiliation", "description": "Junying Chen is affiliated with The Chinese University of Hong Kong, Shenzhen."}, {"entity1": "Wanlong Liu", "entity2": "University of Electronic Science and Technology of China", "relation": "Affiliation", "description": "Wanlong Liu is affiliated with University of Electronic Science and Technology of China."}, {"entity1": "RAG-Instruct", "entity2": "LLMs", "relation": "Application", "description": "RAG-Instruct is used to enhance the capabilities of LLMs."}, {"entity1": "RAG-Instruct", "entity2": "https://github.com/FreedomIntelligence/RAG-Instruct", "relation": "Tool/Resource", "description": "RAG-Instruct is available at https://github.com/FreedomIntelligence/RAG-Instruct."}, {"entity1": "Guu et al.", "entity2": "RAG", "relation": "Research Area", "description": "Guu et al. have researched Retrieval-Augmented Generation (RAG)."}, {"entity1": "Asai et al.", "entity2": "RAG", "relation": "Research Area", "description": "Asai et al. have researched Retrieval-Augmented Generation (RAG)."}, {"entity1": "Wikipedia", "entity2": "RAG-Instruct", "relation": "Dataset-Origin", "description": "Wikipedia is used as a source corpus for RAG-Instruct."}, {"entity1": "Benyou Wang", "entity2": "RAG-Instruct", "relation": "Author-Role", "description": "Benyou Wang is the corresponding author of the RAG-Instruct paper."}, {"entity1": "Wanlong Liu", "entity2": "RAG-Instruct", "relation": "Author-Role", "description": "Wanlong Liu is a co-author of the RAG-Instruct paper and made equal contributions."}, {"entity1": "Junying Chen", "entity2": "RAG-Instruct", "relation": "Author-Role", "description": "Junying Chen is a co-author of the RAG-Instruct paper and made equal contributions."}, {"entity1": "RAG-Instruct", "entity2": "LLMs", "relation": "Methodology", "description": "RAG-Instruct is a method for enhancing LLMs with diverse retrieval-augmented instructions."}, {"entity1": "Petroni et al.", "entity2": "RAG", "relation": "Research Area", "description": "Petroni et al. have researched the limitations of RAG, including the impact of noisy retrieval."}, {"entity1": "Shi et al.", "entity2": "RAG", "relation": "Research Area", "description": "Shi et al. have researched the limitations of RAG, including the impact of noisy retrieval."}, {"entity1": "Maekawa et al.", "entity2": "RAG", "relation": "Research Area", "description": "Maekawa et al. have researched the limitations of RAG, including the impact of noisy retrieval."}, {"entity1": "Wei et al.", "entity2": "RAG", "relation": "Research Area", "description": "Wei et al. have researched methods to enhance the robustness of RAG in handling noisy retrieval contexts."}, {"entity1": "Chan et al.", "entity2": "RAG", "relation": "Research Area", "description": "Chan et al. have researched methods to enhance the robustness of RAG in handling noisy retrieval contexts."}, {"entity1": "Zhang et al.", "entity2": "RAG", "relation": "Research Area", "description": "Zhang et al. have researched methods to enhance the robustness of RAG."}, {"entity1": "Liu et al.", "entity2": "RAG", "relation": "Research Area", "description": "Liu et al. have researched the applications and limitations of RAG."}, {"entity1": "Yoran et al.", "entity2": "RAG", "relation": "Research Area", "description": "Yoran et al. have researched methods to enhance the robustness of RAG."}, {"entity1": "Asai et al.", "entity2": "RAG", "relation": "Research Area", "description": "Asai et al. have researched the applications and limitations of RAG, including adaptive retrieval and query reformulation."}, {"entity1": "Ma et al.", "entity2": "RAG", "relation": "Research Area", "description": "Ma et al. have researched methods to enhance the robustness of RAG, including query reformulation."}, {"entity1": "The Chinese University of Hong Kong, Shenzhen", "entity2": "University of Electronic Science and Technology of China", "relation": "Institution-Collaboration", "description": "The Chinese University of Hong Kong, Shenzhen and University of Electronic Science and Technology of China collaborated on the RAG-Instruct project."}], "8021df6c-fd89-4e98-9f54-b2cdaef963c9": [{"entity1": "Chan et al.", "entity2": "query reformulation", "relation": "Author-Expertise", "description": "Chan et al. are experts in query reformulation."}, {"entity1": "Ma et al.", "entity2": "query reformulation", "relation": "Author-Expertise", "description": "Ma et al. are experts in query reformulation."}, {"entity1": "Zhang et al.", "entity2": "LLM-based RAG systems", "relation": "Author-Expertise", "description": "Zhang et al. are experts in LLM-based RAG systems."}, {"entity1": "Liu et al.", "entity2": "LLM-based RAG systems", "relation": "Author-Expertise", "description": "Liu et al. are experts in LLM-based RAG systems."}, {"entity1": "Yoran et al.", "entity2": "LLM-based RAG systems", "relation": "Author-Expertise", "description": "Yoran et al. are experts in LLM-based RAG systems."}, {"entity1": "RAG methods", "entity2": "LLM-based RAG systems", "relation": "Methodology", "description": "RAG methods are used to enhance the robustness of LLM-based RAG systems."}, {"entity1": "RAG scenarios", "entity2": "RAG methods", "relation": "Limitation", "description": "RAG methods have limited RAG scenarios."}, {"entity1": "RAG methods", "entity2": "task diversity", "relation": "Limitation", "description": "RAG methods have limited task diversity."}, {"entity1": "NQ", "entity2": "RAG methods", "relation": "Dataset-Origin", "description": "NQ is a dataset used for RAG methods."}, {"entity1": "TrivialQA", "entity2": "RAG methods", "relation": "Dataset-Origin", "description": "TrivialQA is a dataset used for RAG methods."}, {"entity1": "RAG-Instruct", "entity2": "Wikipedia", "relation": "Tool/Resource", "description": "RAG-Instruct uses Wikipedia as a source corpus."}, {"entity1": "RAG query paradigms", "entity2": "RAG-Instruct", "relation": "Methodology", "description": "RAG-Instruct defines diverse RAG query paradigms."}, {"entity1": "Wei et al.", "entity2": "RAG methods", "relation": "Author-Expertise", "description": "Wei et al. are experts in RAG methods."}, {"entity1": "Zhang et al.", "entity2": "RAG methods", "relation": "Author-Expertise", "description": "Zhang et al. are experts in RAG methods."}, {"entity1": "Kwiatkowski et al.", "entity2": "NQ", "relation": "Author-Role", "description": "Kwiatkowski et al. are authors of the NQ dataset."}, {"entity1": "Joshi et al.", "entity2": "TrivialQA", "relation": "Author-Role", "description": "Joshi et al. are authors of the TrivialQA dataset."}, {"entity1": "Asai et al.", "entity2": "RAG methods", "relation": "Author-Expertise", "description": "Asai et al. are experts in RAG methods."}, {"entity1": "Liu et al.", "entity2": "RAG methods", "relation": "Author-Expertise", "description": "Liu et al. are experts in RAG methods."}, {"entity1": "arXiv:2501.00353v1", "entity2": "RAG-Instruct", "relation": "Publication Venue", "description": "RAG-Instruct is published in arXiv:2501.00353v1."}], "7b56f631-df65-4eae-b9f8-e3aeaf7978d0": [{"entity1": "RAG-Instruct", "entity2": "Wikipedia", "relation": "Dataset-Origin", "description": "RAG-Instruct dataset is constructed based on Wikipedia."}, {"entity1": "RAG-Instruct", "entity2": "SlimOrca", "relation": "Tool/Resource", "description": "RAG-Instruct utilizes SlimOrca as an existing instruction dataset to guide the generation of RAG instructions."}, {"entity1": "RAG-Instruct", "entity2": "Evol Instruct", "relation": "Tool/Resource", "description": "RAG-Instruct utilizes Evol Instruct as an existing instruction dataset to guide the generation of RAG instructions."}, {"entity1": "RAG-Instruct", "entity2": "Instruction Simulation", "relation": "Methodology", "description": "RAG-Instruct leverages Instruction Simulation to enhance task diversity and data quality."}, {"entity1": "RAG-Instruct", "entity2": "RAG paradigms", "relation": "Methodology", "description": "RAG-Instruct defines five RAG paradigms to cover diverse query-document relationships."}, {"entity1": "RAG-Instruct", "entity2": "Self-RAG", "relation": "Comparison", "description": "RAG-Instruct outperforms Self-RAG in empirical experiments."}, {"entity1": "RAG-Instruct", "entity2": "RQ-RAG", "relation": "Comparison", "description": "RAG-Instruct outperforms RQ-RAG in empirical experiments."}, {"entity1": "RAG-Instruct", "entity2": "Llama2-7b", "relation": "Experiment-Outcome", "description": "RAG-Instruct achieves better results than Llama2-7b in empirical experiments."}, {"entity1": "RAG-Instruct", "entity2": "Llama3-8b", "relation": "Experiment-Outcome", "description": "RAG-Instruct achieves better results than Llama3-8b in empirical experiments."}, {"entity1": "RAG-Instruct", "entity2": "ChatQA-1.5", "relation": "Comparison", "description": "RAG-Instruct outperforms ChatQA-1.5 in empirical experiments."}, {"entity1": "RAG-Instruct", "entity2": "ChatQA-2.0", "relation": "Comparison", "description": "RAG-Instruct outperforms ChatQA-2.0 in empirical experiments."}, {"entity1": "Mitra et al.", "entity2": "SlimOrca", "relation": "Author-Role", "description": "Mitra et al. are the authors of SlimOrca."}, {"entity1": "Xu et al.", "entity2": "Evol Instruct", "relation": "Author-Role", "description": "Xu et al. are the authors of Evol Instruct."}, {"entity1": "Asai et al.", "entity2": "Self-RAG", "relation": "Author-Role", "description": "Asai et al. are the authors of Self-RAG."}, {"entity1": "Chan et al.", "entity2": "RQ-RAG", "relation": "Author-Role", "description": "Chan et al. are the authors of RQ-RAG."}, {"entity1": "RAG-Instruct", "entity2": "knowledge-intensive QA", "relation": "Application", "description": "RAG-Instruct is applied to knowledge-intensive QA tasks."}, {"entity1": "RAG-Instruct", "entity2": "multi-step reasoning", "relation": "Application", "description": "RAG-Instruct is applied to multi-step reasoning tasks."}, {"entity1": "RAG-Instruct", "entity2": "domain-specific benchmarks", "relation": "Application", "description": "RAG-Instruct is applied to domain-specific benchmarks."}, {"entity1": "RAG-Instruct", "entity2": "Instruction Simulation", "relation": "Innovation", "description": "RAG-Instruct introduces Instruction Simulation as a novel technique."}, {"entity1": "RAG-Instruct", "entity2": "RAG paradigms", "relation": "Innovation", "description": "RAG-Instruct introduces five RAG paradigms as a novel approach."}], "1e12525d-2864-4a74-a7f0-dc146c72aadf": [{"entity1": "RAG-Instruct", "entity2": "InstructionDiversityTarget", "relation": "Methodology", "description": "RAG-Instruct ensures instruction data diversity through five RAG paradigms and Instruction Simulation to explore their performance across the five RAG scenarios."}, {"entity1": "RAG-Instruct", "entity2": "RAG Scenarios", "relation": "Experiment-Design", "description": "RAG-Instruct is designed to handle complex and diverse RAG scenarios effectively."}, {"entity1": "GPT-4o", "entity2": "RAG-Instruct", "relation": "Tool/Resource", "description": "GPT-4o is a proprietary model used in the RAG-Instruct process for synthesizing RAG instructions."}, {"entity1": "LongAlpaca", "entity2": "RAG-Instruct", "relation": "Comparison", "description": "LongAlpaca is compared to RAG-Instruct in terms of handling complex and diverse RAG scenarios."}, {"entity1": "Chen et al.", "entity2": "LongAlpaca", "relation": "Author-Role", "description": "Chen et al. are the authors of LongAlpaca."}, {"entity1": "Rajpurkar et al.", "entity2": "SQuAD2.0", "relation": "Author-Role", "description": "Rajpurkar et al. are the authors of SQuAD2.0."}, {"entity1": "Ko\u02c7cisk`y et al.", "entity2": "NarrativeQA", "relation": "Author-Role", "description": "Ko\u02c7cisk`y et al. are the authors of NarrativeQA."}, {"entity1": "RAG-Instruct", "entity2": "Table 1", "relation": "Result", "description": "RAG-Instruct demonstrates significant improvements across all five scenarios as shown in Table 1."}, {"entity1": "RAG-Instruct", "entity2": "Table 2", "relation": "Result", "description": "RAG-Instruct effectively balances both RAG scenario and task diversity as shown in Table 2."}, {"entity1": "Achiam et al.", "entity2": "GPT-4o", "relation": "Author-Role", "description": "Achiam et al. are the authors of GPT-4o."}, {"entity1": "RAG-Instruct", "entity2": "Figure 1", "relation": "Methodology", "description": "The detailed architecture of RAG-Instruct is illustrated in Figure 1."}, {"entity1": "ShareGPT", "entity2": "Evol-Instruct", "relation": "Tool/Resource", "description": "ShareGPT is related to Evol-Instruct in the context of RAG-Instruct."}, {"entity1": "AlpacaSlimOrca", "entity2": "RAG-Instruct", "relation": "Comparison", "description": "AlpacaSlimOrca is compared to RAG-Instruct in terms of handling complex and diverse RAG scenarios."}, {"entity1": "InstructionSimulationFilter", "entity2": "RAG-Instruct", "relation": "Methodology", "description": "InstructionSimulationFilter is used in the RAG-Instruct process for synthesizing RAG instructions."}], "0e687dd9-826b-4555-80a5-81ba87caebcb": [{"entity1": "GPT-4o", "entity2": "Achiam et al.", "relation": "Author-Role", "description": "Achiam et al. are the authors of GPT-4o."}, {"entity1": "GPT-4o", "entity2": "RAG-Instruct", "relation": "Tool/Resource", "description": "GPT-4o is used as a tool for synthesizing RAG instructions in RAG-Instruct."}, {"entity1": "Zheng et al.", "entity2": "GPT-4o", "relation": "Author-Expertise", "description": "Zheng et al. have expertise related to GPT-4o, as they have worked on synthetic datasets based on similar models."}, {"entity1": "Xu et al.", "entity2": "GPT-4o", "relation": "Author-Expertise", "description": "Xu et al. have expertise related to GPT-4o, as they have worked on synthetic datasets based on similar models."}, {"entity1": "Chen et al.", "entity2": "GPT-4o", "relation": "Author-Expertise", "description": "Chen et al. have expertise related to GPT-4o, as they have worked on synthetic datasets based on similar models."}, {"entity1": "Zhang et al.", "entity2": "RAG-Instruct", "relation": "Institution-Collaboration", "description": "Zhang et al. have collaborated with the RAG-Instruct project, as their work inspired the introduction of documents D\u2212."}, {"entity1": "D\u2217", "entity2": "D\u2212", "relation": "Comparison", "description": "D\u2217 and D\u2212 are compared in terms of their relevance to q\u2217, with D\u2217 being relevant and D\u2212 being unrelated."}, {"entity1": "D\u2217", "entity2": "q\u2217", "relation": "Dataset-Origin", "description": "D\u2217 is the source document used to create the instruction q\u2217."}, {"entity1": "q\u2217", "entity2": "a\u2217", "relation": "Result", "description": "q\u2217 is the instruction that results in the response a\u2217."}, {"entity1": "RAG-Instruct", "entity2": "Figure 1", "relation": "Publication Venue", "description": "RAG-Instruct is illustrated in Figure 1, which is part of the publication."}, {"entity1": "2023", "entity2": "GPT-4o", "relation": "Publication Date", "description": "GPT-4o was published in 2023."}, {"entity1": "2024", "entity2": "Zhang et al.", "relation": "Publication Date", "description": "Zhang et al. published their work in 2024."}], "f62ce201-16b3-4a8c-8b1c-be3193ddf287": [{"entity1": "D\u2217", "entity2": "q\u2217", "relation": "D\u2217-q\u2217 Relationship", "description": "Describes the usefulness of D\u2217 in relation to q\u2217, categorized into five RAG paradigms"}, {"entity1": "r0", "entity2": "Useless Doc", "relation": "Relationship Description", "description": "D\u2217 offers no help in answering q\u2217, even if related"}, {"entity1": "r1", "entity2": "Single-Doc Support", "relation": "Relationship Description", "description": "One doc aids q\u2217, providing supporting info or clues without explicit answers"}, {"entity1": "r2", "entity2": "Multi-Doc Support", "relation": "Relationship Description", "description": "Multiple documents support q\u2217 by providing clues or supporting information without explicitly answering it"}, {"entity1": "r3", "entity2": "Single-Doc Answer", "relation": "Relationship Description", "description": "One doc directly provides the answer a\u2217 to q\u2217"}, {"entity1": "r4", "entity2": "Multi-Doc Answer", "relation": "Relationship Description", "description": "Multiple docs provide a full answer to q\u2217, requiring integration (multi-hop reasoning)"}, {"entity1": "Table 3", "entity2": "RAG paradigms", "relation": "Description", "description": "Provides descriptions of 5 RAG paradigms"}, {"entity1": "Appendix B.2", "entity2": "RAG paradigms", "relation": "Reference", "description": "Contains specific prompts for RAG paradigms"}, {"entity1": "Documents", "entity2": "q\u2217", "relation": "Input", "description": "Documents are used to generate an English question q\u2217 and a corresponding response a\u2217"}, {"entity1": "RAG-Instruct", "entity2": "Figure 2", "relation": "Illustration", "description": "Figure 2 illustrates the prompt of RAG-Instruct"}, {"entity1": "RAG Paradigms", "entity2": "Real-world RAG scenarios", "relation": "Complexity", "description": "Real-world RAG scenarios are complex, involving various relationships between D\u2217 and q\u2217"}], "a71eb1d7-d3aa-4744-8ab2-4a5f7824bf94": [{"entity1": "RAG Paradigms", "entity2": "q\u2217", "relation": "Relationship Characterization", "description": "RAG paradigms characterize the relationship between D\u2217 and q\u2217."}, {"entity1": "RAG Paradigms", "entity2": "D\u2217", "relation": "Relationship Characterization", "description": "RAG paradigms characterize the relationship between D\u2217 and q\u2217."}, {"entity1": "Instruction Simulation", "entity2": "q\u2217", "relation": "Generation", "description": "Instruction Simulation generates q\u2217 from D\u2217."}, {"entity1": "Instruction Simulation", "entity2": "D\u2217", "relation": "Generation", "description": "Instruction Simulation generates q\u2217 from D\u2217."}, {"entity1": "Izacard et al.", "entity2": "Instruction Simulation", "relation": "Methodology Comparison", "description": "Izacard et al.'s approach is compared to Instruction Simulation."}, {"entity1": "Wang et al.", "entity2": "Instruction Simulation", "relation": "Methodology Comparison", "description": "Wang et al.'s approach is compared to Instruction Simulation."}, {"entity1": "ShareGPT", "entity2": "Instruction Simulation", "relation": "Dataset-Origin", "description": "ShareGPT is used as an exemplar dataset in Instruction Simulation."}, {"entity1": "Alpaca", "entity2": "Instruction Simulation", "relation": "Dataset-Origin", "description": "Alpaca is used as an exemplar dataset in Instruction Simulation."}, {"entity1": "WizardLM-70K", "entity2": "Instruction Simulation", "relation": "Dataset-Origin", "description": "WizardLM-70K is used as an exemplar dataset in Instruction Simulation."}, {"entity1": "Lmsys-chat-1M", "entity2": "Instruction Simulation", "relation": "Dataset-Origin", "description": "Lmsys-chat-1M is used as an exemplar dataset in Instruction Simulation."}, {"entity1": "SlimOrca", "entity2": "Instruction Simulation", "relation": "Dataset-Origin", "description": "SlimOrca is used as an exemplar dataset in Instruction Simulation."}, {"entity1": "RAG Paradigms", "entity2": "Table 3", "relation": "Result", "description": "RAG paradigms are described in Table 3."}, {"entity1": "R", "entity2": "RAG Paradigms", "relation": "Definition", "description": "R defines the RAG paradigms."}, {"entity1": "q\u2217", "entity2": "D\u2217", "relation": "Relationship Characterization", "description": "The relationship between q\u2217 and D\u2217 is characterized by RAG paradigms."}, {"entity1": "a\u2217", "entity2": "q\u2217", "relation": "Generation", "description": "a\u2217 is generated from q\u2217."}, {"entity1": "a\u2217", "entity2": "D\u2217", "relation": "Generation", "description": "a\u2217 is generated from D\u2217."}, {"entity1": "Appendix B.2", "entity2": "RAG Paradigms", "relation": "Reference", "description": "Other RAG paradigms are shown in Appendix B.2."}], "a57f6270-6280-4c96-afc8-4d64f2da5019": [{"entity1": "RAG Paradigms", "entity2": "GPT4-Alpaca", "relation": "Comparison", "description": "RAG Paradigms are compared with GPT4-Alpaca in terms of their distributions."}, {"entity1": "RAG Paradigms", "entity2": "WizardLM", "relation": "Comparison", "description": "RAG Paradigms are compared with WizardLM in terms of their distributions."}, {"entity1": "RAG Paradigms", "entity2": "Lmsys-Chat-1M", "relation": "Comparison", "description": "RAG Paradigms are compared with Lmsys-Chat-1M in terms of their distributions."}, {"entity1": "RAG Paradigms", "entity2": "ShareGPT_V3", "relation": "Comparison", "description": "RAG Paradigms are compared with ShareGPT_V3 in terms of their distributions."}, {"entity1": "RAG Paradigms", "entity2": "SlimOcar", "relation": "Comparison", "description": "RAG Paradigms are compared with SlimOcar in terms of their distributions."}, {"entity1": "GPT-4o", "entity2": "RAG Paradigms", "relation": "Tool/Resource", "description": "GPT-4o is used to filter knowledge-intensive instructions from synthetic datasets for RAG Paradigms."}, {"entity1": "Maekawa et al.", "entity2": "RAG Paradigms", "relation": "Citation", "description": "Maekawa et al. are cited as a reference for the effectiveness of RAG Paradigms in knowledge-intensive task scenarios."}, {"entity1": "Shi et al.", "entity2": "RAG Paradigms", "relation": "Citation", "description": "Shi et al. are cited as a reference for the effectiveness of RAG Paradigms in knowledge-intensive task scenarios."}, {"entity1": "Wikipedia corpus", "entity2": "RAG-Instruct", "relation": "Dataset-Origin", "description": "RAG-Instruct is constructed using the Wikipedia corpus."}, {"entity1": "RAG-Instruct", "entity2": "GPT-4o", "relation": "Tool/Resource", "description": "GPT-4o is used to generate (q\u2217, a\u2217) for RAG-Instruct."}, {"entity1": "Berant et al.", "entity2": "WebQA (WQA)", "relation": "Author-Role", "description": "Berant et al. are the authors of WebQA (WQA)."}, {"entity1": "Mallen et al.", "entity2": "PopQA (PQA)", "relation": "Author-Role", "description": "Mallen et al. are the authors of PopQA (PQA)."}, {"entity1": "Joshi et al.", "entity2": "TriviaQA-unfiltered (TQA)", "relation": "Author-Role", "description": "Joshi et al. are the authors of TriviaQA-unfiltered (TQA)."}, {"entity1": "Mihaylov et al.", "entity2": "OpenbookQA (OBQA)", "relation": "Author-Role", "description": "Mihaylov et al. are the authors of OpenbookQA (OBQA)."}, {"entity1": "Zhang et al.", "entity2": "PubHealth (Pub)", "relation": "Author-Role", "description": "Zhang et al. are the authors of PubHealth (Pub)."}, {"entity1": "Clark et al.", "entity2": "ARC-Challenge (ARC)", "relation": "Author-Role", "description": "Clark et al. are the authors of ARC-Challenge (ARC)."}, {"entity1": "Ho et al.", "entity2": "2WikiMultiHopQA (2WIKI)", "relation": "Author-Role", "description": "Ho et al. are the authors of 2WikiMultiHopQA (2WIKI)."}, {"entity1": "Yang et al.", "entity2": "HotpotQA (HotQ)", "relation": "Author-Role", "description": "Yang et al. are the authors of HotpotQA (HotQ)."}, {"entity1": "Trivedi et al.", "entity2": "Musique (MSQ)", "relation": "Author-Role", "description": "Trivedi et al. are the authors of Musique (MSQ)."}], "af09640a-0a12-4ee6-9bb8-bcc4eb9846a8": [{"entity1": "Extract Match (EM)", "entity2": "Multi-Hop Tasks", "relation": "Metric", "description": "Extract Match (EM) is used as the metric for Multi-Hop Tasks"}, {"entity1": "2WikiMultiHopQA (2WIKI)", "entity2": "Ho et al.", "relation": "Authorship", "description": "Ho et al. are the authors of 2WikiMultiHopQA (2WIKI)"}, {"entity1": "HotpotQA (HotQ)", "entity2": "Yang et al.", "relation": "Authorship", "description": "Yang et al. are the authors of HotpotQA (HotQ)"}, {"entity1": "Musique (MSQ)", "entity2": "Trivedi et al.", "relation": "Authorship", "description": "Trivedi et al. are the authors of Musique (MSQ)"}, {"entity1": "CFQA", "entity2": "Chen et al.", "relation": "Authorship", "description": "Chen et al. are the authors of CFQA"}, {"entity1": "PubMedQA", "entity2": "Jin et al.", "relation": "Authorship", "description": "Jin et al. are the authors of PubMedQA"}, {"entity1": "GPT-4o", "entity2": "GPT-4o-mini", "relation": "Model Variant", "description": "GPT-4o-mini is a variant of GPT-4o"}, {"entity1": "Llama2-7b", "entity2": "Touvron et al.", "relation": "Authorship", "description": "Touvron et al. are the authors of Llama2-7b"}, {"entity1": "Llama3-8b", "entity2": "Dubey et al.", "relation": "Authorship", "description": "Dubey et al. are the authors of Llama3-8b"}, {"entity1": "Llama3-8b-Instruct", "entity2": "Llama3-70B-Instruct", "relation": "Model Variant", "description": "Llama3-8b-Instruct and Llama3-70B-Instruct are variants of the same model"}, {"entity1": "OpenAI", "entity2": "GPT-4o", "relation": "Affiliation", "description": "GPT-4o is affiliated with OpenAI"}, {"entity1": "2018", "entity2": "HotpotQA (HotQ)", "relation": "Publication Date", "description": "HotpotQA (HotQ) was published in 2018"}, {"entity1": "2019", "entity2": "PubMedQA", "relation": "Publication Date", "description": "PubMedQA was published in 2019"}, {"entity1": "2020", "entity2": "2WikiMultiHopQA (2WIKI)", "relation": "Publication Date", "description": "2WikiMultiHopQA (2WIKI) was published in 2020"}, {"entity1": "2022", "entity2": "Musique (MSQ)", "relation": "Publication Date", "description": "Musique (MSQ) was published in 2022"}, {"entity1": "2022", "entity2": "CFQA", "relation": "Publication Date", "description": "CFQA was published in 2022"}, {"entity1": "2023", "entity2": "Llama2-7b", "relation": "Publication Date", "description": "Llama2-7b was published in 2023"}, {"entity1": "2024", "entity2": "Llama3-8b", "relation": "Publication Date", "description": "Llama3-8b was published in 2024"}], "8639071c-ba92-4a81-ab43-005fa4af3151": [{"entity1": "RAG-Instruct", "entity2": "Wikipedia", "relation": "Dataset-Origin", "description": "RAG-Instruct dataset is constructed using Wikipedia."}, {"entity1": "Contriever-MS MARCO", "entity2": "Izacard et al.", "relation": "Author-Role", "description": "Izacard et al. are the authors of Contriever-MS MARCO."}, {"entity1": "Llama2-7B", "entity2": "RAG-Instruct", "relation": "Tool/Resource", "description": "Llama2-7B is used as a base model for RAG-Instruct."}, {"entity1": "Qwen2.5-7B", "entity2": "RAG-Instruct", "relation": "Tool/Resource", "description": "Qwen2.5-7B is used as a base model for RAG-Instruct."}, {"entity1": "vLLM", "entity2": "Kwon et al.", "relation": "Author-Role", "description": "Kwon et al. are the authors of vLLM."}, {"entity1": "RAG-Instruct", "entity2": "GPT-4o", "relation": "Comparison", "description": "RAG-Instruct is compared to GPT-4o in terms of performance."}, {"entity1": "RAG-Instruct", "entity2": "GPT-4o-mini", "relation": "Comparison", "description": "RAG-Instruct is compared to GPT-4o-mini in terms of performance."}, {"entity1": "Self-RAG", "entity2": "RAG-Instruct", "relation": "Comparison", "description": "Self-RAG is compared to RAG-Instruct in terms of performance."}, {"entity1": "RQ-RAG", "entity2": "RAG-Instruct", "relation": "Comparison", "description": "RQ-RAG is compared to RAG-Instruct in terms of performance."}, {"entity1": "RAG-Instruct", "entity2": "PQA", "relation": "Experiment-Outcome", "description": "RAG-Instruct achieves good performance on PQA task."}, {"entity1": "RAG-Instruct", "entity2": "TQA", "relation": "Experiment-Outcome", "description": "RAG-Instruct achieves good performance on TQA task."}, {"entity1": "RAG-Instruct", "entity2": "HotQA", "relation": "Experiment-Outcome", "description": "RAG-Instruct achieves good performance on HotQA task."}, {"entity1": "RAG-Instruct", "entity2": "MSQ", "relation": "Experiment-Outcome", "description": "RAG-Instruct achieves good performance on MSQ task."}, {"entity1": "RAG-Instruct", "entity2": "PubMedQA", "relation": "Experiment-Outcome", "description": "RAG-Instruct achieves good performance on PubMedQA task."}], "71e712d3-4a08-49ae-aef9-dea256f6be2e": [{"entity1": "seedling survival", "entity2": "Nutrients", "relation": "Influence", "description": "Nutrients influence seedling survival"}, {"entity1": "handkerchief tree", "entity2": "OTC plots", "relation": "Studied In", "description": "Handkerchief tree is studied in the OTC plots"}, {"entity1": "temperature", "entity2": "handkerchief tree", "relation": "Environmental Factor", "description": "Temperature is an environmental factor for handkerchief tree"}, {"entity1": "sunshine rate", "entity2": "handkerchief tree", "relation": "Environmental Factor", "description": "Sunshine rate is an environmental factor for handkerchief tree"}, {"entity1": "rainfall", "entity2": "handkerchief tree", "relation": "Environmental Factor", "description": "Rainfall is an environmental factor for handkerchief tree"}, {"entity1": "humidity rate", "entity2": "handkerchief tree", "relation": "Environmental Factor", "description": "Humidity rate is an environmental factor for handkerchief tree"}, {"entity1": "soil type", "entity2": "handkerchief tree", "relation": "Environmental Factor", "description": "Soil type is an environmental factor for handkerchief tree"}, {"entity1": "policy makers", "entity2": "data", "relation": "User", "description": "Policy makers use data"}, {"entity1": "researchers", "entity2": "data", "relation": "User", "description": "Researchers use data"}, {"entity1": "RAG-Instruct", "entity2": "Instruction Simulation", "relation": "Methodology", "description": "RAG-Instruct uses Instruction Simulation as a methodology"}, {"entity1": "Llama3-8B", "entity2": "RAG-Instruct", "relation": "Tool/Resource", "description": "Llama3-8B is a tool/resource used for RAG-Instruct"}, {"entity1": "population pyramids", "entity2": "demographic trends", "relation": "Comparison Tool", "description": "Population pyramids are used to compare demographic trends"}, {"entity1": "Python", "entity2": "web page", "relation": "Tool/Resource", "description": "Python is used to generate a web page"}, {"entity1": "RAG scenarios", "entity2": "RAG-Instruct", "relation": "Application", "description": "RAG-Instruct is applied to RAG scenarios"}, {"entity1": "Table 4", "entity2": "RAG-Instruct", "relation": "Publication Venue", "description": "Table 4 is a publication venue for RAG-Instruct"}, {"entity1": "Table 5", "entity2": "RAG-Instruct", "relation": "Publication Venue", "description": "Table 5 is a publication venue for RAG-Instruct"}, {"entity1": "Figure 4", "entity2": "RAG-Instruct", "relation": "Publication Venue", "description": "Figure 4 is a publication venue for RAG-Instruct"}], "6707413a-f748-4b90-ad2b-c52862716d7f": [{"entity1": "Instruction Simulation", "entity2": "GPT-4o", "relation": "Methodology", "description": "Instruction Simulation is used to train GPT-4o, enabling it to adapt to various tasks."}, {"entity1": "RAG paradigms", "entity2": "RAG-Instruct", "relation": "Component", "description": "RAG paradigms are a part of RAG-Instruct, contributing to its effectiveness."}, {"entity1": "Llama3-8B", "entity2": "Ds and D\u2032s", "relation": "Training Data", "description": "Llama3-8B is trained using datasets Ds and D\u2032s."}, {"entity1": "Instruction Simulation", "entity2": "Task Performance", "relation": "Impact", "description": "Instruction Simulation has a significant impact on task performance, particularly for closed-set, multi-hop, and domain-specific tasks."}, {"entity1": "RAG paradigms", "entity2": "Multi-hop Tasks", "relation": "Challenge", "description": "RAG paradigms, especially multi-document paradigms (r2 and r4), are crucial for handling multi-hop tasks."}, {"entity1": "GPT-4o", "entity2": "Open-ended Tasks", "relation": "Performance", "description": "GPT-4o performs relatively well on open-ended tasks, even without Instruction Simulation."}, {"entity1": "Table 5", "entity2": "Instruction Simulation", "relation": "Evidence", "description": "Table 5 provides evidence of the performance decline when Instruction Simulation is removed."}, {"entity1": "Figure 4", "entity2": "Instruction Simulation", "relation": "Illustration", "description": "Figure 4 illustrates the effectiveness of Instruction Simulation in generating diverse questions."}, {"entity1": "RAG-Instruct", "entity2": "RAG Scenarios", "relation": "Inclusion", "description": "RAG-Instruct includes all five RAG scenarios, which are essential for its performance."}, {"entity1": "Multi-hop Reasoning", "entity2": "Multi-hop Tasks", "relation": "Methodology", "description": "Multi-hop reasoning is a methodology used to handle multi-hop tasks."}, {"entity1": "Domain-specific Tasks", "entity2": "RAG Paradigms", "relation": "Challenge", "description": "Domain-specific tasks pose a challenge that RAG paradigms help to address."}], "8b4e4094-3a05-48a9-bd37-1785c91794e0": [{"entity1": "RAG-Instruct", "entity2": "Llama3-8B", "relation": "Methodology", "description": "RAG-Instruct is built on the Llama3-8B model."}, {"entity1": "RAG-Instruct", "entity2": "Wikipedia", "relation": "Dataset-Origin", "description": "RAG-Instruct is built on the Wikipedia corpus."}, {"entity1": "RAG-Instruct", "entity2": "Simulation20k", "relation": "Methodology", "description": "RAG-Instruct uses Simulation20k for instruction simulation."}, {"entity1": "RAG-Instruct", "entity2": "Table 5", "relation": "Publication Venue", "description": "RAG-Instruct's ablation study is presented in Table 5."}, {"entity1": "RAG-Instruct", "entity2": "Table 6", "relation": "Publication Venue", "description": "RAG-Instruct's ablation study on role of query paradigms is presented in Table 6."}, {"entity1": "Llama3-8B", "entity2": "RAG-Instruct", "relation": "Author-Role", "description": "Llama3-8B is used as the base model for RAG-Instruct."}, {"entity1": "DuckDuckGo", "entity2": "Bing Search", "relation": "Comparison", "description": "DuckDuckGo and Bing Search are compared as retrieval sources for RAG-Instruct."}, {"entity1": "RAG-Instruct", "entity2": "TriviaQA", "relation": "Experiment-Outcome", "description": "RAG-Instruct achieves a performance of 86.9 on TriviaQA."}, {"entity1": "RAG-Instruct", "entity2": "HotpotQA", "relation": "Experiment-Outcome", "description": "RAG-Instruct achieves a performance of 73.1 on HotpotQA."}, {"entity1": "RAG-Instruct", "entity2": "ARC", "relation": "Experiment-Outcome", "description": "RAG-Instruct achieves a performance of 79.4 on ARC."}, {"entity1": "RAG-Instruct", "entity2": "PQA", "relation": "Experiment-Outcome", "description": "RAG-Instruct achieves a performance of 64.6 on PQA."}, {"entity1": "RAG-Instruct", "entity2": "TQA", "relation": "Experiment-Outcome", "description": "RAG-Instruct achieves a performance of 77.0 on TQA."}, {"entity1": "RAG-Instruct", "entity2": "OBQA", "relation": "Experiment-Outcome", "description": "RAG-Instruct achieves a performance of 80.2 on OBQA."}, {"entity1": "RAG-Instruct", "entity2": "4.5 Further Analysis", "relation": "Conclusion", "description": "RAG-Instruct's performance is further analyzed in section 4.5."}, {"entity1": "RAG-Instruct", "entity2": "Simulation20k", "relation": "Tool/Resource", "description": "Simulation20k is used as a tool/resource for RAG-Instruct."}, {"entity1": "RAG-Instruct", "entity2": "Instruction Simulation", "relation": "Methodology", "description": "RAG-Instruct uses instruction simulation as a methodology."}, {"entity1": "Llama3-8B", "entity2": "RAG-Instruct", "relation": "Funding", "description": "Llama3-8B is used to train RAG-Instruct, implying funding for the research."}], "32158c2a-2c9e-4eaf-9ef6-4e79957ae59a": [{"entity1": "Bing Search", "entity2": "RAG", "relation": "Tool/Resource", "description": "Bing Search is used as a retrieval source to enhance RAG capabilities."}, {"entity1": "DuckDuckGo", "entity2": "RAG", "relation": "Tool/Resource", "description": "DuckDuckGo is used as a retrieval source to enhance RAG capabilities."}, {"entity1": "ARC", "entity2": "RAG", "relation": "Application", "description": "RAG is applied to the ARC task to evaluate its performance."}, {"entity1": "PQA", "entity2": "RAG", "relation": "Application", "description": "RAG is applied to the PQA task to evaluate its performance."}, {"entity1": "TQA", "entity2": "RAG", "relation": "Application", "description": "RAG is applied to the TQA task to evaluate its performance."}, {"entity1": "OBQA", "entity2": "RAG", "relation": "Application", "description": "RAG is applied to the OBQA task to evaluate its performance."}, {"entity1": "Petroni et al.", "entity2": "RAG", "relation": "Research Area", "description": "Petroni et al. research focuses on the imperfections of retrievers in RAG."}, {"entity1": "Shi et al.", "entity2": "RAG", "relation": "Research Area", "description": "Shi et al. research focuses on aligning retrievers with LLM needs in RAG."}, {"entity1": "Maekawa et al.", "entity2": "RAG", "relation": "Research Area", "description": "Maekawa et al. research highlights the imperfections of retrievers in RAG."}, {"entity1": "Trivedi et al.", "entity2": "RAG", "relation": "Research Area", "description": "Trivedi et al. research focuses on multi-step retrieval processes in RAG."}, {"entity1": "Jiang et al.", "entity2": "RAG", "relation": "Research Area", "description": "Jiang et al. research focuses on multi-step retrieval processes in RAG."}, {"entity1": "Jeong et al.", "entity2": "RAG", "relation": "Research Area", "description": "Jeong et al. research focuses on query reformulation in RAG."}, {"entity1": "Shao et al.", "entity2": "RAG", "relation": "Research Area", "description": "Shao et al. research focuses on multi-step retrieval processes in RAG."}, {"entity1": "Yu et al.", "entity2": "RAG", "relation": "Research Area", "description": "Yu et al. research focuses on multi-step retrieval processes in RAG."}, {"entity1": "Asai et al.", "entity2": "RAG", "relation": "Research Area", "description": "Asai et al. research focuses on multi-step retrieval processes in RAG."}, {"entity1": "Wei et al.", "entity2": "RAG", "relation": "Research Area", "description": "Wei et al. research focuses on multi-step retrieval processes in RAG."}, {"entity1": "Ma et al.", "entity2": "RAG", "relation": "Research Area", "description": "Ma et al. research focuses on query reformulation in RAG."}, {"entity1": "Chan et al.", "entity2": "RAG", "relation": "Research Area", "description": "Chan et al. research focuses on enhancing RAG capabilities in noisy retrieval contexts."}, {"entity1": "Zhang et al.", "entity2": "RAG", "relation": "Research Area", "description": "Zhang et al. research focuses on enhancing RAG capabilities in noisy retrieval contexts."}, {"entity1": "Liu et al.", "entity2": "RAG", "relation": "Research Area", "description": "Liu et al. research focuses on enhancing RAG capabilities in noisy retrieval contexts."}, {"entity1": "Yoran et al.", "entity2": "RAG", "relation": "Research Area", "description": "Yoran et al. research focuses on enhancing RAG capabilities in noisy retrieval contexts."}, {"entity1": "RAFT", "entity2": "RAG", "relation": "Tool/Resource", "description": "RAFT is a dataset used to fine-tune RAG models."}, {"entity1": "LLMs", "entity2": "RAG", "relation": "Methodology", "description": "RAG is used to supplement the parametric knowledge of LLMs with external information sources."}, {"entity1": "Appendix C", "entity2": "RAG", "relation": "Publication Venue", "description": "Appendix C contains detailed results of the evaluation of RAG on single-hop QA tasks."}], "97d66cc3-091d-4461-825f-6558bdd12922": [{"entity1": "RAG-Instruct", "entity2": "Wikipedia", "relation": "Dataset-Origin", "description": "RAG-Instruct constructs a 40K instruction dataset from Wikipedia."}, {"entity1": "RAG-Instruct", "entity2": "chain-of-thought (CoT)", "relation": "Future Work", "description": "RAG-Instruct plans to incorporate chain-of-thought (CoT) characteristics in future work."}, {"entity1": "RAG paradigms", "entity2": "query-document relationships", "relation": "Methodology", "description": "RAG paradigms capture diverse query-document relationships."}, {"entity1": "GPT-4", "entity2": "RAG instructions", "relation": "Tool/Resource", "description": "GPT-4 is used to generate RAG instructions."}, {"entity1": "SlimOrca", "entity2": "Evol Instruct", "relation": "Dataset-Origin", "description": "SlimOrca and Evol Instruct are used to improve the diversity and quality of generated data."}, {"entity1": "Achiam et al.", "entity2": "GPT-4 technical report", "relation": "Publication Venue", "description": "Achiam et al. published the GPT-4 technical report on arXiv."}, {"entity1": "Akari Asai", "entity2": "Association for Computational Linguistics", "relation": "Author-Role", "description": "Akari Asai is an author in the proceedings of the 61st Annual Meeting of the Association for Computational Linguistics."}, {"entity1": "Zeqiu Wu", "entity2": "Hannaneh Hajishirzi", "relation": "Co-author", "description": "Zeqiu Wu and Hannaneh Hajishirzi are co-authors in the paper."}, {"entity1": "RAG-Instruct", "entity2": "Retrieval-based language models", "relation": "Application", "description": "RAG-Instruct is applied to retrieval-based language models."}, {"entity1": "RAG paradigms", "entity2": "Granularity", "relation": "Limitation", "description": "The current set of RAG paradigms has a coarse granularity and does not explore more granular or specialized paradigms."}], "073ac472-f650-49c7-bb3e-53e6823b95f7": [{"entity1": "Akari Asai", "entity2": "Association for Computational Linguistics", "relation": "Affiliation", "description": "Akari Asai is affiliated with the Association for Computational Linguistics as indicated by their publication in the ACL volume."}, {"entity1": "Akari Asai", "entity2": "Self-rag", "relation": "Author-Role", "description": "Akari Asai is an author of the Self-rag paper."}, {"entity1": "Jonathan Berant", "entity2": "Semantic parsing on freebase from question-answer pairs", "relation": "Author-Role", "description": "Jonathan Berant is an author of the Semantic parsing on freebase from question-answer pairs paper."}, {"entity1": "Chi-Min Chan", "entity2": "Rq-rag", "relation": "Author-Role", "description": "Chi-Min Chan is an author of the Rq-rag paper."}, {"entity1": "Junying Chen", "entity2": "Huatuogpt-ii", "relation": "Author-Role", "description": "Junying Chen is an author of the Huatuogpt-ii paper."}, {"entity1": "Yukang Chen", "entity2": "Long alpaca", "relation": "Author-Role", "description": "Yukang Chen is an author of the Long alpaca paper."}, {"entity1": "Zhiyu Chen", "entity2": "Convfinqa", "relation": "Author-Role", "description": "Zhiyu Chen is an author of the Convfinqa paper."}, {"entity1": "Peter Clark", "entity2": "Think you have solved question answering? try arc, the ai2 reasoning challenge", "relation": "Author-Role", "description": "Peter Clark is an author of the Think you have solved question answering? try arc, the ai2 reasoning challenge paper."}, {"entity1": "Tri Dao", "entity2": "Flashattention", "relation": "Author-Role", "description": "Tri Dao is an author of the Flashattention paper."}, {"entity1": "Abhimanyu Dubey", "entity2": "The llama 3 herd of models", "relation": "Author-Role", "description": "Abhimanyu Dubey is an author of The llama 3 herd of models paper."}, {"entity1": "The Twelfth International Conference on Learning Representations", "entity2": "Akari Asai", "relation": "Publication Venue", "description": "Akari Asai's paper was published at The Twelfth International Conference on Learning Representations."}, {"entity1": "arXiv", "entity2": "Akari Asai", "relation": "Publication Venue", "description": "Akari Asai's paper was published on arXiv."}, {"entity1": "Proceedings of the 2013 conference on empirical methods in natural language processing", "entity2": "Jonathan Berant", "relation": "Publication Venue", "description": "Jonathan Berant's paper was published in the Proceedings of the 2013 conference on empirical methods in natural language processing."}, {"entity1": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing", "entity2": "Zhiyu Chen", "relation": "Publication Venue", "description": "Zhiyu Chen's paper was published in the Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing."}, {"entity1": "Advances in Neural Information Processing Systems", "entity2": "Tri Dao", "relation": "Publication Venue", "description": "Tri Dao's paper was published in Advances in Neural Information Processing Systems."}], "d177ed53-2313-4ce8-b222-cf8881e80e5a": [{"entity1": "Mandar Joshi", "entity2": "Eunsol Choi", "relation": "Co-author", "description": "Mandar Joshi and Eunsol Choi co-authored the paper 'Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension'."}, {"entity1": "Vladimir Karpukhin", "entity2": "Barlas Oguz", "relation": "Co-author", "description": "Vladimir Karpukhin and Barlas Oguz co-authored the paper 'Dense passage retrieval for open-domain question answering'."}, {"entity1": "Tom\u00e1\u0161 Ko\u02c7cisk`y", "entity2": "Jonathan Schwarz", "relation": "Co-author", "description": "Tom\u00e1\u0161 Ko\u02c7cisk`y and Jonathan Schwarz co-authored the paper 'The narrativeqa reading comprehension challenge'."}, {"entity1": "Tom Kwiatkowski", "entity2": "Jennimaria Palomaki", "relation": "Co-author", "description": "Tom Kwiatkowski and Jennimaria Palomaki co-authored the paper 'Natural questions: a benchmark for question answering research'."}, {"entity1": "Woosuk Kwon", "entity2": "Zhuohan Li", "relation": "Co-author", "description": "Woosuk Kwon and Zhuohan Li co-authored the paper 'Efficient memory management for large language model serving with pagedattention'."}, {"entity1": "Xi Victoria Lin", "entity2": "Xilun Chen", "relation": "Co-author", "description": "Xi Victoria Lin and Xilun Chen co-authored the paper 'Ra-dit: Retrieval-augmented dual instruction tuning'."}, {"entity1": "Wanlong Liu", "entity2": "Enqi Zhang", "relation": "Co-author", "description": "Wanlong Liu and Enqi Zhang co-authored the paper 'A compressive memory-based retrieval approach for event argument extraction'."}, {"entity1": "Zihan Liu", "entity2": "Wei Ping", "relation": "Co-author", "description": "Zihan Liu and Wei Ping co-authored the paper 'Chatqa: Building gpt-4 level conversational qa models'."}, {"entity1": "Shuai Lu", "entity2": "Nan Duan", "relation": "Co-author", "description": "Shuai Lu and Nan Duan co-authored the paper 'Reacc: A retrieval-augmented code completion framework'."}, {"entity1": "Xinbei Ma", "entity2": "Yeyun Gong", "relation": "Co-author", "description": "Xinbei Ma and Yeyun Gong co-authored a paper."}, {"entity1": "Mandar Joshi", "entity2": "Triviaqa", "relation": "Author-Role", "description": "Mandar Joshi is an author of the Triviaqa dataset."}, {"entity1": "Vladimir Karpukhin", "entity2": "Dense passage retrieval", "relation": "Author-Role", "description": "Vladimir Karpukhin is an author of the Dense passage retrieval paper."}, {"entity1": "Tom\u00e1\u0161 Ko\u02c7cisk`y", "entity2": "Narrativeqa", "relation": "Author-Role", "description": "Tom\u00e1\u0161 Ko\u02c7cisk`y is an author of the Narrativeqa dataset."}, {"entity1": "Tom Kwiatkowski", "entity2": "Natural questions", "relation": "Author-Role", "description": "Tom Kwiatkowski is an author of the Natural questions dataset."}, {"entity1": "Woosuk Kwon", "entity2": "Pagedattention", "relation": "Author-Role", "description": "Woosuk Kwon is an author of the Pagedattention paper."}, {"entity1": "Xi Victoria Lin", "entity2": "Radit", "relation": "Author-Role", "description": "Xi Victoria Lin is an author of the Radit paper."}, {"entity1": "Wanlong Liu", "entity2": "Compressive memory-based retrieval", "relation": "Author-Role", "description": "Wanlong Liu is an author of the Compressive memory-based retrieval paper."}, {"entity1": "Zihan Liu", "entity2": "Chatqa", "relation": "Author-Role", "description": "Zihan Liu is an author of the Chatqa paper."}, {"entity1": "Shuai Lu", "entity2": "Reacc", "relation": "Author-Role", "description": "Shuai Lu is an author of the Reacc paper."}, {"entity1": "EMNLP-IJCNLP", "entity2": "Association for Computational Linguistics", "relation": "Affiliation", "description": "EMNLP-IJCNLP is affiliated with the Association for Computational Linguistics."}, {"entity1": "GPT-4", "entity2": "Zihan Liu", "relation": "Tool/Resource", "description": "GPT-4 is a tool/resource used by Zihan Liu in the Chatqa paper."}, {"entity1": "Triviaqa", "entity2": "Mandar Joshi", "relation": "Publication Venue", "description": "Triviaqa was published in the Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics."}, {"entity1": "Dense passage retrieval", "entity2": "Vladimir Karpukhin", "relation": "Publication Venue", "description": "Dense passage retrieval was published in the Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing."}, {"entity1": "Narrativeqa", "entity2": "Tom\u00e1\u0161 Ko\u02c7cisk`y", "relation": "Publication Venue", "description": "Narrativeqa was published in the Transactions of the Association for Computational Linguistics."}, {"entity1": "Natural questions", "entity2": "Tom Kwiatkowski", "relation": "Publication Venue", "description": "Natural questions was published in the Transactions of the Association for Computational Linguistics."}, {"entity1": "Pagedattention", "entity2": "Woosuk Kwon", "relation": "Publication Venue", "description": "Pagedattention was published in the Proceedings of the 29th Symposium on Operating Systems Principles."}, {"entity1": "Radit", "entity2": "Xi Victoria Lin", "relation": "Publication Venue", "description": "Radit was published in the arXiv preprint arXiv:2310.01352."}, {"entity1": "Compressive memory-based retrieval", "entity2": "Wanlong Liu", "relation": "Publication Venue", "description": "Compressive memory-based retrieval was published in the arXiv preprint arXiv:2409.09322."}, {"entity1": "Chatqa", "entity2": "Zihan Liu", "relation": "Publication Venue", "description": "Chatqa was published in the arXiv preprint arXiv:2401.10225."}, {"entity1": "Reacc", "entity2": "Shuai Lu", "relation": "Publication Venue", "description": "Reacc was published in the Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics."}], "9560f9ca-418b-4109-aa44-5a0705c1bd7d": [{"entity1": "Seiji Maekawa", "entity2": "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "relation": "Publication Venue", "description": "Seiji Maekawa published in Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"}, {"entity1": "Nan Duan", "entity2": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing", "relation": "Publication Venue", "description": "Nan Duan published in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"}, {"entity1": "Association for Computational Linguistics", "entity2": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics", "relation": "Affiliation", "description": "Association for Computational Linguistics is affiliated with Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"}, {"entity1": "retrieval augmentation", "entity2": "retrieval-augmented code completion framework", "relation": "Application", "description": "Retrieval augmentation is applied in retrieval-augmented code completion framework"}, {"entity1": "retrieval-augmented large language models", "entity2": "query rewriting", "relation": "Methodology", "description": "Query rewriting is a methodology used in retrieval-augmented large language models"}, {"entity1": "Pengcheng He", "entity2": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing", "relation": "Publication Venue", "description": "Pengcheng He published in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"}, {"entity1": "Xinbei Ma", "entity2": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing", "relation": "Publication Venue", "description": "Xinbei Ma published in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"}, {"entity1": "Sairam Gurajada", "entity2": "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "relation": "Publication Venue", "description": "Sairam Gurajada published in Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"}, {"entity1": "Hayate Iso", "entity2": "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "relation": "Publication Venue", "description": "Hayate Iso published in Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"}, {"entity1": "Nikita Bhutani", "entity2": "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "relation": "Publication Venue", "description": "Nikita Bhutani published in Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"}, {"entity1": "Yeyun Gong", "entity2": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing", "relation": "Publication Venue", "description": "Yeyun Gong published in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"}, {"entity1": "language models", "entity2": "retrieval augmentation", "relation": "Application", "description": "Retrieval augmentation is applied to language models"}, {"entity1": "Hai Zhao", "entity2": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing", "relation": "Publication Venue", "description": "Hai Zhao published in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"}, {"entity1": "Seiji Maekawa", "entity2": "Hayate Iso", "relation": "Co-author", "description": "Seiji Maekawa and Hayate Iso are co-authors"}, {"entity1": "Seiji Maekawa", "entity2": "Sairam Gurajada", "relation": "Co-author", "description": "Seiji Maekawa and Sairam Gurajada are co-authors"}, {"entity1": "Seiji Maekawa", "entity2": "Nikita Bhutani", "relation": "Co-author", "description": "Seiji Maekawa and Nikita Bhutani are co-authors"}, {"entity1": "Xinbei Ma", "entity2": "Yeyun Gong", "relation": "Co-author", "description": "Xinbei Ma and Yeyun Gong are co-authors"}, {"entity1": "Xinbei Ma", "entity2": "Pengcheng He", "relation": "Co-author", "description": "Xinbei Ma and Pengcheng He are co-authors"}, {"entity1": "Xinbei Ma", "entity2": "Hai Zhao", "relation": "Co-author", "description": "Xinbei Ma and Hai Zhao are co-authors"}, {"entity1": "Xinbei Ma", "entity2": "Nan Duan", "relation": "Co-author", "description": "Xinbei Ma and Nan Duan are co-authors"}, {"entity1": "Yeyun Gong", "entity2": "Pengcheng He", "relation": "Co-author", "description": "Yeyun Gong and Pengcheng He are co-authors"}, {"entity1": "Yeyun Gong", "entity2": "Hai Zhao", "relation": "Co-author", "description": "Yeyun Gong and Hai Zhao are co-authors"}, {"entity1": "Yeyun Gong", "entity2": "Nan Duan", "relation": "Co-author", "description": "Yeyun Gong and Nan Duan are co-authors"}, {"entity1": "Pengcheng He", "entity2": "Hai Zhao", "relation": "Co-author", "description": "Pengcheng He and Hai Zhao are co-authors"}, {"entity1": "Pengcheng He", "entity2": "Nan Duan", "relation": "Co-author", "description": "Pengcheng He and Nan Duan are co-authors"}, {"entity1": "Hai Zhao", "entity2": "Nan Duan", "relation": "Co-author", "description": "Hai Zhao and Nan Duan are co-authors"}], "8e66ac05-c053-4505-8473-c9319c349506": [{"entity1": "Alex Mallen", "entity2": "Association for Computational Linguistics", "relation": "Affiliation", "description": "Alex Mallen is affiliated with the Association for Computational Linguistics as indicated by the publication in their proceedings."}, {"entity1": "Todor Mihaylov", "entity2": "Conference on Empirical Methods in Natural Language Processing", "relation": "Publication Venue", "description": "Todor Mihaylov published a paper at the Conference on Empirical Methods in Natural Language Processing."}, {"entity1": "Arindam Mitra", "entity2": "Orca 2", "relation": "Research Area", "description": "Arindam Mitra is involved in the research area of Orca 2, which focuses on teaching small language models how to reason."}, {"entity1": "Fabio Petroni", "entity2": "Automated Knowledge Base Construction", "relation": "Publication Venue", "description": "Fabio Petroni published a paper in Automated Knowledge Base Construction."}, {"entity1": "Samyam Rajbhandari", "entity2": "SC20: International Conference for High Performance Computing, Networking, Storage and Analysis", "relation": "Publication Venue", "description": "Samyam Rajbhandari published a paper at SC20: International Conference for High Performance Computing, Networking, Storage and Analysis."}, {"entity1": "Pranav Rajpurkar", "entity2": "SQuAD", "relation": "Research Area", "description": "Pranav Rajpurkar is involved in the research area of SQuAD, focusing on unanswerable questions."}, {"entity1": "Zhihong Shao", "entity2": "Association for Computational Linguistics", "relation": "Affiliation", "description": "Zhihong Shao is affiliated with the Association for Computational Linguistics as indicated by the publication in their findings."}, {"entity1": "Freda Shi", "entity2": "International Conference on Machine Learning", "relation": "Publication Venue", "description": "Freda Shi published a paper at the International Conference on Machine Learning."}, {"entity1": "Weijia Shi", "entity2": "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics", "relation": "Publication Venue", "description": "Weijia Shi published a paper in the Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics."}, {"entity1": "Akari Asai", "entity2": "Alex Mallen", "relation": "Co-author", "description": "Akari Asai and Alex Mallen are co-authors of a paper."}, {"entity1": "Victor Zhong", "entity2": "Rajarshi Das", "relation": "Co-author", "description": "Victor Zhong and Rajarshi Das are co-authors of a paper."}, {"entity1": "Daniel Khashabi", "entity2": "Hannaneh Hajishirzi", "relation": "Co-author", "description": "Daniel Khashabi and Hannaneh Hajishirzi are co-authors of a paper."}, {"entity1": "Todor Mihaylov", "entity2": "Peter Clark", "relation": "Co-author", "description": "Todor Mihaylov and Peter Clark are co-authors of a paper."}, {"entity1": "Arindam Mitra", "entity2": "Luciano Del Corro", "relation": "Co-author", "description": "Arindam Mitra and Luciano Del Corro are co-authors of a paper."}, {"entity1": "Fabio Petroni", "entity2": "Patrick Lewis", "relation": "Co-author", "description": "Fabio Petroni and Patrick Lewis are co-authors of a paper."}, {"entity1": "Samyam Rajbhandari", "entity2": "Jeff Rasley", "relation": "Co-author", "description": "Samyam Rajbhandari and Jeff Rasley are co-authors of a paper."}, {"entity1": "Pranav Rajpurkar", "entity2": "Robin Jia", "relation": "Co-author", "description": "Pranav Rajpurkar and Robin Jia are co-authors of a paper."}, {"entity1": "Zhihong Shao", "entity2": "Yeyun Gong", "relation": "Co-author", "description": "Zhihong Shao and Yeyun Gong are co-authors of a paper."}, {"entity1": "Freda Shi", "entity2": "Xinyun Chen", "relation": "Co-author", "description": "Freda Shi and Xinyun Chen are co-authors of a paper."}, {"entity1": "Weijia Shi", "entity2": "Sewon Min", "relation": "Co-author", "description": "Weijia Shi and Sewon Min are co-authors of a paper."}], "7ded0298-43c1-4ad8-9ea9-da54d5e061c0": [{"entity1": "Joon Seo", "entity2": "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "relation": "Publication Venue", "description": "Joon Seo published in Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"}, {"entity1": "Hugo Touvron", "entity2": "arXiv:2307.09288", "relation": "Publication Venue", "description": "Hugo Touvron published in arXiv:2307.09288"}, {"entity1": "Harsh Trivedi", "entity2": "Transactions of the Association for Computational Linguistics", "relation": "Publication Venue", "description": "Harsh Trivedi published in Transactions of the Association for Computational Linguistics"}, {"entity1": "Yizhong Wang", "entity2": "arXiv:2306.04751", "relation": "Publication Venue", "description": "Yizhong Wang published in arXiv:2306.04751"}, {"entity1": "Zhepei Wei", "entity2": "arXiv:2406.13629", "relation": "Publication Venue", "description": "Zhepei Wei published in arXiv:2406.13629"}, {"entity1": "Can Xu", "entity2": "arXiv:2304.12244", "relation": "Publication Venue", "description": "Can Xu published in arXiv:2304.12244"}, {"entity1": "An Yang", "entity2": "arXiv:2407.10671", "relation": "Publication Venue", "description": "An Yang published in arXiv:2407.10671"}, {"entity1": "Zhilin Yang", "entity2": "Hotpotqa", "relation": "Dataset-Origin", "description": "Zhilin Yang is associated with the Hotpotqa dataset"}, {"entity1": "Joon Seo", "entity2": "Replug", "relation": "Tool/Resource", "description": "Joon Seo is associated with the Replug tool/resource"}, {"entity1": "Hugo Touvron", "entity2": "Llama 2", "relation": "Tool/Resource", "description": "Hugo Touvron is associated with the Llama 2 tool/resource"}, {"entity1": "Harsh Trivedi", "entity2": "Musique", "relation": "Tool/Resource", "description": "Harsh Trivedi is associated with the Musique tool/resource"}, {"entity1": "Yizhong Wang", "entity2": "Self-instruct", "relation": "Tool/Resource", "description": "Yizhong Wang is associated with the Self-instruct tool/resource"}, {"entity1": "Zhepei Wei", "entity2": "Instructrag", "relation": "Tool/Resource", "description": "Zhepei Wei is associated with the Instructrag tool/resource"}, {"entity1": "Can Xu", "entity2": "Wizardlm", "relation": "Tool/Resource", "description": "Can Xu is associated with the Wizardlm tool/resource"}, {"entity1": "An Yang", "entity2": "Qwen2", "relation": "Tool/Resource", "description": "An Yang is associated with the Qwen2 tool/resource"}, {"entity1": "Joon Seo", "entity2": "Richard James", "relation": "Co-author", "description": "Joon Seo and Richard James are co-authors"}, {"entity1": "Hugo Touvron", "entity2": "Louis Martin", "relation": "Co-author", "description": "Hugo Touvron and Louis Martin are co-authors"}, {"entity1": "Harsh Trivedi", "entity2": "Niranjan Balasubramanian", "relation": "Co-author", "description": "Harsh Trivedi and Niranjan Balasubramanian are co-authors"}, {"entity1": "Yizhong Wang", "entity2": "Yeganeh Kordi", "relation": "Co-author", "description": "Yizhong Wang and Yeganeh Kordi are co-authors"}, {"entity1": "Zhepei Wei", "entity2": "Wei-Lin Chen", "relation": "Co-author", "description": "Zhepei Wei and Wei-Lin Chen are co-authors"}, {"entity1": "Can Xu", "entity2": "Qingfeng Sun", "relation": "Co-author", "description": "Can Xu and Qingfeng Sun are co-authors"}, {"entity1": "An Yang", "entity2": "Baosong Yang", "relation": "Co-author", "description": "An Yang and Baosong Yang are co-authors"}, {"entity1": "Zhilin Yang", "entity2": "Peng Qi", "relation": "Co-author", "description": "Zhilin Yang and Peng Qi are co-authors"}], "1f5d362f-c215-4fbf-b692-64d4d2ae8ab7": [{"entity1": "Zhilin Yang", "entity2": "Hotpotqa", "relation": "Author-Role", "description": "Zhilin Yang is an author of the Hotpotqa dataset."}, {"entity1": "Peng Qi", "entity2": "Hotpotqa", "relation": "Author-Role", "description": "Peng Qi is an author of the Hotpotqa dataset."}, {"entity1": "Saizheng Zhang", "entity2": "Hotpotqa", "relation": "Author-Role", "description": "Saizheng Zhang is an author of the Hotpotqa dataset."}, {"entity1": "Yoshua Bengio", "entity2": "Hotpotqa", "relation": "Author-Role", "description": "Yoshua Bengio is an author of the Hotpotqa dataset."}, {"entity1": "William Cohen", "entity2": "Hotpotqa", "relation": "Author-Role", "description": "William Cohen is an author of the Hotpotqa dataset."}, {"entity1": "Ruslan Salakhutdinov", "entity2": "Hotpotqa", "relation": "Author-Role", "description": "Ruslan Salakhutdinov is an author of the Hotpotqa dataset."}, {"entity1": "Christopher D Manning", "entity2": "Hotpotqa", "relation": "Author-Role", "description": "Christopher D Manning is an author of the Hotpotqa dataset."}, {"entity1": "Hotpotqa", "entity2": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing", "relation": "Publication Venue", "description": "Hotpotqa was published in the Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing."}, {"entity1": "Ori Yoran", "entity2": "Making retrieval-augmented language models robust to irrelevant context", "relation": "Author-Role", "description": "Ori Yoran is an author of the paper 'Making retrieval-augmented language models robust to irrelevant context'."}, {"entity1": "Tomer Wolfson", "entity2": "Making retrieval-augmented language models robust to irrelevant context", "relation": "Author-Role", "description": "Tomer Wolfson is an author of the paper 'Making retrieval-augmented language models robust to irrelevant context'."}, {"entity1": "Ori Ram", "entity2": "Making retrieval-augmented language models robust to irrelevant context", "relation": "Author-Role", "description": "Ori Ram is an author of the paper 'Making retrieval-augmented language models robust to irrelevant context'."}, {"entity1": "Jonathan Berant", "entity2": "Making retrieval-augmented language models robust to irrelevant context", "relation": "Author-Role", "description": "Jonathan Berant is an author of the paper 'Making retrieval-augmented language models robust to irrelevant context'."}, {"entity1": "Making retrieval-augmented language models robust to irrelevant context", "entity2": "The Twelfth International Conference on Learning Representations", "relation": "Publication Venue", "description": "The paper 'Making retrieval-augmented language models robust to irrelevant context' was published in The Twelfth International Conference on Learning Representations."}], "82a54603-088b-4641-ab16-d0de6d20b1dd": [{"entity1": "W. Yu", "entity2": "Hongming Zhang", "relation": "Co-author", "description": "W. Yu and Hongming Zhang co-authored the paper 'Chain-of-note: Enhancing robustness in retrieval-augmented language models'"}, {"entity1": "W. Yu", "entity2": "Xiaoman Pan", "relation": "Co-author", "description": "W. Yu and Xiaoman Pan co-authored the paper 'Chain-of-note: Enhancing robustness in retrieval-augmented language models'"}, {"entity1": "W. Yu", "entity2": "Kaixin Ma", "relation": "Co-author", "description": "W. Yu and Kaixin Ma co-authored the paper 'Chain-of-note: Enhancing robustness in retrieval-augmented language models'"}, {"entity1": "W. Yu", "entity2": "Hongwei Wang", "relation": "Co-author", "description": "W. Yu and Hongwei Wang co-authored the paper 'Chain-of-note: Enhancing robustness in retrieval-augmented language models'"}, {"entity1": "W. Yu", "entity2": "Dong Yu", "relation": "Co-author", "description": "W. Yu and Dong Yu co-authored the paper 'Chain-of-note: Enhancing robustness in retrieval-augmented language models'"}, {"entity1": "Tianhua Zhang", "entity2": "Hongyin Luo", "relation": "Co-author", "description": "Tianhua Zhang and Hongyin Luo co-authored the paper 'Interpretable unified language checking'"}, {"entity1": "Tianhua Zhang", "entity2": "Yung-Sung Chuang", "relation": "Co-author", "description": "Tianhua Zhang and Yung-Sung Chuang co-authored the paper 'Interpretable unified language checking'"}, {"entity1": "Tianhua Zhang", "entity2": "Wei Fang", "relation": "Co-author", "description": "Tianhua Zhang and Wei Fang co-authored the paper 'Interpretable unified language checking'"}, {"entity1": "Tianhua Zhang", "entity2": "Luc Gaitskell", "relation": "Co-author", "description": "Tianhua Zhang and Luc Gaitskell co-authored the paper 'Interpretable unified language checking'"}, {"entity1": "Tianhua Zhang", "entity2": "Thomas Hartvigsen", "relation": "Co-author", "description": "Tianhua Zhang and Thomas Hartvigsen co-authored the paper 'Interpretable unified language checking'"}, {"entity1": "Tianhua Zhang", "entity2": "Xixin Wu", "relation": "Co-author", "description": "Tianhua Zhang and Xixin Wu co-authored the paper 'Interpretable unified language checking'"}, {"entity1": "Tianhua Zhang", "entity2": "Danny Fox", "relation": "Co-author", "description": "Tianhua Zhang and Danny Fox co-authored the paper 'Interpretable unified language checking'"}, {"entity1": "Tianhua Zhang", "entity2": "Helen Meng", "relation": "Co-author", "description": "Tianhua Zhang and Helen Meng co-authored the paper 'Interpretable unified language checking'"}, {"entity1": "Tianhua Zhang", "entity2": "James Glass", "relation": "Co-author", "description": "Tianhua Zhang and James Glass co-authored the paper 'Interpretable unified language checking'"}, {"entity1": "Tianjun Zhang", "entity2": "Shishir G Patil", "relation": "Co-author", "description": "Tianjun Zhang and Shishir G Patil co-authored the paper 'Raft: Adapting language model to domain specific rag'"}, {"entity1": "Tianjun Zhang", "entity2": "Naman Jain", "relation": "Co-author", "description": "Tianjun Zhang and Naman Jain co-authored the paper 'Raft: Adapting language model to domain specific rag'"}, {"entity1": "Tianjun Zhang", "entity2": "Sheng Shen", "relation": "Co-author", "description": "Tianjun Zhang and Sheng Shen co-authored the paper 'Raft: Adapting language model to domain specific rag'"}, {"entity1": "Tianjun Zhang", "entity2": "Matei Zaharia", "relation": "Co-author", "description": "Tianjun Zhang and Matei Zaharia co-authored the paper 'Raft: Adapting language model to domain specific rag'"}, {"entity1": "Tianjun Zhang", "entity2": "Ion Stoica", "relation": "Co-author", "description": "Tianjun Zhang and Ion Stoica co-authored the paper 'Raft: Adapting language model to domain specific rag'"}, {"entity1": "Tianjun Zhang", "entity2": "Joseph E Gonzalez", "relation": "Co-author", "description": "Tianjun Zhang and Joseph E Gonzalez co-authored the paper 'Raft: Adapting language model to domain specific rag'"}, {"entity1": "Lianmin Zheng", "entity2": "Wei-Lin Chiang", "relation": "Co-author", "description": "Lianmin Zheng and Wei-Lin Chiang co-authored the paper 'Lmsys-chat-1m: A large-scale real-world llm conversation dataset'"}, {"entity1": "Lianmin Zheng", "entity2": "Ying Sheng", "relation": "Co-author", "description": "Lianmin Zheng and Ying Sheng co-authored the paper 'Lmsys-chat-1m: A large-scale real-world llm conversation dataset'"}, {"entity1": "Lianmin Zheng", "entity2": "Tianle Li", "relation": "Co-author", "description": "Lianmin Zheng and Tianle Li co-authored the paper 'Lmsys-chat-1m: A large-scale real-world llm conversation dataset'"}, {"entity1": "Lianmin Zheng", "entity2": "Siyuan Zhuang", "relation": "Co-author", "description": "Lianmin Zheng and Siyuan Zhuang co-authored the paper 'Lmsys-chat-1m: A large-scale real-world llm conversation dataset'"}, {"entity1": "Lianmin Zheng", "entity2": "Zhanghao Wu", "relation": "Co-author", "description": "Lianmin Zheng and Zhanghao Wu co-authored the paper 'Lmsys-chat-1m: A large-scale real-world llm conversation dataset'"}, {"entity1": "Lianmin Zheng", "entity2": "Yonghao Zhuang", "relation": "Co-author", "description": "Lianmin Zheng and Yonghao Zhuang co-authored the paper 'Lmsys-chat-1m: A large-scale real-world llm conversation dataset'"}, {"entity1": "Lianmin Zheng", "entity2": "Zhuohan Li", "relation": "Co-author", "description": "Lianmin Zheng and Zhuohan Li co-authored the paper 'Lmsys-chat-1m: A large-scale real-world llm conversation dataset'"}, {"entity1": "Lianmin Zheng", "entity2": "Zi Lin", "relation": "Co-author", "description": "Lianmin Zheng and Zi Lin co-authored the paper 'Lmsys-chat-1m: A large-scale real-world llm conversation dataset'"}, {"entity1": "Lianmin Zheng", "entity2": "Eric P Xing", "relation": "Co-author", "description": "Lianmin Zheng and Eric P Xing co-authored the paper 'Lmsys-chat-1m: A large-scale real-world llm conversation dataset'"}, {"entity1": "Yang Zheng", "entity2": "Adam W Harley", "relation": "Co-author", "description": "Yang Zheng and Adam W Harley co-authored the paper 'Pointodyssey: A large-scale synthetic dataset for long-term point tracking'"}, {"entity1": "Yang Zheng", "entity2": "Bokui Shen", "relation": "Co-author", "description": "Yang Zheng and Bokui Shen co-authored the paper 'Pointodyssey: A large-scale synthetic dataset for long-term point tracking'"}, {"entity1": "Yang Zheng", "entity2": "Gordon Wetzstein", "relation": "Co-author", "description": "Yang Zheng and Gordon Wetzstein co-authored the paper 'Pointodyssey: A large-scale synthetic dataset for long-term point tracking'"}, {"entity1": "Yang Zheng", "entity2": "Leonidas J Guibas", "relation": "Co-author", "description": "Yang Zheng and Leonidas J Guibas co-authored the paper 'Pointodyssey: A large-scale synthetic dataset for long-term point tracking'"}, {"entity1": "W. Yu", "entity2": "ArXiv", "relation": "Publication Venue", "description": "The paper 'Chain-of-note: Enhancing robustness in retrieval-augmented language models' by W. Yu was published on ArXiv"}, {"entity1": "Tianhua Zhang", "entity2": "ArXiv", "relation": "Publication Venue", "description": "The paper 'Interpretable unified language checking' by Tianhua Zhang was published on ArXiv"}, {"entity1": "Tianjun Zhang", "entity2": "ArXiv", "relation": "Publication Venue", "description": "The paper 'Raft: Adapting language model to domain specific rag' by Tianjun Zhang was published on ArXiv"}, {"entity1": "Lianmin Zheng", "entity2": "ArXiv", "relation": "Publication Venue", "description": "The paper 'Lmsys-chat-1m: A large-scale real-world llm conversation dataset' by Lianmin Zheng was published on ArXiv"}, {"entity1": "Yang Zheng", "entity2": "IEEE/CVF International Conference on Computer Vision", "relation": "Publication Venue", "description": "The paper 'Pointodyssey: A large-scale synthetic dataset for long-term point tracking' by Yang Zheng was published at the IEEE/CVF International Conference on Computer Vision"}], "f49f8e6a-171e-48d1-a372-a9d4d215d4f7": [{"entity1": "RAG-Instruct", "entity2": "Wikipedia", "relation": "Dataset-Origin", "description": "RAG-Instruct corpus is built using Wikipedia."}, {"entity1": "Karpukhin et al.", "entity2": "RAG-Instruct", "relation": "Methodology", "description": "RAG-Instruct follows the approach of Karpukhin et al. for dataset construction."}, {"entity1": "Shi et al.", "entity2": "RAG-Instruct", "relation": "Methodology", "description": "RAG-Instruct generates Wikipedia document embeddings following the work of Shi et al."}, {"entity1": "ShareGPT", "entity2": "RAG-Instruct", "relation": "Dataset-Origin", "description": "ShareGPT is one of the datasets used to select exemplar data for RAG-Instruct."}, {"entity1": "Alpaca", "entity2": "RAG-Instruct", "relation": "Dataset-Origin", "description": "Alpaca is one of the datasets used to select exemplar data for RAG-Instruct."}, {"entity1": "WizardLM-70K", "entity2": "RAG-Instruct", "relation": "Dataset-Origin", "description": "WizardLM-70K is one of the datasets used to select exemplar data for RAG-Instruct."}, {"entity1": "Lmsys-chat-1M", "entity2": "RAG-Instruct", "relation": "Dataset-Origin", "description": "Lmsys-chat-1M is one of the datasets used to select exemplar data for RAG-Instruct."}, {"entity1": "SlimOrca", "entity2": "RAG-Instruct", "relation": "Dataset-Origin", "description": "SlimOrca is one of the datasets used to select exemplar data for RAG-Instruct."}, {"entity1": "GPT-4o", "entity2": "RAG-Instruct", "relation": "Tool/Resource", "description": "GPT-4o is used to further filter for knowledge-intensive instructions from synthetic datasets for RAG-Instruct."}, {"entity1": "Nvidia A800", "entity2": "RAG-Instruct", "relation": "Tool/Resource", "description": "Nvidia A800 GPUs are used for training RAG-Instruct models."}, {"entity1": "DeepSpeed Stage 3", "entity2": "RAG-Instruct", "relation": "Tool/Resource", "description": "DeepSpeed Stage 3 is used for multi-GPU distributed training of RAG-Instruct models."}, {"entity1": "FlashAttention", "entity2": "RAG-Instruct", "relation": "Tool/Resource", "description": "FlashAttention is employed to improve efficiency during long-context training of RAG-Instruct models."}, {"entity1": "WebQA", "entity2": "RAG-Instruct", "relation": "Publication Venue", "description": "WebQA is one of the downstream tasks used to evaluate RAG-Instruct."}, {"entity1": "PopQA", "entity2": "RAG-Instruct", "relation": "Publication Venue", "description": "PopQA is one of the downstream tasks used to evaluate RAG-Instruct."}, {"entity1": "TriviaQA-unfiltered", "entity2": "RAG-Instruct", "relation": "Publication Venue", "description": "TriviaQA-unfiltered is one of the downstream tasks used to evaluate RAG-Instruct."}, {"entity1": "Asai et al.", "entity2": "RAG-Instruct", "relation": "Methodology", "description": "RAG-Instruct follows the approach of Asai et al. for evaluating performance based on accuracy."}, {"entity1": "Rajbhandari et al.", "entity2": "DeepSpeed Stage 3", "relation": "Author-Role", "description": "Rajbhandari et al. are the authors of DeepSpeed Stage 3."}, {"entity1": "Dao et al.", "entity2": "FlashAttention", "relation": "Author-Role", "description": "Dao et al. are the authors of FlashAttention."}, {"entity1": "Berant et al.", "entity2": "WebQA", "relation": "Author-Role", "description": "Berant et al. are the authors of WebQA."}, {"entity1": "Joshi et al.", "entity2": "TriviaQA-unfiltered", "relation": "Author-Role", "description": "Joshi et al. are the authors of TriviaQA-unfiltered."}, {"entity1": "Mallen et al.", "entity2": "PopQA", "relation": "Author-Role", "description": "Mallen et al. are the authors of PopQA."}, {"entity1": "Wang et al.", "entity2": "ShareGPT", "relation": "Author-Role", "description": "Wang et al. are the authors of ShareGPT."}, {"entity1": "Cheung and Lam", "entity2": "Alpaca", "relation": "Author-Role", "description": "Cheung and Lam are the authors of Alpaca."}, {"entity1": "Xu et al.", "entity2": "WizardLM-70K", "relation": "Author-Role", "description": "Xu et al. are the authors of WizardLM-70K."}, {"entity1": "Zheng et al.", "entity2": "Lmsys-chat-1M", "relation": "Author-Role", "description": "Zheng et al. are the authors of Lmsys-chat-1M."}, {"entity1": "Mitra et al.", "entity2": "SlimOrca", "relation": "Author-Role", "description": "Mitra et al. are the authors of SlimOrca."}], "f379da8a-93b3-4b45-8503-1ddee720be6c": [{"entity1": "Asai et al.", "entity2": "evaluation metric", "relation": "Methodology", "description": "Asai et al. is used as a reference for evaluating performance based on accuracy"}, {"entity1": "Mihaylov et al.", "entity2": "OpenbookQA", "relation": "Author-Role", "description": "Mihaylov et al. is the author of OpenbookQA"}, {"entity1": "OpenbookQA", "entity2": "Closed-Set Tasks", "relation": "Research Area", "description": "OpenbookQA is a dataset in the Closed-Set Tasks category"}, {"entity1": "Zhang et al.", "entity2": "PubHealth", "relation": "Author-Role", "description": "Zhang et al. is the author of PubHealth"}, {"entity1": "PubHealth", "entity2": "Closed-Set Tasks", "relation": "Research Area", "description": "PubHealth is a dataset in the Closed-Set Tasks category"}, {"entity1": "Clark et al.", "entity2": "ARC-Challenge", "relation": "Author-Role", "description": "Clark et al. is the author of ARC-Challenge"}, {"entity1": "ARC-Challenge", "entity2": "Closed-Set Tasks", "relation": "Research Area", "description": "ARC-Challenge is a dataset in the Closed-Set Tasks category"}, {"entity1": "Chan et al.", "entity2": "evaluation metric", "relation": "Methodology", "description": "Chan et al. is used as a reference for adopting a reading comprehension setup"}, {"entity1": "2WikiMultiHopQA", "entity2": "Multi-Hop Tasks", "relation": "Research Area", "description": "2WikiMultiHopQA is a dataset in the Multi-Hop Tasks category"}, {"entity1": "HotpotQA", "entity2": "Multi-Hop Tasks", "relation": "Research Area", "description": "HotpotQA is a dataset in the Multi-Hop Tasks category"}, {"entity1": "Musique", "entity2": "Multi-Hop Tasks", "relation": "Research Area", "description": "Musique is a dataset in the Multi-Hop Tasks category"}, {"entity1": "Chen et al.", "entity2": "CFQA", "relation": "Author-Role", "description": "Chen et al. is the author of CFQA"}, {"entity1": "CFQA", "entity2": "Domain-Specific Tasks", "relation": "Research Area", "description": "CFQA is a dataset in the Domain-Specific Tasks category"}, {"entity1": "Jin et al.", "entity2": "PubMedQA", "relation": "Author-Role", "description": "Jin et al. is the author of PubMedQA"}, {"entity1": "PubMedQA", "entity2": "Domain-Specific Tasks", "relation": "Research Area", "description": "PubMedQA is a dataset in the Domain-Specific Tasks category"}, {"entity1": "GPT-4o", "entity2": "RAG scenarios", "relation": "Tool/Resource", "description": "GPT-4o is used to categorize questions into RAG scenarios"}, {"entity1": "Table 9", "entity2": "RAG scenarios", "relation": "Publication Venue", "description": "Table 9 shows the final data volume for each RAG scenario subset"}, {"entity1": "Figure 6", "entity2": "Single-hop QA", "relation": "Publication Venue", "description": "Figure 6 shows the prompts used for categorizing Single-hop QA questions"}, {"entity1": "Figure 7", "entity2": "Multi-hop QA", "relation": "Publication Venue", "description": "Figure 7 shows the prompts used for categorizing Multi-hop QA questions"}, {"entity1": "Figure 8", "entity2": "RAG scenarios", "relation": "Publication Venue", "description": "Figure 8 describes one of the RAG paradigms"}, {"entity1": "Figure 9", "entity2": "RAG scenarios", "relation": "Publication Venue", "description": "Figure 9 describes one of the RAG paradigms"}, {"entity1": "Figure 10", "entity2": "RAG scenarios", "relation": "Publication Venue", "description": "Figure 10 describes one of the RAG paradigms"}, {"entity1": "Figure 11", "entity2": "RAG scenarios", "relation": "Publication Venue", "description": "Figure 11 describes one of the RAG paradigms"}], "2297d787-6fa0-49c3-b32a-3d5b56f54943": [{"entity1": "ARC", "entity2": "PQA", "relation": "Comparison", "description": "Both are single-hop QA tasks used to evaluate the method's performance with different retrieval sources."}, {"entity1": "Self-RAG", "entity2": "Llama2-7B", "relation": "Tool/Resource", "description": "Self-RAG utilizes Llama2-7B as its model."}, {"entity1": "DuckDuckGo", "entity2": "WIKI", "relation": "Comparison", "description": "Both are retrieval sources used in experiments to evaluate the method's performance."}, {"entity1": "RQ-RAG", "entity2": "Llama2-7B", "relation": "Tool/Resource", "description": "RQ-RAG utilizes Llama2-7B as its model."}, {"entity1": "RAG-Instruct", "entity2": "Llama2-7B", "relation": "Tool/Resource", "description": "RAG-Instruct utilizes Llama2-7B as its model."}, {"entity1": "Table 7", "entity2": "Performance comparison", "relation": "Publication Venue", "description": "Table 7 presents a performance comparison of different retrieval sources."}, {"entity1": "Knowledge-Intensive Data Selection Prompt", "entity2": "Dividing Prompt for Single-hop Question", "relation": "Methodology", "description": "Both are prompts used in the methodology for data selection and division."}, {"entity1": "Figure 5", "entity2": "Knowledge-Intensive Data Selection Prompt", "relation": "Publication Venue", "description": "Figure 5 illustrates the prompt for filtering knowledge-intensive instructions."}, {"entity1": "Figure 6", "entity2": "Dividing Prompt for Single-hop Question", "relation": "Publication Venue", "description": "Figure 6 illustrates the prompt for dividing single-hop question answering datasets."}, {"entity1": "BingSearch", "entity2": "DuckDuckGo", "relation": "Comparison", "description": "Both are retrieval sources used in experiments to evaluate the method's performance."}, {"entity1": "WQA", "entity2": "OBQA", "relation": "Comparison", "description": "Both are single-hop QA tasks used to evaluate the method's performance with different retrieval sources."}, {"entity1": "TQA", "entity2": "PQA", "relation": "Comparison", "description": "Both are single-hop QA tasks used to evaluate the method's performance with different retrieval sources."}, {"entity1": "Llama2-7B", "entity2": "RAG-Instruct", "relation": "Tool/Resource", "description": "RAG-Instruct is a model that utilizes Llama2-7B."}, {"entity1": "DuckDuckGo", "entity2": "WIKI", "relation": "Tool/Resource", "description": "Both are retrieval sources used with Llama2-7B models like Self-RAG and RQ-RAG."}], "530db349-48eb-40d4-bcb1-06cb62b01b35": [{"entity1": "RAG-Instruct", "entity2": "Robustness", "relation": "Methodology", "description": "RAG-Instruct demonstrates strong resilience to changes in retrieval sources, showing minimal performance fluctuations."}, {"entity1": "RAG-Instruct", "entity2": "Stability", "relation": "Methodology", "description": "RAG-Instruct maintains exceptional stability across diverse retrieval sources."}, {"entity1": "RAG-Instruct", "entity2": "Generalization capabilities", "relation": "Methodology", "description": "RAG-Instruct exhibits superior generalization capabilities compared to existing methods."}, {"entity1": "Self-RAG", "entity2": "Performance fluctuations", "relation": "Experiment-Outcome", "description": "Self-RAG shows notable performance drops when switching to Bing Search."}, {"entity1": "RQ-RAG", "entity2": "Performance fluctuations", "relation": "Experiment-Outcome", "description": "RQ-RAG experiences performance inconsistencies across different data sources."}, {"entity1": "RAG-Instruct", "entity2": "Bing Search", "relation": "Tool/Resource", "description": "RAG-Instruct uses Bing Search as a retrieval source."}, {"entity1": "RAG-Instruct", "entity2": "Duck-DuckGo", "relation": "Tool/Resource", "description": "RAG-Instruct uses Duck-DuckGo as a retrieval source."}, {"entity1": "RAG-Instruct", "entity2": "Wikipedia", "relation": "Tool/Resource", "description": "RAG-Instruct uses Wikipedia as a retrieval source."}, {"entity1": "RAG-Instruct", "entity2": "API", "relation": "Tool/Resource", "description": "RAG-Instruct uses the official API to obtain retrieval results."}, {"entity1": "Self-RAG", "entity2": "Wikipedia", "relation": "Tool/Resource", "description": "Self-RAG is primarily curated using Wikipedia."}, {"entity1": "RAG-Instruct", "entity2": "Multi-hop Question Answering", "relation": "Application", "description": "RAG-Instruct is applied to multi-hop question answering datasets."}, {"entity1": "RAG-Instruct", "entity2": "Figure 7", "relation": "Publication Venue", "description": "RAG-Instruct is presented in Figure 7, which shows the prompt for dividing the multi-hop question answering datasets into five RAG scenarios."}, {"entity1": "RAG-Instruct", "entity2": "Table 7", "relation": "Publication Venue", "description": "RAG-Instruct is presented in Table 7, which shows its strong resilience to changes in retrieval sources."}], "4577cd4a-3bb6-4f2d-815d-b26dca515b66": [{"entity1": "Useless Doc (r0)", "entity2": "Documents", "relation": "Dataset-Origin", "description": "Useless Doc (r0) originates from the provided Documents, but the Documents do not contain useful information for answering questions."}, {"entity1": "Single-Doc Support (r1)", "entity2": "Documents", "relation": "Dataset-Origin", "description": "Single-Doc Support (r1) originates from the provided Documents, which contain useful information or hints for answering questions."}, {"entity1": "q*", "entity2": "Documents", "relation": "Research Area", "description": "q* is related to the research area of the provided Documents, which can be used to generate questions and answers."}, {"entity1": "a*", "entity2": "Documents", "relation": "Research Area", "description": "a* is related to the research area of the provided Documents, which can be used to generate accurate and comprehensive answers."}, {"entity1": "RAG Paradigms", "entity2": "q*", "relation": "Methodology", "description": "RAG Paradigms are used as a methodology to generate q* and a* based on the provided Documents."}, {"entity1": "RAG Paradigms", "entity2": "a*", "relation": "Methodology", "description": "RAG Paradigms are used as a methodology to generate a* based on the provided Documents and q*."}, {"entity1": "Figure 8", "entity2": "Useless Doc (r0)", "relation": "Affiliation", "description": "Figure 8 is affiliated with Useless Doc (r0) as it provides a prompt for synthesizing Useless Doc (r0) data."}, {"entity1": "Document 1", "entity2": "Documents", "relation": "Affiliation", "description": "Document 1 is affiliated with the provided Documents as it is one of the documents listed."}], "ba60ccf4-7039-4298-a790-f15e7dc9c7ae": [{"entity1": "OBQA", "entity2": "ARC", "relation": "Comparison", "description": "Both are domain-specific datasets used for evaluation."}, {"entity1": "FEVER", "entity2": "Pub", "relation": "Publication Venue", "description": "FEVER is a dataset published in a venue referred to as 'Pub'."}, {"entity1": "Multi-hop", "entity2": "Open-ended", "relation": "Task Template", "description": "Multi-hop and Open-ended are task templates used in question answering."}, {"entity1": "CFQA", "entity2": "Domain-specific", "relation": "Research Area", "description": "CFQA is a dataset focused on domain-specific question answering."}, {"entity1": "PubMed", "entity2": "Wikipedia", "relation": "Dataset-Origin", "description": "PubMed and Wikipedia are sources of datasets used in question answering tasks."}, {"entity1": "Table 8", "entity2": "Prompt templates", "relation": "Tool/Resource", "description": "Table 8 provides information on prompt templates used in evaluation."}, {"entity1": "Table 9", "entity2": "RAG scenarios", "relation": "Research Funding-Grant", "description": "Table 9 categorizes dataset subsets into RAG scenarios, possibly funded by research grants."}, {"entity1": "HotpotQA", "entity2": "TriviaQA", "relation": "Comparison", "description": "HotpotQA and TriviaQA are compared in terms of their question answering tasks."}], "34f0fcec-47fa-4d26-9412-a34c34776edf": [{"entity1": "Multi-Doc Support", "entity2": "Documents", "relation": "Support", "description": "Multi-Doc Support is supported by multiple documents providing useful information or hints."}, {"entity1": "Single-Doc Answer", "entity2": "Documents", "relation": "Answer", "description": "Single-Doc Answer can be answered directly using the content of a single document."}, {"entity1": "RAG Paradigms", "entity2": "Multi-Doc Support", "relation": "Methodology", "description": "RAG Paradigms is used to generate question-answer pairs for Multi-Doc Support."}, {"entity1": "RAG Paradigms", "entity2": "Single-Doc Answer", "relation": "Methodology", "description": "RAG Paradigms is used to generate question-answer pairs for Single-Doc Answer."}, {"entity1": "q*", "entity2": "Documents", "relation": "Information Source", "description": "q* is generated based on the information provided in the documents."}, {"entity1": "a*", "entity2": "Documents", "relation": "Information Source", "description": "a* is generated based on the information provided in the documents."}, {"entity1": "Figure 10", "entity2": "Multi-Doc Support", "relation": "Illustration", "description": "Figure 10 illustrates the prompt for synthesizing Multi-Doc Support data."}, {"entity1": "Document 1", "entity2": "Documents", "relation": "Part Of", "description": "Document 1 is part of the documents."}, {"entity1": "Document 2", "entity2": "Documents", "relation": "Part Of", "description": "Document 2 is part of the documents."}, {"entity1": "r2", "entity2": "Multi-Doc Support", "relation": "Identifier", "description": "r2 is an identifier for Multi-Doc Support."}, {"entity1": "r3", "entity2": "Single-Doc Answer", "relation": "Identifier", "description": "r3 is an identifier for Single-Doc Answer."}], "64da7818-fdfa-4a69-b697-25d16f663ebd": [{"entity1": "Multi-Doc Answer", "entity2": "Documents", "relation": "Dataset-Origin", "description": "The Multi-Doc Answer dataset originates from the provided Documents."}, {"entity1": "RAG Paradigms", "entity2": "Multi-Doc Answer", "relation": "Methodology", "description": "RAG Paradigms are used as the methodology for generating the Multi-Doc Answer."}, {"entity1": "q*", "entity2": "a*", "relation": "Result", "description": "The question q* and the answer a* are generated as a result of following the RAG Paradigms."}, {"entity1": "Figure 12", "entity2": "RAG Paradigms", "relation": "Tool/Resource", "description": "Figure 12 is used as a tool or resource for understanding and implementing the RAG Paradigms."}, {"entity1": "JSON", "entity2": "Documents", "relation": "Format", "description": "The Documents are formatted in JSON."}], "5e7fd8ba-6524-490f-95da-d9d8c93ed55b": [{"entity1": "RAG-WM", "entity2": "Retrieval-Augmented Generation", "relation": "Methodology", "description": "RAG-WM is a black-box watermarking approach for Retrieval-Augmented Generation of Large Language Models."}, {"entity1": "RAG-WM", "entity2": "Intellectual Property", "relation": "Application", "description": "RAG-WM is used to detect Intellectual Property infringement of RAGs."}, {"entity1": "Peizhuo Lv", "entity2": "Institute of Information Engineering", "relation": "Affiliation", "description": "Peizhuo Lv is affiliated with the Institute of Information Engineering."}, {"entity1": "Mengjie Sun", "entity2": "Institute of Information Engineering", "relation": "Affiliation", "description": "Mengjie Sun is affiliated with the Institute of Information Engineering."}, {"entity1": "Hao Wang", "entity2": "School of Cyber Science and Technology", "relation": "Affiliation", "description": "Hao Wang is affiliated with the School of Cyber Science and Technology."}, {"entity1": "Xiaofeng Wang", "entity2": "Indiana University Bloomington", "relation": "Affiliation", "description": "Xiaofeng Wang is affiliated with Indiana University Bloomington."}, {"entity1": "Shengzhi Zhang", "entity2": "Department of Computer Science", "relation": "Affiliation", "description": "Shengzhi Zhang is affiliated with the Department of Computer Science."}, {"entity1": "Yuxuan Chen", "entity2": "School of Cyber Science and Technology", "relation": "Affiliation", "description": "Yuxuan Chen is affiliated with the School of Cyber Science and Technology."}, {"entity1": "Kai Chen", "entity2": "Institute of Information Engineering", "relation": "Affiliation", "description": "Kai Chen is affiliated with the Institute of Information Engineering."}, {"entity1": "Limin Sun", "entity2": "Institute of Information Engineering", "relation": "Affiliation", "description": "Limin Sun is affiliated with the Institute of Information Engineering."}, {"entity1": "RAG-WM", "entity2": "Watermark Generator", "relation": "Tool/Resource", "description": "RAG-WM uses a Watermark Generator to create watermark texts."}, {"entity1": "RAG-WM", "entity2": "Shadow LLM & RAG", "relation": "Tool/Resource", "description": "RAG-WM uses a Shadow LLM & RAG to inject watermark texts into the target RAG."}, {"entity1": "RAG-WM", "entity2": "Watermark Discriminator", "relation": "Tool/Resource", "description": "RAG-WM uses a Watermark Discriminator to detect IP infringement."}, {"entity1": "RAG-WM", "entity2": "Large Language Models", "relation": "Application", "description": "RAG-WM is used to detect IP infringement of Large Language Models."}, {"entity1": "Chinese Academy of Sciences", "entity2": "Institute of Information Engineering", "relation": "Institution-Collaboration", "description": "The Chinese Academy of Sciences collaborates with the Institute of Information Engineering."}, {"entity1": "Shandong University", "entity2": "School of Cyber Science and Technology", "relation": "Institution-Collaboration", "description": "Shandong University collaborates with the School of Cyber Science and Technology."}, {"entity1": "Indiana University Bloomington", "entity2": "Department of Computer Science", "relation": "Institution-Collaboration", "description": "Indiana University Bloomington collaborates with the Department of Computer Science."}, {"entity1": "Boston University", "entity2": "Metropolitan College", "relation": "Institution-Collaboration", "description": "Boston University collaborates with the Metropolitan College."}, {"entity1": "RAG-WM", "entity2": "Experimental results", "relation": "Result", "description": "RAG-WM shows promising results in detecting IP infringement of RAGs."}, {"entity1": "RAG-WM", "entity2": "Robustness", "relation": "Conclusion", "description": "RAG-WM is robust against paraphrasing, unrelated content removal, knowledge insertion, and knowledge expansion attacks."}], "eb0f895d-d880-4823-8bc4-cdfda2f83452": [{"entity1": "Microsoft", "entity2": "Azure OpenAI service", "relation": "Affiliation", "description": "Microsoft has incorporated RAG into its Azure OpenAI service."}, {"entity1": "Meta", "entity2": "Llama", "relation": "Affiliation", "description": "The Llama models developed by Meta support RAG integration in certain applications."}, {"entity1": "AnythingLLM AI application", "entity2": "Llama", "relation": "Tool/Resource", "description": "Users can implement a team-specific RAG knowledge base on a preferred model (e.g., Llama) using the AnythingLLM AI application."}, {"entity1": "RAG-WM", "entity2": "Digital watermarking", "relation": "Methodology", "description": "RAG-WM utilizes digital watermarking to protect the IP of RAG systems."}, {"entity1": "Large Language Models (LLMs)", "entity2": "Healthcare", "relation": "Application", "description": "LLMs are applied in healthcare."}, {"entity1": "Large Language Models (LLMs)", "entity2": "Content generation", "relation": "Application", "description": "LLMs are applied in content generation."}, {"entity1": "Large Language Models (LLMs)", "entity2": "Finance", "relation": "Application", "description": "LLMs are applied in finance."}, {"entity1": "Retrieval-Augmented Generation (RAG)", "entity2": "Knowledge base", "relation": "Component", "description": "RAG consists of a retriever model and a knowledge base."}, {"entity1": "RAG system", "entity2": "Intellectual Property (IP)", "relation": "Protection", "description": "IP protection of the RAG system is essential to protect the investment of the original RAG developers."}, {"entity1": "Digital watermarking", "entity2": "Relational databases", "relation": "Application", "description": "Digital watermarking has been demonstrated successful in relational databases."}, {"entity1": "Digital watermarking", "entity2": "DNN models", "relation": "Application", "description": "Digital watermarking has been demonstrated successful in DNN models."}, {"entity1": "CyC", "entity2": "DBpedia", "relation": "Comparison", "description": "The cost of building CyC and DBpedia is compared."}, {"entity1": "CyC", "entity2": "YAGO", "relation": "Comparison", "description": "The cost of building CyC and YAGO is compared."}, {"entity1": "RAG-WM", "entity2": "Watermark detection approaches", "relation": "Challenge", "description": "RAG-WM can evade watermark detection approaches."}, {"entity1": "RAG-WM", "entity2": "IP infringement", "relation": "Application", "description": "RAG-WM has a promising application in detecting IP infringement of RAG systems."}], "0c9cb2ee-3c85-4f2c-9460-e1422b9a1946": [{"entity1": "RAG-WM", "entity2": "RAG", "relation": "Methodology", "description": "RAG-WM is a novel black-box watermarking method for RAG systems."}, {"entity1": "RAG-WM", "entity2": "knowledge base", "relation": "Tool/Resource", "description": "RAG-WM embeds a knowledge watermark into the knowledge base."}, {"entity1": "owners", "entity2": "RAG", "relation": "Author-Role", "description": "Owners have to embed watermarks into the knowledge database to protect the IP of their RAGs."}, {"entity1": "WARD", "entity2": "RAG", "relation": "Tool/Resource", "description": "WARD was proposed to detect unauthorized usage of RAG using an LLM red-green list watermark."}, {"entity1": "attackers", "entity2": "RAG", "relation": "Adversaries", "description": "Attackers often deploy stolen RAGs with LLMs of their choice, making direct access to the outputs of the knowledge database impossible."}, {"entity1": "knowledge watermark", "entity2": "knowledge base", "relation": "Application", "description": "The knowledge watermark is embedded into the knowledge base to protect the IP of valuable RAGs."}, {"entity1": "LLMs", "entity2": "knowledge database", "relation": "Impact", "description": "LLMs might have destroyed the watermark embedded in the knowledge database."}, {"entity1": "RAG-WM", "entity2": "black-box access", "relation": "Methodology", "description": "RAG-WM is a black-box watermarking method that enables IP infringement detection."}, {"entity1": "entities", "entity2": "relationships", "relation": "Dataset-Origin", "description": "Entities and relationships are extracted from the knowledge base to generate tuples of watermark entities and corresponding relations."}, {"entity1": "keyed hash function", "entity2": "secret key", "relation": "Funding", "description": "The keyed hash function involves a secret key only known to the owner, thus enhancing the security of the watermark."}], "3675aca9-e805-4ae5-a92d-a9b6c4023eaf": [{"entity1": "RAG-WM", "entity2": "Watermark Generator", "relation": "Methodology", "description": "RAG-WM employs a Watermark Generator to produce watermark texts based on entity-relationship tuples."}, {"entity1": "RAG-WM", "entity2": "Shadow LLM&RAG", "relation": "Methodology", "description": "RAG-WM uses Shadow LLM&RAG in its multi-LLM interaction watermarking technique."}, {"entity1": "RAG-WM", "entity2": "Watermark Discriminator", "relation": "Methodology", "description": "RAG-WM utilizes a Watermark Discriminator in its multi-LLM interaction watermarking technique."}, {"entity1": "RAG-WM", "entity2": "binomial test", "relation": "Methodology", "description": "RAG-WM applies a binomial test to detect IP infringement."}, {"entity1": "RAG-WM", "entity2": "GPT-3.5-Turbo", "relation": "Evaluation", "description": "RAG-WM is evaluated on GPT-3.5-Turbo as one of the benchmark LLMs."}, {"entity1": "RAG-WM", "entity2": "PaLM 2", "relation": "Evaluation", "description": "RAG-WM is evaluated on PaLM 2 as one of the benchmark LLMs."}, {"entity1": "RAG-WM", "entity2": "Llama-2-7B", "relation": "Evaluation", "description": "RAG-WM is evaluated on Llama-2-7B as one of the benchmark LLMs."}, {"entity1": "RAG-WM", "entity2": "Vicuna-13B", "relation": "Evaluation", "description": "RAG-WM is evaluated on Vicuna-13B as one of the benchmark LLMs."}, {"entity1": "RAG-WM", "entity2": "NQ", "relation": "Evaluation", "description": "RAG-WM is evaluated on the NQ task."}, {"entity1": "RAG-WM", "entity2": "HotpotQA", "relation": "Evaluation", "description": "RAG-WM is evaluated on the HotpotQA task."}, {"entity1": "RAG-WM", "entity2": "MS-MARCO", "relation": "Evaluation", "description": "RAG-WM is evaluated on the MS-MARCO task."}, {"entity1": "RAG-WM", "entity2": "TREC-COVID", "relation": "Evaluation", "description": "RAG-WM is evaluated on the TREC-COVID task."}, {"entity1": "RAG-WM", "entity2": "NF-Corpus", "relation": "Evaluation", "description": "RAG-WM is evaluated on the NF-Corpus task."}, {"entity1": "RAG-WM", "entity2": "Paraphrasing", "relation": "Robustness", "description": "RAG-WM is robust against Paraphrasing attacks."}, {"entity1": "RAG-WM", "entity2": "Unrelated Content Removal", "relation": "Robustness", "description": "RAG-WM is robust against Unrelated Content Removal attacks."}, {"entity1": "RAG-WM", "entity2": "Knowledge Insertion", "relation": "Robustness", "description": "RAG-WM is robust against Knowledge Insertion attacks."}, {"entity1": "RAG-WM", "entity2": "Knowledge Expansion Attacks", "relation": "Robustness", "description": "RAG-WM is robust against Knowledge Expansion Attacks."}, {"entity1": "RAG-WM", "entity2": "Detection by Perplexity", "relation": "Stealthiness", "description": "RAG-WM is stealthy and not easily detectable by Detection by Perplexity methods."}, {"entity1": "RAG-WM", "entity2": "Duplicate Text Filtering", "relation": "Stealthiness", "description": "RAG-WM is stealthy and not easily detectable by Duplicate Text Filtering methods."}], "df869ea6-6ab4-42d5-9253-80e6ac394d87": [{"entity1": "RAG-WM", "entity2": "RAG system", "relation": "Methodology", "description": "RAG-WM is a novel method for RAG systems to generate high-quality watermark texts."}, {"entity1": "RAG-WM", "entity2": "Multi-LLM Interaction technique", "relation": "Methodology", "description": "RAG-WM uses the Multi-LLM Interaction technique to generate watermark texts."}, {"entity1": "RAG system", "entity2": "knowledge database", "relation": "Component", "description": "An RAG system comprises a knowledge database as one of its components."}, {"entity1": "RAG system", "entity2": "retriever", "relation": "Component", "description": "An RAG system comprises a retriever as one of its components."}, {"entity1": "RAG system", "entity2": "large language model (LLM)", "relation": "Component", "description": "An RAG system comprises a large language model (LLM) as one of its components."}, {"entity1": "Retrieval-Augmented Generation (RAG)", "entity2": "question-answering", "relation": "Application", "description": "RAG enhances large language models for knowledge-intensive tasks like question-answering."}, {"entity1": "Retrieval-Augmented Generation (RAG)", "entity2": "medical applications", "relation": "Application", "description": "RAG is used in medical applications."}, {"entity1": "Retrieval-Augmented Generation (RAG)", "entity2": "dialogue systems", "relation": "Application", "description": "RAG is used in dialogue systems."}, {"entity1": "retriever", "entity2": "Cosine Similarity", "relation": "Tool/Resource", "description": "The retriever applies a similarity metric such as Cosine Similarity."}, {"entity1": "retriever", "entity2": "Euclidean Distance", "relation": "Tool/Resource", "description": "The retriever applies a similarity metric such as Euclidean Distance."}, {"entity1": "RAG-WM", "entity2": "GitHub", "relation": "Publication Venue", "description": "The watermark implementation of RAG-WM is released on GitHub."}, {"entity1": "RAG system", "entity2": "\ud835\udc44", "relation": "Input", "description": "The RAG system takes a question \ud835\udc44 as input."}, {"entity1": "RAG system", "entity2": "\ud835\udc3e\ud835\udc37", "relation": "Input", "description": "The RAG system takes a knowledge database \ud835\udc3e\ud835\udc37 as input."}, {"entity1": "RAG system", "entity2": "\ud835\udc52", "relation": "Component", "description": "The RAG system uses a text encoder \ud835\udc52."}, {"entity1": "RAG system", "entity2": "\ud835\udc60\ud835\udc56\ud835\udc5a", "relation": "Methodology", "description": "The RAG system applies a similarity metric \ud835\udc60\ud835\udc56\ud835\udc5a."}, {"entity1": "RAG-WM", "entity2": "watermark texts", "relation": "Output", "description": "RAG-WM generates high-quality watermark texts."}, {"entity1": "RAG system", "entity2": "answer", "relation": "Output", "description": "The RAG system generates an answer."}, {"entity1": "RAG-WM", "entity2": "IP protection", "relation": "Objective", "description": "RAG-WM aims to protect the IP of RAGs."}, {"entity1": "RAG system", "entity2": "knowledge-intensive tasks", "relation": "Research Area", "description": "RAG systems are used for knowledge-intensive tasks."}], "87fd0b28-bc53-4dca-942f-d793759da016": [{"entity1": "RAG", "entity2": "LLM", "relation": "Tool/Resource", "description": "RAG uses LLM to generate answers"}, {"entity1": "RAG", "entity2": "black-box manner", "relation": "Methodology", "description": "RAG knowledge database is accessible in a black-box manner"}, {"entity1": "RAG", "entity2": "IP infringement", "relation": "Challenge", "description": "RAG owner can only detect IP infringement in a black-box manner"}, {"entity1": "Self-RAG", "entity2": "LLM", "relation": "Tool/Resource", "description": "Self-RAG trains an LLM to adaptively retrieve contexts"}, {"entity1": "CRAG", "entity2": "retrieval evaluator", "relation": "Methodology", "description": "CRAG introduces a lightweight retrieval evaluator to assess retrieved contexts"}, {"entity1": "FLARE", "entity2": "knowledge retrieval", "relation": "Methodology", "description": "FLARE predicts upcoming sentence to anticipate future content for knowledge retrieval"}, {"entity1": "IRCoT", "entity2": "chain-of-thought (CoT)", "relation": "Methodology", "description": "IRCoT integrates CoT with the retrieval process"}, {"entity1": "WARD", "entity2": "LLM watermarking", "relation": "Tool/Resource", "description": "WARD proposes a black-box RAG dataset inference method based on LLM watermarking"}, {"entity1": "WARD", "entity2": "red-green list watermark", "relation": "Methodology", "description": "WARD uses a red-green list watermark to paraphrase RAG texts"}, {"entity1": "RAG-WM", "entity2": "paraphrasing attacks", "relation": "Robustness", "description": "RAG-WM is robust against paraphrasing attacks"}, {"entity1": "RAG-WM", "entity2": "piracy attacks", "relation": "Robustness", "description": "RAG-WM is robust against piracy attacks"}, {"entity1": "RAG membership inference attacks", "entity2": "watermarking approaches", "relation": "Methodology", "description": "RAG membership inference attacks may be extended as watermarking approaches"}, {"entity1": "degree of similarity", "entity2": "LLM", "relation": "Methodology", "description": "degree of similarity is calculated by comparing two scores from LLM responses"}], "513a2cd6-5665-4762-9931-fba092410e5f": [{"entity1": "RAG", "entity2": "watermarking approaches", "relation": "Application", "description": "Watermarking approaches can be extended to detect IP infringement of RAG."}, {"entity1": "LLM", "entity2": "perplexity calculation", "relation": "Methodology", "description": "The LLM is queried with a target sample and the response is used for perplexity calculation."}, {"entity1": "gray-box attack", "entity2": "LLM", "relation": "Experiment-Outcome", "description": "The gray-box attack requires gray-box access to the LLM for perplexity calculation."}, {"entity1": "Relational Databases", "entity2": "watermarks", "relation": "Application", "description": "Watermarks are proposed for ownership protection and proving data integrity in relational databases."}, {"entity1": "white-box approaches", "entity2": "Relational Databases", "relation": "Methodology", "description": "Most current database watermarks are white-box approaches that require access to the suspicious databases' inner information."}, {"entity1": "Bit-resetting watermarks", "entity2": "Relational Databases", "relation": "Tool/Resource", "description": "Bit-resetting watermarks are a type of watermark that can be used in relational databases."}, {"entity1": "data statistic-modifying watermarks", "entity2": "Relational Databases", "relation": "Tool/Resource", "description": "Data statistic-modifying watermarks are a type of watermark that can be used in relational databases."}, {"entity1": "constrained data content-modifying watermarks", "entity2": "Relational Databases", "relation": "Tool/Resource", "description": "Constrained data content-modifying watermarks are a type of watermark that can be used in relational databases."}, {"entity1": "relational datasets", "entity2": "structured data", "relation": "Dataset-Origin", "description": "Relational datasets contain structured data."}, {"entity1": "RAG", "entity2": "knowledge base", "relation": "Dataset-Origin", "description": "The knowledge base of RAGs differs significantly from relational datasets."}, {"entity1": "watermarking approaches", "entity2": "IP infringement", "relation": "Application", "description": "Watermarking approaches can be used to detect IP infringement."}, {"entity1": "LLM", "entity2": "membership inference", "relation": "Experiment-Outcome", "description": "The LLM can be used for membership inference."}, {"entity1": "black-box watermarks", "entity2": "RAGs", "relation": "Application", "description": "Black-box watermarks can be used for RAGs."}, {"entity1": "hash function", "entity2": "watermarking process", "relation": "Methodology", "description": "A hash function can be used in the watermarking process."}, {"entity1": "primary key", "entity2": "watermarking process", "relation": "Methodology", "description": "The primary key can be used in the watermarking process."}, {"entity1": "secret key", "entity2": "watermarking process", "relation": "Methodology", "description": "A secret key can be used in the watermarking process."}], "7211bc6b-cd66-4c7f-b27b-8c564b0e64e3": [{"entity1": "relational datasets", "entity2": "RAGs", "relation": "Difference", "description": "Relational datasets contain structured data, which differs significantly from the knowledge base of RAGs."}, {"entity1": "RAGs", "entity2": "white-box watermarks", "relation": "Incompatibility", "description": "RAGs are accessible to the owner only in a black-box manner, making white-box watermarks inapplicable."}, {"entity1": "Text watermarking algorithms", "entity2": "format-based watermarks", "relation": "Methodology", "description": "Text watermarking algorithms propose format-based watermarks that change the text format to embed watermarks."}, {"entity1": "format-based watermarks", "entity2": "line/word shift", "relation": "Example", "description": "Format-based watermarks can be line/word shift, Unicode space characters, etc."}, {"entity1": "syntactic-based watermarks", "entity2": "syntax transformations", "relation": "Methodology", "description": "Syntactic-based watermarks introduce syntax transformations (e.g., Movement, Clefting, Passivization) to embed watermarks."}, {"entity1": "Generation-based watermarks", "entity2": "pre-trained language models", "relation": "Utilization", "description": "Generation-based watermarks utilize pre-trained language models to directly generate watermark texts from original texts and watermark messages."}, {"entity1": "KGWs", "entity2": "z-metric", "relation": "Methodology", "description": "KGWs utilize z-metric (based on z-test) to calculate the green token ratio for ownership verification."}, {"entity1": "watermarking methods", "entity2": "RAG systems", "relation": "Inapplicability", "description": "Watermarking methods cannot directly apply to Retrieval-Augmented Generation (RAG) systems due to post-processing by an adversary's deployed LLM."}, {"entity1": "paraphrasing attacks", "entity2": "watermarks", "relation": "Threat", "description": "Paraphrasing attacks pose significant threats to watermarks."}, {"entity1": "LLMs", "entity2": "watermark information", "relation": "Removal", "description": "Post-processing by an adversary's deployed LLM typically removes the watermark information, including format, syntax, or the red/green token table."}], "cdc17770-58de-4905-ad6a-a6cb8b4340e7": [{"entity1": "RAG system", "entity2": "watermark", "relation": "Tool/Resource", "description": "The RAG system can embed a watermark to detect IP infringement."}, {"entity1": "LLM", "entity2": "RAG system", "relation": "Comparison", "description": "The owner may suspect the LLM is using a stolen version of their RAG if it exhibits exceptional performance in a domain where the owner's RAG holds specialized knowledge."}, {"entity1": "insider attack", "entity2": "RAG system", "relation": "Threat", "description": "An attacker might steal the RAG through an insider attack."}, {"entity1": "intrusion", "entity2": "RAG system", "relation": "Threat", "description": "An attacker might steal the RAG through an intrusion, such as malware infection."}, {"entity1": "Paraphrasing Attack", "entity2": "watermark", "relation": "Attack", "description": "The Paraphrasing Attack can be used to perturb the watermark information and evade verification."}, {"entity1": "Unrelated Content Removal", "entity2": "watermark", "relation": "Attack", "description": "Unrelated Content Removal can be used to remove watermark information from the RAG's knowledge base."}, {"entity1": "Knowledge Insertion Attack", "entity2": "RAG's knowledge base", "relation": "Attack", "description": "Knowledge Insertion Attack involves inserting additional knowledge or misleading information into the RAG's knowledge base to undermine ownership verification."}, {"entity1": "RAG system", "entity2": "availability", "relation": "Objective", "description": "The RAG system aims to ensure its availability is not compromised."}, {"entity1": "owner", "entity2": "RAG system", "relation": "Author-Role", "description": "The owner of the RAG system can embed a watermark to detect IP infringement."}, {"entity1": "RAG system", "entity2": "IP infringement", "relation": "Challenge", "description": "The RAG system faces the challenge of IP infringement."}], "b26cc204-bb7e-4bda-abbb-c3c9209d35e0": [{"entity1": "RAG-WM", "entity2": "watermark", "relation": "Effectiveness", "description": "RAG-WM's effectiveness may be reduced by increasing the volume of non-watermarked information in the retrieved texts."}, {"entity1": "RAG-based systems", "entity2": "database insertion attack", "relation": "Comparison", "description": "The attack on RAG-based systems is similar to the traditional database insertion attack."}, {"entity1": "Knowledge Expansion Attack", "entity2": "RAG-WM", "relation": "Challenge", "description": "The Knowledge Expansion Attack can dilute the presence of the watermark in RAG-WM."}, {"entity1": "perplexity", "entity2": "text quality", "relation": "Measurement", "description": "Perplexity is used to measure the text's quality, with a large perplexity indicating low-quality text."}, {"entity1": "HMAC", "entity2": "Function", "relation": "Application", "description": "HMAC is used as a function for watermark information generation."}, {"entity1": "Multi-LLM Interaction Watermarking", "entity2": "WM-Gen", "relation": "Tool/Resource", "description": "Multi-LLM Interaction Watermarking uses WM-Gen for watermark information generation."}, {"entity1": "RAG", "entity2": "Knowledge Base", "relation": "Affiliation", "description": "RAG is affiliated with its Knowledge Base."}, {"entity1": "Harry Potter", "entity2": "Pirates of the Caribbean", "relation": "Inclusive Relationship", "description": "Harry Potter and Pirates of the Caribbean have an inclusive relationship, intertwining adventure spirit and fantasy elements."}, {"entity1": "Shadow-LLM&RAG", "entity2": "RAG-WM", "relation": "Comparison", "description": "Shadow-LLM&RAG is compared to RAG-WM in terms of watermark detection."}, {"entity1": "Duplicate Text Filtering", "entity2": "watermark content retrieval", "relation": "Challenge", "description": "Duplicate Text Filtering can bypass watermark verification, challenging the success rate of watermark content retrieval."}, {"entity1": "RAG Watermarks", "entity2": "effectiveness", "relation": "Requirement", "description": "An ideal RAG watermarking solution should achieve effectiveness, ensuring watermark information remains intact."}, {"entity1": "RAG Watermarks", "entity2": "robustness", "relation": "Requirement", "description": "An ideal RAG watermarking solution should achieve robustness, allowing watermarks to be detected even after manipulation."}, {"entity1": "RAG Watermarks", "entity2": "security", "relation": "Requirement", "description": "An ideal RAG watermarking solution should achieve security, making it difficult for attackers to forge new watermarks."}, {"entity1": "RAG Watermarks", "entity2": "integrity", "relation": "Requirement", "description": "An ideal RAG watermarking solution should achieve integrity, making it unlikely for owners to detect IP infringement over innocent RAGs."}, {"entity1": "RAG Watermarks", "entity2": "stealthiness", "relation": "Requirement", "description": "An ideal RAG watermarking solution should achieve stealthiness, making it difficult for attackers to detect the watermark."}], "d283118a-6e4c-41ab-a75f-4137a81f49c5": [{"entity1": "RAG-WM", "entity2": "watermark texts", "relation": "Application", "description": "RAG-WM uses watermark texts to embed watermark knowledge"}, {"entity1": "\ud835\udc45\ud835\udc64\ud835\udc5a", "entity2": "\ud835\udc52\ud835\udc57 \ud835\udc64\ud835\udc5a", "relation": "Dataset-Origin", "description": "\ud835\udc45\ud835\udc64\ud835\udc5a and \ud835\udc52\ud835\udc57 \ud835\udc64\ud835\udc5a are related as watermark entities and relations"}, {"entity1": "\ud835\udc5f\ud835\udc56,\ud835\udc57 \ud835\udc64\ud835\udc5a", "entity2": "\ud835\udc38\ud835\udc64\ud835\udc5a", "relation": "Dataset-Origin", "description": "\ud835\udc5f\ud835\udc56,\ud835\udc57 \ud835\udc64\ud835\udc5a and \ud835\udc38\ud835\udc64\ud835\udc5a are related as watermark relations and entities"}, {"entity1": "LLM", "entity2": "watermark knowledge", "relation": "Tool/Resource", "description": "LLM is used to generate watermark texts embedded with watermark knowledge"}, {"entity1": "owners", "entity2": "IP infringement", "relation": "Challenge", "description": "owners face the challenge of detecting IP infringement"}, {"entity1": "attackers", "entity2": "RAG", "relation": "Adversarial", "description": "attackers try to forge a new watermark for the stolen RAG"}, {"entity1": "HMAC", "entity2": "\ud835\udc38\ud835\udc64\ud835\udc5a", "relation": "Methodology", "description": "HMAC is used to generate watermark entities and relations"}, {"entity1": "WM-Gen", "entity2": "Shadow-LLM&RAG", "relation": "Collaboration-Type", "description": "WM-Gen and Shadow-LLM&RAG collaborate to generate watermark texts"}, {"entity1": "WM-Disc", "entity2": "watermark texts", "relation": "Tool/Resource", "description": "WM-Disc is used to discriminate watermark texts"}, {"entity1": "RAG", "entity2": "watermark knowledge", "relation": "Application", "description": "RAG is used to embed watermark knowledge"}, {"entity1": "Figure 1", "entity2": "RAG-WM", "relation": "Publication Venue", "description": "Figure 1 illustrates the workflow of RAG-WM"}], "9167c777-baea-4cdc-bc55-9cd9a656ac11": [{"entity1": "knowledge base", "entity2": "retriever", "relation": "Component", "description": "The knowledge base and retriever are components of the RAG system."}, {"entity1": "knowledge base", "entity2": "LLM", "relation": "Component", "description": "The knowledge base and LLM are components of the RAG system."}, {"entity1": "knowledge base", "entity2": "watermark", "relation": "Injection Target", "description": "The watermark is injected into the knowledge base."}, {"entity1": "LLM", "entity2": "retriever", "relation": "Component", "description": "The LLM and retriever are components of the RAG system."}, {"entity1": "watermark", "entity2": "entities", "relation": "Composition", "description": "The watermark is composed of entities and relations."}, {"entity1": "watermark", "entity2": "relations", "relation": "Composition", "description": "The watermark is composed of entities and relations."}, {"entity1": "entities", "entity2": "relations", "relation": "Extraction Target", "description": "Entities and relations are extracted from the knowledge base."}, {"entity1": "LLM Graph Transformer", "entity2": "entities", "relation": "Extraction Tool", "description": "The LLM Graph Transformer is used to extract entities from the knowledge base."}, {"entity1": "LLM Graph Transformer", "entity2": "relations", "relation": "Extraction Tool", "description": "The LLM Graph Transformer is used to extract relations from the knowledge base."}, {"entity1": "ParseER(\u00b7)", "entity2": "entities", "relation": "Extraction Process", "description": "ParseER(\u00b7) is the process of parsing entities from the sampled documents."}, {"entity1": "ParseER(\u00b7)", "entity2": "relations", "relation": "Extraction Process", "description": "ParseER(\u00b7) is the process of parsing relations from the sampled documents."}, {"entity1": "Sample(\ud835\udc3e\ud835\udc37,\ud835\udc51 )", "entity2": "entities", "relation": "Extraction Source", "description": "The sample of documents from the knowledge base is the source for extracting entities."}, {"entity1": "Sample(\ud835\udc3e\ud835\udc37,\ud835\udc51 )", "entity2": "relations", "relation": "Extraction Source", "description": "The sample of documents from the knowledge base is the source for extracting relations."}, {"entity1": "\ud835\udc3e\ud835\udc37", "entity2": "entities", "relation": "Containment", "description": "The knowledge base \ud835\udc3e\ud835\udc37 contains entities."}, {"entity1": "\ud835\udc3e\ud835\udc37", "entity2": "relations", "relation": "Containment", "description": "The knowledge base \ud835\udc3e\ud835\udc37 contains relations."}, {"entity1": "\ud835\udc38", "entity2": "entities", "relation": "Representation", "description": "\ud835\udc38 represents the list of entities extracted from the knowledge base."}, {"entity1": "\ud835\udc45", "entity2": "relations", "relation": "Representation", "description": "\ud835\udc45 represents the list of relations extracted from the knowledge base."}, {"entity1": "RAG system", "entity2": "knowledge base", "relation": "Component", "description": "The knowledge base is a component of the RAG system."}, {"entity1": "RAG system", "entity2": "retriever", "relation": "Component", "description": "The retriever is a component of the RAG system."}, {"entity1": "RAG system", "entity2": "LLM", "relation": "Component", "description": "The LLM is a component of the RAG system."}], "a0f2f93c-8acc-4cde-afdc-e688ffcdc783": [{"entity1": "Sample(\ud835\udc3e\ud835\udc37,\ud835\udc51 )", "entity2": "ParseER(\u00b7)", "relation": "Methodology", "description": "ParseER(\u00b7) is used to parse entities and relations from the sampled documents in Sample(\ud835\udc3e\ud835\udc37,\ud835\udc51 )."}, {"entity1": "LLM", "entity2": "ParseER(\u00b7)", "relation": "Tool/Resource", "description": "LLM is used in the process of ParseER(\u00b7) for parsing entities and relations."}, {"entity1": "\ud835\udc3e\ud835\udc37", "entity2": "Sample(\ud835\udc3e\ud835\udc37,\ud835\udc51 )", "relation": "Affiliation", "description": "Sample(\ud835\udc3e\ud835\udc37,\ud835\udc51 ) is a random sample of documents from the knowledge base \ud835\udc3e\ud835\udc37."}, {"entity1": "\ud835\udc38", "entity2": "\ud835\udc45", "relation": "Co-author", "description": "\ud835\udc38 and \ud835\udc45 are generated together through the ParseER(\u00b7) process."}, {"entity1": "\ud835\udc38", "entity2": "LLM", "relation": "Dataset-Origin", "description": "The entity list \ud835\udc38 is generated using the LLM."}, {"entity1": "\ud835\udc45", "entity2": "LLM", "relation": "Dataset-Origin", "description": "The relations list \ud835\udc45 is generated using the LLM."}, {"entity1": "\ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a", "entity2": "\ud835\udc5f\ud835\udc56,\ud835\udc57\ud835\udc64\ud835\udc5a", "relation": "Experiment-Outcome", "description": "The watermark entity \ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a is related to the relation \ud835\udc5f\ud835\udc56,\ud835\udc57\ud835\udc64\ud835\udc5a through the HMAC process."}, {"entity1": "HMAC", "entity2": "\ud835\udc58\ud835\udc52\ud835\udc66", "relation": "Methodology", "description": "HMAC uses the secret key \ud835\udc58\ud835\udc52\ud835\udc66 to generate watermark tuples."}, {"entity1": "\ud835\udc520\ud835\udc64\ud835\udc5a", "entity2": "\ud835\udc52\ud835\udc56+1\ud835\udc64\ud835\udc5a", "relation": "Author-Role", "description": "\ud835\udc520\ud835\udc64\ud835\udc5a is the initial entity, and subsequent entities like \ud835\udc52\ud835\udc56+1\ud835\udc64\ud835\udc5a are generated based on it."}, {"entity1": "\ud835\udc38\ud835\udc64\ud835\udc5a", "entity2": "\ud835\udc45\ud835\udc64\ud835\udc5a", "relation": "Collaboration-Type", "description": "\ud835\udc38\ud835\udc64\ud835\udc5a and \ud835\udc45\ud835\udc64\ud835\udc5a are used together to establish relations between watermark entities."}, {"entity1": "\ud835\udc5d1", "entity2": "\ud835\udc5f\ud835\udc56,\ud835\udc57\ud835\udc64\ud835\udc5a", "relation": "Experiment-Design", "description": "The probability \ud835\udc5d1 determines the existence of a relation like \ud835\udc5f\ud835\udc56,\ud835\udc57\ud835\udc64\ud835\udc5a between entities."}], "bbe54066-8c68-4c27-a81b-9b9911fe3c12": [{"entity1": "HMAC", "entity2": "watermarking", "relation": "Tool/Resource", "description": "HMAC is used as a tool for watermarking and protecting intellectual property."}, {"entity1": "DNN models", "entity2": "watermarking", "relation": "Application", "description": "Watermarking is applied to DNN models for protection."}, {"entity1": "Multi-LLM interaction watermarking", "entity2": "WM-Gen", "relation": "Component", "description": "WM-Gen is a component of the Multi-LLM interaction watermarking approach."}, {"entity1": "Multi-LLM interaction watermarking", "entity2": "Shadow-LLM&RAG", "relation": "Component", "description": "Shadow-LLM&RAG is a component of the Multi-LLM interaction watermarking approach."}, {"entity1": "Multi-LLM interaction watermarking", "entity2": "WM-Disc", "relation": "Component", "description": "WM-Disc is a component of the Multi-LLM interaction watermarking approach."}, {"entity1": "WM-Gen", "entity2": "RAG systems", "relation": "Interaction", "description": "WM-Gen generates watermark texts and stores them into the RAG of Shadow-LLM&RAG system."}, {"entity1": "WM-Disc", "entity2": "Shadow-LLM&RAG", "relation": "Interaction", "description": "WM-Disc queries the Shadow-LLM&RAG system using the watermark verification question \ud835\udc4a\ud835\udc44."}, {"entity1": "\ud835\udc4a\ud835\udc44", "entity2": "WM-Disc", "relation": "Tool/Resource", "description": "\ud835\udc4a\ud835\udc44 is used by WM-Disc for watermark verification."}, {"entity1": "\ud835\udc5f\ud835\udc56,\ud835\udc57", "entity2": "\ud835\udc64\ud835\udc5a", "relation": "Relation", "description": "\ud835\udc5f\ud835\udc56,\ud835\udc57 represents the relation between \ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a and \ud835\udc52\ud835\udc57 \ud835\udc64\ud835\udc5a."}, {"entity1": "IP infringement detection", "entity2": "watermarking", "relation": "Application", "description": "Watermarking is used for IP infringement detection."}, {"entity1": "RAG systems", "entity2": "LLM", "relation": "Component", "description": "RAG systems are related to LLM in the context of watermarking and IP protection."}, {"entity1": "HMAC", "entity2": "intellectual property", "relation": "Protection", "description": "HMAC is used for protecting intellectual property."}], "74c6c121-d9fe-454d-9b6a-12b9fe714ae4": [{"entity1": "WM-Gen", "entity2": "RAG", "relation": "Tool/Resource", "description": "WM-Gen generates watermark text and inserts it into RAG to create a watermarked RAG."}, {"entity1": "WM-Disc", "entity2": "Shadow-LLM", "relation": "Tool/Resource", "description": "WM-Disc queries the Shadow-LLM to improve the quality of the watermark text."}, {"entity1": "Shadow-LLM", "entity2": "RAG", "relation": "Tool/Resource", "description": "Shadow-LLM is used with RAG to simulate the scenario where adversaries deploy the stolen RAG with their LLM."}, {"entity1": "WM-Gen", "entity2": "LLM", "relation": "Tool/Resource", "description": "WM-Gen uses an LLM to generate watermark texts."}, {"entity1": "WM-Disc", "entity2": "WM-Gen", "relation": "Collaboration-Type", "description": "WM-Disc provides feedback to WM-Gen to retry watermark generation until successful embedding of the information."}, {"entity1": "Equation 6", "entity2": "Shadow-LLM", "relation": "Mathematical Model", "description": "Equation 6 defines the retrieval and answer generation process using Shadow-LLM and RAG."}, {"entity1": "Equation 7", "entity2": "WM-Disc", "relation": "Mathematical Model", "description": "Equation 7 defines the discrimination function used by WM-Disc to analyze the processed answers."}, {"entity1": "Equation 8", "entity2": "WM-Gen", "relation": "Mathematical Model", "description": "Equation 8 defines the process of generating watermark text using WM-Gen and LLM."}, {"entity1": "\ud835\udc5f\ud835\udc56,\ud835\udc57", "entity2": "\ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a", "relation": "Author-Expertise", "description": "\ud835\udc5f\ud835\udc56,\ud835\udc57 is related to \ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a as part of the watermark entity-relation pairs."}, {"entity1": "\ud835\udc52\ud835\udc57", "entity2": "\ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a", "relation": "Author-Expertise", "description": "\ud835\udc52\ud835\udc57 is related to \ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a as part of the watermark entity-relation pairs."}, {"entity1": "RAG", "entity2": "LLM", "relation": "Tool/Resource", "description": "RAG is used with LLM to generate processed answers."}, {"entity1": "WM-Gen", "entity2": "\ud835\udc38\ud835\udc64\ud835\udc5a", "relation": "Research Area", "description": "WM-Gen generates watermark texts based on the set of watermark entity-relation pairs {\ud835\udc38\ud835\udc64\ud835\udc5a,\ud835\udc45\ud835\udc64\ud835\udc5a}."}, {"entity1": "WM-Disc", "entity2": "\ud835\udc4a\ud835\udc44", "relation": "Tool/Resource", "description": "WM-Disc uses a set of watermark verification questions \ud835\udc4a\ud835\udc44 to improve the quality of the watermark text."}], "be0aa8d2-efa3-4297-bb68-08f32a122d76": [{"entity1": "RAG", "entity2": "Watermark Text", "relation": "Watermark Injection", "description": "The watermark text is injected into the original RAG to generate the watermarked RAGwm."}, {"entity1": "Watermark Query", "entity2": "RAG", "relation": "Retrieval", "description": "The watermark query is used to retrieve the most relevant text from the original RAG."}, {"entity1": "WM-Gen", "entity2": "Watermark Text", "relation": "Generation", "description": "WM-Gen generates the watermark text based on the input parameters."}, {"entity1": "WM-Gen", "entity2": "Retrieved Text", "relation": "Concatenation", "description": "WM-Gen performs a text concatenation operation to entangle the watermark text with the retrieved text."}, {"entity1": "LLM", "entity2": "Concatenated Text", "relation": "Semantic Coherence Evaluation", "description": "The LLM evaluates the semantic coherence of the concatenated text to ensure its quality."}, {"entity1": "RAGwm", "entity2": "Watermark Text", "relation": "Embedding", "description": "The watermark text is embedded into the RAG to generate the watermarked RAGwm."}, {"entity1": "WM-Disc", "entity2": "RAGwm", "relation": "IP Infringement Detection", "description": "WM-Disc conducts IP infringement detection by executing the watermark discrimination operation on the watermarked RAGwm."}, {"entity1": "IP Infringement Detection", "entity2": "LLM", "relation": "Suspicion", "description": "The owner may suspect the LLM of using a stolen version of RAGwm if it demonstrates exceptional performance in a domain where the owner's RAG contains specialized knowledge."}, {"entity1": "Watermark Query", "entity2": "Watermark Text", "relation": "Generation", "description": "The watermark query is generated based on the input parameters to retrieve the watermark text."}, {"entity1": "RAG", "entity2": "Knowledge Base", "relation": "Retrieval", "description": "RAG retrieves top k text highly relevant to the question from the knowledge base."}], "45e5727f-5e98-4fe5-96f5-81f6a56628df": [{"entity1": "Null Hypothesis \ud835\udc3b0", "entity2": "Alternative Hypothesis \ud835\udc3b1", "relation": "Hypothesis Comparison", "description": "Null Hypothesis \ud835\udc3b0 and Alternative Hypothesis \ud835\udc3b1 are compared to determine if the suspicious LLM is equipped with the watermarked RAG."}, {"entity1": "Binomial Test", "entity2": "Null Hypothesis \ud835\udc3b0", "relation": "Methodology", "description": "The Binomial Test is used to verify the watermark and test the Null Hypothesis \ud835\udc3b0."}, {"entity1": "Binomial Test", "entity2": "Alternative Hypothesis \ud835\udc3b1", "relation": "Methodology", "description": "The Binomial Test is used to verify the watermark and test the Alternative Hypothesis \ud835\udc3b1."}, {"entity1": "\ud835\udc5d-value", "entity2": "Binomial Test", "relation": "Result", "description": "The calculated p-value from the binomial test is used to determine the significance of the results."}, {"entity1": "\ud835\udefc", "entity2": "\ud835\udc5d-value", "relation": "Comparison", "description": "The p-value is compared to the common significance level \ud835\udefc = 0.05 to determine the significance of the results."}, {"entity1": "LLM", "entity2": "RAG", "relation": "Tool/Resource", "description": "The LLM is equipped with the watermarked RAG."}, {"entity1": "\ud835\udc5f\ud835\udc56,\ud835\udc57", "entity2": "\ud835\udc64\ud835\udc5a", "relation": "Relation", "description": "The relation \ud835\udc5f\ud835\udc56,\ud835\udc57 is associated with the watermark \ud835\udc64\ud835\udc5a."}, {"entity1": "\ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a", "entity2": "\ud835\udc5f\ud835\udc56,\ud835\udc57", "relation": "Relation", "description": "The entity \ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a is associated with the relation \ud835\udc5f\ud835\udc56,\ud835\udc57."}, {"entity1": "\ud835\udc4a\ud835\udc44", "entity2": "Equation (6)", "relation": "Methodology", "description": "The watermark queries \ud835\udc4a\ud835\udc44 are generated using Equation (6)."}, {"entity1": "\ud835\udc4a\ud835\udc44", "entity2": "Equation (7)", "relation": "Methodology", "description": "The watermark queries \ud835\udc4a\ud835\udc44 are executed using Equation (7)."}], "8a74c3cd-fa8d-4420-99af-af8113fca296": [{"entity1": "RAG", "entity2": "LLM", "relation": "Deployment", "description": "The suspect LLM is deployed with our watermark RAG"}, {"entity1": "TREC-COVID", "entity2": "NFCorpus", "relation": "Comparison", "description": "Both are mainstream knowledge bases with numerous relations"}, {"entity1": "NQ", "entity2": "TREC-COVID", "relation": "Dataset-Origin", "description": "NQ is one of the benchmark datasets used in RAG for question-answering tasks, similar to TREC-COVID"}, {"entity1": "RAG-WM", "entity2": "RAG", "relation": "Methodology", "description": "RAG-WM is a watermarking approach for RAG"}, {"entity1": "GPT-3.5-Turbo", "entity2": "PaLM 2", "relation": "Comparison", "description": "Both are mainstream and representative large language models used in the experiment"}, {"entity1": "Llama-2-7B", "entity2": "GPT-3.5-Turbo", "relation": "Tool/Resource", "description": "Llama-2-7B is another large language model used in the experiment, similar to GPT-3.5-Turbo"}, {"entity1": "Self-RAG", "entity2": "CRAG", "relation": "Innovation", "description": "Both are advanced RAG systems that the watermark should effectively protect"}, {"entity1": "RAG", "entity2": "Watermark", "relation": "Application", "description": "The watermark is applied to RAG to protect its intellectual property"}, {"entity1": "MS-MARCO", "entity2": "HotpotQA", "relation": "Dataset-Origin", "description": "Both are benchmark datasets used in RAG for question-answering tasks"}, {"entity1": "NFCorpus", "entity2": "TREC-COVID", "relation": "Research Area", "description": "Both datasets are used in research areas related to question-answering tasks"}, {"entity1": "RAG-WM", "entity2": "Black-box", "relation": "Methodology", "description": "RAG-WM is evaluated in a black-box manner"}, {"entity1": "\ud835\udc5d0", "entity2": "\ud835\udc5b\ud835\udc5f", "relation": "Mathematical Model", "description": "\ud835\udc5d0 is related to \ud835\udc5b\ud835\udc5f in the mathematical model of the watermarking approach"}, {"entity1": "\ud835\udc50\ud835\udc64\ud835\udc5a", "entity2": "\ud835\udefc", "relation": "Mathematical Model", "description": "\ud835\udc50\ud835\udc64\ud835\udc5a is related to \ud835\udefc in the mathematical model of the watermarking approach"}, {"entity1": "RAG", "entity2": "LLM", "relation": "Collaboration-Type", "description": "RAG and LLM collaborate in the watermarking approach"}], "7081482c-0b24-438b-923a-e313411d8807": [{"entity1": "GPT-3.5-Turbo", "entity2": "RAG-WM", "relation": "Evaluation", "description": "GPT-3.5-Turbo is used to evaluate the effectiveness of RAG-WM."}, {"entity1": "PaLM 2", "entity2": "RAG-WM", "relation": "Evaluation", "description": "PaLM 2 is used to evaluate the effectiveness of RAG-WM."}, {"entity1": "Llama-2-7B", "entity2": "RAG-WM", "relation": "Evaluation", "description": "Llama-2-7B is used to evaluate the effectiveness of RAG-WM."}, {"entity1": "Vicuna-13B", "entity2": "RAG-WM", "relation": "Evaluation", "description": "Vicuna-13B is used to evaluate the effectiveness of RAG-WM."}, {"entity1": "Contriever", "entity2": "RAG Systems", "relation": "Component", "description": "Contriever is a retriever model used in RAG Systems."}, {"entity1": "Contriever-ms", "entity2": "RAG Systems", "relation": "Component", "description": "Contriever-ms is a retriever model used in RAG Systems."}, {"entity1": "ANCE", "entity2": "RAG Systems", "relation": "Component", "description": "ANCE is a retriever model used in RAG Systems."}, {"entity1": "Euclidean distance", "entity2": "RAG Systems", "relation": "Distance Metric", "description": "Euclidean distance is used as a distance metric in RAG Systems."}, {"entity1": "Inner Product", "entity2": "RAG Systems", "relation": "Distance Metric", "description": "Inner Product is used as a distance metric in RAG Systems."}, {"entity1": "Cosine similarity", "entity2": "RAG Systems", "relation": "Distance Metric", "description": "Cosine similarity is used as a distance metric in RAG Systems."}, {"entity1": "Chroma", "entity2": "RAG Systems", "relation": "Database", "description": "Chroma is used as an embedding database in RAG Systems."}, {"entity1": "TREC", "entity2": "RAG-WM", "relation": "Evaluation Dataset", "description": "TREC is used as a dataset to evaluate RAG-WM."}, {"entity1": "COVID", "entity2": "RAG-WM", "relation": "Evaluation Dataset", "description": "COVID is used as a dataset to evaluate RAG-WM."}, {"entity1": "NFCorpus", "entity2": "RAG-WM", "relation": "Evaluation Dataset", "description": "NFCorpus is used as a dataset to evaluate RAG-WM."}, {"entity1": "NQ", "entity2": "RAG-WM", "relation": "Evaluation Dataset", "description": "NQ is used as a dataset to evaluate RAG-WM."}, {"entity1": "HotpotQA", "entity2": "RAG-WM", "relation": "Evaluation Dataset", "description": "HotpotQA is used as a dataset to evaluate RAG-WM."}, {"entity1": "MS-MARCO", "entity2": "RAG-WM", "relation": "Evaluation Dataset", "description": "MS-MARCO is used as a dataset to evaluate RAG-WM."}, {"entity1": "Watermark Information Retrieval Ratio (WIRR)", "entity2": "RAG-WM", "relation": "Evaluation Metric", "description": "WIRR is used to evaluate the performance of RAG-WM."}, {"entity1": "Watermark Success Number (WSN)", "entity2": "RAG-WM", "relation": "Evaluation Metric", "description": "WSN is used to evaluate the performance of RAG-WM."}, {"entity1": "Clean Data Performance Alignment (CDPA)", "entity2": "RAG-WM", "relation": "Evaluation Metric", "description": "CDPA is used to evaluate the performance of RAG-WM."}, {"entity1": "Clean Information Retrieval Alignment (CIRA)", "entity2": "RAG-WM", "relation": "Evaluation Metric", "description": "CIRA is used to evaluate the performance of RAG-WM."}, {"entity1": "RAG3", "entity2": "IP infringement detection", "relation": "Application", "description": "RAG3 is used for IP infringement detection."}], "3a989ff0-15da-4d29-9c53-ffdde506e79c": [{"entity1": "RAG-WM", "entity2": "watermark verification", "relation": "Effectiveness", "description": "RAG-WM is evaluated for its effectiveness in terms of watermark verification"}, {"entity1": "RAG-WM", "entity2": "main-task performance", "relation": "Effectiveness", "description": "RAG-WM is evaluated for its effectiveness in terms of main-task performance"}, {"entity1": "RAG-WM", "entity2": "integrity", "relation": "Effectiveness", "description": "RAG-WM is evaluated for its effectiveness in terms of integrity"}, {"entity1": "RAG-WM", "entity2": "time consumption", "relation": "Effectiveness", "description": "RAG-WM is evaluated for its effectiveness in terms of time consumption"}, {"entity1": "RAG-WM", "entity2": "human evaluation", "relation": "Effectiveness", "description": "RAG-WM is evaluated for its effectiveness in terms of human evaluation"}, {"entity1": "GPT-3.5-Turbo", "entity2": "WSN", "relation": "Measurement", "description": "GPT-3.5-Turbo is used to measure WSN"}, {"entity1": "GPT-3.5-Turbo", "entity2": "CDPA", "relation": "Measurement", "description": "GPT-3.5-Turbo is used to measure CDPA"}, {"entity1": "human evaluation", "entity2": "LLM-based evaluation", "relation": "Comparison", "description": "Human evaluation is compared to LLM-based evaluation, showing similar performance results"}, {"entity1": "Ubuntu 20.04 LTS", "entity2": "Intel(R) Xeon(R) Silver 4214 CPU", "relation": "Platform", "description": "Ubuntu 20.04 LTS is used as the operating system on a server with Intel(R) Xeon(R) Silver 4214 CPU"}, {"entity1": "Tesla V100 GPUs", "entity2": "Server", "relation": "Hardware", "description": "Tesla V100 GPUs are used on the server"}, {"entity1": "watermark queries", "entity2": "watermark success number", "relation": "Calculation", "description": "Watermark queries are used to calculate the watermark success number"}, {"entity1": "p-value", "entity2": "significance level", "relation": "Comparison", "description": "The p-value is compared to the significance level, indicating 100% verification success"}, {"entity1": "RAG-WM", "entity2": "CIRA", "relation": "Evaluation Metric", "description": "CIRA is used to evaluate the effectiveness of RAG-WM"}, {"entity1": "\ud835\udc50\ud835\udc64\ud835\udc5a", "entity2": "equation: \ud835\udc50\ud835\udc64\ud835\udc5a = f(\ud835\udc5b)", "relation": "Calculation", "description": "The watermark success number (\ud835\udc50\ud835\udc64\ud835\udc5a) is calculated using the equation \ud835\udc50\ud835\udc64\ud835\udc5a = f(\ud835\udc5b)"}, {"entity1": "p-value", "entity2": "equation: p-value < \ud835\udefc", "relation": "Calculation", "description": "The p-value is calculated using the equation p-value < \ud835\udefc"}], "228f390e-ad05-4c06-8e31-f322c073c4d2": [{"entity1": "RAG-WM", "entity2": "watermark texts", "relation": "Methodology", "description": "RAG-WM injects watermark texts into knowledge bases for watermark verification."}, {"entity1": "RAG-WM", "entity2": "knowledge bases", "relation": "Affiliation", "description": "RAG-WM is associated with knowledge bases such as TREC-COVID, NFCorpus, NQ, HotpotQA, and MS-MARCO."}, {"entity1": "watermark texts", "entity2": "knowledge bases", "relation": "Dataset-Origin", "description": "Watermark texts are inserted into knowledge bases."}, {"entity1": "RAG-WM", "entity2": "LLMs", "relation": "Tool/Resource", "description": "RAG-WM uses LLMs such as GPT-3.5-Turbo, PaLM 2, Llama-2, and Vicuna-13B for watermark verification."}, {"entity1": "watermark embedding method", "entity2": "watermark texts", "relation": "Methodology", "description": "The watermark embedding method integrates watermark texts into knowledge bases."}, {"entity1": "RAG-WM", "entity2": "IP infringement detection", "relation": "Application", "description": "RAG-WM is used for IP infringement detection."}, {"entity1": "watermark questions", "entity2": "RAG systems", "relation": "Experiment-Design", "description": "Watermark questions are used to query RAG systems for watermark verification."}, {"entity1": "Llama-2-7B", "entity2": "Vicuna-13B", "relation": "Comparison", "description": "Llama-2-7B and Vicuna-13B are compared in terms of their white-box nature and control over token sampling strategies."}, {"entity1": "GPT-3.5-Turbo", "entity2": "PaLM 2", "relation": "Comparison", "description": "GPT-3.5-Turbo and PaLM 2 are compared in terms of their black-box nature and stability."}, {"entity1": "CDPA", "entity2": "clean data performance alignment", "relation": "Definition", "description": "CDPA is defined as clean data performance alignment between watermarked and clean RAGs."}, {"entity1": "WIRR", "entity2": "watermark information retrieval ratio", "relation": "Definition", "description": "WIRR is defined as the watermark information retrieval ratio."}, {"entity1": "RAG-WM", "entity2": "watermark success counts (WSN)", "relation": "Result", "description": "RAG-WM achieves effective watermark verification with high watermark success counts (WSN)."}, {"entity1": "MS-MARCO", "entity2": "watermark retrieval", "relation": "Challenge", "description": "MS-MARCO poses a challenge for watermark retrieval due to its large size."}], "dafe6e06-810c-460d-968e-b12c74f16ebd": [{"entity1": "RAG-WM", "entity2": "token sampling strategies", "relation": "Methodology", "description": "RAG-WM utilizes token sampling strategies for easier control compared to black-box models."}, {"entity1": "GPT-3.5-Turbo", "entity2": "PaLM-2", "relation": "Comparison", "description": "GPT-3.5-Turbo and PaLM-2 are compared in terms of stability, with the former being less stable due to latent variable states."}, {"entity1": "Llama-2-7B", "entity2": "Vicuna-13B", "relation": "Comparison", "description": "The evaluation results for Llama-2-7B and Vicuna-13B are similar, with only the results for Llama-2-7B being presented in Table 2."}, {"entity1": "RAG-WM", "entity2": "CIRA", "relation": "Evaluation Metric", "description": "The clean information retrieval alignment (CIRA) of RAG-WM is evaluated, with an average value of 95.17%."}, {"entity1": "MS-MARCO", "entity2": "NFCorpus", "relation": "Comparison", "description": "MS-MARCO and NFCorpus are compared in terms of CIRA, with MS-MARCO having a higher CIRA value (97.34%) than NFCorpus (89.16%)."}, {"entity1": "RAG-WM", "entity2": "IP infringement", "relation": "Integrity", "description": "RAG-WM is evaluated for its integrity in detecting IP infringement, with results showing that it does not falsely detect IP infringement in clean RAGs."}, {"entity1": "GPT-3.5-Turbo", "entity2": "Human Evaluation", "relation": "Validation", "description": "The results of the LLM-based evaluation using GPT-3.5-Turbo are validated through human evaluation, showing high consistency between the two."}, {"entity1": "RAG-WM", "entity2": "Watermark detection", "relation": "Application", "description": "RAG-WM is used for watermark detection, with the results showing high consistency between LLM-based and human evaluations."}, {"entity1": "GPT-3.5-Turbo", "entity2": "Time Consumption", "relation": "Limitation", "description": "The watermarking process using GPT-3.5-Turbo introduces additional time compared to a clean RAG."}], "32c872be-8b46-4c95-af09-9879617816d3": [{"entity1": "LLM", "entity2": "human evaluations", "relation": "Comparison", "description": "LLM-based evaluations are compared to human evaluations for watermark detection, showing consistency and reliability."}, {"entity1": "RAG", "entity2": "watermarking process", "relation": "Methodology", "description": "The watermarking process introduces additional time compared to a clean RAG."}, {"entity1": "GPT-3.5-Turbo", "entity2": "watermark texts", "relation": "Tool/Resource", "description": "GPT-3.5-Turbo is used to generate watermark texts."}, {"entity1": "Table 3", "entity2": "time consumption", "relation": "Result", "description": "Table 3 shows the average time for generating each watermark text."}, {"entity1": "RAG", "entity2": "knowledge base", "relation": "Affiliation", "description": "The watermarking process is applied to the RAG's knowledge base."}, {"entity1": "TREC", "entity2": "NFCorpus", "relation": "Dataset-Origin", "description": "TREC and NFCorpus are datasets used in the evaluation."}, {"entity1": "HotpotQA", "entity2": "MS-COVID", "relation": "Dataset-Origin", "description": "HotpotQA and MS-COVID are datasets used in the evaluation."}, {"entity1": "MARCO", "entity2": "RAG-WM", "relation": "Dataset-Origin", "description": "MARCO is a dataset used to evaluate the performance of RAG-WM."}, {"entity1": "RAG-WM", "entity2": "parameters", "relation": "Methodology", "description": "The performance of RAG-WM is related to several factors, including parameters."}, {"entity1": "\ud835\udc58", "entity2": "retrieval top\ud835\udc58related texts", "relation": "Methodology", "description": "The \ud835\udc58 value is used for retrieval top\ud835\udc58related texts, as defined in equation (1)."}, {"entity1": "equation (1)", "entity2": "\ud835\udc58", "relation": "mathematical model", "description": "Equation (1) defines the \ud835\udc58 value for retrieval top\ud835\udc58related texts."}, {"entity1": "RAG-WM", "entity2": "LLaMA-2-7B", "relation": "Tool/Resource", "description": "LLaMA-2-7B is used as the adversary's LLM for the RAG system."}, {"entity1": "Contriever", "entity2": "Contriever-ms", "relation": "Comparison", "description": "Contriever and Contriever-ms are compared as retriever models."}, {"entity1": "ANCE", "entity2": "Contriever", "relation": "Comparison", "description": "ANCE is compared to Contriever as a retriever model."}, {"entity1": "Figure 2", "entity2": "evaluation results", "relation": "Result", "description": "Figure 2 shows the evaluation results of the effectiveness of RAG-WM on different retrievers."}, {"entity1": "RAG-WM", "entity2": "retrievers", "relation": "Methodology", "description": "RAG-WM is evaluated on different retrievers, including Contriever, Contriever-ms, and ANCE."}], "33b66a0b-fcc9-4cd2-a708-8f4ef8f78064": [{"entity1": "Watermark texts", "entity2": "Watermark queries", "relation": "Semantic Alignment", "description": "Watermark texts are semantically aligned with the watermark queries."}, {"entity1": "Watermark queries", "entity2": "Retrievers", "relation": "Retrieval Rate", "description": "The average retrieval rate of watermark queries by various retrievers is 95.93%."}, {"entity1": "ContrieverContriever-msANCE", "entity2": "TREC-COVID", "relation": "Experiment-Outcome", "description": "ContrieverContriever-msANCE achieves a certain outcome on the TREC-COVID dataset."}, {"entity1": "ContrieverContriever-msANCE", "entity2": "NFCorpus", "relation": "Experiment-Outcome", "description": "ContrieverContriever-msANCE achieves a certain outcome on the NFCorpus dataset."}, {"entity1": "ContrieverContriever-msANCE", "entity2": "MS-MARCO", "relation": "Experiment-Outcome", "description": "ContrieverContriever-msANCE achieves a certain outcome on the MS-MARCO dataset."}, {"entity1": "RAG-WM", "entity2": "Similarity metrics", "relation": "Impact Assessment", "description": "RAG-WM is evaluated with three similarity metrics: cosine similarity, inner product, and Euclidean distance."}, {"entity1": "Cosine similarity", "entity2": "Inner product", "relation": "Comparison", "description": "Cosine similarity and inner product are compared as similarity metrics."}, {"entity1": "Cosine similarity", "entity2": "Euclidean distance", "relation": "Comparison", "description": "Cosine similarity and Euclidean distance are compared as similarity metrics."}, {"entity1": "Inner product", "entity2": "Euclidean distance", "relation": "Comparison", "description": "Inner product and Euclidean distance are compared as similarity metrics."}, {"entity1": "RAG-WM", "entity2": "k", "relation": "Parameter Evaluation", "description": "RAG-WM is evaluated with different values of k."}, {"entity1": "k", "entity2": "WSN", "relation": "Impact", "description": "The value of k affects the WSN of RAG-WM."}, {"entity1": "k", "entity2": "WIRR", "relation": "Impact", "description": "The value of k affects the WIRR of RAG-WM."}, {"entity1": "Watermark tuples", "entity2": "Watermark verification", "relation": "Impact", "description": "The number of injected watermark tuples affects the watermark verification process."}, {"entity1": "RAG system", "entity2": "IP infringement", "relation": "Detection", "description": "The RAG system is used to detect IP infringement."}, {"entity1": "RAGs", "entity2": "LLMs", "relation": "Collaboration", "description": "RAGs collaborate with LLMs."}, {"entity1": "Adversary", "entity2": "k", "relation": "Parameter Setting", "description": "The adversary sets the value of k."}, {"entity1": "Watermark information", "entity2": "Watermark tuples", "relation": "Containment", "description": "Watermark tuples contain watermark information."}], "8a26a41c-525b-44b4-93c2-fe5ddfc75b4a": [{"entity1": "RAG system", "entity2": "watermark tuples", "relation": "Detection", "description": "The RAG system is used to detect IP infringement by querying it with watermark tuples."}, {"entity1": "watermark tuples", "entity2": "verification performance", "relation": "Impact", "description": "The number of embedded watermark tuples affects the verification performance."}, {"entity1": "WSN", "entity2": "WIRR", "relation": "Comparison", "description": "The WSN and WIRR are compared in terms of their performance."}, {"entity1": "TREC-COVID", "entity2": "NFCorpus", "relation": "Comparison", "description": "The performance of the watermark is compared across different datasets, including TREC-COVID and NFCorpus."}, {"entity1": "MS-MARCO", "entity2": "TREC-COVID", "relation": "Comparison", "description": "The performance of the watermark is compared across different datasets, including MS-MARCO and TREC-COVID."}, {"entity1": "\ud835\udc58Value", "entity2": "watermark verification", "relation": "Influence", "description": "The \ud835\udc58Value affects the subsequent watermark verification process."}, {"entity1": "CDPA", "entity2": "CIRA", "relation": "Trade-off", "description": "There is a trade-off between CDPA and CIRA as the number of watermark tuples increases."}, {"entity1": "knowledge base", "entity2": "watermark texts", "relation": "Impact", "description": "The size of the knowledge base affects the impact of injecting watermark texts."}, {"entity1": "NFCorpus", "entity2": "knowledge base", "relation": "Affiliation", "description": "NFCorpus is a dataset with a smaller knowledge base."}, {"entity1": "TREC-COVID", "entity2": "knowledge base", "relation": "Affiliation", "description": "TREC-COVID is a dataset with a larger knowledge base."}, {"entity1": "MS-MARCO", "entity2": "knowledge base", "relation": "Affiliation", "description": "MS-MARCO is a dataset with a larger knowledge base."}, {"entity1": "watermark tuples", "entity2": "\ud835\udc41\ud835\udc64\ud835\udc5a", "relation": "Influence", "description": "The number of injected texts per watermark tuple (\ud835\udc41\ud835\udc64\ud835\udc5a) affects the proportion of watermark texts retrieved."}, {"entity1": "RAG system", "entity2": "\ud835\udc58", "relation": "Parameter", "description": "The RAG system retrieves the top \ud835\udc58 most relevant texts for a given question."}], "e24f9337-baf4-44cc-b0c6-5a0e39044275": [{"entity1": "RAG-WM", "entity2": "Paraphrasing Attack", "relation": "Robustness", "description": "RAG-WM remains detectable even after paraphrasing attack"}, {"entity1": "RAG-WM", "entity2": "GPT-3.5-Turbo", "relation": "Tool/Resource", "description": "GPT-3.5-Turbo is used for paraphrasing attack on RAG-WM"}, {"entity1": "NFCorpus", "entity2": "TREC-COVID", "relation": "Dataset-Origin", "description": "NFCorpus and TREC-COVID are datasets used in the experiment"}, {"entity1": "RAG-WM", "entity2": "LLM", "relation": "Application", "description": "RAG-WM is applied to LLM for watermarking"}, {"entity1": "WSN", "entity2": "Number of Injected Texts", "relation": "Experiment-Outcome", "description": "WSN value increases as the number of injected watermark texts increases"}, {"entity1": "CDPA", "entity2": "RAG-WM", "relation": "Performance Metric", "description": "CDPA remains stable for RAG-WM, consistently above 95%"}, {"entity1": "Figure 6", "entity2": "Impact of Number of Injected Texts", "relation": "Result", "description": "Figure 6 shows the impact of number of injected texts on WSN value"}, {"entity1": "RAG-WM", "entity2": "Watermark Tuple", "relation": "Methodology", "description": "RAG-WM generates multiple watermark texts for each watermark tuple"}, {"entity1": "GPT-3.5-Turbo", "entity2": "Paraphrasing", "relation": "Tool/Resource", "description": "GPT-3.5-Turbo is used for paraphrasing"}, {"entity1": "RAG-WM", "entity2": "MS-MARCO", "relation": "Dataset-Origin", "description": "RAG-WM is evaluated on MS-MARCO dataset"}, {"entity1": "Equation: \ud835\udc58 = 1", "entity2": "RAG-WM", "relation": "Mathematical Model", "description": "Equation: \ud835\udc58 = 1 is used to set the number of retrieved most related texts in RAG-WM"}, {"entity1": "Equation: \ud835\udc41\ud835\udc64\ud835\udc5a = [1,5]", "entity2": "RAG-WM", "relation": "Mathematical Model", "description": "Equation: \ud835\udc41\ud835\udc64\ud835\udc5a = [1,5] is used to set the range of \ud835\udc41\ud835\udc64\ud835\udc5a values in RAG-WM"}], "288e1c91-80dc-4bf8-b551-5a952c04256c": [{"entity1": "RAG", "entity2": "Paraphrasing Attack", "relation": "Vulnerability", "description": "RAG is vulnerable to paraphrasing attacks, where an adversary can replace words from the green list with those from the red list, rendering the WARD watermark ineffective."}, {"entity1": "WARD", "entity2": "Paraphrasing Attack", "relation": "Ineffective Against", "description": "The WARD watermark is ineffective against paraphrasing attacks."}, {"entity1": "RAG-WM", "entity2": "Unrelated Content Removal Attack", "relation": "Robustness", "description": "RAG-WM is robust against unrelated content removal attacks, with an average WSN of 12 after the attack."}, {"entity1": "GPT-3.5-Turbo", "entity2": "Unrelated Content Removal Attack", "relation": "Tool", "description": "GPT-3.5-Turbo is used to remove unrelated content based on a designed prompt."}, {"entity1": "RAG-WM", "entity2": "Knowledge Insertion Attack", "relation": "Vulnerability", "description": "RAG-WM is vulnerable to knowledge insertion attacks, where an adversary can insert misleading information into the RAG's knowledge base to interfere with the watermark retrieval process."}, {"entity1": "Watermark Retrieval Process", "entity2": "Knowledge Insertion Attack", "relation": "Interference", "description": "Knowledge insertion attacks can interfere with the watermark retrieval process by inserting records with randomly selected entities and relations."}, {"entity1": "TREC-COVID", "entity2": "NFCorpus", "relation": "Comparison", "description": "The performance of RAG-WM on TREC-COVID and NFCorpus is compared in terms of WSN values."}, {"entity1": "MS-MARCO", "entity2": "WSN", "relation": "Evaluation Metric", "description": "WSN is used as an evaluation metric for the performance of RAG-WM on MS-MARCO."}, {"entity1": "RAG", "entity2": "Watermark Relation Retrieval", "relation": "Failure", "description": "The insertion of misleading information into the RAG's knowledge base can cause the failure of watermark relation retrieval."}, {"entity1": "Figure 7", "entity2": "Knowledge Insertion Attack", "relation": "Experimental Results", "description": "Figure 7 shows the experimental results of the knowledge insertion attack on RAG-WM."}], "cbe80a7f-8ff2-4958-8feb-bb13957083bc": [{"entity1": "RAG-WM", "entity2": "IP infringement detection", "relation": "Application", "description": "RAG-WM is used for IP infringement detection"}, {"entity1": "WSN", "entity2": "TREC-COVID", "relation": "Result", "description": "WSN values for TREC-COVID are 18 and decrease to 12 with injected texts"}, {"entity1": "WSN", "entity2": "NFCorpus", "relation": "Result", "description": "WSN values for NFCorpus are 24 and decrease to 13 with injected texts"}, {"entity1": "WSN", "entity2": "MS-MARCO", "relation": "Result", "description": "WSN values for MS-MARCO are 19 and decrease to 12 with injected texts"}, {"entity1": "RAG-WM", "entity2": "Knowledge Expansion Attack", "relation": "Challenge", "description": "RAG-WM is challenged by Knowledge Expansion Attack"}, {"entity1": "Knowledge Expansion Attack", "entity2": "LLMs", "relation": "Impact", "description": "Knowledge Expansion Attack affects the effectiveness of LLMs"}, {"entity1": "RAG-WM", "entity2": "WSN", "relation": "Methodology", "description": "RAG-WM uses WSN to measure the effectiveness of watermarking"}, {"entity1": "WSN", "entity2": "Injected texts", "relation": "Experiment-Outcome", "description": "WSN values decrease with the increase of injected texts"}, {"entity1": "RAG-WM", "entity2": "WIRR", "relation": "Result", "description": "RAG-WM's WIRR decreases from 100.00% to 46.67% with injected texts"}, {"entity1": "Knowledge Expansion Attack", "entity2": "RAG-WM", "relation": "Adversary", "description": "Knowledge Expansion Attack is an adversary to RAG-WM"}, {"entity1": "RAG-WM", "entity2": "LLMs", "relation": "Tool/Resource", "description": "RAG-WM uses LLMs for watermarking"}], "55fca813-c19f-4a94-b0b8-1dc759dd17de": [{"entity1": "Knowledge Insertion Attack", "entity2": "RAG-WM", "relation": "Attack-Defense", "description": "Knowledge Insertion Attack undermines RAG-WM's effectiveness by retrieving clean texts."}, {"entity1": "RAG-WM", "entity2": "PaLM-2", "relation": "Tool/Resource", "description": "RAG-WM uses PaLM-2 for the RAG system."}, {"entity1": "Llama-2-7B", "entity2": "Vicuna-13B", "relation": "Comparison", "description": "Llama-2-7B and Vicuna-13B are excluded from evaluation due to input text length limits."}, {"entity1": "Knowledge Expansion Attack", "entity2": "RAG-WM", "relation": "Attack-Defense", "description": "Knowledge Expansion Attack leads to significant computational costs on RAG-WM."}, {"entity1": "Perplexity Analysis", "entity2": "Duplicate Text Filtering", "relation": "Comparison", "description": "Perplexity Analysis and Duplicate Text Filtering are techniques used to detect watermarks."}, {"entity1": "PPL", "entity2": "Llama-2-7B", "relation": "Tool/Resource", "description": "PPL is calculated using the Llama-2-7B model."}, {"entity1": "K-means algorithm", "entity2": "PPL values", "relation": "Methodology", "description": "K-means algorithm is used to partition PPL values into two clusters."}, {"entity1": "TREC-COVID", "entity2": "NFCorpus", "relation": "Dataset-Origin", "description": "TREC-COVID and NFCorpus are datasets used in the evaluation."}, {"entity1": "MS-MARCO", "entity2": "RAG-WM", "relation": "Dataset-Origin", "description": "MS-MARCO is a dataset used to evaluate RAG-WM."}, {"entity1": "F1-score", "entity2": "Perplexity Analysis", "relation": "Evaluation Metric", "description": "F1-score is used to evaluate the effectiveness of Perplexity Analysis in detecting watermarks."}], "64a3dd50-f1fc-40e5-bbc1-8290b4c788c9": [{"entity1": "watermark texts", "entity2": "clean texts", "relation": "Comparison", "description": "The perplexity distribution of watermark texts closely overlaps with regions of low perplexity and high-frequency values in clean texts."}, {"entity1": "RAG-WM", "entity2": "watermark texts", "relation": "Tool/Resource", "description": "RAG-WM generates watermark texts that exhibit high quality, making them difficult to distinguish from clean texts based solely on perplexity."}, {"entity1": "perplexity", "entity2": "watermark texts", "relation": "Limitation", "description": "Using perplexity is not an effective method for detecting watermark texts."}, {"entity1": "SHA-256 hash function", "entity2": "duplicate texts", "relation": "Methodology", "description": "The SHA-256 hash function is used to compute the hash value for each text in the top 50 retrieval results from the watermarked knowledge base to filter out duplicate texts."}, {"entity1": "watermark query", "entity2": "watermark texts", "relation": "Experiment-Design", "description": "The owner may inject multiple watermark texts for each watermark tuple to improve the success rate of watermark retrieval in response to a watermark query."}, {"entity1": "WSN", "entity2": "WIRR", "relation": "Comparison", "description": "Both WSN and WIRR remain unchanged after the attack, indicating that duplicate text filtering is ineffective at detecting and removing watermark texts."}, {"entity1": "multi-LLM interaction watermarking technique", "entity2": "watermark texts", "relation": "Methodology", "description": "The multi-LLM interaction watermarking technique generates watermark texts that differ for each watermark tuple."}, {"entity1": "knowledge base", "entity2": "watermark texts", "relation": "Affiliation", "description": "The watermark texts are stored in the knowledge base."}, {"entity1": "TREC-COVID", "entity2": "NFcorpus", "relation": "Comparison", "description": "The F1-scores for the evaluation are 15.88% on TREC-COVID, 6.93% on NFcorpus, and 16.36% on MS-MARCO tasks."}, {"entity1": "Figure 11", "entity2": "heatmaps", "relation": "Publication Venue", "description": "The results are visualized as heatmaps in Figure 11 of Appendix."}], "cf7724fe-0e4e-45c7-9892-814ba204d2f2": [{"entity1": "RAG-WM", "entity2": "Self-RAG", "relation": "Comparison", "description": "RAG-WM is compared to Self-RAG in terms of WSN and WIRR values in advanced RAG systems."}, {"entity1": "RAG-WM", "entity2": "CRAG", "relation": "Comparison", "description": "RAG-WM is compared to CRAG in terms of WSN and WIRR values in advanced RAG systems."}, {"entity1": "RAG-WM", "entity2": "TREC-WIRR", "relation": "Experiment-Outcome", "description": "RAG-WM achieves high WSNs (average 22 WSN) in the TREC-WIRR dataset."}, {"entity1": "RAG-WM", "entity2": "NFCorpus", "relation": "Experiment-Outcome", "description": "RAG-WM achieves high WSNs (average 22 WSN) in the NFCorpus dataset."}, {"entity1": "RAG-WM", "entity2": "MS-MARCO", "relation": "Experiment-Outcome", "description": "RAG-WM achieves high WSNs (average 22 WSN) in the MS-MARCO dataset."}, {"entity1": "Relevant-text concatenation", "entity2": "Direct insertion", "relation": "Comparison", "description": "Relevant-text concatenation is compared to direct insertion in terms of WSN and WIRR values."}, {"entity1": "RAG-WM", "entity2": "LLM", "relation": "Tool/Resource", "description": "RAG-WM uses LLM to generate correct watermark relationships based on retrieved watermark contexts."}, {"entity1": "Watermark injection method", "entity2": "RAG\ud835\udc64\ud835\udc5a", "relation": "Methodology", "description": "The watermark injection method is used to inject watermark texts into RAG\ud835\udc64\ud835\udc5a."}, {"entity1": "Pre-retrieval", "entity2": "Watermark retrieval performance", "relation": "Impact", "description": "Pre-retrieval improves watermark retrieval performance and increases WSN values."}, {"entity1": "Direct insertion", "entity2": "Large-scale knowledge bases", "relation": "Limitation", "description": "Direct insertion has poor performance in large-scale knowledge bases, which worsens as the size of the base increases."}], "6a2ffadd-f006-486c-9570-ec08f01680f2": [{"entity1": "Knowledge Graph Distillation Attack", "entity2": "Domain-specific Knowledge Distillation Attack", "relation": "Type Of", "description": "Knowledge Graph Distillation Attack is a type of Domain-specific Knowledge Distillation Attack."}, {"entity1": "NQ", "entity2": "HotpotQA", "relation": "Comparison", "description": "NQ and HotpotQA are compared in terms of their performance on the watermarking task."}, {"entity1": "RAG-WM", "entity2": "RAGs", "relation": "Affiliation", "description": "RAG-WM is a type of RAGs."}, {"entity1": "LLama", "entity2": "Vicuna", "relation": "Comparison", "description": "LLama and Vicuna are compared in terms of their performance on the watermarking task."}, {"entity1": "GPT-3.5", "entity2": "PaLM", "relation": "Comparison", "description": "GPT-3.5 and PaLM are compared in terms of their performance on the watermarking task."}, {"entity1": "TREC-COVID", "entity2": "NFCorpus", "relation": "Dataset-Origin", "description": "TREC-COVID and NFCorpus are datasets used in the evaluation of the watermarking task."}, {"entity1": "WSN", "entity2": "WIRR", "relation": "Metric", "description": "WSN and WIRR are metrics used to evaluate the performance of the watermarking task."}, {"entity1": "Knowledge Graph Distillation Attack", "entity2": "RAGs", "relation": "Attack-Target", "description": "Knowledge Graph Distillation Attack is an attack targeting RAGs."}, {"entity1": "Domain-specific Knowledge Distillation Attack", "entity2": "Knowledge Base", "relation": "Attack-Target", "description": "Domain-specific Knowledge Distillation Attack is an attack targeting the knowledge base."}, {"entity1": "Figure 9", "entity2": "Table 8", "relation": "Publication Venue", "description": "Figure 9 and Table 8 are publication venues used to present the results of the evaluation."}, {"entity1": "CDPA", "entity2": "RAG-WM", "relation": "Metric", "description": "CDPA is a metric used to evaluate the performance of RAG-WM."}, {"entity1": "IP", "entity2": "RAG-WM", "relation": "Infringement Detection", "description": "IP infringement of RAG-WM can be detected using the watermarking technique."}], "deab9380-03e1-4785-9a33-ab36e4c46b20": [{"entity1": "RAG-WM", "entity2": "knowledge graph distillation attack", "relation": "Robustness", "description": "RAG-WM is robust against knowledge graph distillation attack due to the high degree of watermark entities in the extracted graph."}, {"entity1": "RAG-WM", "entity2": "domain-specific knowledge", "relation": "Robustness", "description": "RAG-WM is robust against domain-specific knowledge distillation attack by ensuring effective retention of watermark entities in the extracted graph."}, {"entity1": "Wikidata", "entity2": "domain-specific knowledge", "relation": "Containment", "description": "Wikidata contains extensive domain knowledge."}, {"entity1": "DBpedia", "entity2": "domain-specific knowledge", "relation": "Containment", "description": "DBpedia contains extensive domain knowledge."}, {"entity1": "YAGO", "entity2": "domain-specific knowledge", "relation": "Containment", "description": "YAGO contains extensive domain knowledge."}, {"entity1": "RAG-WM", "entity2": "relational database watermarking techniques", "relation": "Inspiration", "description": "RAG-WM can refer to traditional relational database watermarking techniques to partition the knowledge base into groups."}, {"entity1": "RAG-WM", "entity2": "partitioning approach", "relation": "Enhancement", "description": "The partitioning approach enhances the robustness of RAG-WM against knowledge graph distillation attack."}, {"entity1": "watermark embedding algorithm", "entity2": "piracy attack", "relation": "Vulnerability", "description": "The watermark embedding algorithm can be used to insert a pirated watermark into stolen RAGs, allowing attackers to fraudulently claim ownership."}, {"entity1": "RAG", "entity2": "knowledge graph distillation attack", "relation": "Robustness", "description": "RAG is robust against knowledge graph distillation attack."}, {"entity1": "RAG-WM", "entity2": "Section 5.4", "relation": "Reference", "description": "RAG-WM's robustness is discussed in Section 5.4."}], "0237aba9-317e-43a2-aad4-fa0faefe8adf": [{"entity1": "RAG-WM", "entity2": "RAG", "relation": "Methodology", "description": "RAG-WM generates watermarked RAGs by leveraging a multi-LLM interaction watermarking technique."}, {"entity1": "RAG-WM", "entity2": "LLM", "relation": "Methodology", "description": "RAG-WM generates watermarked RAGs by leveraging a multi-LLM interaction watermarking technique."}, {"entity1": "RAG-WM", "entity2": "Watermark entity-relationship tuples", "relation": "Methodology", "description": "RAG-WM creates watermark texts based on watermark entity-relationship tuples."}, {"entity1": "RAG-WM", "entity2": "IP infringement detection", "relation": "Application", "description": "RAG-WM detects IP infringement by querying the suspect LLM and RAG systems with the watermark queries in a black-box manner."}, {"entity1": "RAG-WM", "entity2": "Distillation Rate", "relation": "Experiment-Outcome", "description": "The results demonstrate that our watermark can effectively detect IP infringement of RAGs in various adversary's deployed LLMs."}, {"entity1": "RAG-WM", "entity2": "CDPA", "relation": "Comparison", "description": "Our watermark is robust against piracy attack (achieving security)."}, {"entity1": "Yossi Adi", "entity2": "Carsten Baum", "relation": "Co-author", "description": "Yossi Adi and Carsten Baum co-authored a paper on watermarking deep neural networks."}, {"entity1": "Yossi Adi", "entity2": "Moustapha Cisse", "relation": "Co-author", "description": "Yossi Adi and Moustapha Cisse co-authored a paper on watermarking deep neural networks."}, {"entity1": "Yossi Adi", "entity2": "Benny Pinkas", "relation": "Co-author", "description": "Yossi Adi and Benny Pinkas co-authored a paper on watermarking deep neural networks."}, {"entity1": "Yossi Adi", "entity2": "Joseph Keshet", "relation": "Co-author", "description": "Yossi Adi and Joseph Keshet co-authored a paper on watermarking deep neural networks."}, {"entity1": "Rakesh Agrawal", "entity2": "Peter J Haas", "relation": "Co-author", "description": "Rakesh Agrawal and Peter J Haas co-authored a paper on watermarking relational databases."}, {"entity1": "Rakesh Agrawal", "entity2": "Jerry Kiernan", "relation": "Co-author", "description": "Rakesh Agrawal and Jerry Kiernan co-authored a paper on watermarking relational databases."}, {"entity1": "Chroma AI", "entity2": "Chroma", "relation": "Affiliation", "description": "Chroma AI is affiliated with Chroma."}, {"entity1": "Ahmet Yusuf Alan", "entity2": "Enis Karaarslan", "relation": "Co-author", "description": "Ahmet Yusuf Alan and Enis Karaarslan co-authored a paper on a rag-based question answering system proposal."}, {"entity1": "Ahmet Yusuf Alan", "entity2": "\u00d6mer Aydin", "relation": "Co-author", "description": "Ahmet Yusuf Alan and \u00d6mer Aydin co-authored a paper on a rag-based question answering system proposal."}, {"entity1": "Gabriel Alon", "entity2": "Michael Kamfonas", "relation": "Co-author", "description": "Gabriel Alon and Michael Kamfonas co-authored a paper on detecting language model attacks with perplexity."}, {"entity1": "Maya Anderson", "entity2": "Guy Amit", "relation": "Co-author", "description": "Maya Anderson and Guy Amit co-authored a paper on a related topic."}, {"entity1": "Maya Anderson", "entity2": "Abigail Goldsteen", "relation": "Co-author", "description": "Maya Anderson and Abigail Goldsteen co-authored a paper on a related topic."}, {"entity1": "RAG-WM", "entity2": "USENIX Security 18", "relation": "Publication Venue", "description": "RAG-WM was presented at USENIX Security 18."}, {"entity1": "RAG-WM", "entity2": "ACM SIGMOD", "relation": "Publication Venue", "description": "RAG-WM was presented at ACM SIGMOD."}, {"entity1": "RAG-WM", "entity2": "VLDB", "relation": "Publication Venue", "description": "RAG-WM was presented at VLDB."}, {"entity1": "RAG-WM", "entity2": "arXiv", "relation": "Publication Venue", "description": "RAG-WM was presented at arXiv."}, {"entity1": "Figure 9: Knowledge Graph Distillation Attack", "entity2": "RAG-WM", "relation": "Result", "description": "Figure 9 shows the result of the knowledge graph distillation attack on RAG-WM."}, {"entity1": "Mufassirqas llm", "entity2": "Chroma AI", "relation": "Tool/Resource", "description": "Mufassirqas llm uses Chroma AI as a tool or resource."}, {"entity1": "RAG-WM", "entity2": "Wikidata", "relation": "Dataset-Origin", "description": "RAG-WM uses Wikidata as a dataset or origin."}], "1ee33b1b-b477-4f6f-9163-aeb69eecd394": [{"entity1": "Gabriel Alon", "entity2": "Michael Kamfonas", "relation": "Co-author", "description": "Gabriel Alon and Michael Kamfonas co-authored the paper 'Detecting language model attacks with perplexity'."}, {"entity1": "Maya Anderson", "entity2": "Guy Amit", "relation": "Co-author", "description": "Maya Anderson and Guy Amit co-authored the paper 'Is My Data in Your Retrieval Database? Membership Inference Attacks Against Retrieval Augmented Generation'."}, {"entity1": "Rohan Anil", "entity2": "Andrew M Dai", "relation": "Co-author", "description": "Rohan Anil and Andrew M Dai co-authored the paper 'Palm 2 technical report'."}, {"entity1": "Akari Asai", "entity2": "Zeqiu Wu", "relation": "Co-author", "description": "Akari Asai and Zeqiu Wu co-authored the paper 'Self-rag: Learning to retrieve, generate, and critique through self-reflection'."}, {"entity1": "Mikhail J Atallah", "entity2": "Victor Raskin", "relation": "Co-author", "description": "Mikhail J Atallah and Victor Raskin co-authored the paper 'Natural language watermarking: Design, analysis, and a proof-of-concept implementation'."}, {"entity1": "Payal Bajaj", "entity2": "Daniel Campos", "relation": "Co-author", "description": "Payal Bajaj and Daniel Campos co-authored the paper 'Ms marco: A human generated machine reading comprehension dataset'."}, {"entity1": "Mahbuba Begum", "entity2": "Mohammad Shorif Uddin", "relation": "Co-author", "description": "Mahbuba Begum and Mohammad Shorif Uddin co-authored the paper 'Digital image watermarking techniques: a review'."}, {"entity1": "Sukriti Bhattacharya", "entity2": "Agostino Cortesi", "relation": "Co-author", "description": "Sukriti Bhattacharya and Agostino Cortesi co-authored the paper 'A Distortion Free Watermark Framework for Relational Databases'."}, {"entity1": "Vera Boteva", "entity2": "Demian Gholipour", "relation": "Co-author", "description": "Vera Boteva and Demian Gholipour co-authored the paper 'A full-text learning to rank dataset for medical information retrieval'."}, {"entity1": "Tom Brown", "entity2": "Benjamin Mann", "relation": "Co-author", "description": "Tom Brown and Benjamin Mann co-authored the paper 'Language models are few-shot learners'."}, {"entity1": "Wei-Lin Chiang", "entity2": "Zhuohan Li", "relation": "Co-author", "description": "Wei-Lin Chiang and Zhuohan Li co-authored the paper 'Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality'."}, {"entity1": "arXiv", "entity2": "Gabriel Alon", "relation": "Publication Venue", "description": "Gabriel Alon's paper 'Detecting language model attacks with perplexity' was published on arXiv."}, {"entity1": "arXiv", "entity2": "Maya Anderson", "relation": "Publication Venue", "description": "Maya Anderson's paper 'Is My Data in Your Retrieval Database? Membership Inference Attacks Against Retrieval Augmented Generation' was published on arXiv."}, {"entity1": "Springer", "entity2": "Mikhail J Atallah", "relation": "Publication Venue", "description": "Mikhail J Atallah's paper 'Natural language watermarking: Design, analysis, and a proof-of-concept implementation' was published by Springer."}, {"entity1": "ECIR", "entity2": "Vera Boteva", "relation": "Publication Venue", "description": "Vera Boteva's paper 'A full-text learning to rank dataset for medical information retrieval' was published at ECIR."}, {"entity1": "Advances in neural information processing systems", "entity2": "Tom Brown", "relation": "Publication Venue", "description": "Tom Brown's paper 'Language models are few-shot learners' was published in Advances in neural information processing systems."}], "93023948-3337-435f-8407-81ab55c1e9a1": [{"entity1": "Wei-Lin Chiang", "entity2": "Vicuna", "relation": "Author-Role", "description": "Wei-Lin Chiang is an author of the Vicuna open-source chatbot."}, {"entity1": "Vicuna", "entity2": "GPT-4", "relation": "Comparison", "description": "Vicuna is compared to GPT-4 in terms of chat quality."}, {"entity1": "Miranda Christ", "entity2": "The Thirty Seventh Annual Conference on Learning Theory", "relation": "Publication Venue", "description": "Miranda Christ published a paper at The Thirty Seventh Annual Conference on Learning Theory."}, {"entity1": "DBpedia Community", "entity2": "DBpedia", "relation": "Affiliation", "description": "DBpedia Community is affiliated with DBpedia."}, {"entity1": "Florin Cuconasu", "entity2": "Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval", "relation": "Publication Venue", "description": "Florin Cuconasu published a paper at the Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval."}, {"entity1": "Zhengmian Hu", "entity2": "arXiv", "relation": "Publication Venue", "description": "Zhengmian Hu published a preprint on arXiv."}, {"entity1": "Mintplex Labs Inc.", "entity2": "Anything LLM AI", "relation": "Affiliation", "description": "Mintplex Labs Inc. is affiliated with Anything LLM AI."}, {"entity1": "Gautier Izacard", "entity2": "arXiv", "relation": "Publication Venue", "description": "Gautier Izacard published a preprint on arXiv."}, {"entity1": "Neel Jain", "entity2": "arXiv", "relation": "Publication Venue", "description": "Neel Jain published a preprint on arXiv."}, {"entity1": "Eric Jang", "entity2": "Gumbel-softmax", "relation": "Author-Role", "description": "Eric Jang is an author of the Gumbel-softmax paper."}, {"entity1": "Hengrui Jia", "entity2": "USENIX security symposium", "relation": "Publication Venue", "description": "Hengrui Jia published a paper at the USENIX security symposium."}, {"entity1": "Zhengbao Jiang", "entity2": "arXiv", "relation": "Publication Venue", "description": "Zhengbao Jiang published a preprint on arXiv."}, {"entity1": "Nikola Jovanovi\u0107", "entity2": "arXiv", "relation": "Publication Venue", "description": "Nikola Jovanovi\u0107 published a preprint on arXiv."}, {"entity1": "Muhammad Kamran", "entity2": "Muddassar Farooq", "relation": "Co-author", "description": "Muhammad Kamran and Muddassar Farooq are co-authors of a survey paper."}], "b986822c-1952-431b-89d1-3ed8412434ab": [{"entity1": "Shaochen Xu", "entity2": "Haixing Dai", "relation": "Co-author", "description": "Shaochen Xu and Haixing Dai are co-authors of the paper 'Revolutionizing finance with llms: An overview of applications and insights'."}, {"entity1": "Pengyuan Zhou", "entity2": "Lin Wang", "relation": "Co-author", "description": "Pengyuan Zhou and Lin Wang are co-authors of the paper 'A survey on generative ai and llm for video generation, understanding, and streaming'."}, {"entity1": "Wei Zou", "entity2": "Runpeng Geng", "relation": "Co-author", "description": "Wei Zou and Runpeng Geng are co-authors of the paper 'Poisonedrag: Knowledge poisoning attacks to retrieval-augmented generation of large language models'."}, {"entity1": "TREC-COVID", "entity2": "COVID-19", "relation": "Dataset-Origin", "description": "TREC-COVID is a dataset based on COVID-19 literature."}, {"entity1": "NFCorpus", "entity2": "PubMed", "relation": "Dataset-Origin", "description": "NFCorpus is a full-text English dataset for medical information retrieval, containing documents primarily sourced from PubMed."}, {"entity1": "NQ", "entity2": "Wikipedia", "relation": "Dataset-Origin", "description": "NQ includes questions from real users and texts from Wikipedia articles."}, {"entity1": "HotpotQA", "entity2": "Wikipedia", "relation": "Dataset-Origin", "description": "HotpotQA dataset is collected from Wikipedia."}, {"entity1": "MS-MARCO", "entity2": "Microsoft Bing", "relation": "Dataset-Origin", "description": "MS-MARCO is a question-answering dataset containing text samples collected from web documents using the Microsoft Bing search engine."}, {"entity1": "Shaochen Xu", "entity2": "arXiv", "relation": "Publication Venue", "description": "Shaochen Xu's paper 'Revolutionizing finance with llms: An overview of applications and insights' is published on arXiv."}, {"entity1": "Pengyuan Zhou", "entity2": "arXiv", "relation": "Publication Venue", "description": "Pengyuan Zhou's paper 'A survey on generative ai and llm for video generation, understanding, and streaming' is published on arXiv."}, {"entity1": "Wei Zou", "entity2": "arXiv", "relation": "Publication Venue", "description": "Wei Zou's paper 'Poisonedrag: Knowledge poisoning attacks to retrieval-augmented generation of large language models' is published on arXiv."}, {"entity1": "\ud835\udc38", "entity2": "\ud835\udc45", "relation": "Watermark Setting", "description": "The size of the entity list \ud835\udc38 is set to 100 and the size of the relations list \ud835\udc45 is set to 20."}, {"entity1": "LLM", "entity2": "Retrieval-augmented generation", "relation": "Application", "description": "LLM is used for retrieval-augmented generation."}, {"entity1": "Generative AI", "entity2": "Video generation", "relation": "Application", "description": "Generative AI is used for video generation."}, {"entity1": "Natural language processing", "entity2": "Open-domain question answering", "relation": "Research Area", "description": "Natural language processing is used for open-domain question answering."}, {"entity1": "Watermarking", "entity2": "Knowledge base", "relation": "Methodology", "description": "Watermarking is used to embed watermarks into a knowledge base."}], "a3f319be-50c1-4a76-a43f-709d3f6d586a": [{"entity1": "GPT-3.5", "entity2": "Llama", "relation": "Comparison", "description": "GPT-3.5 and Llama are compared in evaluations for WSN values as shown in Figure 10."}, {"entity1": "GPT-3.5", "entity2": "Vicuna", "relation": "Comparison", "description": "GPT-3.5 and Vicuna are compared in evaluations for WSN values as shown in Figure 10."}, {"entity1": "GPT-3.5", "entity2": "PaLM", "relation": "Comparison", "description": "GPT-3.5 and PaLM are compared in evaluations for WSN values as shown in Figure 10."}, {"entity1": "Llama", "entity2": "Vicuna", "relation": "Comparison", "description": "Llama and Vicuna are compared in evaluations for WSN values as shown in Figure 10."}, {"entity1": "Llama", "entity2": "PaLM", "relation": "Comparison", "description": "Llama and PaLM are compared in evaluations for WSN values as shown in Figure 10."}, {"entity1": "Vicuna", "entity2": "PaLM", "relation": "Comparison", "description": "Vicuna and PaLM are compared in evaluations for WSN values as shown in Figure 10."}, {"entity1": "GPT-3.5", "entity2": "SHA-256", "relation": "Utilization", "description": "GPT-3.5 utilizes SHA-256 as the HMAC function for watermark injection."}, {"entity1": "GPT-3.5-Turbo", "entity2": "WM-Gen", "relation": "Tool/Resource", "description": "GPT-3.5-Turbo is used to generate watermark texts in WM-Gen."}, {"entity1": "GPT-3.5-Turbo", "entity2": "Shadow-LLM&RAG", "relation": "Tool/Resource", "description": "GPT-3.5-Turbo is used as the shadow model in Shadow-LLM&RAG."}, {"entity1": "Llama-2-7B", "entity2": "RAG-WM", "relation": "Deployment", "description": "Llama-2-7B is deployed as the default LLM in RAG-WM."}, {"entity1": "TREC-COVID", "entity2": "NFCorpus", "relation": "Dataset-Origin", "description": "TREC-COVID and NFCorpus are datasets used in the evaluation, with statistics listed in Table 9."}, {"entity1": "NQ", "entity2": "HotpotQA", "relation": "Dataset-Origin", "description": "NQ and HotpotQA are datasets used in the evaluation, with statistics listed in Table 9."}, {"entity1": "MS-MARCO", "entity2": "Table 9", "relation": "Dataset-Origin", "description": "MS-MARCO is a dataset used in the evaluation, with statistics listed in Table 9."}, {"entity1": "GPT-3.5-Turbo", "entity2": "PaLM 2", "relation": "Comparison", "description": "GPT-3.5-Turbo and PaLM 2 are compared as potential adversary-deployed models."}, {"entity1": "Llama-2-7B", "entity2": "Vicuna-13B", "relation": "Comparison", "description": "Llama-2-7B and Vicuna-13B are compared as potential adversary-deployed models."}, {"entity1": "RAG-WM", "entity2": "Black-box LLMs", "relation": "Effectiveness", "description": "RAG-WM is effective in both black-box and white-box LLMs."}, {"entity1": "RAG-WM", "entity2": "White-box LLMs", "relation": "Effectiveness", "description": "RAG-WM is effective in both black-box and white-box LLMs."}, {"entity1": "GPT-3.5-Turbo", "entity2": "Llama-2-7B", "relation": "Comparison", "description": "GPT-3.5-Turbo and Llama-2-7B are compared as LLMs deployed in the adversary's RAG system."}, {"entity1": "Vicuna-13B", "entity2": "PaLM 2", "relation": "Comparison", "description": "Vicuna-13B and PaLM 2 are compared as potential adversary-deployed models."}, {"entity1": "GPT-3.5", "entity2": "Figure 10", "relation": "Publication Venue", "description": "GPT-3.5 is mentioned in Figure 10, which compares Human and LLM Evaluations for WSN values."}, {"entity1": "Llama", "entity2": "Figure 10", "relation": "Publication Venue", "description": "Llama is mentioned in Figure 10, which compares Human and LLM Evaluations for WSN values."}, {"entity1": "Vicuna", "entity2": "Figure 10", "relation": "Publication Venue", "description": "Vicuna is mentioned in Figure 10, which compares Human and LLM Evaluations for WSN values."}, {"entity1": "PaLM", "entity2": "Figure 10", "relation": "Publication Venue", "description": "PaLM is mentioned in Figure 10, which compares Human and LLM Evaluations for WSN values."}, {"entity1": "GPT-3.5-Turbo", "entity2": "Table 9", "relation": "Publication Venue", "description": "GPT-3.5-Turbo is used to generate watermark texts, with statistics listed in Table 9."}, {"entity1": "TREC-COVID", "entity2": "Table 9", "relation": "Dataset-Origin", "description": "TREC-COVID is a dataset used in the evaluation, with statistics listed in Table 9."}, {"entity1": "NFCorpus", "entity2": "Table 9", "relation": "Dataset-Origin", "description": "NFCorpus is a dataset used in the evaluation, with statistics listed in Table 9."}, {"entity1": "NQ", "entity2": "Table 9", "relation": "Dataset-Origin", "description": "NQ is a dataset used in the evaluation, with statistics listed in Table 9."}, {"entity1": "HotpotQA", "entity2": "Table 9", "relation": "Dataset-Origin", "description": "HotpotQA is a dataset used in the evaluation, with statistics listed in Table 9."}, {"entity1": "MS-MARCO", "entity2": "Table 9", "relation": "Dataset-Origin", "description": "MS-MARCO is a dataset used in the evaluation, with statistics listed in Table 9."}], "db950de3-9925-460a-a20c-e27c3d9e1ff5": [{"entity1": "GPT-3.5-Turbo", "entity2": "watermark", "relation": "Tool/Resource", "description": "GPT-3.5-Turbo is used to improve the effectiveness of the watermark."}, {"entity1": "Contriever", "entity2": "cosine similarity", "relation": "Tool/Resource", "description": "Contriever utilizes cosine similarity as the default retriever."}, {"entity1": "WM-Gen", "entity2": "watermark text (WT)", "relation": "Author-Role", "description": "WM-Gen generates watermark text (WT) that encodes the relationship between entities."}, {"entity1": "Watermark Discriminator 1 (WD1)", "entity2": "watermark text (WT)", "relation": "Evaluation", "description": "WD1 evaluates whether the watermark text accurately implies the relationship between entities."}, {"entity1": "Watermark Extractor (WE)", "entity2": "watermark text (WT)", "relation": "Experiment-Outcome", "description": "WE attempts to extract the relationship from the restored watermark text."}, {"entity1": "Watermark Discriminator 2 (WD2)", "entity2": "relationship (R1)", "relation": "Evaluation", "description": "WD2 assesses whether the relationship is still clearly implied after extraction."}, {"entity1": "E1", "entity2": "E2", "relation": "Research Area", "description": "E1 and E2 are connected by a relationship (R1) in the knowledge graph."}, {"entity1": "R1", "entity2": "watermark text (WT)", "relation": "Theoretical Framework", "description": "The relationship (R1) is encoded in the watermark text (WT)."}, {"entity1": "TEXT", "entity2": "watermark text (WT)", "relation": "Dataset-Origin", "description": "The watermark text (WT) is appended to the database content (TEXT)."}, {"entity1": "GPT-3.5-Turbo", "entity2": "Contriever", "relation": "Collaboration-Type", "description": "GPT-3.5-Turbo and Contriever are used together in the evaluation."}], "5f5dafd7-f704-460e-b717-19e90546932f": [{"entity1": "WM-Disc", "entity2": "Watermark Queries", "relation": "Tool/Resource", "description": "WM-Disc uses watermark queries to extract watermark information."}, {"entity1": "WM-Disc", "entity2": "Watermark Verification", "relation": "Methodology", "description": "WM-Disc uses a prompt for watermark verification to measure WSN."}, {"entity1": "Knowledge Graphs", "entity2": "Linguistics", "relation": "Research Area", "description": "The expert in the prompt is knowledgeable in both knowledge graphs and linguistics."}, {"entity1": "RAGs", "entity2": "CDPA", "relation": "Research Funding-Grant", "description": "RAGs are evaluated for their performance on the CDPA task."}, {"entity1": "Shadow-LLM&RAG", "entity2": "RAGs", "relation": "Tool/Resource", "description": "Shadow-LLM&RAG is a prompt used in conjunction with RAGs."}, {"entity1": "TREC-COVID", "entity2": "Watermark Tuple", "relation": "Dataset-Origin", "description": "The watermark tuple is used in the context of TREC-COVID."}, {"entity1": "Angiogenesis", "entity2": "Antagonists", "relation": "Research Area", "description": "Angiogenesis and antagonists are related concepts in the context of blood vessel formation."}, {"entity1": "RAGs", "entity2": "LLM", "relation": "Tool/Resource", "description": "RAGs are related to LLMs in the context of natural language processing."}, {"entity1": "WM-Disc", "entity2": "R1", "relation": "Methodology", "description": "WM-Disc uses a prompt to evaluate the relationship R1 between entities E1 and E2."}, {"entity1": "E1", "entity2": "E2", "relation": "Entity-Relationship", "description": "E1 and E2 are entities related by the relationship R1."}], "648033c0-56c7-4b27-84dd-5e71b4f5e4f6": [{"entity1": "angiogenesis", "entity2": "blood vessel", "relation": "Regulation", "description": "Angiogenesis regulates blood vessel formation."}, {"entity1": "angiogenesis", "entity2": "antagonists", "relation": "Utilization", "description": "Angiogenesis employs antagonists to modulate its processes."}, {"entity1": "antagonists", "entity2": "angiogenesis", "relation": "Modulation", "description": "Antagonists play a crucial role in regulating the multi-step process of angiogenesis."}, {"entity1": "Idiopathic Urticaria", "entity2": "Calciferols", "relation": "Association", "description": "Idiopathic Urticaria is associated with Calciferols."}, {"entity1": "Preston", "entity2": "Liquidity", "relation": "Involvement", "description": "Preston is involved with Liquidity."}], "85f5d448-08ab-4726-a4ef-cc6f1277d5e8": [{"entity1": "Shamshad Hussain", "entity2": "Boston University", "relation": "Affiliation", "description": "Shamshad Hussain is affiliated with Boston University."}, {"entity1": "Mozilla", "entity2": "Scrofula", "relation": "CAUSES", "description": "Mozilla is known to cause Scrofula."}, {"entity1": "TREC-COVID", "entity2": "Figure 11", "relation": "Publication Venue", "description": "TREC-COVID is a publication venue related to Figure 11."}, {"entity1": "NFCorpus", "entity2": "Figure 11", "relation": "Publication Venue", "description": "NFCorpus is a publication venue related to Figure 11."}, {"entity1": "MS-MARCO", "entity2": "Figure 11", "relation": "Publication Venue", "description": "MS-MARCO is a publication venue related to Figure 11."}, {"entity1": "\ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a", "entity2": "\ud835\udc52\ud835\udc57\ud835\udc64\ud835\udc5a", "relation": "Correlation", "description": "\ud835\udc52\ud835\udc56\ud835\udc64\ud835\udc5a and \ud835\udc52\ud835\udc57\ud835\udc64\ud835\udc5a have a correlation."}, {"entity1": "HotpotQA", "entity2": "Shamshad Hussain", "relation": "Dataset-Origin", "description": "HotpotQA is a dataset that contains information about Shamshad Hussain."}, {"entity1": "MS-MARCO", "entity2": "Mozilla", "relation": "Dataset-Origin", "description": "MS-MARCO is a dataset that contains information about Mozilla."}, {"entity1": "Table 10", "entity2": "Watermark Queries", "relation": "Publication Venue", "description": "Table 10 is a publication venue related to Watermark Queries."}, {"entity1": "Table 11", "entity2": "Entities and Relations", "relation": "Publication Venue", "description": "Table 11 is a publication venue related to Entities and Relations."}, {"entity1": "NFCorpus", "entity2": "\ud835\udc38,\ud835\udc45", "relation": "Dataset-Origin", "description": "NFCorpus is a dataset that contributes to the entity and relation list {\ud835\udc38,\ud835\udc45}."}, {"entity1": "TREC-COVID", "entity2": "\ud835\udc38,\ud835\udc45", "relation": "Dataset-Origin", "description": "TREC-COVID is a dataset that contributes to the entity and relation list {\ud835\udc38,\ud835\udc45}."}, {"entity1": "MS-MARCO", "entity2": "\ud835\udc38,\ud835\udc45", "relation": "Dataset-Origin", "description": "MS-MARCO is a dataset that contributes to the entity and relation list {\ud835\udc38,\ud835\udc45}."}], "35382242-5ef0-42f1-8133-91f2d8b29915": [{"entity1": "TREC-COVID", "entity2": "Table 11", "relation": "Dataset", "description": "TREC-COVID dataset is listed in Table 11 with an extraction rate of 5.00%, containing 74,176 entities and 127,764 relations."}, {"entity1": "NFCorpus", "entity2": "Table 11", "relation": "Dataset", "description": "NFCorpus dataset is listed in Table 11 with an extraction rate of 100.00%, containing 38,194 entities and 75,179 relations."}, {"entity1": "NQ", "entity2": "Table 11", "relation": "Dataset", "description": "NQ dataset is listed in Table 11 with an extraction rate of 0.19%, containing 34,659 entities and 41,763 relations."}, {"entity1": "HotPotQA", "entity2": "Table 11", "relation": "Dataset", "description": "HotPotQA dataset is listed in Table 11 with an extraction rate of 0.096%, containing 25,300 entities and 27,707 relations."}, {"entity1": "MS-MARCO", "entity2": "Table 11", "relation": "Dataset", "description": "MS-MARCO dataset is listed in Table 11 with an extraction rate of 0.057%, containing 29,530 entities and 36,167 relations."}, {"entity1": "TREC-COVID", "entity2": "Entities", "relation": "Extraction Rate", "description": "TREC-COVID has an extraction rate of 5.00% for entities."}, {"entity1": "NFCorpus", "entity2": "Entities", "relation": "Extraction Rate", "description": "NFCorpus has an extraction rate of 100.00% for entities."}, {"entity1": "NQ", "entity2": "Entities", "relation": "Extraction Rate", "description": "NQ has an extraction rate of 0.19% for entities."}, {"entity1": "HotPotQA", "entity2": "Entities", "relation": "Extraction Rate", "description": "HotPotQA has an extraction rate of 0.096% for entities."}, {"entity1": "MS-MARCO", "entity2": "Entities", "relation": "Extraction Rate", "description": "MS-MARCO has an extraction rate of 0.057% for entities."}, {"entity1": "TREC-COVID", "entity2": "Relations", "relation": "Extraction Rate", "description": "TREC-COVID has an extraction rate of 5.00% for relations."}, {"entity1": "NFCorpus", "entity2": "Relations", "relation": "Extraction Rate", "description": "NFCorpus has an extraction rate of 100.00% for relations."}, {"entity1": "NQ", "entity2": "Relations", "relation": "Extraction Rate", "description": "NQ has an extraction rate of 0.19% for relations."}, {"entity1": "HotPotQA", "entity2": "Relations", "relation": "Extraction Rate", "description": "HotPotQA has an extraction rate of 0.096% for relations."}, {"entity1": "MS-MARCO", "entity2": "Relations", "relation": "Extraction Rate", "description": "MS-MARCO has an extraction rate of 0.057% for relations."}, {"entity1": "TREC-COVID", "entity2": "Table 10", "relation": "Dataset Metrics", "description": "TREC-COVID dataset metrics are shown in Table 10."}, {"entity1": "NFCorpus", "entity2": "Table 10", "relation": "Dataset Metrics", "description": "NFCorpus dataset metrics are shown in Table 10."}, {"entity1": "NQ", "entity2": "Table 10", "relation": "Dataset Metrics", "description": "NQ dataset metrics are shown in Table 10."}, {"entity1": "HotPotQA", "entity2": "Table 10", "relation": "Dataset Metrics", "description": "HotPotQA dataset metrics are shown in Table 10."}, {"entity1": "MS-MARCO", "entity2": "Table 10", "relation": "Dataset Metrics", "description": "MS-MARCO dataset metrics are shown in Table 10."}], "bd4377a5-7b86-4388-a620-3f7b62cbec00": [{"entity1": "Jiaxing Li", "entity2": "Simon Fraser University", "relation": "Affiliation", "description": "Jiaxing Li is affiliated with Simon Fraser University."}, {"entity1": "Chi Xu", "entity2": "Simon Fraser University", "relation": "Affiliation", "description": "Chi Xu is affiliated with Simon Fraser University."}, {"entity1": "Lianchen Jia", "entity2": "Tsinghua University", "relation": "Affiliation", "description": "Lianchen Jia is affiliated with Tsinghua University."}, {"entity1": "Feng Wang", "entity2": "University of Mississippi", "relation": "Affiliation", "description": "Feng Wang is affiliated with University of Mississippi."}, {"entity1": "Cong Zhang", "entity2": "Jiangxing Intelligence Inc.", "relation": "Affiliation", "description": "Cong Zhang is affiliated with Jiangxing Intelligence Inc."}, {"entity1": "Jiangchuan Liu", "entity2": "Simon Fraser University", "relation": "Affiliation", "description": "Jiangchuan Liu is affiliated with Simon Fraser University."}, {"entity1": "EACO-RAG", "entity2": "edge-assisted distributed RAG system", "relation": "Tool/Resource", "description": "EACO-RAG is an edge-assisted distributed RAG system."}, {"entity1": "Large Language Models", "entity2": "Web", "relation": "Application", "description": "Large Language Models are applied in Web systems."}, {"entity1": "Large Language Models", "entity2": "mobile", "relation": "Application", "description": "Large Language Models are applied in mobile systems."}, {"entity1": "Large Language Models", "entity2": "Web of Things", "relation": "Application", "description": "Large Language Models are applied in Web of Things systems."}, {"entity1": "Retrieval-Augmented Generation", "entity2": "Large Language Models", "relation": "Extension", "description": "Retrieval-Augmented Generation extends Large Language Models' capabilities."}, {"entity1": "EACO-RAG", "entity2": "adaptive knowledge updates", "relation": "Methodology", "description": "EACO-RAG employs adaptive knowledge updates."}, {"entity1": "EACO-RAG", "entity2": "inter-node collaboration", "relation": "Methodology", "description": "EACO-RAG leverages inter-node collaboration."}, {"entity1": "EACO-RAG", "entity2": "multi-armed bandit framework", "relation": "Methodology", "description": "EACO-RAG uses a multi-armed bandit framework."}, {"entity1": "EACO-RAG", "entity2": "safe online Bayesian methods", "relation": "Methodology", "description": "EACO-RAG employs safe online Bayesian methods."}, {"entity1": "EACO-RAG", "entity2": "vector datasets", "relation": "Dataset-Origin", "description": "EACO-RAG distributes vector datasets across edge nodes."}, {"entity1": "Simon Fraser University", "entity2": "Tsinghua University", "relation": "Institution-Collaboration", "description": "Simon Fraser University and Tsinghua University collaborate through the EACO-RAG project."}, {"entity1": "Simon Fraser University", "entity2": "University of Mississippi", "relation": "Institution-Collaboration", "description": "Simon Fraser University and University of Mississippi collaborate through the EACO-RAG project."}, {"entity1": "Jiangxing Intelligence Inc.", "entity2": "Simon Fraser University", "relation": "Institution-Collaboration", "description": "Jiangxing Intelligence Inc. and Simon Fraser University collaborate through the EACO-RAG project."}, {"entity1": "EACO-RAG", "entity2": "edge nodes", "relation": "Tool/Resource", "description": "EACO-RAG distributes vector datasets across edge nodes."}, {"entity1": "Large Language Models", "entity2": "autonomous processing", "relation": "Application", "description": "Large Language Models enable autonomous processing."}, {"entity1": "Large Language Models", "entity2": "recommendations", "relation": "Application", "description": "Large Language Models enable recommendations."}, {"entity1": "Large Language Models", "entity2": "decision-making", "relation": "Application", "description": "Large Language Models enable decision-making."}], "34fb97aa-7e15-4ab2-a865-8968967e6c0b": [{"entity1": "RAG services", "entity2": "healthcare", "relation": "Application", "description": "RAG services are being used in the healthcare industry, driving rapid adoption."}, {"entity1": "RAG services", "entity2": "education", "relation": "Application", "description": "RAG services are being used in the education industry, driving rapid adoption."}, {"entity1": "RAG services", "entity2": "legal services", "relation": "Application", "description": "RAG services are being used in the legal services industry, driving rapid adoption."}, {"entity1": "RAG services", "entity2": "2024", "relation": "Publication Date", "description": "The market for RAG services is projected to grow at a compound annual growth rate of 44.7% between 2024 and 2030."}, {"entity1": "RAG services", "entity2": "2030", "relation": "Publication Date", "description": "The market for RAG services is projected to grow at a compound annual growth rate of 44.7% between 2024 and 2030."}, {"entity1": "RAG systems", "entity2": "centralized data centers", "relation": "Affiliation", "description": "RAG systems are typically hosted alongside language models in centralized data centers."}, {"entity1": "RAG systems", "entity2": "databases", "relation": "Tool/Resource", "description": "RAG systems convert text into vector representations stored in databases."}, {"entity1": "Edge-assisted and Collaborative RAG (EACO-RAG)", "entity2": "edge nodes", "relation": "Methodology", "description": "EACO-RAG is a distributed architecture that uses edge nodes to improve answer quality, response time, and cost efficiency."}, {"entity1": "edge nodes", "entity2": "end users", "relation": "Collaboration-Type", "description": "Edge nodes can analyze behaviors locally to preemptively optimize retrievals for end users."}, {"entity1": "edge nodes", "entity2": "central databases", "relation": "Collaboration-Type", "description": "Edge nodes can escalate complex queries to central databases in the cloud."}, {"entity1": "knowledge bases", "entity2": "edge nodes", "relation": "Tool/Resource", "description": "Edge nodes can dynamically update their knowledge bases to enhance localized query handling capabilities."}, {"entity1": "RAG services", "entity2": "Quality of Service (QoS)", "relation": "Challenge", "description": "RAG services face scalability challenges that can result in QoS degradation, including reduced answer quality and increased response delay."}, {"entity1": "LLMs", "entity2": "edge", "relation": "Deployment", "description": "LLMs are being deployed at the edge to reduce generation delay."}, {"entity1": "arXiv:2410.20299v1", "entity2": "cs.DC", "relation": "Publication Venue", "description": "The paper arXiv:2410.20299v1 is published in the cs.DC category."}, {"entity1": "arXiv:2410.20299v1", "entity2": "27 Oct 2024", "relation": "Publication Date", "description": "The paper arXiv:2410.20299v1 was published on 27 Oct 2024."}], "67027d79-e7b6-468d-ab13-da611a1cc876": [{"entity1": "Li et al.", "entity2": "EACO-RAG", "relation": "Author-Role", "description": "Li et al. introduced EACO-RAG, a distributed RAG system."}, {"entity1": "EACO-RAG", "entity2": "Edge Server Limit", "relation": "Methodology", "description": "EACO-RAG optimizes the retrieval process by dynamically updating local knowledge bases at edge servers."}, {"entity1": "Model", "entity2": "Average FLOPs", "relation": "Result", "description": "The model's performance is measured by Average FLOPs (TFLOPs)."}, {"entity1": "Chunk Size", "entity2": "Accuracy", "relation": "Experiment-Outcome", "description": "The experiment shows the impact of Chunk Size on Accuracy."}, {"entity1": "Top K", "entity2": "Accuracy", "relation": "Experiment-Outcome", "description": "The experiment shows the impact of Top K on Accuracy and Generation Time."}, {"entity1": "RTT Delay", "entity2": "Cloud Model Deploy Location", "relation": "Result", "description": "RTT Delay is affected by the Cloud Model Deploy Location."}, {"entity1": "City A", "entity2": "SAR", "relation": "Comparison", "description": "The RTT Delay is compared across different cities: City A, SF, NY, BJ, and SAR."}, {"entity1": "EACO-RAG", "entity2": "RAG", "relation": "Innovation", "description": "EACO-RAG is an innovative RAG system that integrates edge assistance through adaptive knowledge updates and collaboration at the edge."}, {"entity1": "Figure 2", "entity2": "Model", "relation": "Publication Venue", "description": "Figure 2 shows the performance and bottleneck overview of LLM-only applications across varying parameter sizes."}, {"entity1": "Figure 3", "entity2": "Chunk Size", "relation": "Publication Venue", "description": "Figure 3 shows the impact of RAG parameters on performance: chunk size and Top K vs. accuracy and generation time."}], "3ffc97ac-c101-40d7-a2db-21a369e630ad": [{"entity1": "EACO-RAG", "entity2": "edge computing", "relation": "Utilization", "description": "EACO-RAG leverages edge computing to address scalability, delay reduction, and resource efficiency challenges."}, {"entity1": "EACO-RAG", "entity2": "inter-node collaboration", "relation": "Mechanism", "description": "EACO-RAG uses inter-node collaboration to minimize delay and communication overhead."}, {"entity1": "EACO-RAG", "entity2": "safe online Bayesian methods", "relation": "Methodology", "description": "EACO-RAG utilizes safe online Bayesian methods to manage the trade-off between accuracy, delay, and cost."}, {"entity1": "EACO-RAG", "entity2": "multi-armed bandit", "relation": "Model", "description": "The optimization problem in EACO-RAG is modeled as a multi-armed bandit."}, {"entity1": "EACO-RAG", "entity2": "KGRAG-3B", "relation": "Comparison", "description": "EACO-RAG achieves a 76.7% reduction in cost and a 74.2% reduction in delay compared to KGRAG-3B."}, {"entity1": "EACO-RAG", "entity2": "11.5% sacrifice in accuracy", "relation": "Tradeoff", "description": "EACO-RAG achieves reductions in cost and delay with only an 11.5% sacrifice in accuracy."}, {"entity1": "qwen2.5:0.5b", "entity2": "Mobile Edge", "relation": "Deployment", "description": "qwen2.5:0.5b is deployed on Mobile Edge."}, {"entity1": "qwen2.5:1.5b", "entity2": "Mobile Edge", "relation": "Deployment", "description": "qwen2.5:1.5b is deployed on Mobile Edge."}, {"entity1": "qwen2.5:3b", "entity2": "Mobile Edge", "relation": "Deployment", "description": "qwen2.5:3b is deployed on Mobile Edge."}, {"entity1": "qwen2.5:7b", "entity2": "Edge Server", "relation": "Deployment", "description": "qwen2.5:7b is deployed on Edge Server."}, {"entity1": "qwen2.5:14b", "entity2": "Edge Server", "relation": "Deployment", "description": "qwen2.5:14b is deployed on Edge Server."}, {"entity1": "qwen2.5:32b", "entity2": "Cloud Server", "relation": "Deployment", "description": "qwen2.5:32b is deployed on Cloud Server."}, {"entity1": "qwen2.5:72b", "entity2": "Cloud Server", "relation": "Deployment", "description": "qwen2.5:72b is deployed on Cloud Server."}, {"entity1": "EACO-RAG", "entity2": "Figure 1", "relation": "Illustration", "description": "Figure 1 illustrates the effectiveness of EACO-RAG in reducing delay and resource expenditure."}, {"entity1": "EACO-RAG", "entity2": "Table 1", "relation": "Reference", "description": "Table 1 provides information about LLM models, including those used in EACO-RAG."}, {"entity1": "edge computing", "entity2": "scalability", "relation": "Contribution", "description": "Edge computing contributes to scalability in EACO-RAG."}, {"entity1": "inter-node collaboration", "entity2": "delay reduction", "relation": "Contribution", "description": "Inter-node collaboration contributes to delay reduction in EACO-RAG."}, {"entity1": "safe online Bayesian methods", "entity2": "resource efficiency", "relation": "Contribution", "description": "Safe online Bayesian methods contribute to resource efficiency in EACO-RAG."}], "82c42b2c-328d-4010-90b1-fa020808dacf": [{"entity1": "qwen2.5:3b", "entity2": "Mobile Edge", "relation": "Deployment", "description": "qwen2.5:3b is deployed on Mobile Edge with 1.9GB"}, {"entity1": "qwen2.5:7b", "entity2": "Edge Server", "relation": "Deployment", "description": "qwen2.5:7b is deployed on Edge Server with 4.7GB"}, {"entity1": "qwen2.5:14b", "entity2": "Edge Server", "relation": "Deployment", "description": "qwen2.5:14b is deployed on Edge Server with 9GB"}, {"entity1": "qwen2.5:32b", "entity2": "Cloud Server", "relation": "Deployment", "description": "qwen2.5:32b is deployed on Cloud Server with 20GB"}, {"entity1": "qwen2.5:72b", "entity2": "Cloud Server", "relation": "Deployment", "description": "qwen2.5:72b is deployed on Cloud Server with 47GB"}, {"entity1": "RAG system", "entity2": "generator", "relation": "Component", "description": "The generator is a component of the RAG system"}, {"entity1": "RAG system", "entity2": "knowledge database", "relation": "Component", "description": "The knowledge database is a component of the RAG system"}, {"entity1": "RAG system", "entity2": "accuracy", "relation": "Performance Metric", "description": "Accuracy is a performance metric of the RAG system"}, {"entity1": "RAG system", "entity2": "delay", "relation": "Performance Metric", "description": "Delay is a performance metric of the RAG system"}, {"entity1": "RAG system", "entity2": "computational resource consumption", "relation": "Performance Metric", "description": "Computational resource consumption is a performance metric of the RAG system"}, {"entity1": "generator", "entity2": "LLM", "relation": "Alias", "description": "The generator is also known as LLM"}], "c9cb3bd6-f9e1-4b4a-84f5-05b0a9dba097": [{"entity1": "LLM", "entity2": "RAG", "relation": "Application", "description": "LLM is applied in RAG systems"}, {"entity1": "EACO-RAG", "entity2": "Edge-Assisted", "relation": "Affiliation", "description": "EACO-RAG is affiliated with Edge-Assisted"}, {"entity1": "NVIDIA RTX 4090", "entity2": "Edge configuration", "relation": "Tool/Resource", "description": "NVIDIA RTX 4090 is used as a tool/resource for edge configuration"}, {"entity1": "GPT-4o1", "entity2": "500 question-answer pairs", "relation": "Experiment-Outcome", "description": "GPT-4o1 is used to evaluate the outcome of experiments with 500 question-answer pairs"}, {"entity1": "Model size", "entity2": "Accuracy", "relation": "Result", "description": "Model size has a significant impact on accuracy"}, {"entity1": "Model size", "entity2": "Generation time", "relation": "Result", "description": "Model size affects generation time"}, {"entity1": "Model size", "entity2": "Resource consumption", "relation": "Result", "description": "Model size impacts resource consumption"}, {"entity1": "0.5B\u20137B parameters", "entity2": "7B\u201332B parameters", "relation": "Comparison", "description": "Models with 0.5B\u20137B parameters are compared to models with 7B\u201332B parameters"}, {"entity1": "Cloud deployment", "entity2": "Network delays", "relation": "Challenge", "description": "Cloud deployment introduces additional network delays, which is a challenge"}, {"entity1": "ICMP protocol", "entity2": "Network delay", "relation": "Methodology", "description": "ICMP protocol is used to measure network delay"}, {"entity1": "City A", "entity2": "Network delay", "relation": "Experiment-Outcome", "description": "Network delay is measured in City A"}, {"entity1": "Edge configuration", "entity2": "Real-time performance", "relation": "Objective", "description": "Edge configuration aims to achieve real-time performance"}, {"entity1": "LLM-based RAG systems", "entity2": "Edge deployment", "relation": "Application", "description": "LLM-based RAG systems are deployed at the edge"}, {"entity1": "Model accuracy", "entity2": "Real-time performance", "relation": "Trade-off", "description": "There is a trade-off between model accuracy and real-time performance"}, {"entity1": "NVIDIA RTX 4090", "entity2": "Computational power", "relation": "Tool/Resource", "description": "NVIDIA RTX 4090 represents typical edge computational power"}, {"entity1": "EACO-RAG", "entity2": "Adaptive Knowledge Update", "relation": "Methodology", "description": "EACO-RAG uses adaptive knowledge update"}, {"entity1": "Table 1", "entity2": "LLM configurations", "relation": "Publication Venue", "description": "Table 1 presents a selection of LLM configurations"}, {"entity1": "Figure 2(a)", "entity2": "Model size", "relation": "Result", "description": "Figure 2(a) shows the impact of model size on accuracy, generation time, and resource consumption"}, {"entity1": "Figure 2(b)", "entity2": "Model size", "relation": "Result", "description": "Figure 2(b) shows the impact of model size on accuracy, generation time, and resource consumption"}, {"entity1": "Figure 2(c)", "entity2": "Network delay", "relation": "Result", "description": "Figure 2(c) shows the network delay measured via ICMP protocol"}], "9d3b0b52-3e2f-4a08-8d6b-40b5c11b1ca5": [{"entity1": "computational resources", "entity2": "network delays", "relation": "Impact", "description": "Network delays predominantly affect the overall service time of computational resources, especially across longer distances."}, {"entity1": "cloud-based large models", "entity2": "network delays", "relation": "Challenge", "description": "Cloud-based large models face the challenge of network delays, which affects their real-time performance."}, {"entity1": "edge deployment", "entity2": "model accuracy", "relation": "Trade-off", "description": "Edge deployment presents a trade-off between model accuracy and real-time performance."}, {"entity1": "RAG systems", "entity2": "Knowledge Base", "relation": "Integration", "description": "RAG systems integrate a Knowledge Base to improve performance, especially in edge deployment scenarios."}, {"entity1": "chunk size", "entity2": "token count", "relation": "Measurement", "description": "Chunk size is measured in token count, defining how the knowledge base is segmented."}, {"entity1": "Top K", "entity2": "retrieval efficiency", "relation": "Influence", "description": "Top K directly influences retrieval efficiency and the quality of generated responses in RAG systems."}, {"entity1": "Figure 3(a)", "entity2": "chunk size", "relation": "Exploration", "description": "Figure 3(a) explores the relationship between chunk size and accuracy in RAG systems."}, {"entity1": "Figure 3(b)", "entity2": "Top K", "relation": "Examination", "description": "Figure 3(b) examines the trade-off between the number of retrieved chunks (Top K) and system performance."}, {"entity1": "LLM RAG KGRAG", "entity2": "edge deployment", "relation": "Optimization", "description": "A chunk size of 300 tokens and a Top K of 20 achieve an optimal balance between retrieval efficiency and answer accuracy for edge-deployed LLM-based RAG systems."}, {"entity1": "Harry Potter series", "entity2": "QA", "relation": "Dataset-Origin", "description": "500 QA questions were collected from a public dataset about the Harry Potter series."}, {"entity1": "semantic correlations", "entity2": "clusters", "relation": "Categorization", "description": "The QA questions were categorized into 20 clusters based on their semantic correlations."}], "8aec1d0c-dc8f-41be-9c5e-3fdc98f9b886": [{"entity1": "LLM-only", "entity2": "domain-specific knowledge", "relation": "Limitation", "description": "The accuracy of standalone LLMs drops to nearly zero when domain-specific knowledge is required."}, {"entity1": "RAG systems", "entity2": "local knowledge bases", "relation": "Constraint", "description": "RAG systems are constrained by the scope of their local knowledge bases."}, {"entity1": "KGRAG systems", "entity2": "structured knowledge graphs", "relation": "Application", "description": "KGRAG systems use structured knowledge graphs to improve accuracy."}, {"entity1": "KGRAG systems", "entity2": "LLM-only", "relation": "Comparison", "description": "KGRAG systems show an average improvement of over 40% in accuracy compared to LLMs."}, {"entity1": "RAG systems", "entity2": "LLM-only", "relation": "Comparison", "description": "RAG systems improve accuracy by 15% to 20% over LLMs."}, {"entity1": "clusters 5, 9 and 17", "entity2": "context-dependent questions", "relation": "Example", "description": "Clusters 5, 9, and 17 are examples of context-dependent questions where the accuracy of standalone LLMs drops to nearly zero."}, {"entity1": "Harry Potter series", "entity2": "public dataset", "relation": "Dataset-Origin", "description": "The public dataset is about the Harry Potter series."}, {"entity1": "500 QA", "entity2": "20 clusters", "relation": "Categorization", "description": "The 500 QA were categorized into 20 clusters based on their semantic correlations."}, {"entity1": "KGRAG systems", "entity2": "cloud", "relation": "Tool/Resource", "description": "KGRAG systems are cloud-based."}, {"entity1": "Figure 4(a)", "entity2": "system accuracy", "relation": "Result", "description": "Figure 4(a) shows the accuracy of the systems across different classes."}], "c27c8a8b-73e4-4b84-9f63-c78bfc87678d": [{"entity1": "KGRAG", "entity2": "LLM", "relation": "Comparison", "description": "KGRAG dramatically enhances accuracy compared to LLM, but increases generation time."}, {"entity1": "KGRAG", "entity2": "RAG systems", "relation": "Comparison", "description": "KGRAG has fluctuating generation times due to knowledge base retrieval overhead, unlike basic RAG systems."}, {"entity1": "KGRAG", "entity2": "knowledge base", "relation": "Tool/Resource", "description": "KGRAG retrieves from huge knowledge bases, affecting generation times."}, {"entity1": "RAG systems", "entity2": "domain-specific knowledge", "relation": "Integration", "description": "RAG systems improve accuracy by integrating domain-specific knowledge."}, {"entity1": "EACO-RAG", "entity2": "distributed edge devices", "relation": "Collaboration-Type", "description": "EACO-RAG leverages distributed edge devices for adaptive knowledge update and query addressing."}, {"entity1": "O1 models", "entity2": "edge devices", "relation": "Challenge", "description": "O1 models pose significant challenges for service providers due to computational requirements and extended generation times, affecting QoS."}, {"entity1": "EACO-RAG", "entity2": "user queries", "relation": "Adaptive Knowledge Update", "description": "EACO-RAG updates knowledge base dynamically based on user queries for improved contextual understanding."}, {"entity1": "KGRAG", "entity2": "generation time", "relation": "Limitation", "description": "KGRAG's generation time can spike from 0.5 seconds to over 7 seconds due to retrieval complexity."}, {"entity1": "RAG systems", "entity2": "knowledge base coverage", "relation": "Limitation", "description": "Traditional RAG systems are limited by issues like fluctuating generation times and limited knowledge base coverage."}, {"entity1": "EACO-RAG", "entity2": "context-dependent applications", "relation": "Application", "description": "EACO-RAG is particularly valuable for context-dependent applications, improving contextual understanding while maintaining reasonable generation times."}, {"entity1": "Li et al.", "entity2": "EACO-RAG", "relation": "Author-Role", "description": "Li et al. propose EACO-RAG, an online adaptive knowledge update system."}], "3737c119-cd6f-4313-b784-72018d4454ae": [{"entity1": "LLMs", "entity2": "RAG", "relation": "Comparison", "description": "The system adaptively selects from LLMs, RAG, and KGRAG based on contextual information for efficient processing."}, {"entity1": "Edge nodes", "entity2": "Cloud", "relation": "Collaboration-Type", "description": "Edge nodes and cloud collaborate in a cascading structure to manage resource allocation, with simpler queries processed by edge nodes and complex queries escalated to the cloud."}, {"entity1": "User queries", "entity2": "Edge nodes", "relation": "Data Collection", "description": "User queries are initially stored in the cloud as they pass through the gate in edge nodes, updating local databases at the edge."}, {"entity1": "GPT-4o", "entity2": "Abstract", "relation": "Generation", "description": "GPT-4o generates an abstract using user queries, producing summaries of common topics."}, {"entity1": "Abstract", "entity2": "Community Summary", "relation": "Comparison", "description": "Abstracts are compared to community summaries stored in the knowledge graph by embedding and assessing similarity."}, {"entity1": "Similarity", "entity2": "Embedding", "relation": "Calculation", "description": "Similarity is calculated by comparing the embedding of user queries with summaries from each edge."}, {"entity1": "Query complexity", "entity2": "Classifier", "relation": "Assessment", "description": "Query complexity is assessed using a simple classifier."}, {"entity1": "Gate mechanism", "entity2": "Generation time", "relation": "Optimization", "description": "The gate mechanism reduces generation time and operational costs while preserving service quality by determining the necessity of retrieval and selecting the most efficient approach."}, {"entity1": "Edge DB", "entity2": "Edge LM", "relation": "Integration", "description": "Edge DB and Edge LM are integrated for local RAG operations."}, {"entity1": "Knowledge Graph", "entity2": "Embedding", "relation": "Storage", "description": "Community summaries are stored in the knowledge graph and compared with abstracts through embedding."}, {"entity1": "Cloud Database", "entity2": "Chunk Data", "relation": "Storage", "description": "Chunk data is stored in the cloud database."}, {"entity1": "EACO-RAG", "entity2": "Figure 5", "relation": "Illustration", "description": "Figure 5 illustrates the workflow of the EACO-RAG system design."}, {"entity1": "OpenAI", "entity2": "GPT-4o", "relation": "Affiliation", "description": "GPT-4o is affiliated with OpenAI."}, {"entity1": "Quidditch", "entity2": "Wizarding world", "relation": "Context", "description": "Quidditch is a topic within the context of the wizarding world, used as an example of user queries."}, {"entity1": "Edge 1", "entity2": "Edge nodes", "relation": "Instance", "description": "Edge 1 is an instance of edge nodes."}, {"entity1": "Network delay", "entity2": "Query complexity", "relation": "Influence", "description": "Network delay and query complexity influence the determination of the optimal retrieval and generation strategy."}, {"entity1": "GraphRAG", "entity2": "Knowledge Graph", "relation": "Extension", "description": "GraphRAG is an extension or related concept to the knowledge graph, used in the context of RAG operations."}], "836e86d4-356f-47f1-be8b-78b0a8bc286e": [{"entity1": "service provider", "entity2": "operational costs", "relation": "Objective", "description": "The service provider aims to minimize operational costs."}, {"entity1": "gate architecture", "entity2": "response quality", "relation": "Optimization", "description": "The gate architecture optimizes response quality."}, {"entity1": "local model", "entity2": "cloud models", "relation": "Comparison", "description": "The gate decides whether to use a local model or delegate to cloud models."}, {"entity1": "knowledge base", "entity2": "RAG retrieval", "relation": "Methodology", "description": "The knowledge base is updated, and the gate optimizes RAG retrieval."}, {"entity1": "Safe Online Bayesian Optimization", "entity2": "gate architecture", "relation": "Tool/Resource", "description": "Safe Online Bayesian Optimization is used in the gate architecture."}, {"entity1": "contextual multi-armed bandit", "entity2": "Cost Modelling", "relation": "Methodology", "description": "Contextual multi-armed bandit is used for cost modelling."}, {"entity1": "edge", "entity2": "cloud resources", "relation": "Collaboration-Type", "description": "The approach balances the use of edge and cloud resources."}, {"entity1": "network latency", "entity2": "delay costs", "relation": "Cause-Effect", "description": "Network latency contributes to delay costs."}, {"entity1": "model generation delays", "entity2": "delay costs", "relation": "Cause-Effect", "description": "Model generation delays contribute to delay costs."}, {"entity1": "resource usage", "entity2": "operational costs", "relation": "Component", "description": "Resource usage is a component of operational costs."}, {"entity1": "delay costs", "entity2": "operational costs", "relation": "Component", "description": "Delay costs are a component of operational costs."}, {"entity1": "service provider", "entity2": "response quality", "relation": "Objective", "description": "The service provider aims to ensure response quality."}, {"entity1": "service provider", "entity2": "generation delay", "relation": "Objective", "description": "The service provider aims to reduce generation delay."}, {"entity1": "COST MODELLING AND OPTIMIZATION", "entity2": "contextual multi-armed bandit", "relation": "Methodology", "description": "Contextual multi-armed bandit is used for cost modelling and optimization."}, {"entity1": "LLM", "entity2": "service provider", "relation": "Affiliation", "description": "The service provider offers LLM-based services."}], "77d00e46-c3d4-4218-9a63-07d8d864689c": [{"entity1": "EACO-RAG", "entity2": "answer accuracy", "relation": "Objective", "description": "EACO-RAG aims to satisfy the constraint that answer accuracy must exceed a defined threshold \ud835\udf0cmin."}, {"entity1": "EACO-RAG", "entity2": "response time", "relation": "Objective", "description": "EACO-RAG aims to satisfy the constraint that response time must remain below a real-time limit \u210emax."}, {"entity1": "context", "entity2": "\ud835\udc50\ud835\udc61", "relation": "Representation", "description": "The context is represented as \ud835\udc50\ud835\udc61 := [\ud835\udc51\ud835\udc61 cloud,\ud835\udc51\ud835\udc61 edge,\ud835\udc60\ud835\udc61,\ud835\udc5e\ud835\udc61]\u2208C."}, {"entity1": "control policy", "entity2": "\ud835\udc51\ud835\udc61", "relation": "Representation", "description": "The control policy at time \ud835\udc61 is represented as \ud835\udc51\ud835\udc61 := [\ud835\udc5f\ud835\udc61,\ud835\udc54\ud835\udc61]\u2208D."}, {"entity1": "service provider", "entity2": "cost", "relation": "Optimization", "description": "The system aims to minimize the service provider's cost while balancing delay and accuracy."}, {"entity1": "local database", "entity2": "edge database", "relation": "Comparison", "description": "Local and edge-based databases provide low-cost, low-delay responses, while cloud-based databases provide more accurate and comprehensive responses."}, {"entity1": "\ud835\udeff1", "entity2": "resource cost", "relation": "Weight", "description": "\ud835\udeff1 is the weight for resource costs in the total cost function."}, {"entity1": "\ud835\udeff2", "entity2": "delay cost", "relation": "Weight", "description": "\ud835\udeff2 is the weight for delay costs in the total cost function."}, {"entity1": "\ud835\udc62\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61)", "entity2": "total cost", "relation": "Representation", "description": "The total cost function is represented as \ud835\udc62\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61)= \ud835\udeff1 \u00b7\ud835\udc62\ud835\udc61 r (\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61)+\ud835\udeff2 \u00b7\ud835\udc62\ud835\udc61 d (\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61)."}, {"entity1": "optimization problem", "entity2": "minimization", "relation": "Objective", "description": "The optimization problem aims to minimize the total cost over a time horizon \ud835\udc47."}, {"entity1": "\ud835\udf0c\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61)", "entity2": "answer accuracy", "relation": "Constraint", "description": "The answer accuracy \ud835\udf0c\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61) must exceed a defined threshold \ud835\udf0cmin."}, {"entity1": "\u210e\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61)", "entity2": "response time", "relation": "Constraint", "description": "The response time \u210e\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61) must remain below a real-time limit \u210emax."}], "79f51534-5ac1-4102-a7b1-e93884ef950f": [{"entity1": "Contextual multi-armed bandit problem", "entity2": "Safe Online Bayesian Optimization", "relation": "Methodology", "description": "Safe Online Bayesian Optimization is used to solve the contextual multi-armed bandit problem."}, {"entity1": "Gaussian Processes (GPs)", "entity2": "Function Approximator", "relation": "Tool/Resource", "description": "Gaussian Processes (GPs) are used as a Function Approximator to estimate cost and constraint functions."}, {"entity1": "Algorithm 1", "entity2": "Safe Online Bayesian Optimization", "relation": "Methodology", "description": "Algorithm 1 is a part of the Safe Online Bayesian Optimization approach."}, {"entity1": "\ud835\udf0c\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61)", "entity2": "Accuracy", "relation": "Result", "description": "\ud835\udf0c\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61) represents the accuracy of the generated answer."}, {"entity1": "\u210e\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61)", "entity2": "Response Time", "relation": "Result", "description": "\u210e\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc51\ud835\udc61) represents the model response time at time \ud835\udc61."}, {"entity1": "\ud835\udf07(\ud835\udc65)", "entity2": "Gaussian Processes (GPs)", "relation": "Mathematical Model", "description": "\ud835\udf07(\ud835\udc65) is the mean function used in Gaussian Processes (GPs)."}, {"entity1": "\ud835\udc58(\ud835\udc65,\ud835\udc65\u2032)", "entity2": "Gaussian Processes (GPs)", "relation": "Mathematical Model", "description": "\ud835\udc58(\ud835\udc65,\ud835\udc65\u2032) is the kernel function used in Gaussian Processes (GPs) to capture covariance between points."}, {"entity1": "Bayesian Optimization", "entity2": "Contextual multi-armed bandit problem", "relation": "Research Area", "description": "Bayesian Optimization is applied to the contextual multi-armed bandit problem."}, {"entity1": "Online learning techniques", "entity2": "Dynamic environments", "relation": "Methodology", "description": "Online learning techniques are used to adapt to dynamic environments."}, {"entity1": "Service provider", "entity2": "Operational costs", "relation": "Objective", "description": "The objective is to minimize the service provider's operational costs."}], "ea6cc7c4-346e-4e8b-af89-034517921b5b": [{"entity1": "y(0)T", "entity2": "cost", "relation": "Denotation", "description": "y(0)T denotes the observations for cost"}, {"entity1": "y(1)T", "entity2": "accuracy", "relation": "Denotation", "description": "y(1)T denotes the observations for accuracy"}, {"entity1": "y(2)T", "entity2": "response time", "relation": "Denotation", "description": "y(2)T denotes the observations for response time"}, {"entity1": "u", "entity2": "Control policies", "relation": "Denotation", "description": "u represents the control policies"}, {"entity1": "x", "entity2": "context-decision pairs", "relation": "Representation", "description": "x represents the context-decision pairs"}, {"entity1": "\u03c32", "entity2": "Gaussian noise", "relation": "Attribute", "description": "\u03c32 is the variance of the Gaussian noise"}, {"entity1": "\u03bc(i)T(x)", "entity2": "Posterior distribution", "relation": "Derivation", "description": "\u03bc(i)T(x) is derived from the posterior distribution"}, {"entity1": "k(i)T(x,x\u2032)", "entity2": "Posterior distribution", "relation": "Derivation", "description": "k(i)T(x,x\u2032) is derived from the posterior distribution"}, {"entity1": "Equation (3)", "entity2": "\u03bc(i)T(x)", "relation": "Definition", "description": "Equation (3) defines \u03bc(i)T(x)"}, {"entity1": "Equation (4)", "entity2": "k(i)T(x,x\u2032)", "relation": "Definition", "description": "Equation (4) defines k(i)T(x,x\u2032)"}, {"entity1": "Safe set", "entity2": "Control policies", "relation": "Dependency", "description": "The safe set depends on the control policies"}, {"entity1": "Safe set", "entity2": "System constraints", "relation": "Constraint", "description": "The safe set is constrained by the system constraints"}, {"entity1": "Local resources", "entity2": "Cloud-based resources", "relation": "Alternative", "description": "Local resources and cloud-based resources are alternatives"}, {"entity1": "Network delay", "entity2": "Response time constraint", "relation": "Impact", "description": "Network delay impacts the response time constraint"}, {"entity1": "Prior", "entity2": "Posterior distribution", "relation": "Update", "description": "The prior is updated to obtain the posterior distribution"}, {"entity1": "XT", "entity2": "x", "relation": "Collection", "description": "XT is a collection of x"}, {"entity1": "KT", "entity2": "k(i)T(x,x\u2032)", "relation": "Component", "description": "KT is a component of k(i)T(x,x\u2032)"}, {"entity1": "IT", "entity2": "KT", "relation": "Attribute", "description": "IT is an attribute of KT"}], "cc3d1224-b5b0-4a70-9093-d16529bee78c": [{"entity1": "Li et al.", "entity2": "AdaptiveEdge SafeOBO", "relation": "Author-Role", "description": "Li et al. proposed the AdaptiveEdge SafeOBO algorithm."}, {"entity1": "\ud835\udf07(\ud835\udc56) \ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)", "entity2": "Gaussian Process", "relation": "Methodology", "description": "\ud835\udf07(\ud835\udc56) \ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65) is estimated using Gaussian Process."}, {"entity1": "\ud835\udf0e(\ud835\udc56) \ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65)", "entity2": "Gaussian Process", "relation": "Methodology", "description": "\ud835\udf0e(\ud835\udc56) \ud835\udc61 (\ud835\udc50\ud835\udc61,\ud835\udc65) is estimated using Gaussian Process."}, {"entity1": "\ud835\udefd", "entity2": "Exploration", "relation": "Parameter", "description": "\ud835\udefd is the exploration parameter that balances exploration and exploitation."}, {"entity1": "\ud835\udf0cmin", "entity2": "System Constraints", "relation": "Constraint", "description": "\ud835\udf0cmin is the minimum accuracy required by the system constraints."}, {"entity1": "\u210emax", "entity2": "System Constraints", "relation": "Constraint", "description": "\u210emax is the maximum response time allowed by the system constraints."}, {"entity1": "\ud835\udc65\ud835\udc61", "entity2": "X", "relation": "Element", "description": "\ud835\udc65\ud835\udc61 is an element of the control space X."}, {"entity1": "\ud835\udc50\ud835\udc61", "entity2": "Context", "relation": "Instance", "description": "\ud835\udc50\ud835\udc61 is an instance of the context at time \ud835\udc61."}, {"entity1": "AdaptiveEdge SafeOBO", "entity2": "Gaussian Process", "relation": "Tool/Resource", "description": "AdaptiveEdge SafeOBO uses Gaussian Process for estimation."}, {"entity1": "\ud835\udf0c\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61)", "entity2": "Accuracy", "relation": "Measurement", "description": "\ud835\udf0c\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61) measures the accuracy at time \ud835\udc61."}, {"entity1": "\u210e\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61)", "entity2": "Response Time", "relation": "Measurement", "description": "\u210e\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61) measures the response time at time \ud835\udc61."}, {"entity1": "\ud835\udc62\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61)", "entity2": "Cost", "relation": "Measurement", "description": "\ud835\udc62\ud835\udc61(\ud835\udc50\ud835\udc61,\ud835\udc65\ud835\udc61) measures the cost at time \ud835\udc61."}], "8a195056-75d2-425d-ad5b-f579490c32bf": [{"entity1": "AdaptiveEdge SafeOBO Algorithm", "entity2": "Gaussian Process", "relation": "Utilization", "description": "The AdaptiveEdge SafeOBO Algorithm utilizes the Gaussian Process posteriors for the cost, accuracy, and response time functions."}, {"entity1": "Gaussian Process", "entity2": "GP posteriors", "relation": "Update", "description": "The Gaussian Process posteriors are updated based on the observations of response time, accuracy, and total cost."}, {"entity1": "AdaptiveEdge SafeOBO Algorithm", "entity2": "Exploration phase", "relation": "Transition", "description": "The AdaptiveEdge SafeOBO Algorithm transitions from the exploration phase to the exploitation phase."}, {"entity1": "Exploration phase", "entity2": "Random exploration", "relation": "Methodology", "description": "In the exploration phase, the algorithm uses random exploration to select decisions."}, {"entity1": "Exploitation phase", "entity2": "Strategic decision-making", "relation": "Methodology", "description": "In the exploitation phase, the algorithm uses strategic decision-making to select decisions."}, {"entity1": "AdaptiveEdge SafeOBO Algorithm", "entity2": "Safe set", "relation": "Estimation", "description": "The AdaptiveEdge SafeOBO Algorithm estimates a safe set of context-decision pairs that meet the system's constraints on accuracy and response time."}, {"entity1": "Safe set", "entity2": "Minimum accuracy", "relation": "Constraint", "description": "The safe set includes decisions expected to satisfy the minimum accuracy constraint."}, {"entity1": "Safe set", "entity2": "Maximum response time", "relation": "Constraint", "description": "The safe set includes decisions expected to satisfy the maximum response time constraint."}, {"entity1": "AdaptiveEdge SafeOBO Algorithm", "entity2": "Acquisition function", "relation": "Utilization", "description": "The AdaptiveEdge SafeOBO Algorithm utilizes an acquisition function to select the next decision."}, {"entity1": "Acquisition function", "entity2": "GP posteriors", "relation": "Dependency", "description": "The acquisition function depends on the GP posteriors."}, {"entity1": "AdaptiveEdge SafeOBO Algorithm", "entity2": "Total cost", "relation": "Minimization", "description": "The AdaptiveEdge SafeOBO Algorithm aims to minimize the total cost."}, {"entity1": "Total cost", "entity2": "Resource cost", "relation": "Component", "description": "The total cost includes the resource cost."}, {"entity1": "Total cost", "entity2": "Delay cost", "relation": "Component", "description": "The total cost includes the delay cost."}], "9372eccf-2134-4691-87a2-0a842ffacae0": [{"entity1": "\ud835\udc66(1)", "entity2": "\ud835\udc61", "relation": "Update", "description": "Update accuracy posterior"}, {"entity1": "\ud835\udc66(2)", "entity2": "\ud835\udc61", "relation": "Update", "description": "Update response time posterior"}, {"entity1": "\ud835\udc46\ud835\udc61", "entity2": "\ud835\udc460", "relation": "Extension", "description": "Extension of the safe set"}, {"entity1": "\ud835\udf07(1)\ud835\udc61", "entity2": "\ud835\udf0e(1)\ud835\udc61", "relation": "Trade-off", "description": "Trade-off between mean and variance in the safe set"}, {"entity1": "\ud835\udf07(2)\ud835\udc61", "entity2": "\ud835\udf0e(2)\ud835\udc61", "relation": "Trade-off", "description": "Trade-off between mean and variance in the safe set"}, {"entity1": "\ud835\udc65\ud835\udc61", "entity2": "\ud835\udc46\ud835\udc61", "relation": "Selection", "description": "Selection of the decision that minimizes the expected total cost"}, {"entity1": "acquisition function", "entity2": "GP model", "relation": "Optimization", "description": "Optimization of the acquisition function using the GP model"}, {"entity1": "\ud835\udf07(0)\ud835\udc61", "entity2": "\ud835\udf0e(0)\ud835\udc61", "relation": "Trade-off", "description": "Trade-off between mean and variance in the acquisition function"}, {"entity1": "\ud835\udefd\ud835\udc61", "entity2": "exploration", "relation": "Control", "description": "Control of the trade-off between exploration and exploitation"}, {"entity1": "\ud835\udefd\ud835\udc61", "entity2": "exploitation", "relation": "Control", "description": "Control of the trade-off between exploration and exploitation"}, {"entity1": "Equation (7)", "entity2": "\ud835\udc46\ud835\udc61", "relation": "Definition", "description": "Definition of the safe set"}, {"entity1": "Equation (8)", "entity2": "acquisition function", "relation": "Definition", "description": "Definition of the acquisition function"}], "30e7b354-cbbd-4eb2-9ced-dd8f94384345": [{"entity1": "EACO-RAG", "entity2": "GP", "relation": "Methodology", "description": "EACO-RAG uses GP models to refine decision-making over time."}, {"entity1": "EACO-RAG", "entity2": "safety", "relation": "Objective", "description": "EACO-RAG aims to optimize safety and utility over time."}, {"entity1": "EACO-RAG", "entity2": "utility", "relation": "Objective", "description": "EACO-RAG aims to optimize safety and utility over time."}, {"entity1": "EACO-RAG", "entity2": "accuracy", "relation": "Constraint", "description": "EACO-RAG respects accuracy constraints."}, {"entity1": "EACO-RAG", "entity2": "response time", "relation": "Constraint", "description": "EACO-RAG respects response time constraints."}, {"entity1": "Edge 3B Model", "entity2": "edge", "relation": "Deployment", "description": "The Edge 3B Model is deployed on edge nodes."}, {"entity1": "Cloud 72B Model", "entity2": "cloud", "relation": "Deployment", "description": "The Cloud 72B Model is deployed in the cloud."}, {"entity1": "Edge 3B Model", "entity2": "NVIDIA RTX 4090", "relation": "Tool/Resource", "description": "The Edge 3B Model is powered by NVIDIA RTX 4090 GPUs."}, {"entity1": "EACO-RAG", "entity2": "Edge 3B Model", "relation": "Component", "description": "The Edge 3B Model is a component of the EACO-RAG system."}, {"entity1": "EACO-RAG", "entity2": "Cloud 72B Model", "relation": "Component", "description": "The Cloud 72B Model is a component of the EACO-RAG system."}, {"entity1": "EACO-RAG", "entity2": "\ud835\udeff2", "relation": "Experiment-Design", "description": "EACO-RAG evaluates the impact of varying delay cost weights \ud835\udeff2."}, {"entity1": "EACO-RAG", "entity2": "\ud835\udc470", "relation": "Experiment-Design", "description": "EACO-RAG evaluates the convergence with different exploration rounds \ud835\udc470."}], "44b06616-b200-45e9-80fd-ce11b1955be7": [{"entity1": "EACO-RAG algorithm", "entity2": "convergence rate", "relation": "Methodology", "description": "The EACO-RAG algorithm's convergence rate is affected by the number of exploration rounds (\ud835\udc470)."}, {"entity1": "\ud835\udc470", "entity2": "convergence rate", "relation": "Experiment-Outcome", "description": "Increasing \ud835\udc470 initially accelerates the convergence rate, but then slows it down."}, {"entity1": "\ud835\udc470", "entity2": "exploration rounds", "relation": "Research Area", "description": "The number of exploration rounds (\ud835\udc470) is a key parameter in the experiment."}, {"entity1": "delay cost weights (\ud835\udeff2)", "entity2": "convergence rate", "relation": "Experiment-Outcome", "description": "The delay cost weights (\ud835\udeff2) affect the convergence rate of the system."}, {"entity1": "NVIDIA A100 Tensor Core", "entity2": "TFLOPS", "relation": "Tool/Resource", "description": "The NVIDIA A100 Tensor Core has a TFLOPS consumption of 9.70."}, {"entity1": "NVIDIA Tesla P100", "entity2": "TFLOPS", "relation": "Tool/Resource", "description": "The NVIDIA Tesla P100 has a TFLOPS consumption of 4.70."}, {"entity1": "NVIDIA Tesla V100", "entity2": "TFLOPS", "relation": "Tool/Resource", "description": "The NVIDIA Tesla V100 has a TFLOPS consumption of 7.80."}, {"entity1": "NVIDIA H100 Tensor Core", "entity2": "TFLOPS", "relation": "Tool/Resource", "description": "The NVIDIA H100 Tensor Core has a TFLOPS consumption of 60.00."}, {"entity1": "NVIDIA GeForce RTX 4090", "entity2": "TFLOPS", "relation": "Tool/Resource", "description": "The NVIDIA GeForce RTX 4090 has a TFLOPS consumption of 1.29."}, {"entity1": "equation: cost = f(\ud835\udc470, \ud835\udeff2, iterations)", "entity2": "cost", "relation": "Mathematical Model", "description": "The equation models the cost as a function of \ud835\udc470, \ud835\udeff2, and iterations."}, {"entity1": "Figure 6", "entity2": "convergence rate", "relation": "Result", "description": "Figure 6 illustrates the convergence rate of the system for different \ud835\udc470 values."}, {"entity1": "Figure 7", "entity2": "delay cost weights (\ud835\udeff2)", "relation": "Result", "description": "Figure 7 presents the system's performance under different delay cost weights (\ud835\udeff2)."}, {"entity1": "Table 2", "entity2": "TFLOPS", "relation": "Result", "description": "Table 2 lists the TFLOPS consumption per second of waiting for various server-side NVIDIA GPUs."}, {"entity1": "EACO-RAG algorithm", "entity2": "robustness", "relation": "Conclusion", "description": "The EACO-RAG algorithm demonstrates robustness, as it converges to a similar cost level regardless of the \ud835\udc470 value."}], "f4d546f2-c89b-4f23-91a4-5976c8240ce5": [{"entity1": "Cost", "entity2": "\ud835\udeff2", "relation": "Cost variation as a function of", "description": "The cost varies with \ud835\udeff2 under different accuracy and delay thresholds, showing a nonlinear relationship."}, {"entity1": "Cost", "entity2": "Accuracy", "relation": "Trade-off", "description": "There is a trade-off between cost and accuracy, with stricter accuracy thresholds leading to higher costs."}, {"entity1": "Cost", "entity2": "Delay", "relation": "Trade-off", "description": "There is a trade-off between cost and delay, with increased weighting of delay in the cost function leading to higher costs but lower delays."}, {"entity1": "EACO-RAG", "entity2": "LLM-3b", "relation": "Comparison", "description": "EACO-RAG has higher accuracy than LLM-3b but also higher cost and delay."}, {"entity1": "EACO-RAG", "entity2": "KGRAG-3b", "relation": "Comparison", "description": "EACO-RAG achieves a 76.7% reduction in cost and a 74.2% reduction in delay compared to KGRAG-3b, while sacrificing 11.5% in accuracy."}, {"entity1": "EACO-RAG", "entity2": "RAG-3b", "relation": "Comparison", "description": "EACO-RAG has higher accuracy than RAG-3b."}, {"entity1": "KGRAG-3b", "entity2": "KGRAG-72b", "relation": "Comparison", "description": "KGRAG-72b has higher cost and delay than KGRAG-3b due to its use of a large model and complex knowledge graph retrieval process."}, {"entity1": "Li et al.", "entity2": "EACO-RAG", "relation": "Author-Role", "description": "Li et al. proposed the EACO-RAG method."}, {"entity1": "Server GPUs", "entity2": "FP64", "relation": "Application", "description": "Server GPUs are used for double-precision (FP64) computations."}, {"entity1": "Table 2", "entity2": "TFLOPs", "relation": "Reference", "description": "Table 2 details the typical TFLOP performance of server GPUs in double-precision (FP64) computations."}, {"entity1": "Figure 8", "entity2": "Cost variation", "relation": "Illustration", "description": "Figure 8 illustrates the cost variation as a function of \ud835\udeff2 under different accuracy and delay thresholds."}, {"entity1": "Figure 9", "entity2": "Performance Comparison", "relation": "Illustration", "description": "Figure 9 illustrates the performance comparison of LLM, RAG, KGRAG, and EACO-RAG methods across cost, delay, and accuracy metrics."}], "86f06f9e-5553-402c-b080-cd333eae9c2a": [{"entity1": "EACO-RAG", "entity2": "RAG-3B", "relation": "Comparison", "description": "EACO-RAG achieves a 51% improvement in accuracy compared to RAG-3B with comparable cost and delay."}, {"entity1": "EACO-RAG", "entity2": "LLM-3B", "relation": "Comparison", "description": "EACO-RAG has significantly higher accuracy than LLM-3B."}, {"entity1": "EACO-RAG", "entity2": "KGRAG-3B", "relation": "Comparison", "description": "EACO-RAG's accuracy approaches the performance of KGRAG-3B."}, {"entity1": "Adaptive RAG", "entity2": "Corrective RAG", "relation": "Methodology", "description": "Both Adaptive RAG and Corrective RAG are extensions of Retrieval-Augmented Generation that address retrieval strategy limitations and query complexity."}, {"entity1": "Mallen et al.", "entity2": "Query complexity", "relation": "Research Area", "description": "Mallen et al. classify query complexity using entity frequency."}, {"entity1": "Qi et al.", "entity2": "Retrieval-Augmented Generation", "relation": "Methodology", "description": "Qi et al. use fixed operations (retrieval, reading, reranking) for Retrieval-Augmented Generation."}, {"entity1": "Self-RAG", "entity2": "Retrieval-Augmented Generation", "relation": "Methodology", "description": "Self-RAG retrieves, critiques, and generates dynamically for Retrieval-Augmented Generation."}, {"entity1": "EACO-RAG", "entity2": "Edge computing", "relation": "Tool/Resource", "description": "EACO-RAG uses edge computing to distribute knowledge across multiple nodes."}, {"entity1": "Model quantization", "entity2": "LLM deployment", "relation": "Methodology", "description": "Model quantization is a technique used to reduce LLM deployment costs."}, {"entity1": "Model pruning", "entity2": "LLM deployment", "relation": "Methodology", "description": "Model pruning is a technique used to reduce LLM deployment costs."}, {"entity1": "LLM distillation", "entity2": "LLM deployment", "relation": "Methodology", "description": "LLM distillation is a technique used to reduce LLM deployment costs."}, {"entity1": "FrugalGPT", "entity2": "Prompt engineering", "relation": "Methodology", "description": "FrugalGPT uses prompt engineering and model multiplexing to select model size based on query complexity."}, {"entity1": "EACO-RAG", "entity2": "6G edge networks", "relation": "Application", "description": "EACO-RAG is integrated with 6G edge networks."}], "0efd9a04-6be2-4908-b1af-a289f3ec01f1": [{"entity1": "LLM distillation", "entity2": "computational costs", "relation": "Reduction", "description": "LLM distillation reduces computational costs."}, {"entity1": "model quantization", "entity2": "computational costs", "relation": "Reduction", "description": "Model quantization reduces computational costs."}, {"entity1": "EACO-RAG", "entity2": "edge and cloud resources", "relation": "Combination", "description": "EACO-RAG combines edge and cloud resources to optimize retrieval and generation."}, {"entity1": "EACO-RAG", "entity2": "multi-armed bandit framework", "relation": "Utilization", "description": "EACO-RAG utilizes a multi-armed bandit framework to minimize resource use and operational costs."}, {"entity1": "Resource Allocation Strategies", "entity2": "edge computing", "relation": "Optimization", "description": "Resource Allocation Strategies optimize resource allocation in edge computing."}, {"entity1": "task offloading", "entity2": "latency", "relation": "Minimization", "description": "Task offloading aims to minimize latency."}, {"entity1": "task offloading", "entity2": "energy consumption", "relation": "Minimization", "description": "Task offloading aims to minimize energy consumption."}, {"entity1": "resource scheduling", "entity2": "latency", "relation": "Minimization", "description": "Resource scheduling aims to minimize latency."}, {"entity1": "resource scheduling", "entity2": "energy consumption", "relation": "Minimization", "description": "Resource scheduling aims to minimize energy consumption."}, {"entity1": "edge-cloud collaboration", "entity2": "edge nodes", "relation": "Resource Allocation", "description": "Edge-cloud collaboration dynamically allocates resources between edge nodes and cloud servers."}, {"entity1": "edge-cloud collaboration", "entity2": "cloud servers", "relation": "Resource Allocation", "description": "Edge-cloud collaboration dynamically allocates resources between edge nodes and cloud servers."}, {"entity1": "AI", "entity2": "heterogeneous edge environments", "relation": "Improvement", "description": "Recent advancements in AI improve adaptability and scalability in heterogeneous edge environments."}, {"entity1": "reinforcement learning", "entity2": "heterogeneous edge environments", "relation": "Improvement", "description": "Recent advancements in reinforcement learning improve adaptability and scalability in heterogeneous edge environments."}, {"entity1": "delay", "entity2": "energy", "relation": "Balancing", "description": "Optimizing resource allocation in edge computing focuses on balancing delay, energy, and processing power."}, {"entity1": "delay", "entity2": "processing power", "relation": "Balancing", "description": "Optimizing resource allocation in edge computing focuses on balancing delay, energy, and processing power."}, {"entity1": "energy", "entity2": "processing power", "relation": "Balancing", "description": "Optimizing resource allocation in edge computing focuses on balancing delay, energy, and processing power."}], "bf1a2254-3513-42a4-9c03-e3485f1f29bb": [{"entity1": "EACO-RAG", "entity2": "edge computing", "relation": "Application", "description": "EACO-RAG utilizes edge computing for task offloading and adaptive knowledge updates."}, {"entity1": "EACO-RAG", "entity2": "contextual multi-armed bandit framework", "relation": "Methodology", "description": "EACO-RAG employs a contextual multi-armed bandit framework for optimizing performance."}, {"entity1": "EACO-RAG", "entity2": "Safe Online Bayesian Optimization", "relation": "Methodology", "description": "EACO-RAG uses Safe Online Bayesian Optimization for balancing exploration and exploitation."}, {"entity1": "EACO-RAG", "entity2": "RAG", "relation": "Innovation", "description": "EACO-RAG is an innovative edge-assisted collaborative RAG system."}, {"entity1": "Y. Wu", "entity2": "Advances in Neural Information Processing Systems", "relation": "Publication Venue", "description": "Y. Wu published a paper in Advances in Neural Information Processing Systems."}, {"entity1": "X. Ren", "entity2": "Proceedings of the ACM on Web Conference 2024", "relation": "Publication Venue", "description": "X. Ren published a paper in Proceedings of the ACM on Web Conference 2024."}, {"entity1": "R. Yang", "entity2": "Health Care Science", "relation": "Publication Venue", "description": "R. Yang published a paper in Health Care Science."}, {"entity1": "Grand View Research", "entity2": "Retrieval augmented generation market", "relation": "Research Area", "description": "Grand View Research conducted a study on the retrieval augmented generation market."}, {"entity1": "EACO-RAG", "entity2": "edge devices", "relation": "Tool/Resource", "description": "EACO-RAG is designed to work with a network of distributed edge devices."}, {"entity1": "EACO-RAG", "entity2": "real-time applications", "relation": "Application", "description": "EACO-RAG is a scalable solution for real-time applications."}], "4b7d91bc-af61-4e6d-841a-a2ff0deb569f": [{"entity1": "D. Duvenaud", "entity2": "University of Cambridge Repository", "relation": "Affiliation", "description": "D. Duvenaud's Ph.D. dissertation is hosted at the University of Cambridge Repository."}, {"entity1": "P. Lewis", "entity2": "Advances in Neural Information Processing Systems", "relation": "Publication Venue", "description": "P. Lewis published a paper titled 'Retrieval-augmented generation for knowledge-intensive nlp tasks' in Advances in Neural Information Processing Systems."}, {"entity1": "S.-Q. Yan", "entity2": "arXiv", "relation": "Publication Venue", "description": "S.-Q. Yan published a preprint titled 'Corrective retrieval augmented generation' on arXiv."}, {"entity1": "A. Asai", "entity2": "arXiv", "relation": "Publication Venue", "description": "A. Asai published a preprint titled 'Self-rag: Learning to retrieve, generate, and critique through self-reflection' on arXiv."}, {"entity1": "A. Mallen", "entity2": "Association for Computational Linguistics", "relation": "Publication Venue", "description": "A. Mallen published a paper titled 'When not to trust language models: Investigating effectiveness of parametric and non-parametric memories' in the Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics."}, {"entity1": "P. Qi", "entity2": "Association for Computational Linguistics", "relation": "Publication Venue", "description": "P. Qi published a paper titled 'Answering open-domain questions of varying reasoning steps from text' in the Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing."}, {"entity1": "X. Huang", "entity2": "IEEE Network", "relation": "Publication Venue", "description": "X. Huang published a paper titled 'Toward effective retrieval augmented generative services in 6g networks' in IEEE Network."}, {"entity1": "G. Xiao", "entity2": "International Conference on Machine Learning", "relation": "Publication Venue", "description": "G. Xiao published a paper titled 'Smoothquant: Accurate and efficient post-training quantization for large language models' in the International Conference on Machine Learning."}, {"entity1": "G. Park", "entity2": "The Twelfth International Conference on Learning Representations", "relation": "Publication Venue", "description": "G. Park published a paper titled 'Lut-gemm: Quantized matrix multiplication based on luts for efficient inference in large-scale generative language models' in The Twelfth International Conference on Learning Representations."}, {"entity1": "X. Ma", "entity2": "Advances in Neural Information Processing Systems", "relation": "Publication Venue", "description": "X. Ma published a paper titled 'Llm-pruner: On the structural pruning of large language models' in Advances in Neural Information Processing Systems."}, {"entity1": "D. Duvenaud", "entity2": "Gaussian processes", "relation": "Research Area", "description": "D. Duvenaud's Ph.D. dissertation is about automatic model construction with Gaussian processes."}, {"entity1": "P. Lewis", "entity2": "Retrieval-augmented generation", "relation": "Research Area", "description": "P. Lewis published a paper about retrieval-augmented generation for knowledge-intensive NLP tasks."}, {"entity1": "A. Asai", "entity2": "Self-rag", "relation": "Tool/Resource", "description": "A. Asai published a preprint about Self-rag, a method for learning to retrieve, generate, and critique through self-reflection."}, {"entity1": "G. Xiao", "entity2": "Smoothquant", "relation": "Tool/Resource", "description": "G. Xiao published a paper about Smoothquant, a method for accurate and efficient post-training quantization for large language models."}, {"entity1": "G. Park", "entity2": "Lut-gemm", "relation": "Tool/Resource", "description": "G. Park published a paper about Lut-gemm, a method for quantized matrix multiplication based on LUTs for efficient inference in large-scale generative language models."}, {"entity1": "X. Ma", "entity2": "Llm-pruner", "relation": "Tool/Resource", "description": "X. Ma published a paper about Llm-pruner, a method for the structural pruning of large language models."}], "fb13a5c3-480a-43c1-a12b-d9469f22a394": [{"entity1": "X. Ma", "entity2": "Advances in neural information processing systems", "relation": "Publication Venue", "description": "X. Ma published in Advances in neural information processing systems"}, {"entity1": "I. Stogiannidis", "entity2": "arXiv", "relation": "Publication Venue", "description": "I. Stogiannidis published in arXiv"}, {"entity1": "B. Zhu", "entity2": "arXiv", "relation": "Publication Venue", "description": "B. Zhu published in arXiv"}, {"entity1": "W. Gill", "entity2": "arXiv", "relation": "Publication Venue", "description": "W. Gill published in arXiv"}, {"entity1": "J. Li", "entity2": "arXiv", "relation": "Publication Venue", "description": "J. Li published in arXiv"}, {"entity1": "Y. Liu", "entity2": "ACM SIGCOMM 2024 Conference", "relation": "Publication Venue", "description": "Y. Liu published in ACM SIGCOMM 2024 Conference"}, {"entity1": "J. Yao", "entity2": "arXiv", "relation": "Publication Venue", "description": "J. Yao published in arXiv"}, {"entity1": "L. Chen", "entity2": "arXiv", "relation": "Publication Venue", "description": "L. Chen published in arXiv"}, {"entity1": "F. Bang", "entity2": "Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023)", "relation": "Publication Venue", "description": "F. Bang published in Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023)"}, {"entity1": "S. Kim", "entity2": "arXiv", "relation": "Publication Venue", "description": "S. Kim published in arXiv"}, {"entity1": "P. Wang", "entity2": "IEEE Internet of Things Journal", "relation": "Publication Venue", "description": "P. Wang published in IEEE Internet of Things Journal"}, {"entity1": "A. Naouri", "entity2": "IEEE Internet of Things Journal", "relation": "Publication Venue", "description": "A. Naouri published in IEEE Internet of Things Journal"}, {"entity1": "H. Gu", "entity2": "Ai-enhanced cloud-edge-terminal", "relation": "Research Area", "description": "H. Gu researched in Ai-enhanced cloud-edge-terminal"}, {"entity1": "Cacheblend", "entity2": "J. Yao", "relation": "Author-Role", "description": "J. Yao is an author of Cacheblend"}, {"entity1": "Cacheblend", "entity2": "arXiv", "relation": "Publication Venue", "description": "Cacheblend published in arXiv"}, {"entity1": "Cachegen", "entity2": "Y. Liu", "relation": "Author-Role", "description": "Y. Liu is an author of Cachegen"}, {"entity1": "Cachegen", "entity2": "ACM SIGCOMM 2024 Conference", "relation": "Publication Venue", "description": "Cachegen published in ACM SIGCOMM 2024 Conference"}, {"entity1": "Scalm", "entity2": "J. Li", "relation": "Author-Role", "description": "J. Li is an author of Scalm"}, {"entity1": "Scalm", "entity2": "arXiv", "relation": "Publication Venue", "description": "Scalm published in arXiv"}, {"entity1": "Gptcache", "entity2": "F. Bang", "relation": "Author-Role", "description": "F. Bang is an author of Gptcache"}, {"entity1": "Gptcache", "entity2": "Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023)", "relation": "Publication Venue", "description": "Gptcache published in Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023)"}, {"entity1": "Frugalgpt", "entity2": "L. Chen", "relation": "Author-Role", "description": "L. Chen is an author of Frugalgpt"}, {"entity1": "Frugalgpt", "entity2": "arXiv", "relation": "Publication Venue", "description": "Frugalgpt published in arXiv"}, {"entity1": "Big little transformer decoder", "entity2": "S. Kim", "relation": "Author-Role", "description": "S. Kim is an author of Big little transformer decoder"}, {"entity1": "Big little transformer decoder", "entity2": "arXiv", "relation": "Publication Venue", "description": "Big little transformer decoder published in arXiv"}, {"entity1": "Llm-pruner", "entity2": "X. Ma", "relation": "Author-Role", "description": "X. Ma is an author of Llm-pruner"}, {"entity1": "Llm-pruner", "entity2": "Advances in neural information processing systems", "relation": "Publication Venue", "description": "Llm-pruner published in Advances in neural information processing systems"}], "fa9d47b5-2038-4356-8a39-f7e192658eb7": [{"entity1": "H. Gu", "entity2": "L. Zhao", "relation": "Co-author", "description": "H. Gu and L. Zhao are co-authors of the publication 'Ai-enhanced cloud-edge-terminal collaborative network: Survey, applications, and future directions'"}, {"entity1": "H. Gu", "entity2": "Z. Han", "relation": "Co-author", "description": "H. Gu and Z. Han are co-authors of the publication 'Ai-enhanced cloud-edge-terminal collaborative network: Survey, applications, and future directions'"}, {"entity1": "H. Gu", "entity2": "G. Zheng", "relation": "Co-author", "description": "H. Gu and G. Zheng are co-authors of the publication 'Ai-enhanced cloud-edge-terminal collaborative network: Survey, applications, and future directions'"}, {"entity1": "H. Gu", "entity2": "S. Song", "relation": "Co-author", "description": "H. Gu and S. Song are co-authors of the publication 'Ai-enhanced cloud-edge-terminal collaborative network: Survey, applications, and future directions'"}, {"entity1": "X. Xiong", "entity2": "K. Zheng", "relation": "Co-author", "description": "X. Xiong and K. Zheng are co-authors of the publication 'Resource allocation based on deep reinforcement learning in iot edge computing'"}, {"entity1": "X. Xiong", "entity2": "L. Lei", "relation": "Co-author", "description": "X. Xiong and L. Lei are co-authors of the publication 'Resource allocation based on deep reinforcement learning in iot edge computing'"}, {"entity1": "X. Xiong", "entity2": "L. Hou", "relation": "Co-author", "description": "X. Xiong and L. Hou are co-authors of the publication 'Resource allocation based on deep reinforcement learning in iot edge computing'"}, {"entity1": "IEEE Internet of Things Journal", "entity2": "mobile-edge computing", "relation": "Publication Venue", "description": "The publication 'mobile-edge computing by optimizing task offloading' was published in the IEEE Internet of Things Journal"}, {"entity1": "IEEE Communications Surveys & Tutorials", "entity2": "Ai-enhanced cloud-edge-terminal collaborative network", "relation": "Publication Venue", "description": "The publication 'Ai-enhanced cloud-edge-terminal collaborative network: Survey, applications, and future directions' was published in the IEEE Communications Surveys & Tutorials"}, {"entity1": "IEEE Journal on Selected Areas in Communications", "entity2": "Resource allocation based on deep reinforcement learning in iot edge computing", "relation": "Publication Venue", "description": "The publication 'Resource allocation based on deep reinforcement learning in iot edge computing' was published in the IEEE Journal on Selected Areas in Communications"}, {"entity1": "deep reinforcement learning", "entity2": "IoT", "relation": "Application", "description": "Deep reinforcement learning is applied in IoT edge computing"}, {"entity1": "task offloading", "entity2": "mobile-edge computing", "relation": "Methodology", "description": "Task offloading is a methodology used in mobile-edge computing"}], "e5885620-6125-42da-8f65-3b76b034d79f": [{"entity1": "RAG Foundry", "entity2": "Intel Labs", "relation": "Affiliation", "description": "RAG Foundry is affiliated with Intel Labs."}, {"entity1": "Daniel Fleischer", "entity2": "Intel Labs", "relation": "Affiliation", "description": "Daniel Fleischer is affiliated with Intel Labs."}, {"entity1": "Moshe Berchansky", "entity2": "Intel Labs", "relation": "Affiliation", "description": "Moshe Berchansky is affiliated with Intel Labs."}, {"entity1": "Moshe Wasserblat", "entity2": "Intel Labs", "relation": "Affiliation", "description": "Moshe Wasserblat is affiliated with Intel Labs."}, {"entity1": "Peter Izsak", "entity2": "Intel Labs", "relation": "Affiliation", "description": "Peter Izsak is affiliated with Intel Labs."}, {"entity1": "RAG Foundry", "entity2": "Retrieval-Augmented Generation (RAG)", "relation": "Tool/Resource", "description": "RAG Foundry is a framework for enhancing LLMs for Retrieval-Augmented Generation (RAG)."}, {"entity1": "RAG Foundry", "entity2": "Large Language Models (LLMs)", "relation": "Application", "description": "RAG Foundry is used to augment and fine-tune Large Language Models (LLMs) for RAG use cases."}, {"entity1": "Llama-3", "entity2": "RAG Foundry", "relation": "Experiment-Outcome", "description": "Llama-3 was augmented and fine-tuned using RAG Foundry, showing consistent improvements."}, {"entity1": "Phi-3", "entity2": "RAG Foundry", "relation": "Experiment-Outcome", "description": "Phi-3 was augmented and fine-tuned using RAG Foundry, showing consistent improvements."}, {"entity1": "Brown et al.", "entity2": "Large Language Models (LLMs)", "relation": "Citation", "description": "Brown et al. is cited as a reference for the capabilities of Large Language Models (LLMs)."}, {"entity1": "Kojima et al.", "entity2": "Large Language Models (LLMs)", "relation": "Citation", "description": "Kojima et al. is cited as a reference for the capabilities of Large Language Models (LLMs)."}, {"entity1": "Huang et al.", "entity2": "Large Language Models (LLMs)", "relation": "Citation", "description": "Huang et al. is cited as a reference for the limitations of Large Language Models (LLMs)."}, {"entity1": "Liu et al.", "entity2": "Large Language Models (LLMs)", "relation": "Citation", "description": "Liu et al. is cited as a reference for the limitations of Large Language Models (LLMs)."}, {"entity1": "RAG Foundry", "entity2": "https://github.com/IntelLabs/RAGFoundry", "relation": "Publication Venue", "description": "RAG Foundry is published as open-source on https://github.com/IntelLabs/RAGFoundry."}, {"entity1": "RAG Foundry", "entity2": "Retrieval-Augmented Generation (RAG)", "relation": "Research Area", "description": "RAG Foundry is a framework for enhancing LLMs for Retrieval-Augmented Generation (RAG), which is a research area."}, {"entity1": "Large Language Models (LLMs)", "entity2": "Retrieval-Augmented Generation (RAG)", "relation": "Research Area", "description": "Large Language Models (LLMs) are used in Retrieval-Augmented Generation (RAG), which is a research area."}, {"entity1": "RAG Foundry", "entity2": "LoRA", "relation": "Methodology", "description": "RAG Foundry uses LoRA as a methodology for data augmentation."}, {"entity1": "RAG Foundry", "entity2": "EM", "relation": "Methodology", "description": "RAG Foundry uses EM as a methodology for evaluation."}, {"entity1": "RAG Foundry", "entity2": "F1", "relation": "Methodology", "description": "RAG Foundry uses F1 as a methodology for evaluation."}, {"entity1": "RAG Foundry", "entity2": "Faithfulness", "relation": "Methodology", "description": "RAG Foundry uses Faithfulness as a methodology for evaluation."}, {"entity1": "RAG Foundry", "entity2": "Relevancy", "relation": "Methodology", "description": "RAG Foundry uses Relevancy as a methodology for evaluation."}, {"entity1": "RAG Foundry", "entity2": "ROUGE", "relation": "Methodology", "description": "RAG Foundry uses ROUGE as a methodology for evaluation."}], "de8f5827-7789-46d8-9a79-d718e78940c9": [{"entity1": "RAG systems", "entity2": "LLMs", "relation": "Methodology", "description": "RAG systems enhance LLMs performance by integrating external information using retrieval mechanisms."}, {"entity1": "RAG systems", "entity2": "knowledge limitations", "relation": "Challenge", "description": "RAG systems address knowledge limitations by combining retrieval that leverages vast knowledge-bases outside the knowledge of the model."}, {"entity1": "RAG systems", "entity2": "hallucinations", "relation": "Impact", "description": "RAG systems can reduce hallucinations by providing external information."}, {"entity1": "RAG systems", "entity2": "interpretability", "relation": "Result", "description": "RAG systems provide interpretability by integrating external information."}, {"entity1": "RAG systems", "entity2": "cost-efficiency", "relation": "Result", "description": "RAG systems can be vastly more cost-efficient."}, {"entity1": "Lewis et al.", "entity2": "RAG systems", "relation": "Citation", "description": "Lewis et al. is cited as a reference for RAG systems."}, {"entity1": "Mallen et al.", "entity2": "RAG systems", "relation": "Citation", "description": "Mallen et al. is cited as a reference for RAG systems."}, {"entity1": "Gao et al.", "entity2": "RAG systems", "relation": "Citation", "description": "Gao et al. is cited as a reference for RAG systems."}, {"entity1": "Asai et al.", "entity2": "RAG systems", "relation": "Citation", "description": "Asai et al. is cited as a reference for RAG systems."}, {"entity1": "Borgeaud et al.", "entity2": "RAG systems", "relation": "Citation", "description": "Borgeaud et al. is cited as a reference for RAG systems."}, {"entity1": "Peng et al.", "entity2": "RAG systems", "relation": "Citation", "description": "Peng et al. is cited as a reference for RAG systems."}, {"entity1": "de Jong et al.", "entity2": "RAG systems", "relation": "Citation", "description": "de Jong et al. is cited as a reference for RAG systems."}, {"entity1": "Yu et al.", "entity2": "RAG systems", "relation": "Citation", "description": "Yu et al. is cited as a reference for RAG systems."}, {"entity1": "Liu et al.", "entity2": "RAG systems", "relation": "Citation", "description": "Liu et al. is cited as a reference for RAG systems."}, {"entity1": "fine-tuning", "entity2": "RAG systems", "relation": "Methodology", "description": "Fine-tuning LLMs for RAG can achieve state-of-the-art performance."}, {"entity1": "state-of-the-art performance", "entity2": "RAG systems", "relation": "Result", "description": "RAG systems can achieve state-of-the-art performance."}, {"entity1": "arXiv:2408.02545v1", "entity2": "RAG systems", "relation": "Publication Venue", "description": "The paper arXiv:2408.02545v1 is published and discusses RAG systems."}], "5bffcb44-f7fc-4c99-abe6-b2b59d7cc769": [{"entity1": "RAG FOUNDRY", "entity2": "python", "relation": "Tool/Resource", "description": "RAG FOUNDRY is an open-source python framework for developing retrieval-augmented LLMs."}, {"entity1": "Barnett et al.", "entity2": "RAG design decisions", "relation": "Author-Expertise", "description": "Barnett et al. are experts in RAG design decisions, including text embedding, indexing parameters, retrieval algorithms, query building, and prompt design."}, {"entity1": "Wang et al.", "entity2": "LLM configuration", "relation": "Author-Expertise", "description": "Wang et al. are experts in LLM configuration, beyond RAG design decisions."}, {"entity1": "Chen et al.", "entity2": "evaluation suite", "relation": "Author-Expertise", "description": "Chen et al. are experts in evaluation suites for RAG systems, accounting for retrieval accuracy and generative quality."}, {"entity1": "Yu et al.", "entity2": "evaluation suite", "relation": "Author-Expertise", "description": "Yu et al. are experts in evaluation suites for RAG systems, accounting for retrieval accuracy and generative quality."}, {"entity1": "Es et al.", "entity2": "evaluation suite", "relation": "Author-Expertise", "description": "Es et al. are experts in evaluation suites for RAG systems, accounting for retrieval accuracy and generative quality."}, {"entity1": "RAG FOUNDRY", "entity2": "data selection", "relation": "Tool/Resource", "description": "RAG FOUNDRY supports data selection for RAG use cases."}, {"entity1": "RAG FOUNDRY", "entity2": "aggregation and filtering", "relation": "Tool/Resource", "description": "RAG FOUNDRY supports aggregation and filtering for RAG use cases."}, {"entity1": "RAG FOUNDRY", "entity2": "retrieval", "relation": "Tool/Resource", "description": "RAG FOUNDRY supports retrieval for RAG use cases."}, {"entity1": "RAG FOUNDRY", "entity2": "text processing", "relation": "Tool/Resource", "description": "RAG FOUNDRY supports text processing for RAG use cases."}, {"entity1": "RAG FOUNDRY", "entity2": "document ranking", "relation": "Tool/Resource", "description": "RAG FOUNDRY supports document ranking for RAG use cases."}, {"entity1": "RAG FOUNDRY", "entity2": "few-shot generation", "relation": "Tool/Resource", "description": "RAG FOUNDRY supports few-shot generation for RAG use cases."}, {"entity1": "RAG FOUNDRY", "entity2": "prompt design", "relation": "Tool/Resource", "description": "RAG FOUNDRY supports prompt design using templates for RAG use cases."}, {"entity1": "RAG FOUNDRY", "entity2": "fine-tuning", "relation": "Tool/Resource", "description": "RAG FOUNDRY supports fine-tuning for RAG use cases."}, {"entity1": "RAG FOUNDRY", "entity2": "inference", "relation": "Tool/Resource", "description": "RAG FOUNDRY supports inference for RAG use cases."}, {"entity1": "RAG FOUNDRY", "entity2": "evaluation", "relation": "Tool/Resource", "description": "RAG FOUNDRY supports evaluation for RAG use cases."}, {"entity1": "Wu", "entity2": "chain-of-thought (CoT) reasoning", "relation": "Author-Expertise", "description": "Wu is an expert in chain-of-thought (CoT) reasoning."}], "99deb00c-827d-45aa-bf1a-aa08427ece00": [{"entity1": "RAG FOUNDRY", "entity2": "retrieval", "relation": "Tool/Resource", "description": "RAG FOUNDRY facilitates experimentation on retrieval aspects of RAG."}, {"entity1": "RAG FOUNDRY", "entity2": "ranking", "relation": "Tool/Resource", "description": "RAG FOUNDRY enables experimentation on ranking aspects of RAG."}, {"entity1": "RAG FOUNDRY", "entity2": "reasoning", "relation": "Tool/Resource", "description": "RAG FOUNDRY facilitates experimentation on reasoning aspects of RAG."}, {"entity1": "Wu et al.", "entity2": "chain-of-thought reasoning", "relation": "Author-Expertise", "description": "Wu et al. are associated with chain-of-thought reasoning."}, {"entity1": "Zhang et al.", "entity2": "negative distractor-documents technique", "relation": "Author-Expertise", "description": "Zhang et al. are associated with the negative distractor-documents technique."}, {"entity1": "Liu", "entity2": "LlamaIndex", "relation": "Author-Role", "description": "Liu is the author of LlamaIndex."}, {"entity1": "Chase", "entity2": "LangChain", "relation": "Author-Role", "description": "Chase is the author of LangChain."}, {"entity1": "Pietsch et al.", "entity2": "Haystack", "relation": "Author-Role", "description": "Pietsch et al. are the authors of Haystack."}, {"entity1": "Hoshi et al.", "entity2": "RAG-based LLMs", "relation": "Author-Expertise", "description": "Hoshi et al. are associated with RAG-based LLMs."}, {"entity1": "Khattab et al.", "entity2": "LLM prompting as a programming language", "relation": "Author-Expertise", "description": "Khattab et al. are associated with representing LLM prompting as a programming language."}, {"entity1": "Saad-Falcon et al.", "entity2": "evaluation of RAG systems", "relation": "Author-Expertise", "description": "Saad-Falcon et al. are associated with evaluating RAG systems."}, {"entity1": "Hsia et al.", "entity2": "retrieval aspects of RAG", "relation": "Author-Expertise", "description": "Hsia et al. are associated with studying retrieval aspects of RAG."}, {"entity1": "Jin et al.", "entity2": "RAG building framework", "relation": "Author-Expertise", "description": "Jin et al. are associated with a RAG building framework."}, {"entity1": "Rau et al.", "entity2": "framework with extensibility-through-configuration", "relation": "Author-Expertise", "description": "Rau et al. are associated with a framework that shares a similar design principle of extensibility-through-configuration."}, {"entity1": "RAG FOUNDRY", "entity2": "custom components", "relation": "Tool/Resource", "description": "RAG FOUNDRY allows users to define custom components."}, {"entity1": "RAG FOUNDRY", "entity2": "custom pipelines", "relation": "Tool/Resource", "description": "RAG FOUNDRY enables users to define custom pipelines."}, {"entity1": "RAG FOUNDRY", "entity2": "extensibility", "relation": "Tool/Resource", "description": "RAG FOUNDRY focuses on extensibility."}, {"entity1": "RAG FOUNDRY", "entity2": "evaluation", "relation": "Tool/Resource", "description": "RAG FOUNDRY facilitates evaluation of RAG systems."}, {"entity1": "RAG FOUNDRY", "entity2": "inference", "relation": "Tool/Resource", "description": "RAG FOUNDRY is used for inference in RAG systems."}, {"entity1": "RAG FOUNDRY", "entity2": "fine-tuning", "relation": "Tool/Resource", "description": "RAG FOUNDRY is used for fine-tuning in RAG systems."}, {"entity1": "RAG FOUNDRY", "entity2": "chain-of-thought reasoning", "relation": "Tool/Resource", "description": "RAG FOUNDRY is used for chain-of-thought reasoning in RAG systems."}, {"entity1": "RAG FOUNDRY", "entity2": "negative distractor-documents technique", "relation": "Tool/Resource", "description": "RAG FOUNDRY is used with the negative distractor-documents technique."}], "33523c77-be5d-4ada-b220-24df5866ede2": [{"entity1": "HaystackRetriever", "entity2": "qdrant.yaml", "relation": "Tool/Resource", "description": "HaystackRetriever uses qdrant.yaml as its pipeline path for retrieval."}, {"entity1": "HFLoader", "entity2": "Tevatron/wikipedia-trivia", "relation": "Dataset-Origin", "description": "HFLoader loads data from the Tevatron/wikipedia-trivia dataset."}, {"entity1": "LocalLoader", "entity2": "prepared-fewshot-data.jsonl", "relation": "Dataset-Origin", "description": "LocalLoader loads data from the prepared-fewshot-data.jsonl file."}, {"entity1": "ShuffleSelect", "entity2": "HFLoader", "relation": "Methodology", "description": "ShuffleSelect is used to shuffle the data loaded by HFLoader."}, {"entity1": "FewShot", "entity2": "LocalLoader", "relation": "Methodology", "description": "FewShot is used to sample data from the fewshot-data loaded by LocalLoader."}, {"entity1": "TextPrompter", "entity2": "prompts/basic.txt", "relation": "Tool/Resource", "description": "TextPrompter uses prompts/basic.txt as its prompt file."}, {"entity1": "OutputData", "entity2": "TQA_train_processed.jsonl", "relation": "Publication Venue", "description": "OutputData saves the processed data to TQA_train_processed.jsonl."}, {"entity1": "Berchansky et al.", "entity2": "RAG", "relation": "Author-Expertise", "description": "Berchansky et al. are experts in RAG-oriented training and inference."}, {"entity1": "Liu et al.", "entity2": "RAG", "relation": "Author-Expertise", "description": "Liu et al. are experts in RAG-oriented training and inference."}, {"entity1": "Yu et al.", "entity2": "RAG", "relation": "Author-Expertise", "description": "Yu et al. are experts in RAG-oriented training and inference."}, {"entity1": "my_pipeline", "entity2": "Python", "relation": "Application", "description": "my_pipeline is a Python-based data processing pipeline."}, {"entity1": "HaystackRetriever", "entity2": "my_pipeline", "relation": "Tool/Resource", "description": "HaystackRetriever is a part of the my_pipeline data processing pipeline."}, {"entity1": "RAG", "entity2": "my_pipeline", "relation": "Application", "description": "RAG is used in the my_pipeline data processing pipeline for training and inference."}], "2421798d-ef66-4cd8-ab00-f306f6d2f7fb": [{"entity1": "Loaders", "entity2": "Hugging Face", "relation": "Affiliation", "description": "Loaders load datasets from the Hugging Face hub or from local sources."}, {"entity1": "Retrievers", "entity2": "external databases", "relation": "Tool/Resource", "description": "Retrievers integrate information from external databases, tools, libraries, and pipelines."}, {"entity1": "Samplers", "entity2": "dataset", "relation": "Methodology", "description": "Samplers collect random examples or features from any dataset to compile few-shot or negative examples."}, {"entity1": "Prompters", "entity2": "custom templates", "relation": "Tool/Resource", "description": "Prompters format prompts using custom templates and keyword mappings."}, {"entity1": "processing module", "entity2": "step caching", "relation": "Methodology", "description": "The processing module includes step caching, which caches each pipeline step locally."}, {"entity1": "pipeline", "entity2": "RAG", "relation": "Application", "description": "The modular design allows for building flexible and efficient data processes, tailored to the needs of RAG-oriented training and inference."}, {"entity1": "Q&A Dataset", "entity2": "Retrievers", "relation": "Research Area", "description": "To showcase the effectiveness of the processing module, we demonstrate how to enrich a question-answering dataset with external information."}, {"entity1": "Hugging Face", "entity2": "https://huggingface.co/", "relation": "Publication Venue", "description": "Hugging Face is accessible through the website https://huggingface.co/."}, {"entity1": "Selectors", "entity2": "dataset", "relation": "Methodology", "description": "Selectors filter examples, shuffle datasets, and select subset datasets."}, {"entity1": "processing module", "entity2": "global dataset sharing", "relation": "Methodology", "description": "The processing module supports the handling of multiple datasets at once, through global dataset sharing."}], "cd93c4cc-dc61-4d24-9522-46586cf9fd54": [{"entity1": "HFTrain", "entity2": "TRL2", "relation": "Tool/Resource", "description": "HFTrain relies on the well established training framework TRL2"}, {"entity1": "HFTrain", "entity2": "microsoft/Phi-3-mini-128k-instruct", "relation": "Model", "description": "HFTrain uses the model microsoft/Phi-3-mini-128k-instruct"}, {"entity1": "HFInference", "entity2": "microsoft/Phi-3-mini-128k-instruct", "relation": "Model", "description": "HFInference uses the model microsoft/Phi-3-mini-128k-instruct"}, {"entity1": "LORA", "entity2": "HFTrain", "relation": "Methodology", "description": "LORA is used in HFTrain for training"}, {"entity1": "paged_adamw_8bit", "entity2": "HFTrain", "relation": "Methodology", "description": "paged_adamw_8bit is used as the optimizer in HFTrain"}, {"entity1": "cosine", "entity2": "HFTrain", "relation": "Methodology", "description": "cosine is used as the learning rate scheduler in HFTrain"}, {"entity1": "qa.txt", "entity2": "HFTrain", "relation": "Input", "description": "qa.txt is used as the instruction file in HFTrain"}, {"entity1": "TQA_train_processed.jsonl", "entity2": "HFTrain", "relation": "Input", "description": "TQA_train_processed.jsonl is used as the data file in HFTrain"}, {"entity1": "Haystack", "entity2": "Pietsch et al.", "relation": "Citation", "description": "Haystack is based on the work of Pietsch et al."}, {"entity1": "Liu", "entity2": "Chase", "relation": "Comparison", "description": "Liu and Chase are compared as different retrieval processes"}, {"entity1": "Lin et al.", "entity2": "Liu", "relation": "Comparison", "description": "Lin et al. and Liu are compared as different retrieval processes"}, {"entity1": "TRL2", "entity2": "Hugging Face", "relation": "Affiliation", "description": "TRL2 is a training framework provided by Hugging Face"}, {"entity1": "HFTrain", "entity2": "Hugging Face", "relation": "Affiliation", "description": "HFTrain is a model provided by Hugging Face"}, {"entity1": "YAML", "entity2": "HFTrain", "relation": "Tool/Resource", "description": "YAML is used to define the configuration for HFTrain"}, {"entity1": "my-processed-data.jsnol", "entity2": "HFInference", "relation": "Input", "description": "my-processed-data.jsnol is used as the data file in HFInference"}, {"entity1": "model-predictions.jsonl", "entity2": "HFInference", "relation": "Output", "description": "model-predictions.jsonl is the output file of HFInference"}], "7de4d590-6feb-45d0-8e3f-4f5b08df5c36": [{"entity1": "RAG", "entity2": "LLMs", "relation": "Augmentation", "description": "The framework aims to augment LLMs for RAG techniques."}, {"entity1": "Inference", "entity2": "Evaluation", "relation": "Conceptual Separation", "description": "Inference is conceptually separated from the evaluation step due to computational demands."}, {"entity1": "Listing 3", "entity2": "Inference", "relation": "Example Configuration", "description": "Listing 3 presents an example configuration for generating predictions given a dataset."}, {"entity1": "Hu et al.", "entity2": "LoRA", "relation": "Author-Expertise", "description": "Hu et al. are associated with the development of LoRA, an advanced training technique."}, {"entity1": "Zhang et al.", "entity2": "BERTScore", "relation": "Author-Expertise", "description": "Zhang et al. are associated with the development of BERTScore, a metric for evaluation."}, {"entity1": "Hugging Face evaluate library", "entity2": "Evaluation Module", "relation": "Tool/Resource", "description": "The evaluation module uses the Hugging Face evaluate library as a resource for metrics."}, {"entity1": "DeepEval3", "entity2": "Evaluation Module", "relation": "Tool/Resource", "description": "The evaluation module uses DeepEval3 as a resource for metrics."}, {"entity1": "Recall", "entity2": "Evaluation Module", "relation": "Metric", "description": "Recall is one of the metrics used in the evaluation module for classification-based metrics."}, {"entity1": "Exact Match (EM)", "entity2": "Evaluation Module", "relation": "Metric", "description": "Exact Match (EM) is one of the metrics used in the evaluation module for individual examples."}, {"entity1": "F1", "entity2": "Evaluation Module", "relation": "Metric", "description": "F1 is one of the metrics used in the evaluation module."}, {"entity1": "Semantic Similarity", "entity2": "Evaluation Module", "relation": "Metric", "description": "Semantic Similarity is one of the metrics used in the evaluation module."}], "2fb4747f-ccc2-49e6-8586-0a9d3c37b8f1": [{"entity1": "RAG FOUNDRY", "entity2": "RAGAS", "relation": "Tool/Resource", "description": "RAG FOUNDRY is a library that uses RAGAS metrics for evaluation."}, {"entity1": "RAG FOUNDRY", "entity2": "Answer Processor", "relation": "Component", "description": "RAG FOUNDRY contains an Answer Processor for custom logic and processing."}, {"entity1": "Answer Processor", "entity2": "RegexAnswer", "relation": "Implementation", "description": "The Answer Processor can implement RegexAnswer for extracting answers."}, {"entity1": "RAG FOUNDRY", "entity2": "Es et al.", "relation": "Citation", "description": "RAG FOUNDRY uses metrics from Es et al. for evaluation."}, {"entity1": "RAG FOUNDRY", "entity2": "Zhang et al.", "relation": "Citation", "description": "RAG FOUNDRY uses fine-tuning recipes from Zhang et al. for RAG augmentation."}, {"entity1": "RAG", "entity2": "LLMs", "relation": "Methodology", "description": "RAG is used to improve LLMs through augmentation techniques."}, {"entity1": "RAG-sft", "entity2": "CoT-sft", "relation": "Comparison", "description": "RAG-sft and CoT-sft are compared in terms of their fine-tuning recipes."}, {"entity1": "Baseline", "entity2": "RAG", "relation": "Comparison", "description": "Baseline is compared to RAG in terms of their performance."}, {"entity1": "my-processed-data.jsonl", "entity2": "my-evaluation.yaml", "relation": "Input-Output", "description": "my-processed-data.jsonl is used as input to produce my-evaluation.yaml as output."}, {"entity1": "model-prediction.jsonl", "entity2": "my-evaluation.yaml", "relation": "Input-Output", "description": "model-prediction.jsonl is used as input to produce my-evaluation.yaml as output."}, {"entity1": "RAG FOUNDRY", "entity2": "microsoft/deberta-large-mnli", "relation": "Tool/Resource", "description": "RAG FOUNDRY uses microsoft/deberta-large-mnli as a model for evaluation."}, {"entity1": "RAG FOUNDRY", "entity2": "BAAI/bge-small-en-v1.5", "relation": "Tool/Resource", "description": "RAG FOUNDRY uses BAAI/bge-small-en-v1.5 as an embedding for evaluation."}, {"entity1": "Faithfulness", "entity2": "Relevancy", "relation": "Evaluation Metric", "description": "Faithfulness and Relevancy are used as evaluation metrics in RAG FOUNDRY."}, {"entity1": "RAG", "entity2": "CoT", "relation": "Methodology", "description": "RAG and CoT are used together for chain-of-thought reasoning."}], "76e455a7-d28f-46d3-96b2-12082d343903": [{"entity1": "RAG-sft", "entity2": "RAG", "relation": "Methodology", "description": "RAG-sft is a fine-tuning recipe in the RAG setup."}, {"entity1": "CoT-sft", "entity2": "CoT", "relation": "Methodology", "description": "CoT-sft is a fine-tuning recipe that complements CoT."}, {"entity1": "Zhang et al.", "entity2": "CoT-sft", "relation": "Author-Role", "description": "Zhang et al. introduced the CoT-sft fine-tuning recipe."}, {"entity1": "TriviaQA", "entity2": "Joshi et al.", "relation": "Dataset-Origin", "description": "TriviaQA is a dataset introduced by Joshi et al."}, {"entity1": "PubmedQA", "entity2": "Jin et al.", "relation": "Dataset-Origin", "description": "PubmedQA is a dataset introduced by Jin et al."}, {"entity1": "ASQA", "entity2": "Stelmakh et al.", "relation": "Dataset-Origin", "description": "ASQA is a dataset introduced by Stelmakh et al."}, {"entity1": "Llama-35", "entity2": "Touvron et al.", "relation": "Author-Expertise", "description": "Touvron et al. are associated with the Llama-35 model."}, {"entity1": "Phi-36", "entity2": "Abdin et al.", "relation": "Author-Expertise", "description": "Abdin et al. are associated with the Phi-36 model."}, {"entity1": "RAGAS", "entity2": "Es et al.", "relation": "Author-Role", "description": "Es et al. introduced the RAGAS metrics."}, {"entity1": "GPT4-32k", "entity2": "RAGAS", "relation": "Tool/Resource", "description": "GPT4-32k is used as the critic LLM for RAGAS metrics."}, {"entity1": "7BAAI/bge-small-en-v1.5", "entity2": "RAGAS", "relation": "Tool/Resource", "description": "7BAAI/bge-small-en-v1.5 is an embedder required for relevancy evaluation in RAGAS."}, {"entity1": "Faithfulness", "entity2": "RAGAS", "relation": "Methodology", "description": "Faithfulness is a metric used in RAGAS to measure the relation between generated text and context."}, {"entity1": "Relevancy", "entity2": "RAGAS", "relation": "Methodology", "description": "Relevancy is a metric used in RAGAS to measure the relation between generated text and query."}, {"entity1": "TriviaQA", "entity2": "ASQA", "relation": "Comparison", "description": "TriviaQA and ASQA are compared in the evaluation of RAG augmentation techniques."}, {"entity1": "PubmedQA", "entity2": "ASQA", "relation": "Comparison", "description": "PubmedQA and ASQA are compared in the evaluation of RAG augmentation techniques."}, {"entity1": "Llama-35", "entity2": "Phi-36", "relation": "Comparison", "description": "Llama-35 and Phi-36 are compared in the evaluation of RAG augmentation techniques."}, {"entity1": "RAG-sft", "entity2": "CoT-sft", "relation": "Comparison", "description": "RAG-sft and CoT-sft are compared in the evaluation of fine-tuning recipes."}, {"entity1": "4BAAI/llm-embedder", "entity2": "RAGAS", "relation": "Tool/Resource", "description": "4BAAI/llm-embedder is used in the evaluation of RAGAS metrics."}, {"entity1": "5meta-llama/Meta-Llama-3-8B-Instruct", "entity2": "RAGAS", "relation": "Tool/Resource", "description": "5meta-llama/Meta-Llama-3-8B-Instruct is used in the evaluation of RAGAS metrics."}, {"entity1": "6microsoft/Phi-3-mini-128k-instruct", "entity2": "RAGAS", "relation": "Tool/Resource", "description": "6microsoft/Phi-3-mini-128k-instruct is used in the evaluation of RAGAS metrics."}], "99787138-c00f-436a-bd1d-b94881423416": [{"entity1": "RAG setting", "entity2": "fine-tuning", "relation": "Methodology", "description": "Fine-tuning improves the results of the RAG setting."}, {"entity1": "CoT reasoning", "entity2": "fine-tuning", "relation": "Methodology", "description": "Fine-tuning on CoT reasoning decreases performance for TriviaQA."}, {"entity1": "RAG setting", "entity2": "retrieval", "relation": "Methodology", "description": "Retrieved context improves the results of the RAG setting."}, {"entity1": "CoT", "entity2": "faithfulness", "relation": "Result", "description": "CoT configurations produce a reasoning that can be evaluated for faithfulness."}, {"entity1": "RAG-sft", "entity2": "CoT-sft", "relation": "Comparison", "description": "RAG-sft and CoT-sft are compared in terms of performance."}, {"entity1": "TriviaQA", "entity2": "ASQA", "relation": "Comparison", "description": "The best method is model-dependent for TriviaQA, but CoT reasoning produces consistent improvement in ASQA."}, {"entity1": "PubmedQA", "entity2": "CoT reasoning", "relation": "Result", "description": "CoT reasoning improves upon the untrained RAG setting in PubmedQA."}, {"entity1": "faithfulness", "entity2": "relevancy", "relation": "Trade-off", "description": "Faithfulness and relevancy scores often do not correlate with each other, indicating a trade-off in performance."}, {"entity1": "RAG", "entity2": "CoT", "relation": "Comparison", "description": "RAG and CoT are compared in terms of performance, with CoT producing consistent improvement in ASQA."}, {"entity1": "Llama-3", "entity2": "Phi-3", "relation": "Comparison", "description": "Llama-3 and Phi-3 are compared in terms of performance, with different results for different datasets."}, {"entity1": "Baseline", "entity2": "RAG setting", "relation": "Comparison", "description": "The baseline method is compared to the RAG setting, with the RAG setting improving upon the baseline in most cases."}, {"entity1": "Es et al.", "entity2": "faithfulness", "relation": "Definition", "description": "Faithfulness is defined in Es et al. (2024)."}, {"entity1": "CoT-sft", "entity2": "fine-tuning", "relation": "Methodology", "description": "Fine-tuning of the CoT configuration shows to perform best in ASQA."}, {"entity1": "RAG-sft", "entity2": "fine-tuning", "relation": "Methodology", "description": "Fine-tuning of the RAG setting improves the results."}, {"entity1": "Table 1", "entity2": "evaluation results", "relation": "Publication Venue", "description": "Table 1 displays the evaluation results of baseline and different RAG settings."}], "3fe191ef-dbf6-4dc0-a6dd-984488059275": [{"entity1": "RAG techniques", "entity2": "performance", "relation": "Improvement", "description": "RAG techniques improve performance"}, {"entity1": "RAG systems evaluation", "entity2": "multi-aspect metrics", "relation": "Methodology", "description": "Multi-aspect metrics are used for RAG systems evaluation"}, {"entity1": "RAG FOUNDRY", "entity2": "LLMs", "relation": "Application", "description": "RAG FOUNDRY is used for fine-tuning LLMs"}, {"entity1": "RAG FOUNDRY", "entity2": "RAG techniques", "relation": "Tool/Resource", "description": "RAG FOUNDRY is a library for RAG techniques"}, {"entity1": "Q&A datasets", "entity2": "RAG systems evaluation", "relation": "Dataset-Origin", "description": "Q&A datasets are used for RAG systems evaluation"}, {"entity1": "RAG techniques", "entity2": "RAG settings", "relation": "Methodology", "description": "RAG techniques are used in RAG settings"}, {"entity1": "code repository", "entity2": "suggestions", "relation": "Collaboration-Type", "description": "The code repository accepts suggestions"}, {"entity1": "code repository", "entity2": "bug-fixes", "relation": "Collaboration-Type", "description": "The code repository accepts bug-fixes"}, {"entity1": "code repository", "entity2": "pull requests", "relation": "Collaboration-Type", "description": "The code repository accepts pull requests"}, {"entity1": "RAG FOUNDRY", "entity2": "Limitations and Future Plans", "relation": "Research Area", "description": "RAG FOUNDRY has limitations and future plans"}, {"entity1": "RAG techniques", "entity2": "trade-off", "relation": "Challenge", "description": "RAG techniques represent a trade-off in performance"}], "1f9cd042-d24b-4004-9f1a-5f1a63d3dfd0": [{"entity1": "Ethics Statement", "entity2": "integrity", "relation": "Inclusion", "description": "The Ethics Statement includes integrity as one of its core principles."}, {"entity1": "Ethics Statement", "entity2": "fairness", "relation": "Inclusion", "description": "The Ethics Statement includes fairness as one of its core principles."}, {"entity1": "Ethics Statement", "entity2": "societal benefit", "relation": "Inclusion", "description": "The Ethics Statement includes societal benefit as one of its core principles."}, {"entity1": "Research", "entity2": "data privacy", "relation": "Prioritization", "description": "The research prioritizes data privacy throughout its conduct."}, {"entity1": "Research", "entity2": "security", "relation": "Prioritization", "description": "The research prioritizes security throughout its conduct."}, {"entity1": "Methodologies", "entity2": "transparency", "relation": "Adherence", "description": "The methodologies adhere to the principle of transparency."}, {"entity1": "Methodologies", "entity2": "reproducibility", "relation": "Adherence", "description": "The methodologies adhere to the principle of reproducibility."}, {"entity1": "LLMs", "entity2": "RAG augmentation", "relation": "Advocacy", "description": "The authors advocate for the responsible use of LLMs and RAG augmentation."}, {"entity1": "LLMs", "entity2": "hallucinations", "relation": "Caution", "description": "The authors exercise caution regarding the potential hallucinations produced by LLMs."}, {"entity1": "Authors", "entity2": "References", "relation": "Citation", "description": "The authors cite the references listed."}], "5e5227bf-3c72-4793-be8c-0cddef83a306": [{"entity1": "Witte", "entity2": "Phi-3", "relation": "Author-Role", "description": "Witte is an author of the Phi-3 technical report."}, {"entity1": "Haiping Wu", "entity2": "Phi-3", "relation": "Author-Role", "description": "Haiping Wu is an author of the Phi-3 technical report."}, {"entity1": "AI@Meta", "entity2": "Llama 3", "relation": "Publication Venue", "description": "AI@Meta published the Llama 3 model card."}, {"entity1": "Akari Asai", "entity2": "Self-rag", "relation": "Author-Role", "description": "Akari Asai is an author of the Self-rag paper."}, {"entity1": "Angels Balaguer", "entity2": "RAG", "relation": "Author-Role", "description": "Angels Balaguer is an author of the RAG paper."}, {"entity1": "Scott Barnett", "entity2": "Seven failure points when engineering a retrieval augmented generation system", "relation": "Author-Role", "description": "Scott Barnett is an author of the paper on seven failure points when engineering a retrieval augmented generation system."}, {"entity1": "Moshe Berchansky", "entity2": "Cotar", "relation": "Author-Role", "description": "Moshe Berchansky is an author of the Cotar paper."}, {"entity1": "Sebastian Borgeaud", "entity2": "Improving language models by retrieving from trillions of tokens", "relation": "Author-Role", "description": "Sebastian Borgeaud is an author of the paper on improving language models by retrieving from trillions of tokens."}, {"entity1": "Tom B. Brown", "entity2": "Language Models are Few-Shot Learners", "relation": "Author-Role", "description": "Tom B. Brown is an author of the paper on language models being few-shot learners."}, {"entity1": "Phi-3", "entity2": "arXiv", "relation": "Publication Venue", "description": "The Phi-3 technical report was published on arXiv."}, {"entity1": "Self-rag", "entity2": "arXiv", "relation": "Publication Venue", "description": "The Self-rag paper was published on arXiv."}, {"entity1": "RAG", "entity2": "arXiv", "relation": "Publication Venue", "description": "The RAG paper was published on arXiv."}, {"entity1": "Seven failure points when engineering a retrieval augmented generation system", "entity2": "arXiv", "relation": "Publication Venue", "description": "The paper on seven failure points when engineering a retrieval augmented generation system was published on arXiv."}, {"entity1": "Cotar", "entity2": "arXiv", "relation": "Publication Venue", "description": "The Cotar paper was published on arXiv."}, {"entity1": "Improving language models by retrieving from trillions of tokens", "entity2": "International Conference on Machine Learning", "relation": "Publication Venue", "description": "The paper on improving language models by retrieving from trillions of tokens was published at the International Conference on Machine Learning."}, {"entity1": "Language Models are Few-Shot Learners", "entity2": "arXiv", "relation": "Publication Venue", "description": "The paper on language models being few-shot learners was published on arXiv."}, {"entity1": "Llama 3", "entity2": "AI@Meta", "relation": "Affiliation", "description": "Llama 3 is affiliated with AI@Meta."}, {"entity1": "Phi-3", "entity2": "Witte", "relation": "Author-Expertise", "description": "Witte has expertise in Phi-3."}, {"entity1": "Self-rag", "entity2": "Akari Asai", "relation": "Author-Expertise", "description": "Akari Asai has expertise in Self-rag."}, {"entity1": "RAG", "entity2": "Angels Balaguer", "relation": "Author-Expertise", "description": "Angels Balaguer has expertise in RAG."}, {"entity1": "Seven failure points when engineering a retrieval augmented generation system", "entity2": "Scott Barnett", "relation": "Author-Expertise", "description": "Scott Barnett has expertise in retrieval augmented generation systems."}, {"entity1": "Cotar", "entity2": "Moshe Berchansky", "relation": "Author-Expertise", "description": "Moshe Berchansky has expertise in Cotar."}, {"entity1": "Improving language models by retrieving from trillions of tokens", "entity2": "Sebastian Borgeaud", "relation": "Author-Expertise", "description": "Sebastian Borgeaud has expertise in improving language models by retrieving from trillions of tokens."}, {"entity1": "Language Models are Few-Shot Learners", "entity2": "Tom B. Brown", "relation": "Author-Expertise", "description": "Tom B. Brown has expertise in language models being few-shot learners."}, {"entity1": "Phi-3", "entity2": "arXiv", "relation": "Citation", "description": "The Phi-3 technical report is cited on arXiv."}, {"entity1": "Self-rag", "entity2": "arXiv", "relation": "Citation", "description": "The Self-rag paper is cited on arXiv."}, {"entity1": "RAG", "entity2": "arXiv", "relation": "Citation", "description": "The RAG paper is cited on arXiv."}, {"entity1": "Seven failure points when engineering a retrieval augmented generation system", "entity2": "arXiv", "relation": "Citation", "description": "The paper on seven failure points when engineering a retrieval augmented generation system is cited on arXiv."}, {"entity1": "Cotar", "entity2": "arXiv", "relation": "Citation", "description": "The Cotar paper is cited on arXiv."}, {"entity1": "Improving language models by retrieving from trillions of tokens", "entity2": "International Conference on Machine Learning", "relation": "Citation", "description": "The paper on improving language models by retrieving from trillions of tokens is cited at the International Conference on Machine Learning."}, {"entity1": "Language Models are Few-Shot Learners", "entity2": "arXiv", "relation": "Citation", "description": "The paper on language models being few-shot learners is cited on arXiv."}], "c114abfd-2ed2-47aa-910a-ceab86457d01": [{"entity1": "Sigler", "entity2": "Language Models are Few-Shot Learners", "relation": "Author-Publication", "description": "Sigler is an author of the publication 'Language Models are Few-Shot Learners'."}, {"entity1": "Mateusz Litwin", "entity2": "Language Models are Few-Shot Learners", "relation": "Author-Publication", "description": "Mateusz Litwin is an author of the publication 'Language Models are Few-Shot Learners'."}, {"entity1": "Scott Gray", "entity2": "Language Models are Few-Shot Learners", "relation": "Author-Publication", "description": "Scott Gray is an author of the publication 'Language Models are Few-Shot Learners'."}, {"entity1": "Benjamin Chess", "entity2": "Language Models are Few-Shot Learners", "relation": "Author-Publication", "description": "Benjamin Chess is an author of the publication 'Language Models are Few-Shot Learners'."}, {"entity1": "Jack Clark", "entity2": "Language Models are Few-Shot Learners", "relation": "Author-Publication", "description": "Jack Clark is an author of the publication 'Language Models are Few-Shot Learners'."}, {"entity1": "Christopher Berner", "entity2": "Language Models are Few-Shot Learners", "relation": "Author-Publication", "description": "Christopher Berner is an author of the publication 'Language Models are Few-Shot Learners'."}, {"entity1": "Sam McCandlish", "entity2": "Language Models are Few-Shot Learners", "relation": "Author-Publication", "description": "Sam McCandlish is an author of the publication 'Language Models are Few-Shot Learners'."}, {"entity1": "Alec Radford", "entity2": "Language Models are Few-Shot Learners", "relation": "Author-Publication", "description": "Alec Radford is an author of the publication 'Language Models are Few-Shot Learners'."}, {"entity1": "Ilya Sutskever", "entity2": "Language Models are Few-Shot Learners", "relation": "Author-Publication", "description": "Ilya Sutskever is an author of the publication 'Language Models are Few-Shot Learners'."}, {"entity1": "Dario Amodei", "entity2": "Language Models are Few-Shot Learners", "relation": "Author-Publication", "description": "Dario Amodei is an author of the publication 'Language Models are Few-Shot Learners'."}, {"entity1": "Harrison Chase", "entity2": "LangChain", "relation": "Author-Publication", "description": "Harrison Chase is the author of the publication 'LangChain'."}, {"entity1": "Jiawei Chen", "entity2": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "relation": "Author-Publication", "description": "Jiawei Chen is an author of the publication 'Benchmarking Large Language Models in Retrieval-Augmented Generation'."}, {"entity1": "Hongyu Lin", "entity2": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "relation": "Author-Publication", "description": "Hongyu Lin is an author of the publication 'Benchmarking Large Language Models in Retrieval-Augmented Generation'."}, {"entity1": "Xianpei Han", "entity2": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "relation": "Author-Publication", "description": "Xianpei Han is an author of the publication 'Benchmarking Large Language Models in Retrieval-Augmented Generation'."}, {"entity1": "Le Sun", "entity2": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "relation": "Author-Publication", "description": "Le Sun is an author of the publication 'Benchmarking Large Language Models in Retrieval-Augmented Generation'."}, {"entity1": "Michiel de Jong", "entity2": "Pre-computed memory", "relation": "Author-Publication", "description": "Michiel de Jong is an author of the publication 'Pre-computed memory'."}, {"entity1": "Yury Zemlyanskiy", "entity2": "Pre-computed memory", "relation": "Author-Publication", "description": "Yury Zemlyanskiy is an author of the publication 'Pre-computed memory'."}, {"entity1": "Nicholas FitzGerald", "entity2": "Pre-computed memory", "relation": "Author-Publication", "description": "Nicholas FitzGerald is an author of the publication 'Pre-computed memory'."}, {"entity1": "Joshua Ainslie", "entity2": "Pre-computed memory", "relation": "Author-Publication", "description": "Joshua Ainslie is an author of the publication 'Pre-computed memory'."}, {"entity1": "Sumit Sanghai", "entity2": "Pre-computed memory", "relation": "Author-Publication", "description": "Sumit Sanghai is an author of the publication 'Pre-computed memory'."}, {"entity1": "Fei Sha", "entity2": "Pre-computed memory", "relation": "Author-Publication", "description": "Fei Sha is an author of the publication 'Pre-computed memory'."}, {"entity1": "William Cohen", "entity2": "Pre-computed memory", "relation": "Author-Publication", "description": "William Cohen is an author of the publication 'Pre-computed memory'."}, {"entity1": "Language Models are Few-Shot Learners", "entity2": "arXiv", "relation": "Publication Venue", "description": "The publication 'Language Models are Few-Shot Learners' is published on arXiv."}, {"entity1": "LangChain", "entity2": "arXiv", "relation": "Publication Venue", "description": "The publication 'LangChain' is published on arXiv."}, {"entity1": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "entity2": "arXiv", "relation": "Publication Venue", "description": "The publication 'Benchmarking Large Language Models in Retrieval-Augmented Generation' is published on arXiv."}, {"entity1": "Pre-computed memory", "entity2": "arXiv", "relation": "Publication Venue", "description": "The publication 'Pre-computed memory' is published on arXiv."}], "1a465845-0c96-4462-bce6-18936184463d": [{"entity1": "Shahul Es", "entity2": "RAGAs", "relation": "Author-Role", "description": "Shahul Es is an author of the paper RAGAs: Automated evaluation of retrieval augmented generation."}, {"entity1": "RAGAs", "entity2": "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations", "relation": "Publication Venue", "description": "RAGAs was published in the Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations."}, {"entity1": "Yunfan Gao", "entity2": "Retrieval-Augmented Generation for Large Language Models: A Survey", "relation": "Author-Role", "description": "Yunfan Gao is an author of the paper Retrieval-Augmented Generation for Large Language Models: A Survey."}, {"entity1": "Retrieval-Augmented Generation for Large Language Models: A Survey", "entity2": "arXiv", "relation": "Publication Venue", "description": "Retrieval-Augmented Generation for Large Language Models: A Survey was published on arXiv."}, {"entity1": "Yasuto Hoshi", "entity2": "RaLLe", "relation": "Author-Role", "description": "Yasuto Hoshi is an author of the paper RaLLe: A Framework for Developing and Evaluating Retrieval-Augmented Large Language Models."}, {"entity1": "RaLLe", "entity2": "arXiv", "relation": "Publication Venue", "description": "RaLLe was published on arXiv."}, {"entity1": "Jennifer Hsia", "entity2": "RAGGED", "relation": "Author-Role", "description": "Jennifer Hsia is an author of the paper RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems."}, {"entity1": "RAGGED", "entity2": "arXiv", "relation": "Publication Venue", "description": "RAGGED was published on arXiv."}, {"entity1": "Edward J. Hu", "entity2": "LoRA", "relation": "Author-Role", "description": "Edward J. Hu is an author of the paper LoRA: Low-Rank Adaptation of Large Language Models."}, {"entity1": "LoRA", "entity2": "arXiv", "relation": "Publication Venue", "description": "LoRA was published on arXiv."}, {"entity1": "Lei Huang", "entity2": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions", "relation": "Author-Role", "description": "Lei Huang is an author of the paper A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions."}, {"entity1": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions", "entity2": "arXiv", "relation": "Publication Venue", "description": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions was published on arXiv."}, {"entity1": "Jiajie Jin", "entity2": "FlashRAG", "relation": "Author-Role", "description": "Jiajie Jin is an author of the paper FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research."}, {"entity1": "FlashRAG", "entity2": "arXiv", "relation": "Publication Venue", "description": "FlashRAG was published on arXiv."}, {"entity1": "Qiao Jin", "entity2": "PubMedQA", "relation": "Author-Role", "description": "Qiao Jin is an author of the paper PubMedQA: A Dataset for Biomedical Research Question Answering."}, {"entity1": "PubMedQA", "entity2": "arXiv", "relation": "Publication Venue", "description": "PubMedQA was published on arXiv."}, {"entity1": "Mandar Joshi", "entity2": "TriviaQA", "relation": "Author-Role", "description": "Mandar Joshi is an author of the paper TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension."}, {"entity1": "TriviaQA", "entity2": "arXiv", "relation": "Publication Venue", "description": "TriviaQA was published on arXiv."}, {"entity1": "Omar Khattab", "entity2": "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP", "relation": "Author-Role", "description": "Omar Khattab is an author of the paper Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP."}, {"entity1": "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP", "entity2": "arXiv", "relation": "Publication Venue", "description": "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP was published on arXiv."}], "dadf97ee-929c-4f3b-8d33-b99a2d6077e8": [{"entity1": "Matei Zaharia", "entity2": "Demonstrate-search-predict", "relation": "Author-Role", "description": "Matei Zaharia is an author of the paper 'Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP'."}, {"entity1": "Omar Khattab", "entity2": "Dspy", "relation": "Author-Role", "description": "Omar Khattab is an author of the paper 'Dspy: Compiling declarative language model calls into self-improving pipelines'."}, {"entity1": "Takeshi Kojima", "entity2": "Large Language Models are Zero-Shot Reasoners", "relation": "Author-Role", "description": "Takeshi Kojima is an author of the paper 'Large Language Models are Zero-Shot Reasoners'."}, {"entity1": "Patrick Lewis", "entity2": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "relation": "Author-Role", "description": "Patrick Lewis is an author of the paper 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'."}, {"entity1": "Jimmy Lin", "entity2": "Pyserini", "relation": "Author-Role", "description": "Jimmy Lin is an author of the paper 'Pyserini: A Python toolkit for reproducible information retrieval research with sparse and dense representations'."}, {"entity1": "Jerry Liu", "entity2": "LlamaIndex", "relation": "Author-Role", "description": "Jerry Liu is the creator of LlamaIndex."}, {"entity1": "Nelson F. Liu", "entity2": "Lost in the middle: How language models use long contexts", "relation": "Author-Role", "description": "Nelson F. Liu is an author of the paper 'Lost in the middle: How language models use long contexts'."}, {"entity1": "Zihan Liu", "entity2": "ChatQA", "relation": "Author-Role", "description": "Zihan Liu is an author of the paper 'ChatQA: Surpassing GPT-4 on Conversational QA and RAG'."}, {"entity1": "Alex Troy Mallen", "entity2": "When not to trust language models: Investigating effectiveness of parametric and non-parametric memories", "relation": "Author-Role", "description": "Alex Troy Mallen is an author of the paper 'When not to trust language models: Investigating effectiveness of parametric and non-parametric memories'."}, {"entity1": "Baolin Peng", "entity2": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback", "relation": "Author-Role", "description": "Baolin Peng is an author of the paper 'Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback'."}, {"entity1": "Malte Pietsch", "entity2": "Haystack", "relation": "Author-Role", "description": "Malte Pietsch is an author of the paper 'Haystack: the end-to-end NLP framework for pragmatic builders'."}, {"entity1": "Matei Zaharia", "entity2": "arXiv", "relation": "Publication Venue", "description": "Matei Zaharia published the paper 'Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP' on arXiv."}, {"entity1": "Omar Khattab", "entity2": "arXiv", "relation": "Publication Venue", "description": "Omar Khattab published the paper 'Dspy: Compiling declarative language model calls into self-improving pipelines' on arXiv."}, {"entity1": "Takeshi Kojima", "entity2": "ArXiv", "relation": "Publication Venue", "description": "Takeshi Kojima published the paper 'Large Language Models are Zero-Shot Reasoners' on ArXiv."}, {"entity1": "Patrick Lewis", "entity2": "arXiv", "relation": "Publication Venue", "description": "Patrick Lewis published the paper 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks' on arXiv."}, {"entity1": "Jimmy Lin", "entity2": "ACM SIGIR Conference on Research and Development in Information Retrieval", "relation": "Publication Venue", "description": "Jimmy Lin published the paper 'Pyserini: A Python toolkit for reproducible information retrieval research with sparse and dense representations' at the ACM SIGIR Conference on Research and Development in Information Retrieval."}, {"entity1": "Zihan Liu", "entity2": "arXiv", "relation": "Publication Venue", "description": "Zihan Liu published the paper 'ChatQA: Surpassing GPT-4 on Conversational QA and RAG' on arXiv."}, {"entity1": "Alex Troy Mallen", "entity2": "Annual Meeting of the Association for Computational Linguistics", "relation": "Publication Venue", "description": "Alex Troy Mallen published the paper 'When not to trust language models: Investigating effectiveness of parametric and non-parametric memories' at the Annual Meeting of the Association for Computational Linguistics."}, {"entity1": "Baolin Peng", "entity2": "arXiv", "relation": "Publication Venue", "description": "Baolin Peng published the paper 'Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback' on arXiv."}, {"entity1": "Malte Pietsch", "entity2": "Haystack", "relation": "Tool/Resource", "description": "Malte Pietsch is an author of the paper 'Haystack: the end-to-end NLP framework for pragmatic builders', which introduces the Haystack tool."}, {"entity1": "Jerry Liu", "entity2": "LlamaIndex", "relation": "Tool/Resource", "description": "Jerry Liu is the creator of the LlamaIndex tool."}, {"entity1": "Jimmy Lin", "entity2": "Pyserini", "relation": "Tool/Resource", "description": "Jimmy Lin is an author of the paper 'Pyserini: A Python toolkit for reproducible information retrieval research with sparse and dense representations', which introduces the Pyserini tool."}, {"entity1": "Zihan Liu", "entity2": "ChatQA", "relation": "Tool/Resource", "description": "Zihan Liu is an author of the paper 'ChatQA: Surpassing GPT-4 on Conversational QA and RAG', which introduces the ChatQA tool."}, {"entity1": "GPT-4", "entity2": "ChatQA", "relation": "Comparison", "description": "The paper 'ChatQA: Surpassing GPT-4 on Conversational QA and RAG' compares the performance of ChatQA and GPT-4."}], "63f5f060-0c8c-4b32-af89-debaeb585575": [{"entity1": "Risch", "entity2": "Haystack", "relation": "Author-Role", "description": "Risch is an author of the Haystack framework"}, {"entity1": "Massimiliano Pippi", "entity2": "Haystack", "relation": "Author-Role", "description": "Massimiliano Pippi is an author of the Haystack framework"}, {"entity1": "Mayank Jobanputra", "entity2": "Haystack", "relation": "Author-Role", "description": "Mayank Jobanputra is an author of the Haystack framework"}, {"entity1": "Sara Zanzottera", "entity2": "Haystack", "relation": "Author-Role", "description": "Sara Zanzottera is an author of the Haystack framework"}, {"entity1": "Silvano Cerza", "entity2": "Haystack", "relation": "Author-Role", "description": "Silvano Cerza is an author of the Haystack framework"}, {"entity1": "Vladimir Blagojevic", "entity2": "Haystack", "relation": "Author-Role", "description": "Vladimir Blagojevic is an author of the Haystack framework"}, {"entity1": "Thomas Stadelmann", "entity2": "Haystack", "relation": "Author-Role", "description": "Thomas Stadelmann is an author of the Haystack framework"}, {"entity1": "Tanay Soni", "entity2": "Haystack", "relation": "Author-Role", "description": "Tanay Soni is an author of the Haystack framework"}, {"entity1": "Sebastian Lee", "entity2": "Haystack", "relation": "Author-Role", "description": "Sebastian Lee is an author of the Haystack framework"}, {"entity1": "David Rau", "entity2": "BERGEN", "relation": "Author-Role", "description": "David Rau is an author of the BERGEN library"}, {"entity1": "Herv'e D'ejean", "entity2": "BERGEN", "relation": "Author-Role", "description": "Herv'e D'ejean is an author of the BERGEN library"}, {"entity1": "Nadezhda Chirkova", "entity2": "BERGEN", "relation": "Author-Role", "description": "Nadezhda Chirkova is an author of the BERGEN library"}, {"entity1": "Thibault Formal", "entity2": "BERGEN", "relation": "Author-Role", "description": "Thibault Formal is an author of the BERGEN library"}, {"entity1": "Shuai Wang", "entity2": "BERGEN", "relation": "Author-Role", "description": "Shuai Wang is an author of the BERGEN library"}, {"entity1": "Vassilina Nikoulina", "entity2": "BERGEN", "relation": "Author-Role", "description": "Vassilina Nikoulina is an author of the BERGEN library"}, {"entity1": "S. Clinchant", "entity2": "BERGEN", "relation": "Author-Role", "description": "S. Clinchant is an author of the BERGEN library"}, {"entity1": "Haystack", "entity2": "2019", "relation": "Publication Date", "description": "Haystack was published in 2019"}, {"entity1": "BERGEN", "entity2": "2024", "relation": "Publication Date", "description": "BERGEN was published in 2024"}], "ed93135e-501c-49ee-87f4-ba92a37fa969": [{"entity1": "LoRA", "entity2": "r", "relation": "Parameter Value", "description": "LoRA has a parameter value of r = 16"}, {"entity1": "LoRA", "entity2": "\u03b1", "relation": "Parameter Value", "description": "LoRA has a parameter value of \u03b1 = 16"}, {"entity1": "LoRA", "entity2": "Dropout", "relation": "Parameter Value", "description": "LoRA has a parameter value of Dropout = 0.1"}, {"entity1": "LoRA", "entity2": "Bias", "relation": "Parameter Value", "description": "LoRA has a parameter value of Bias = None"}, {"entity1": "LoRA", "entity2": "qkv_proj", "relation": "Modules", "description": "LoRA includes the module qkv_proj"}, {"entity1": "LoRA", "entity2": "Phi-3", "relation": "Modules", "description": "LoRA includes the module Phi-3"}, {"entity1": "LoRA", "entity2": "q/v_proj", "relation": "Modules", "description": "LoRA includes the module q/v_proj"}, {"entity1": "LoRA", "entity2": "Llama-3", "relation": "Modules", "description": "LoRA includes the module Llama-3"}, {"entity1": "LR", "entity2": "1e-4", "relation": "Parameter Value", "description": "LR has a parameter value of 1e-4"}, {"entity1": "LR", "entity2": "cosine", "relation": "LR Scheduler", "description": "LR uses the cosine scheduler"}, {"entity1": "Warmup Ratio", "entity2": "0.03", "relation": "Parameter Value", "description": "Warmup Ratio has a parameter value of 0.03"}, {"entity1": "Weight Decay", "entity2": "0.001", "relation": "Parameter Value", "description": "Weight Decay has a parameter value of 0.001"}, {"entity1": "Batch Size", "entity2": "1", "relation": "Parameter Value", "description": "Batch Size has a parameter value of 1"}, {"entity1": "Epochs", "entity2": "1", "relation": "Parameter Value", "description": "Epochs has a parameter value of 1"}, {"entity1": "TriviaQA", "entity2": "6000", "relation": "Training Size", "description": "TriviaQA has a training size of 6000"}, {"entity1": "TriviaQA", "entity2": "1000", "relation": "Evaluation Size", "description": "TriviaQA has an evaluation size of 1000"}, {"entity1": "ASQA", "entity2": "4353", "relation": "Training Size", "description": "ASQA has a training size of 4353"}, {"entity1": "ASQA", "entity2": "948", "relation": "Evaluation Size", "description": "ASQA has an evaluation size of 948"}, {"entity1": "PubmedQA", "entity2": "10000", "relation": "Training Size", "description": "PubmedQA has a training size of 10000"}, {"entity1": "PubmedQA", "entity2": "500", "relation": "Evaluation Size", "description": "PubmedQA has an evaluation size of 500"}, {"entity1": "k", "entity2": "5", "relation": "Context Size", "description": "The context size is k = 5"}], "75a42688-24f6-4e01-a30b-b09958ef5651": [{"entity1": "Tan Yu", "entity2": "NVIDIA", "relation": "Affiliation", "description": "Tan Yu is affiliated with NVIDIA."}, {"entity1": "Anbang Xu", "entity2": "NVIDIA", "relation": "Affiliation", "description": "Anbang Xu is affiliated with NVIDIA."}, {"entity1": "Rama Akkiraju", "entity2": "NVIDIA", "relation": "Affiliation", "description": "Rama Akkiraju is affiliated with NVIDIA."}, {"entity1": "NVIDIA", "entity2": "Santa Clara", "relation": "Location", "description": "NVIDIA is located in Santa Clara, California, United States."}, {"entity1": "Tan Yu", "entity2": "United States", "relation": "Location", "description": "Tan Yu is located in the United States."}, {"entity1": "Anbang Xu", "entity2": "United States", "relation": "Location", "description": "Anbang Xu is located in the United States."}, {"entity1": "Rama Akkiraju", "entity2": "United States", "relation": "Location", "description": "Rama Akkiraju is located in the United States."}, {"entity1": "Guu et al.", "entity2": "RAG", "relation": "Author-Expertise", "description": "Guu et al. are experts in RAG."}, {"entity1": "Lewis et al.", "entity2": "RAG", "relation": "Author-Expertise", "description": "Lewis et al. are experts in RAG."}, {"entity1": "OP-RAG", "entity2": "RAG", "relation": "Methodology", "description": "OP-RAG is a type of RAG."}, {"entity1": "Llama3.1-70B", "entity2": "OP-RAG", "relation": "Tool/Resource", "description": "Llama3.1-70B is used as a generator in OP-RAG."}, {"entity1": "GPT-4O", "entity2": "OpenAI", "relation": "Affiliation", "description": "GPT-4O is affiliated with OpenAI."}, {"entity1": "Claudi-3.5", "entity2": "Anthropic", "relation": "Affiliation", "description": "Claudi-3.5 is affiliated with Anthropic."}, {"entity1": "Llama3.1", "entity2": "Meta", "relation": "Affiliation", "description": "Llama3.1 is affiliated with Meta."}, {"entity1": "Phi-3", "entity2": "Abdin et al.", "relation": "Author-Expertise", "description": "Abdin et al. are experts in Phi-3."}, {"entity1": "OP-RAG", "entity2": "En.QA dataset", "relation": "Experiment-Outcome", "description": "OP-RAG was tested on the En.QA dataset and achieved superior results."}, {"entity1": "Llama3.1-70B", "entity2": "OP-RAG", "relation": "Experiment-Design", "description": "Llama3.1-70B was used as a generator in the OP-RAG experiment."}, {"entity1": "F1 score", "entity2": "OP-RAG", "relation": "Result", "description": "The F1 score was used to evaluate the performance of OP-RAG."}, {"entity1": "Input token count", "entity2": "OP-RAG", "relation": "Result", "description": "The input token count was used to evaluate the performance of OP-RAG."}], "22e7b8d5-d4e3-43b8-b251-9eeabfeeb2ae": [{"entity1": "OP-RAG", "entity2": "Llama3.1-70B", "relation": "Tool/Resource", "description": "OP-RAG uses Llama3.1-70B as its generator."}, {"entity1": "Llama3.1-70B", "entity2": "RAG", "relation": "Comparison", "description": "Llama3.1-70B with RAG is compared to Llama3.1-70B without RAG."}, {"entity1": "Llama3.1", "entity2": "Meta", "relation": "Affiliation", "description": "Llama3.1 is affiliated with Meta."}, {"entity1": "Phi-3", "entity2": "Abdin et al.", "relation": "Author-Role", "description": "Abdin et al. are authors of Phi-3."}, {"entity1": "Mistral-Large2", "entity2": "AI", "relation": "Affiliation", "description": "Mistral-Large2 is affiliated with AI."}, {"entity1": "Gemini-1.5-pro", "entity2": "1M context window", "relation": "Application", "description": "Gemini-1.5-pro supports a 1M context window."}, {"entity1": "Li et al.", "entity2": "RAG", "relation": "Comparison", "description": "Li et al. compare RAG with long-context LLMs."}, {"entity1": "RAG", "entity2": "long-context LLMs", "relation": "Comparison", "description": "RAG is compared to long-context LLMs in terms of answer quality."}, {"entity1": "OP-RAG", "entity2": "En.QA dataset", "relation": "Dataset-Origin", "description": "OP-RAG is evaluated on the En.QA dataset."}, {"entity1": "arXiv:2409.01666v1", "entity2": "cs.CL", "relation": "Publication Venue", "description": "arXiv:2409.01666v1 is published in the cs.CL category."}, {"entity1": "Llama3.1-70B", "entity2": "Llama3.1", "relation": "Tool/Resource", "description": "Llama3.1-70B is a variant of Llama3.1."}], "74a9b256-b2ed-451b-9202-6cc4caa4f6f8": [{"entity1": "LLM", "entity2": "Answer quality", "relation": "Impact", "description": "The context of LLM is vital for the answer quality."}, {"entity1": "RAG", "entity2": "Order-preserving mechanism", "relation": "Methodology", "description": "The proposed order-preserving mechanism is used in RAG to preserve the order of retrieved chunks."}, {"entity1": "RAG", "entity2": "Answer quality", "relation": "Result", "description": "The proposed order-preserving RAG significantly improves the answer quality."}, {"entity1": "Retrieved chunks", "entity2": "Answer quality", "relation": "Experiment-Outcome", "description": "As the number of retrieved chunks increases, the answer quality initially rises and then declines."}, {"entity1": "Li et al.", "entity2": "RAG", "relation": "Comparison", "description": "The conclusion from Li et al. differs from the results obtained with the proposed order-preserving mechanism in RAG."}, {"entity1": "RAG", "entity2": "Long-Context LLMs", "relation": "Comparison", "description": "RAG achieves higher answer quality compared to its counterparts that rely solely on Long-Context LLMs."}, {"entity1": "Llama3.1-70B", "entity2": "RAG", "relation": "Experiment-Outcome", "description": "Using RAG, Llama3.1-70B achieves a higher F1 score compared to using only Long-Context LLMs."}, {"entity1": "GPT-4O", "entity2": "RAG", "relation": "Comparison", "description": "GPT-4O achieves a lower F1 score compared to RAG."}, {"entity1": "Gemini-1.5-Pro", "entity2": "RAG", "relation": "Comparison", "description": "Gemini-1.5-Pro obtains a lower F1 score compared to RAG."}, {"entity1": "Guu et al.", "entity2": "RAG", "relation": "Research Area", "description": "Guu et al. are related to the research area of retrieval-augmented generation (RAG)."}, {"entity1": "Lewis et al.", "entity2": "RAG", "relation": "Research Area", "description": "Lewis et al. are related to the research area of retrieval-augmented generation (RAG)."}, {"entity1": "Mialon et al.", "entity2": "RAG", "relation": "Research Area", "description": "Mialon et al. are related to the research area of retrieval-augmented generation (RAG)."}, {"entity1": "Zhang et al.", "entity2": "\u221eBench", "relation": "Affiliation", "description": "Zhang et al. are affiliated with the \u221eBench dataset."}, {"entity1": "En.QA dataset", "entity2": "\u221eBench", "relation": "Dataset-Origin", "description": "The En.QA dataset is part of the \u221eBench dataset."}, {"entity1": "RAG", "entity2": "External knowledge", "relation": "Methodology", "description": "RAG incorporates external knowledge as context to improve factual accuracy."}], "23118466-1ab6-4f9c-9408-19048a6ec359": [{"entity1": "RAG", "entity2": "LLMs", "relation": "Methodology", "description": "RAG is a promising solution to overcoming the limitation of short context window in LLMs."}, {"entity1": "Choromanski et al.", "entity2": "self-attention", "relation": "Research Area", "description": "Choromanski et al. have devoted efforts to improving the computing efficiency of self-attention."}, {"entity1": "Zaheer et al.", "entity2": "self-attention", "relation": "Research Area", "description": "Zaheer et al. have devoted efforts to improving the computing efficiency of self-attention."}, {"entity1": "Tay et al.", "entity2": "self-attention", "relation": "Research Area", "description": "Tay et al. have devoted efforts to improving the computing efficiency of self-attention."}, {"entity1": "Dao et al.", "entity2": "self-attention", "relation": "Research Area", "description": "Dao et al. have devoted efforts to improving the computing efficiency of self-attention."}, {"entity1": "Dao", "entity2": "self-attention", "relation": "Research Area", "description": "Dao has devoted efforts to improving the computing efficiency of self-attention."}, {"entity1": "Press et al.", "entity2": "positional encoding", "relation": "Research Area", "description": "Press et al. have devoted efforts to boosting extensibility of positional encoding."}, {"entity1": "Sun et al.", "entity2": "positional encoding", "relation": "Research Area", "description": "Sun et al. have devoted efforts to boosting extensibility of positional encoding."}, {"entity1": "Chen et al.", "entity2": "positional encoding", "relation": "Research Area", "description": "Chen et al. have devoted efforts to boosting extensibility of positional encoding."}, {"entity1": "GPT-4O", "entity2": "OpenAI", "relation": "Affiliation", "description": "GPT-4O is a flagship LLM developed by OpenAI."}, {"entity1": "Gemini-1.5-Pro", "entity2": "Reid et al.", "relation": "Affiliation", "description": "Gemini-1.5-Pro is a flagship LLM developed by Reid et al."}, {"entity1": "Claudi-3.5", "entity2": "Anthropic", "relation": "Affiliation", "description": "Claudi-3.5 is a flagship LLM developed by Anthropic."}, {"entity1": "Grok-2", "entity2": "xAI", "relation": "Affiliation", "description": "Grok-2 is a flagship LLM developed by xAI."}, {"entity1": "Llama3.1", "entity2": "Meta", "relation": "Affiliation", "description": "Llama3.1 is a flagship LLM developed by Meta."}, {"entity1": "Li et al.", "entity2": "RAG", "relation": "Conclusion", "description": "Li et al. conclude that using long-context without RAG could significantly outperform RAG."}, {"entity1": "RAG", "entity2": "long-context LLMs", "relation": "Comparison", "description": "The proposed order-preserve RAG could beat the long-context LLMs without RAG."}, {"entity1": "d", "entity2": "ci", "relation": "Dataset-Origin", "description": "The long textual context d is split into N chunks {ci}N i=1."}, {"entity1": "q", "entity2": "ci", "relation": "Experiment-Design", "description": "The relevance score of the chunk ci is obtained by computing cosine similarity between the embedding of q and that of ci."}], "8f4378b0-104e-4f1d-b524-602cf913d719": [{"entity1": "EN.QA", "entity2": "\u221eBench", "relation": "Dataset-Origin", "description": "EN.QA is a dataset from the \u221eBench benchmark."}, {"entity1": "EN.MC", "entity2": "\u221eBench", "relation": "Dataset-Origin", "description": "EN.MC is a dataset from the \u221eBench benchmark."}, {"entity1": "RAG", "entity2": "cos(\u00b7, \u00b7)", "relation": "Methodology", "description": "RAG uses the cosine similarity function cos(\u00b7, \u00b7) for similarity scores."}, {"entity1": "RAG", "entity2": "emb(\u00b7)", "relation": "Methodology", "description": "RAG uses the embedding function emb(\u00b7) for query and chunk embeddings."}, {"entity1": "J = {ji}k i=1", "entity2": "RAG", "relation": "Methodology", "description": "J = {ji}k i=1 represents the indices of top k chunks retrieved by RAG."}, {"entity1": "Zhang et al.", "entity2": "\u221eBench", "relation": "Author-Role", "description": "Zhang et al. are the authors of the \u221eBench benchmark."}, {"entity1": "Bai et al.", "entity2": "LongBench", "relation": "Author-Role", "description": "Bai et al. are the authors of the LongBench benchmark."}, {"entity1": "Xiao et al.", "entity2": "BGE-large-env1.5", "relation": "Author-Role", "description": "Xiao et al. are the authors of the BGE-large-env1.5 model."}, {"entity1": "Llama3.1-8B", "entity2": "RAG", "relation": "Tool/Resource", "description": "Llama3.1-8B is a model used with RAG for experiments."}, {"entity1": "Figure 2", "entity2": "RAG", "relation": "Result", "description": "Figure 2 visualizes the difference between vanilla RAG and order-preserve RAG."}, {"entity1": "Figure 3", "entity2": "RAG", "relation": "Result", "description": "Figure 3 shows the influence of context length on the performance of RAG."}, {"entity1": "F1-score", "entity2": "EN.QA", "relation": "Evaluation Metric", "description": "F1-score is used as the evaluation metric for EN.QA."}, {"entity1": "Accuracy", "entity2": "EN.MC", "relation": "Evaluation Metric", "description": "Accuracy is used as the evaluation metric for EN.MC."}, {"entity1": "2024", "entity2": "\u221eBench", "relation": "Publication Date", "description": "\u221eBench was published in 2024."}, {"entity1": "2023", "entity2": "LongBench", "relation": "Publication Date", "description": "LongBench was published in 2023."}, {"entity1": "2023", "entity2": "BGE-large-env1.5", "relation": "Publication Date", "description": "BGE-large-env1.5 was published in 2023."}], "8c454368-50b0-4fd3-b5b5-b4d6819850cc": [{"entity1": "Llama3.1-8B", "entity2": "EN.QA dataset", "relation": "Performance Peak", "description": "Llama3.1-8B achieves its performance peak at context length 16K on EN.QA dataset"}, {"entity1": "Llama3.1-8B", "entity2": "EN.MC dataset", "relation": "Performance Peak", "description": "Llama3.1-8B achieves its performance peak at context length 16K on EN.MC dataset"}, {"entity1": "Llama3.1-70B", "entity2": "EN.QA dataset", "relation": "Performance Peak", "description": "Llama3.1-70B achieves its performance peak at context length 48K on EN.QA dataset"}, {"entity1": "Llama3.1-70B", "entity2": "EN.MC dataset", "relation": "Performance Peak", "description": "Llama3.1-70B achieves its performance peak at context length 32K on EN.MC dataset"}, {"entity1": "Llama3.1-8B", "entity2": "Llama3.1-70B", "relation": "Comparison", "description": "Llama3.1-70B has a stronger capability to distinguish relevant chunks than Llama3.1-8B"}, {"entity1": "16K", "entity2": "Llama3.1-8B", "relation": "Optimal Context Length", "description": "16K is the optimal context length for Llama3.1-8B on both EN.QA and EN.MC datasets"}, {"entity1": "48K", "entity2": "Llama3.1-70B", "relation": "Optimal Context Length", "description": "48K is the optimal context length for Llama3.1-70B on EN.QA dataset"}, {"entity1": "32K", "entity2": "Llama3.1-70B", "relation": "Optimal Context Length", "description": "32K is the optimal context length for Llama3.1-70B on EN.MC dataset"}], "1d8cf616-93d1-4100-88bc-a9f09c73434e": [{"entity1": "OP-RAG", "entity2": "vanilla RAG", "relation": "Comparison", "description": "OP-RAG outperforms vanilla RAG, especially when the number of retrieved chunks is large."}, {"entity1": "OP-RAG", "entity2": "SELF-ROUTE mechanism", "relation": "Comparison", "description": "OP-RAG significantly outperforms SELF-ROUTE mechanism in terms of F1 score and accuracy."}, {"entity1": "OP-RAG", "entity2": "Long-context LLM", "relation": "Comparison", "description": "OP-RAG reduces the number of tokens and improves answer quality compared to Long-context LLM without RAG."}, {"entity1": "Llama3.1-70B", "entity2": "OP-RAG", "relation": "Application", "description": "Llama3.1-70B is used with OP-RAG to achieve improved F1 scores and accuracy."}, {"entity1": "EN.QA", "entity2": "OP-RAG", "relation": "Experiment-Outcome", "description": "OP-RAG achieves a 44.43 F1-score on EN.QA dataset when retrieving 128 chunks."}, {"entity1": "EN.MC", "entity2": "OP-RAG", "relation": "Experiment-Outcome", "description": "OP-RAG achieves an 88.65 accuracy on EN.MC dataset when retrieving 192 chunks."}, {"entity1": "Li et al.", "entity2": "SELF-ROUTE mechanism", "relation": "Author-Role", "description": "Li et al. proposed the SELF-ROUTE mechanism."}, {"entity1": "OP-RAG", "entity2": "Figure 4", "relation": "Result", "description": "Figure 4 shows the comparisons between OP-RAG and vanilla RAG."}, {"entity1": "OP-RAG", "entity2": "Table 1", "relation": "Result", "description": "Table 1 shows the comparisons among OP-RAG, SELF-ROUTE mechanism, and Long-context LLM without RAG."}, {"entity1": "Llama3.1-70B", "entity2": "GPT-4O", "relation": "Comparison", "description": "Llama3.1-70B and GPT-4O are compared in terms of F1 score and accuracy."}, {"entity1": "Gemini-1.5-Pro", "entity2": "Llama3.1-70B", "relation": "Comparison", "description": "Gemini-1.5-Pro and Llama3.1-70B are compared in terms of F1 score and accuracy."}, {"entity1": "OP-RAG", "entity2": "\u221eBench", "relation": "Dataset-Origin", "description": "OP-RAG is evaluated on EN.QA and EN.MC datasets of \u221eBench."}, {"entity1": "SELF-ROUTE mechanism", "entity2": "Li et al.", "relation": "Author-Expertise", "description": "Li et al. have expertise in proposing the SELF-ROUTE mechanism."}, {"entity1": "OP-RAG", "entity2": "order-preserve RAG", "relation": "Tool/Resource", "description": "OP-RAG is an instance of order-preserve RAG."}, {"entity1": "vanilla RAG", "entity2": "order-preserve RAG", "relation": "Comparison", "description": "vanilla RAG is compared to order-preserve RAG in terms of performance."}, {"entity1": "Llama3.1-70B", "entity2": "OP-RAG", "relation": "Methodology", "description": "Llama3.1-70B is used as a methodology to evaluate OP-RAG."}, {"entity1": "EN.QA", "entity2": "EN.MC", "relation": "Comparison", "description": "EN.QA and EN.MC are compared in terms of F1 score and accuracy."}, {"entity1": "OP-RAG", "entity2": "F1-score", "relation": "Experiment-Outcome", "description": "OP-RAG achieves improved F1 scores on EN.QA and EN.MC datasets."}, {"entity1": "OP-RAG", "entity2": "accuracy", "relation": "Experiment-Outcome", "description": "OP-RAG achieves improved accuracy on EN.QA and EN.MC datasets."}], "2b254807-3d71-4b82-88be-29f33cd42162": [{"entity1": "LLMs", "entity2": "RAG", "relation": "Comparison", "description": "The paper compares LLMs and RAG, arguing that while LLMs can incorporate extensive text sequences, RAG can provide better focus on relevant information."}, {"entity1": "OP-RAG", "entity2": "RAG", "relation": "Innovation", "description": "OP-RAG is proposed as an improvement over traditional RAG, addressing the issue of diminished focus on relevant information in long-context LLMs."}, {"entity1": "LLMs", "entity2": "question-answering tasks", "relation": "Application", "description": "LLMs are applied to question-answering tasks, but their performance can be degraded due to extremely long contexts."}, {"entity1": "OP-RAG", "entity2": "public benchmarks", "relation": "Experiment-Outcome", "description": "Experiments on public benchmarks demonstrate that OP-RAG significantly improves performance."}, {"entity1": "RAG", "entity2": "retrieval-augmented generation", "relation": "Synonym", "description": "RAG is another term for retrieval-augmented generation."}, {"entity1": "OP-RAG", "entity2": "long-context language models", "relation": "Methodology", "description": "OP-RAG is a mechanism proposed to improve the performance of long-context language models."}], "c4cc04e2-b009-459b-abed-1b5c51e39a4f": [{"entity1": "RAG", "entity2": "OP-RAG", "relation": "Comparison", "description": "OP-RAG outperforms RAG in long-context question-answer applications due to efficient retrieval and focused context utilization."}, {"entity1": "RAG", "entity2": "Long-context question-answer applications", "relation": "Application", "description": "RAG is used for long-context question-answer applications."}, {"entity1": "OP-RAG", "entity2": "Efficient retrieval", "relation": "Methodology", "description": "OP-RAG utilizes efficient retrieval to outperform RAG."}, {"entity1": "OP-RAG", "entity2": "Focused context utilization", "relation": "Methodology", "description": "OP-RAG uses focused context utilization to improve performance."}, {"entity1": "Marah Abdin", "entity2": "Phi-3 technical report", "relation": "Author-Role", "description": "Marah Abdin is an author of the Phi-3 technical report."}, {"entity1": "Mistral AI", "entity2": "Mistral large 2", "relation": "Tool/Resource", "description": "Mistral AI developed Mistral large 2."}, {"entity1": "Anthropic", "entity2": "Claude 3.5 sonnet", "relation": "Tool/Resource", "description": "Anthropic developed Claude 3.5 sonnet."}, {"entity1": "Yushi Bai", "entity2": "Longbench", "relation": "Author-Role", "description": "Yushi Bai is an author of the Longbench benchmark."}, {"entity1": "Shouyuan Chen", "entity2": "Extending context window of large language models via positional interpolation", "relation": "Author-Role", "description": "Shouyuan Chen is an author of the paper on extending context window of large language models via positional interpolation."}, {"entity1": "Krzysztof Choromanski", "entity2": "Rethinking attention with performers", "relation": "Author-Role", "description": "Krzysztof Choromanski is an author of the paper on rethinking attention with performers."}, {"entity1": "Tri Dao", "entity2": "FlashAttention-2", "relation": "Author-Role", "description": "Tri Dao is an author of the FlashAttention-2 paper."}, {"entity1": "Tri Dao", "entity2": "FlashAttention", "relation": "Author-Role", "description": "Tri Dao is an author of the FlashAttention paper."}, {"entity1": "Kelvin Guu", "entity2": "Retrieval augmented language model pre-training", "relation": "Author-Role", "description": "Kelvin Guu is an author of the paper on retrieval augmented language model pre-training."}, {"entity1": "Patrick Lewis", "entity2": "Retrieval-augmented generation for knowledge-intensive nlp tasks", "relation": "Author-Role", "description": "Patrick Lewis is an author of the paper on retrieval-augmented generation for knowledge-intensive nlp tasks."}, {"entity1": "Zhuowan Li", "entity2": "Retrieval augmented generation or long-context llms? a comprehensive study and hybrid approach", "relation": "Author-Role", "description": "Zhuowan Li is an author of the paper on retrieval augmented generation or long-context llms? a comprehensive study and hybrid approach."}, {"entity1": "Meta", "entity2": "Llama 3.1", "relation": "Tool/Resource", "description": "Meta developed Llama 3.1."}, {"entity1": "arXiv:2404.14219", "entity2": "Phi-3 technical report", "relation": "Publication Venue", "description": "The Phi-3 technical report is published on arXiv:2404.14219."}, {"entity1": "arXiv:2308.14508", "entity2": "Longbench", "relation": "Publication Venue", "description": "The Longbench benchmark is published on arXiv:2308.14508."}, {"entity1": "arXiv:2306.15595", "entity2": "Extending context window of large language models via positional interpolation", "relation": "Publication Venue", "description": "The paper on extending context window of large language models via positional interpolation is published on arXiv:2306.15595."}, {"entity1": "arXiv:2009.14794", "entity2": "Rethinking attention with performers", "relation": "Publication Venue", "description": "The paper on rethinking attention with performers is published on arXiv:2009.14794."}, {"entity1": "ICLR", "entity2": "FlashAttention-2", "relation": "Publication Venue", "description": "The FlashAttention-2 paper is published in ICLR."}, {"entity1": "NeurIPS", "entity2": "FlashAttention", "relation": "Publication Venue", "description": "The FlashAttention paper is published in NeurIPS."}, {"entity1": "PMLR", "entity2": "Retrieval augmented language model pre-training", "relation": "Publication Venue", "description": "The paper on retrieval augmented language model pre-training is published in PMLR."}], "49e0f66e-40e1-49c7-b8df-516a41b12f22": [{"entity1": "Gr\u00e9goire Mialon", "entity2": "Roberto Dess\u00ec", "relation": "Co-author", "description": "Co-authors of the paper 'Augmented language models: a survey' (arXiv:2302.07842)"}, {"entity1": "Meta", "entity2": "Llama 3.1", "relation": "Publication Venue", "description": "Meta introduced Llama 3.1, their most capable models to date"}, {"entity1": "Ofir Press", "entity2": "Noah A Smith", "relation": "Co-author", "description": "Co-authors of the paper 'Train short, test long: Attention with linear biases enables input length extrapolation' (arXiv:2108.12409)"}, {"entity1": "Machel Reid", "entity2": "Nikolay Savinov", "relation": "Co-author", "description": "Co-authors of the paper 'Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context' (arXiv:2403.05530)"}, {"entity1": "Yutao Sun", "entity2": "Li Dong", "relation": "Co-author", "description": "Co-authors of the paper 'A length-extrapolatable transformer' (arXiv:2212.10554)"}, {"entity1": "Yi Tay", "entity2": "Mostafa Dehghani", "relation": "Co-author", "description": "Co-authors of the paper 'Efficient transformers: A survey' (cs.LG/2009.06732)"}, {"entity1": "xAI", "entity2": "Grok-2", "relation": "Tool/Resource", "description": "xAI released Grok-2 beta"}, {"entity1": "Shitao Xiao", "entity2": "Zheng Liu", "relation": "Co-author", "description": "Co-authors of the paper 'C-pack: Packaged resources to advance general chinese embedding' (arXiv:2309.07597)"}, {"entity1": "Manzil Zaheer", "entity2": "Guru Guruganesh", "relation": "Co-author", "description": "Co-authors of the paper 'Big bird: Transformers for longer sequences' (Advances in neural information processing systems, 33:17283\u201317297)"}, {"entity1": "Xinrong Zhang", "entity2": "Yingfa Chen", "relation": "Co-author", "description": "Co-authors of the paper '\u221ebench: Extending long context evaluation beyond 100k tokens' (arXiv:2402.13718)"}, {"entity1": "arXiv:2407.16833", "entity2": "arXiv:2403.05530", "relation": "Citation", "description": "arXiv:2407.16833 cites arXiv:2403.05530"}, {"entity1": "Llama 3.1", "entity2": "GPT-4", "relation": "Comparison", "description": "Llama 3.1 and GPT-4 are compared in the context of long-context LLMs"}, {"entity1": "OpenAI", "entity2": "GPT-4", "relation": "Publication Venue", "description": "OpenAI published the GPT-4 technical report (ArXiv, 2303:08774)"}, {"entity1": "Meta", "entity2": "Llama 3.1 models", "relation": "Research Area", "description": "Meta researches and develops Llama 3.1 models"}, {"entity1": "arXiv:2302.07842", "entity2": "Augmented language models", "relation": "Publication Venue", "description": "arXiv:2302.07842 is a publication about augmented language models"}, {"entity1": "arXiv:2403.05530", "entity2": "Gemini 1.5", "relation": "Publication Venue", "description": "arXiv:2403.05530 is a publication about Gemini 1.5"}, {"entity1": "arXiv:2212.10554", "entity2": "A length-extrapolatable transformer", "relation": "Publication Venue", "description": "arXiv:2212.10554 is a publication about a length-extrapolatable transformer"}, {"entity1": "cs.LG/2009.06732", "entity2": "Efficient transformers", "relation": "Publication Venue", "description": "cs.LG/2009.06732 is a publication about efficient transformers"}, {"entity1": "arXiv:2309.07597", "entity2": "C-pack", "relation": "Publication Venue", "description": "arXiv:2309.07597 is a publication about C-pack"}, {"entity1": "Advances in neural information processing systems", "entity2": "Big bird", "relation": "Publication Venue", "description": "Advances in neural information processing systems published a paper about Big bird"}, {"entity1": "arXiv:2402.13718", "entity2": "\u221ebench", "relation": "Publication Venue", "description": "arXiv:2402.13718 is a publication about \u221ebench"}], "18d96500-fc8d-497e-a044-1e27fe445e9a": [{"entity1": "Sumit Soman", "entity2": "Sujoy Roychowdhury", "relation": "Co-author", "description": "Sumit Soman and Sujoy Roychowdhury are co-authors of the paper published at ICLR 2024."}, {"entity1": "ICLR 2024", "entity2": "Sumit Soman and Sujoy Roychowdhury", "relation": "Publication Venue", "description": "The paper by Sumit Soman and Sujoy Roychowdhury was published at ICLR 2024."}, {"entity1": "Ericsson", "entity2": "Sumit Soman", "relation": "Affiliation", "description": "Sumit Soman is affiliated with Ericsson, as indicated by his email address."}, {"entity1": "Ericsson", "entity2": "Sujoy Roychowdhury", "relation": "Affiliation", "description": "Sujoy Roychowdhury is affiliated with Ericsson, as indicated by his email address."}, {"entity1": "Xu et al.", "entity2": "Sumit Soman and Sujoy Roychowdhury", "relation": "Citation", "description": "Sumit Soman and Sujoy Roychowdhury cite the work of Xu et al. in their paper."}, {"entity1": "Toro et al.", "entity2": "Sumit Soman and Sujoy Roychowdhury", "relation": "Citation", "description": "Sumit Soman and Sujoy Roychowdhury cite the work of Toro et al. in their paper."}, {"entity1": "IEEE", "entity2": "IEEE Wireless LAN Medium Access Control (MAC)", "relation": "Publication", "description": "IEEE published the IEEE Wireless LAN Medium Access Control (MAC) specification."}, {"entity1": "IEEE", "entity2": "IEEE Standard Glossary of Stationary Battery Terminology 1881-2016", "relation": "Publication", "description": "IEEE published the IEEE Standard Glossary of Stationary Battery Terminology 1881-2016."}, {"entity1": "MPNET", "entity2": "Song et al.", "relation": "Author-Role", "description": "Song et al. are the authors of the MPNET model."}, {"entity1": "Llama2-7b-chat", "entity2": "Touvron et al.", "relation": "Author-Role", "description": "Touvron et al. are the authors of the Llama2-7b-chat model."}, {"entity1": "Sumit Soman and Sujoy Roychowdhury", "entity2": "RAG", "relation": "Research Area", "description": "Sumit Soman and Sujoy Roychowdhury are researching Retrieval Augmented Generation (RAG) systems for technical documents."}, {"entity1": "Kernel Density Estimate (KDE)", "entity2": "Sumit Soman and Sujoy Roychowdhury", "relation": "Tool/Resource", "description": "Sumit Soman and Sujoy Roychowdhury use Kernel Density Estimate (KDE) in their experiments."}, {"entity1": "cosine similarity", "entity2": "Sumit Soman and Sujoy Roychowdhury", "relation": "Tool/Resource", "description": "Sumit Soman and Sujoy Roychowdhury use cosine similarity in their experiments."}, {"entity1": "Reimers & Gurevych", "entity2": "Sumit Soman and Sujoy Roychowdhury", "relation": "Citation", "description": "Sumit Soman and Sujoy Roychowdhury cite the work of Reimers & Gurevych in their paper."}, {"entity1": "Soman & HG", "entity2": "Sumit Soman and Sujoy Roychowdhury", "relation": "Citation", "description": "Sumit Soman and Sujoy Roychowdhury cite the work of Soman & HG in their paper."}], "424cd92f-8c17-4757-aa3a-1174be86d7c2": [{"entity1": "sentence embeddings", "entity2": "cosine similarity", "relation": "Methodology", "description": "Sentence embeddings are analyzed using cosine similarity to measure pairwise similarity."}, {"entity1": "sentence length", "entity2": "similarity distribution", "relation": "Result", "description": "The similarity distribution changes with increasing sentence length, showing spurious similarities for longer sentences."}, {"entity1": "query document", "entity2": "queried document", "relation": "Comparison", "description": "The similarity distribution is bimodal when both the query and queried document are over 200 words."}, {"entity1": "H1", "entity2": "definition and terms", "relation": "Hypothesis", "description": "Splitting on definition and terms can help improve results."}, {"entity1": "H2", "entity2": "similarity scores", "relation": "Hypothesis", "description": "Similarity scores are a good measure."}, {"entity1": "H3", "entity2": "position of keywords", "relation": "Hypothesis", "description": "The position of keywords influences results."}, {"entity1": "H4", "entity2": "sentence-based similarity", "relation": "Hypothesis", "description": "Sentence-based similarity results in a better retriever."}, {"entity1": "H5", "entity2": "sentence-based similarity", "relation": "Hypothesis", "description": "Sentence-based similarity results in a better generator."}, {"entity1": "Global AI Accelerator", "entity2": "Ericsson R&D", "relation": "Affiliation", "description": "Both authors are affiliated with Global AI Accelerator and Ericsson R&D."}, {"entity1": "Bangalore", "entity2": "India", "relation": "Location", "description": "The authors are located in Bangalore, India."}, {"entity1": "arXiv:2404.00657v1", "entity2": "cs.LG", "relation": "Publication Venue", "description": "The paper is published on arXiv with the category cs.LG."}, {"entity1": "Git Repo Link", "entity2": "code", "relation": "Tool/Resource", "description": "The Git Repo Link provides access to the code."}, {"entity1": "Appendix B", "entity2": "Fig. 1", "relation": "Reference", "description": "Appendix B contains Fig. 1, which shows the Kernel Density Estimate plot of cosine similarity scores."}, {"entity1": "Appendix C", "entity2": "sample queries and results", "relation": "Reference", "description": "Appendix C provides corresponding sample queries and their results."}, {"entity1": "Table 1", "entity2": "hypotheses and key observations", "relation": "Summary", "description": "Table 1 summarizes the hypotheses and key observations."}], "4705e314-7a60-49ca-a3fd-10e40baaa6a8": [{"entity1": "H1", "entity2": "Splitting definition and defined words", "relation": "Hypothesis", "description": "H1 hypothesis states that splitting definition and defined words helps in queries"}, {"entity1": "H2", "entity2": "Similarity scores", "relation": "Hypothesis", "description": "H2 hypothesis states that similarity scores should not be used to compare retrieved results"}, {"entity1": "H3", "entity2": "Position of keywords", "relation": "Hypothesis", "description": "H3 hypothesis states that the position of keywords matters in retrieval"}, {"entity1": "H4", "entity2": "Sentence Based Similarity", "relation": "Hypothesis", "description": "H4 hypothesis states that sentence based similarity is better"}, {"entity1": "H5", "entity2": "Generator for sentence based similarity", "relation": "Hypothesis", "description": "H5 hypothesis states that generated answer using sentence based similarity gives better results"}, {"entity1": "H6", "entity2": "Definitions with acronyms", "relation": "Hypothesis", "description": "H6 hypothesis states that definitions with acronyms don't perform well"}, {"entity1": "H7", "entity2": "Order of retrieved paragraphs", "relation": "Hypothesis", "description": "H7 hypothesis states that the order of retrieved paragraphs does not affect generator results"}, {"entity1": "ID F1", "entity2": "Table 2", "relation": "Experiment-Outcome", "description": "ID F1 experiment outcome is summarized in Table 2"}, {"entity1": "ID F1", "entity2": "App. Table 3", "relation": "Experiment-Outcome", "description": "ID F1 experiment outcome is also summarized in App. Table 3"}, {"entity1": "ID F2", "entity2": "App. Table 3", "relation": "Experiment-Outcome", "description": "ID F2 experiment outcome is summarized in App. Table 3"}, {"entity1": "ID F3", "entity2": "App. Table 3", "relation": "Experiment-Outcome", "description": "ID F3 experiment outcome is summarized in App. Table 3"}, {"entity1": "Chen et al.", "entity2": "2023a", "relation": "Citation", "description": "Chen et al. is cited as 2023a"}, {"entity1": "ICLR 2024", "entity2": "Tiny Paper", "relation": "Publication Venue", "description": "Tiny Paper is published at ICLR 2024"}, {"entity1": "H2", "entity2": "Chen et al.", "relation": "Hypothesis-Source", "description": "H2 hypothesis is based on Chen et al."}, {"entity1": "H7", "entity2": "Chen et al.", "relation": "Hypothesis-Source", "description": "H7 hypothesis is based on Chen et al."}], "c7057eaf-1c61-42c4-9fe9-d5fa970cb3b4": [{"entity1": "thresholding", "entity2": "retriever results", "relation": "Methodology", "description": "Thresholding is used to filter retriever results based on similarity scores."}, {"entity1": "generator", "entity2": "retrieved results", "relation": "Methodology", "description": "The generator's performance is evaluated based on the retrieved results."}, {"entity1": "similarity scores", "entity2": "retrieval strategies", "relation": "Methodology", "description": "Similarity scores are used to determine the effectiveness of retrieval strategies."}, {"entity1": "Chen et al.", "entity2": "definition QA", "relation": "Research Area", "description": "Chen et al.'s work is related to definition QA."}, {"entity1": "Es et al.", "entity2": "RAG metrics", "relation": "Tool/Resource", "description": "Es et al.'s work provides RAG metrics for evaluation."}, {"entity1": "retrieval strategies", "entity2": "evaluation metrics", "relation": "Methodology", "description": "Retrieval strategies are evaluated using evaluation metrics."}, {"entity1": "generator", "entity2": "abbreviations", "relation": "Limitation", "description": "The generator does not add value for acronyms and their expansions."}, {"entity1": "sentence-based similarity search", "entity2": "paragraph-based similarity search", "relation": "Comparison", "description": "The two approaches are compared for their effectiveness in retrieval."}, {"entity1": "chunk length", "entity2": "retriever embeddings", "relation": "Experiment-Outcome", "description": "Chunk length affects retriever embeddings."}, {"entity1": "technical documents", "entity2": "long form QA", "relation": "Application", "description": "The approach is particularly relevant for long form QA on technical documents."}, {"entity1": "Chen et al.", "entity2": "Es et al.", "relation": "Citation", "description": "Chen et al.'s work is cited by the authors, as well as Es et al.'s work."}, {"entity1": "RAG metrics", "entity2": "evaluation metrics", "relation": "Tool/Resource", "description": "RAG metrics are used as evaluation metrics."}, {"entity1": "follow-up questions", "entity2": "evaluation metrics", "relation": "Future Work", "description": "Developing methods and evaluation metrics for follow-up questions is a future work direction."}], "9ee0d30a-bf93-43f3-b4fc-af12779e2e90": [{"entity1": "ICLR 2024", "entity2": "Tiny Papers Track", "relation": "Publication Venue", "description": "The paper was published as a Tiny Paper at ICLR 2024."}, {"entity1": "URM", "entity2": "ICLR 2024", "relation": "Institution-Collaboration", "description": "The authors acknowledge that at least one key author of this work meets the URM criteria of ICLR 2024."}, {"entity1": "Hung-Ting Chen", "entity2": "Fangyuan Xu", "relation": "Co-author", "description": "Hung-Ting Chen and Fangyuan Xu co-authored the paper 'Understanding retrieval augmentation for long-form question answering'."}, {"entity1": "Jiawei Chen", "entity2": "Hongyu Lin", "relation": "Co-author", "description": "Jiawei Chen and Hongyu Lin co-authored the paper 'Benchmarking large language models in retrieval-augmented generation'."}, {"entity1": "Shahul Es", "entity2": "Jithin James", "relation": "Co-author", "description": "Shahul Es and Jithin James co-authored the paper 'Ragas: Automated evaluation of retrieval augmented generation'."}, {"entity1": "IEEE", "entity2": "IEEE Std 802.11-2020", "relation": "Publication Venue", "description": "IEEE published the standard 'IEEE Std 802.11-2020'."}, {"entity1": "Nils Reimers", "entity2": "Iryna Gurevych", "relation": "Co-author", "description": "Nils Reimers and Iryna Gurevych co-authored the paper 'Sentence-BERT: Sentence embeddings using siamese bert-networks'."}, {"entity1": "Sumit Soman", "entity2": "Ranjani HG", "relation": "Co-author", "description": "Sumit Soman and Ranjani HG co-authored the paper 'Observations on LLMs for telecom domain: Capabilities and limitations'."}, {"entity1": "Kaitao Song", "entity2": "Xu Tan", "relation": "Co-author", "description": "Kaitao Song and Xu Tan co-authored the paper 'MPNET: Masked and permuted pre-training for language understanding'."}, {"entity1": "Sabrina Toro", "entity2": "Anna V Anagnostopoulos", "relation": "Co-author", "description": "Sabrina Toro and Anna V Anagnostopoulos co-authored the paper 'Dynamic retrieval augmented generation of ontologies using artificial intelligence (DRAGON-AI)'."}, {"entity1": "Hugo Touvron", "entity2": "Louis Martin", "relation": "Co-author", "description": "Hugo Touvron and Louis Martin co-authored the paper 'Llama 2: Open foundation'."}, {"entity1": "arXiv:2310.12150", "entity2": "Hung-Ting Chen", "relation": "Author-Role", "description": "Hung-Ting Chen is an author of the paper 'Understanding retrieval augmentation for long-form question answering' published on arXiv:2310.12150."}, {"entity1": "arXiv:2309.01431", "entity2": "Jiawei Chen", "relation": "Author-Role", "description": "Jiawei Chen is an author of the paper 'Benchmarking large language models in retrieval-augmented generation' published on arXiv:2309.01431."}, {"entity1": "arXiv:2309.15217", "entity2": "Shahul Es", "relation": "Author-Role", "description": "Shahul Es is an author of the paper 'Ragas: Automated evaluation of retrieval augmented generation' published on arXiv:2309.15217."}, {"entity1": "IEEE Std 802.11-2020", "entity2": "IEEE", "relation": "Publication Venue", "description": "IEEE Std 802.11-2020 is a standard published by IEEE."}, {"entity1": "EMNLP-IJCNLP", "entity2": "Nils Reimers", "relation": "Publication Venue", "description": "Nils Reimers published the paper 'Sentence-BERT: Sentence embeddings using siamese bert-networks' at EMNLP-IJCNLP."}, {"entity1": "The Third International Conference on Artificial Intelligence and Machine Learning Systems", "entity2": "Sumit Soman", "relation": "Publication Venue", "description": "Sumit Soman published the paper 'Observations on LLMs for telecom domain: Capabilities and limitations' at The Third International Conference on Artificial Intelligence and Machine Learning Systems."}, {"entity1": "ICLR 2024", "entity2": "Tiny Papers Track", "relation": "Publication Venue", "description": "ICLR 2024 is the publication venue for the Tiny Papers Track."}], "a66fd603-b792-4dd3-9d5b-022463b1ab9d": [{"entity1": "Hugo Touvron", "entity2": "Llama 2", "relation": "Author-Role", "description": "Hugo Touvron is an author of the Llama 2 model."}, {"entity1": "Benfeng Xu", "entity2": "Retrieval-augmented domain adaptation of language models", "relation": "Author-Role", "description": "Benfeng Xu is an author of the paper 'Retrieval-augmented domain adaptation of language models'."}, {"entity1": "Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)", "entity2": "Retrieval-augmented domain adaptation of language models", "relation": "Publication Venue", "description": "The paper 'Retrieval-augmented domain adaptation of language models' was published in the Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)."}, {"entity1": "arXiv:2307.09288", "entity2": "Llama 2", "relation": "Citation", "description": "The paper with the citation arXiv:2307.09288 is related to the Llama 2 model."}, {"entity1": "Benfeng Xu", "entity2": "Chunxu Zhao", "relation": "Co-author", "description": "Benfeng Xu and Chunxu Zhao are co-authors of the paper 'Retrieval-augmented domain adaptation of language models'."}, {"entity1": "Hugo Touvron", "entity2": "Louis Martin", "relation": "Co-author", "description": "Hugo Touvron and Louis Martin are co-authors of the paper related to the Llama 2 model."}, {"entity1": "arXiv:2312.10904", "entity2": "2023", "relation": "Publication Date", "description": "The paper with the citation arXiv:2312.10904 was published in 2023."}, {"entity1": "Llama 2", "entity2": "Open foundation and fine-tuned chat models", "relation": "Description", "description": "Llama 2 is described as open foundation and fine-tuned chat models."}, {"entity1": "Retrieval-augmented domain adaptation of language models", "entity2": "pp. 54\u201364", "relation": "Publication Details", "description": "The paper 'Retrieval-augmented domain adaptation of language models' was published on pages 54-64."}], "ed69152d-4f20-473c-ad9a-2edda31146dd": [{"entity1": "Tiny Paper", "entity2": "ICLR 2024", "relation": "Publication Venue", "description": "Tiny Paper was published at ICLR 2024"}, {"entity1": "LLM", "entity2": "System Prompt", "relation": "Tool/Resource", "description": "LLM uses System Prompt for answering questions"}, {"entity1": "LLM", "entity2": "User Prompt", "relation": "Tool/Resource", "description": "LLM uses User Prompt for generating answers"}, {"entity1": "Git repository", "entity2": "source code", "relation": "Affiliation", "description": "Git repository contains anonymized source code"}, {"entity1": "Git repository", "entity2": "Experiment v/s hypothesis tabulation", "relation": "Affiliation", "description": "Git repository contains Experiment v/s hypothesis tabulation"}, {"entity1": "Table 1", "entity2": "hypotheses", "relation": "Research Area", "description": "Table 1 provides details with respect to hypotheses"}, {"entity1": "Appendix A", "entity2": "System Prompt", "relation": "Affiliation", "description": "Appendix A contains System Prompt"}, {"entity1": "Appendix B", "entity2": "Figure 1", "relation": "Affiliation", "description": "Appendix B contains Figure 1"}, {"entity1": "Appendix C", "entity2": "Git repository", "relation": "Affiliation", "description": "Appendix C provides supplementary material including Git repository"}, {"entity1": "Experiment v/s hypothesis tabulation", "entity2": "hypotheses", "relation": "Research Area", "description": "Experiment v/s hypothesis tabulation provides consolidated quantitative results for hypotheses"}], "5b5b8fdb-267b-457f-ac19-65f81e0453f0": [{"entity1": "EIRP", "entity2": "transmitter power", "relation": "Mathematical Model", "description": "EIRP equals the product of the transmitter power and the antenna gain, reduced by any coupling losses."}, {"entity1": "EIRP", "entity2": "antenna gain", "relation": "Mathematical Model", "description": "EIRP equals the product of the transmitter power and the antenna gain, reduced by any coupling losses."}, {"entity1": "EIRP", "entity2": "coupling losses", "relation": "Mathematical Model", "description": "EIRP equals the product of the transmitter power and the antenna gain, reduced by any coupling losses."}, {"entity1": "EAS", "entity2": "U.S. national public warning system", "relation": "Definition", "description": "EAS is a U.S. national public warning system."}, {"entity1": "IEEE 802.1X authentication", "entity2": "Extensible Authentication Protocol (EAP)", "relation": "Methodology", "description": "IEEE 802.1X authentication transports Extensible Authentication Protocol (EAP) authentication."}, {"entity1": "RCPI", "entity2": "Received channel power indicator", "relation": "Definition", "description": "RCPI is an indication of the total channel power of a received frame."}, {"entity1": "ESA", "entity2": "Extended service set (ESS)", "relation": "Definition", "description": "ESA is the area within which members of an ESS can communicate."}, {"entity1": "ESA", "entity2": "Basic service area (BSA)", "relation": "Comparison", "description": "ESA is larger than or equal to a BSA."}, {"entity1": "ESA", "entity2": "Basic service sets (BSSs)", "relation": "Composition", "description": "ESA might involve several BSSs in overlapping, disjointed, or both configurations."}, {"entity1": "MSK", "entity2": "Extensible Authentication Protocol (EAP)", "relation": "Methodology", "description": "MSK is derived between the EAP peer and exported by the EAP method to the Authentication Server (AS)."}, {"entity1": "MSK", "entity2": "Authentication Server (AS)", "relation": "Methodology", "description": "MSK is derived between the EAP peer and exported by the EAP method to the AS."}], "f9402661-1c9c-430f-8053-f55c282c2d9a": [{"entity1": "EAP peer", "entity2": "Authentication Server", "relation": "Authentication", "description": "The EAP peer and Authentication Server derive keying material, known as the Master session key (MSK), through the Extensible Authentication Protocol."}, {"entity1": "beamformer", "entity2": "beamformee", "relation": "Communication", "description": "The beamformer transmits a physical layer protocol data unit (PPDU) using a beamforming steering matrix to the beamformee."}, {"entity1": "transmitter", "entity2": "receiver", "relation": "Signal Transmission", "description": "The transmitter sends signals to the receiver, with the goal of improving the signal power or signal-to-noise ratio (SNR) at the intended receiver using beamforming."}, {"entity1": "beamforming steering matrix", "entity2": "channel", "relation": "Dependency", "description": "The beamforming steering matrix is determined using knowledge of the channel between the transmitter and the intended receiver."}, {"entity1": "beamforming", "entity2": "signal-to-noise ratio (SNR)", "relation": "Improvement", "description": "Beamforming is used to improve the received signal power or signal-to-noise ratio (SNR) at the intended receiver."}, {"entity1": "beamformer", "entity2": "physical layer protocol data unit (PPDU)", "relation": "Transmission", "description": "The beamformer transmits a physical layer protocol data unit (PPDU) using a beamforming steering matrix."}, {"entity1": "beamformee", "entity2": "physical layer protocol data unit (PPDU)", "relation": "Reception", "description": "The beamformee receives a physical layer protocol data unit (PPDU) that was transmitted using a beamforming steering matrix."}, {"entity1": "Extensible Authentication Protocol", "entity2": "Master session key", "relation": "Key Derivation", "description": "The Extensible Authentication Protocol is used to derive the Master session key (MSK) between the EAP peer and the Authentication Server."}], "d73dcabb-a71d-4ea0-9bbf-b486e6ec2431": [{"entity1": "beamforming steering matrix", "entity2": "transmit steering", "relation": "Calculation", "description": "The beamforming steering matrix is used in the calculation of transmit steering."}, {"entity1": "beamforming steering matrix", "entity2": "modulation and coding scheme (MCS)", "relation": "Calculation", "description": "The beamforming steering matrix is used in the calculation of the recommended modulation and coding scheme (MCS)."}, {"entity1": "beamforming steering matrix", "entity2": "calibration parameters", "relation": "Calculation", "description": "The beamforming steering matrix is used in the calculation of calibration parameters."}, {"entity1": "multi-level precedence and preemption (MLPP)", "entity2": "admission control", "relation": "Framework Support", "description": "The multi-level precedence and preemption (MLPP) framework is used with admission control for the treatment of traffic streams based on precedence."}, {"entity1": "multi-level precedence and preemption (MLPP)", "entity2": "traffic streams", "relation": "Preemption", "description": "The multi-level precedence and preemption (MLPP) framework supports the preemption of an active traffic stream by a higher precedence traffic stream when resources are limited."}, {"entity1": "traffic category (TC)", "entity2": "medium access control (MAC)", "relation": "Label", "description": "Traffic category (TC) is a label for medium access control (MAC) service data units (MSDUs) that have a distinct user priority (UP)."}, {"entity1": "traffic category (TC)", "entity2": "quality of service (QoS)", "relation": "Support", "description": "Traffic categories are meaningful only to MAC entities that support quality of service (QoS) within the MAC data service."}, {"entity1": "traffic classification (TCLAS)", "entity2": "protocol data units (PDUs)", "relation": "Classification", "description": "Traffic classification (TCLAS) is the specification of one of several types of matching filter to classify protocol data units (PDUs) or medium access control (MAC) service data units (MSDUs) as belonging to a particular traffic stream (TS)."}, {"entity1": "traffic classification (TCLAS)", "entity2": "medium access control (MAC) service data units (MSDUs)", "relation": "Classification", "description": "Traffic classification (TCLAS) is the specification of one of several types of matching filter to classify medium access control (MAC) service data units (MSDUs) as belonging to a particular traffic stream (TS)."}, {"entity1": "admission control", "entity2": "network", "relation": "Algorithm", "description": "Admission control is an algorithm intended to prevent the violation of parameterized service commitments made by the network to admitted flows by controlling the admittance of a new flow into a resource constrained network."}, {"entity1": "traffic specification (TSPEC)", "entity2": "quality-of-service (QoS)", "relation": "Characteristics", "description": "The traffic specification (TSPEC) is the quality-of-service (QoS) characteristics of a data flow to and from a QoS station (STA)."}, {"entity1": "traffic specification (TSPEC)", "entity2": "QoS station (STA)", "relation": "Specification", "description": "The traffic specification (TSPEC) is the quality-of-service (QoS) characteristics of a data flow to and from a QoS station (STA)."}], "cde23353-1d3f-431f-9b76-e3436103582b": [{"entity1": "traffic specification (TSPEC)", "entity2": "quality-of-service (QoS)", "relation": "Definition", "description": "traffic specification (TSPEC) defines the quality-of-service (QoS) characteristics of a data flow to and from a QoS station (STA)"}, {"entity1": "multi-level precedence and preemption (MLPP)", "entity2": "admission control", "relation": "Usage", "description": "multi-level precedence and preemption (MLPP) is used with admission control for the treatment of traffic streams based on precedence"}, {"entity1": "traffic stream (TS)", "entity2": "medium access control (MAC) service data units (MSDUs)", "relation": "Composition", "description": "traffic stream (TS) is a set of medium access control (MAC) service data units (MSDUs) to be delivered subject to the quality-of-service (QoS) parameter values"}, {"entity1": "traffic stream (TS)", "entity2": "traffic specification (TSPEC)", "relation": "Association", "description": "traffic stream (TS) is associated with a particular traffic specification (TSPEC) for delivery of MSDUs"}, {"entity1": "peer-to-peer traffic specification (PTP TSPEC)", "entity2": "non-access point (non-AP) QoS stations (STAs)", "relation": "Applicability", "description": "peer-to-peer traffic specification (PTP TSPEC) defines the quality-of-service (QoS) characteristics of a data flow between non-access point (non-AP) QoS stations (STAs)"}, {"entity1": "frame", "entity2": "peer protocol entities", "relation": "Exchange", "description": "frame is a unit of data exchanged between peer protocol entities"}, {"entity1": "IETF RFC 3610", "entity2": "Ethertype protocol", "relation": "Reference", "description": "IETF RFC 3610 is referenced in relation to Ethertype protocol discrimination"}, {"entity1": "medium access control (MAC) service data units (MSDUs)", "entity2": "MAC service access point (MAC SAP)", "relation": "Delivery", "description": "MSDUs are delivered using the priority parameter provided with those MSDUs at the MAC service access point (MAC SAP)"}, {"entity1": "quality-of-service (QoS) station (STA)", "entity2": "traffic specification (TSPEC)", "relation": "Characterization", "description": "traffic specification (TSPEC) characterizes the quality-of-service (QoS) of a data flow to and from a QoS station (STA)"}], "8a214f91-7f7c-4839-91e6-3640010a4849": [{"entity1": "MSDU", "entity2": "MAC service access point (SAP)", "relation": "Passing Parameters", "description": "MSDU parameters are passed across the MAC service access point (SAP)"}, {"entity1": "MAC service access point (SAP)", "entity2": "Distribution System", "relation": "Data Delivery", "description": "Data is delivered across the distribution system between access points (APs), mesh gates, and the portal of an extended service set (ESS) via the MAC service access point (SAP)"}, {"entity1": "TCLAS", "entity2": "MAC sublayer management entity (MLME)", "relation": "Filter Application", "description": "TCLAS filter is applied within the MAC sublayer management entity (MLME) for traffic classification"}, {"entity1": "TCLAS", "entity2": "MAC", "relation": "Filter Application", "description": "TCLAS filter is applied within the MAC itself for traffic classification"}, {"entity1": "GAS", "entity2": "RLQP", "relation": "Protocol Usage", "description": "GAS uses the registered location query protocol (RLQP) for transporting registered location information"}, {"entity1": "DSM", "entity2": "DS", "relation": "Medium Usage", "description": "DSM is the medium used by the distribution system (DS) for communications"}, {"entity1": "MAC", "entity2": "MLME", "relation": "Sublayer Management", "description": "MLME is the sublayer management entity of the MAC"}, {"entity1": "EPD", "entity2": "LLC", "relation": "Protocol Discrimination", "description": "EPD and LLC are used for protocol discrimination"}, {"entity1": "IETF RFC 4282", "entity2": "IETF RFC 2903", "relation": "Reference", "description": "IETF RFC 4282 and IETF RFC 2903 are referenced in the context of protocol definitions"}, {"entity1": "IEEE Std 802.11", "entity2": "DL-MU-MIMO", "relation": "Standard Support", "description": "IEEE Std 802.11 supports DL-MU-MIMO"}], "f9a38bce-fe4e-4d69-8cd9-872ed2090615": [{"entity1": "IETF RFC 3610", "entity2": "IETF RFC 4282", "relation": "Citation", "description": "Both are referenced as standards in the context of network services and protocols."}, {"entity1": "Extensible Authentication Protocol (EAP)", "entity2": "Authentication Server (AS)", "relation": "Authentication", "description": "EAP peer master session key (MSK) is derived and exported to the Authentication Server (AS)."}, {"entity1": "IEEE 802.1X authentication", "entity2": "Extensible Authentication Protocol (EAP)", "relation": "Authentication Method", "description": "IEEE 802.1X authentication uses EAP for authentication."}, {"entity1": "peer mesh station (STA)", "entity2": "mesh peering", "relation": "Establishment", "description": "A mesh peering is established with a peer mesh station (STA)."}, {"entity1": "frame", "entity2": "peer protocol entities", "relation": "Data Exchange", "description": "A frame is a unit of data exchanged between peer protocol entities."}, {"entity1": "master session key (MSK)", "entity2": "Extensible Authentication Protocol (EAP) peer", "relation": "Key Derivation", "description": "The master session key (MSK) is derived between the EAP peer and exported to the Authentication Server (AS)."}, {"entity1": "peer-to-peer link", "entity2": "quality-of-service (QoS) basic service set (BSS)", "relation": "Link Establishment", "description": "A peer-to-peer link can be established within a QoS BSS."}, {"entity1": "tunneled direct-link setup (TDLS) link", "entity2": "station-to-station (STA-to-STA) communication", "relation": "Alternative Link", "description": "A TDLS link is an alternative form of peer-to-peer link, similar to STA-to-STA communication in an IBSS."}, {"entity1": "peer-to-peer traffic specification (PTP TSPEC)", "entity2": "non-access point (non-AP) QoS stations (STAs)", "relation": "QoS Specification", "description": "PTP TSPEC defines the QoS characteristics of a data flow between non-AP QoS STAs."}, {"entity1": "subscription service provider (SSP)", "entity2": "network services", "relation": "Service Provision", "description": "An SSP offers connection to network services, possibly for a fee."}, {"entity1": "service hash", "entity2": "service name", "relation": "Hash Representation", "description": "A service hash is a value representing a service, formed from a hash of the service name."}, {"entity1": "transmit steering", "entity2": "modulation and coding scheme (MCS)", "relation": "Calculation", "description": "Transmit steering and MCS are calculated for optimization purposes."}, {"entity1": "calibration parameters", "entity2": "calculation", "relation": "Determination", "description": "Calibration parameters are determined through calculation."}], "9190f5a0-9d19-45ef-934a-d02a4d21b9e7": [{"entity1": "PTP TSPEC", "entity2": "QoS", "relation": "Characteristics", "description": "PTP TSPEC describes the QoS characteristics of a data flow between non-AP QoS STAs."}, {"entity1": "PTP TSPEC", "entity2": "non-AP QoS STAs", "relation": "Data Flow", "description": "PTP TSPEC is related to the data flow between non-AP QoS STAs."}, {"entity1": "MSK", "entity2": "EAP peer", "relation": "Key Derivation", "description": "MSK is derived between the EAP peer and exported by the EAP method."}, {"entity1": "MSK", "entity2": "Authentication Server (AS)", "relation": "Export", "description": "MSK is exported by the EAP method to the Authentication Server (AS)."}, {"entity1": "EAP peer", "entity2": "Authentication Server (AS)", "relation": "Authentication", "description": "EAP peer authenticates with the Authentication Server (AS) using the EAP method."}], "4169a02e-e588-499d-ab86-5436e55f7cc4": [{"entity1": "RAW Group Indication subfield", "entity2": "RAW Group subfield", "relation": "Presence Indication", "description": "The RAW Group Indication subfield indicates whether the RAW Group subfield is present in the RAW Assignment subfield."}, {"entity1": "RAW Group Indication subfield", "entity2": "RAW type", "relation": "Interpretation Dependency", "description": "The interpretation of the RAW Group Indication subfield depends on the type of RAW (generic, sounding, triggering frame, non-TIM, or AP PM)."}, {"entity1": "RAW Group Indication subfield", "entity2": "RAW Assignment subfield", "relation": "Location Indication", "description": "The RAW Group Indication subfield is located within the RAW Assignment subfield and indicates the presence or absence of the RAW Group subfield."}, {"entity1": "RAW Group Indication subfield", "entity2": "AIDs", "relation": "Reference", "description": "The RAW Group Indication subfield in the first RAW assignment references the range of AIDs in all the TIM bitmaps in the S1G Beacon frame."}, {"entity1": "RAW Group Indication subfield", "entity2": "TIM bitmaps", "relation": "Reference", "description": "The RAW Group Indication subfield in the first RAW assignment references the TIM bitmaps in the S1G Beacon frame."}, {"entity1": "RAW Group Indication subfield", "entity2": "S1G Beacon frame", "relation": "Reference", "description": "The RAW Group Indication subfield in the first RAW assignment references the S1G Beacon frame."}, {"entity1": "RAW Group Indication subfield", "entity2": "AP PM RAW", "relation": "Specific Interpretation", "description": "When the RAW is an AP PM RAW, the RAW Group Indication subfield equal to 0 indicates that the RAW group does not include any non-AP STAs, and the RAW Group subfield is not present."}, {"entity1": "RAW Group Indication subfield", "entity2": "non-TIM RAW", "relation": "Specific Interpretation", "description": "When the RAW is a non-TIM RAW, the RAW Group Indication subfield is set to 0 and the RAW Group subfield is not present."}, {"entity1": "RAW Group Indication subfield", "entity2": "Figure 9-672", "relation": "Format Reference", "description": "The format of the RAW Group subfield is shown in Figure 9-672."}, {"entity1": "RAW Group Indication subfield", "entity2": "RAW Assignment subfield", "relation": "Comparison", "description": "The RAW Group Indication subfield is compared to the previous RAW assignment to determine if the RAW group is the same."}], "3b675f2a-3361-4aea-98c5-0e36a2714979": [{"entity1": "RAW Group Indication subfield", "entity2": "RAW Group subfield", "relation": "Indication", "description": "The RAW Group Indication subfield indicates whether the RAW Group subfield is present in the RAW Assignment subfield."}, {"entity1": "RAW Group Indication subfield", "entity2": "RAW Assignment subfield", "relation": "PartOf", "description": "The RAW Group Indication subfield is part of the RAW Assignment subfield."}, {"entity1": "RAW type", "entity2": "RAW Group Indication subfield", "relation": "Influence", "description": "The RAW type influences the interpretation of the RAW Group Indication subfield."}, {"entity1": "RAW Group Indication subfield", "entity2": "RAW group", "relation": "Definition", "description": "The RAW Group Indication subfield defines whether the RAW group in the current RAW assignment is the same as the RAW group in the previous RAW assignment."}, {"entity1": "RAW Group Indication subfield", "entity2": "AIDs", "relation": "Comparison", "description": "The RAW Group Indication subfield in the first RAW assignment is set to 0 to indicate the RAW group is the same as the range of AIDs in all the TIM bitmaps in the S1G Beacon frame."}, {"entity1": "RAW Group Indication subfield", "entity2": "S1G Beacon frame", "relation": "Reference", "description": "The RAW Group Indication subfield references the S1G Beacon frame for the range of AIDs in the TIM bitmaps."}, {"entity1": "RAW Group Indication subfield", "entity2": "TIM bitmaps", "relation": "Reference", "description": "The RAW Group Indication subfield references the TIM bitmaps in the S1G Beacon frame for the range of AIDs."}, {"entity1": "RAW Assignment", "entity2": "RAW Group subfield", "relation": "Containment", "description": "The RAW Assignment may contain the RAW Group subfield based on the RAW Group Indication subfield."}, {"entity1": "RAW Group Indication subfield", "entity2": "0", "relation": "ValueIndication", "description": "When the RAW Group Indication subfield is equal to 0, it indicates the RAW group is the same as in the previous RAW assignment."}, {"entity1": "RAW Group Indication subfield", "entity2": "1", "relation": "ValueIndication", "description": "When the RAW Group Indication subfield is equal to 1, it indicates the RAW Group subfield is present in the RAW Assignment."}], "c028c706-7dfd-420d-88d9-d2f8afe96099": [{"entity1": "RAW Group Indication subfield", "entity2": "RAW Group subfield", "relation": "Presence Indication", "description": "The RAW Group Indication subfield indicates whether the RAW Group subfield is present in the RAW Assignment subfield."}, {"entity1": "RAW Group Indication subfield", "entity2": "RAW Assignment subfield", "relation": "Indicator of RAW Group Subfield Presence", "description": "The RAW Group Indication subfield indicates whether the RAW Group subfield is present in the RAW Assignment subfield based on its value (0 or 1)."}, {"entity1": "RAW Group Indication subfield", "entity2": "RPS element", "relation": "Part Of", "description": "The RAW Group Indication subfield is part of the RPS (Radio Parameter Set) element."}, {"entity1": "RAW Group Indication subfield", "entity2": "RAW type", "relation": "Interpretation Dependency", "description": "The interpretation of the RAW Group Indication subfield depends on the type of RAW (generic, sounding, triggering frame, non-TIM, or AP PM)."}, {"entity1": "RAW Group Indication subfield", "entity2": "Previous RAW Assignment", "relation": "Comparison Basis", "description": "The RAW Group Indication subfield value (0 or 1) is used to compare the current RAW assignment with the previous RAW assignment."}, {"entity1": "RAW Group Indication subfield", "entity2": "AIDs", "relation": "Reference", "description": "The RAW Group Indication subfield in the first RAW assignment references the range of AIDs in all the TIM bitmaps in the S1G Beacon frame."}, {"entity1": "RAW Group Indication subfield", "entity2": "TIM bitmaps", "relation": "Reference", "description": "The RAW Group Indication subfield in the first RAW assignment references the TIM bitmaps in the S1G Beacon frame."}, {"entity1": "RAW Group Indication subfield", "entity2": "S1G Beacon frame", "relation": "Reference", "description": "The RAW Group Indication subfield in the first RAW assignment references the S1G Beacon frame for the range of AIDs in the TIM bitmaps."}, {"entity1": "RAW Group Indication subfield", "entity2": "AP PM RAW", "relation": "Specific Interpretation", "description": "For AP PM RAW, the RAW Group Indication subfield value indicates whether the RAW group includes non-AP STAs."}, {"entity1": "RAW Group Indication subfield", "entity2": "Non-TIM RAW", "relation": "Fixed Value", "description": "For non-TIM RAW, the RAW Group Indication subfield is always set to 0."}], "c715a271-d56a-4012-aa70-28269b24344a": [{"entity1": "IGTK", "entity2": "STA", "relation": "Assignment", "description": "IGTK is assigned by the broadcast/multicast source station (STA)"}, {"entity1": "IGTK", "entity2": "MAC", "relation": "Protection", "description": "IGTK is used to protect group addressed medium access control (MAC) management protocol data units (MMPDUs)"}, {"entity1": "IPI", "entity2": "PHY", "relation": "Indication", "description": "IPI is a physical layer (PHY) indication of the total channel power"}, {"entity1": "IPI", "entity2": "STA", "relation": "Measurement", "description": "IPI is measured in the channel at the receiving antenna connector while the station (STA) is idle"}, {"entity1": "MAC", "entity2": "MMPDUs", "relation": "Management", "description": "MAC management protocol data units (MMPDUs) are protected by IGTK"}, {"entity1": "STA", "entity2": "PHY", "relation": "Interaction", "description": "The station (STA) interacts with the physical layer (PHY) to measure the idle power indicator (IPI)"}], "6f81910f-4063-4f36-a85d-557efb6a6975": [{"entity1": "researchers", "entity2": "engineers", "relation": "Collaboration-Type", "description": "Researchers and engineers are expected to continue obtaining new capabilities of interest to each other."}, {"entity1": "RAG systems", "entity2": "software engineering", "relation": "Research Area", "description": "The paper investigates RAG systems from a software engineering perspective."}, {"entity1": "Amanda Edgar", "entity2": "AI Tutor project", "relation": "Author-Role", "description": "Amanda Edgar was involved in making the AI Tutor project possible."}, {"entity1": "Rajesh Vasa", "entity2": "AI Tutor project", "relation": "Author-Role", "description": "Rajesh Vasa was involved in making the AI Tutor project possible."}, {"entity1": "Kon Mouzakis", "entity2": "AI Tutor project", "relation": "Author-Role", "description": "Kon Mouzakis was involved in making the AI Tutor project possible."}, {"entity1": "Matteo Vergani", "entity2": "AI Tutor project", "relation": "Author-Role", "description": "Matteo Vergani was involved in making the AI Tutor project possible."}, {"entity1": "Trish McCluskey", "entity2": "AI Tutor project", "relation": "Author-Role", "description": "Trish McCluskey was involved in making the AI Tutor project possible."}, {"entity1": "Kathryn Perus", "entity2": "AI Tutor project", "relation": "Author-Role", "description": "Kathryn Perus was involved in making the AI Tutor project possible."}, {"entity1": "Tara Draper", "entity2": "AI Tutor project", "relation": "Author-Role", "description": "Tara Draper was involved in making the AI Tutor project possible."}, {"entity1": "Joan Sutherland", "entity2": "AI Tutor project", "relation": "Author-Role", "description": "Joan Sutherland was involved in making the AI Tutor project possible."}, {"entity1": "Ruary Ross", "entity2": "AI Tutor project", "relation": "Author-Role", "description": "Ruary Ross was involved in making the AI Tutor project possible."}, {"entity1": "researchers", "entity2": "RAG systems", "relation": "Research Area", "description": "Researchers are investigating RAG systems."}, {"entity1": "engineers", "entity2": "RAG systems", "relation": "Application", "description": "RAG systems are of interest to engineers."}], "99ea3b12-d6e8-4964-ab45-c03207453c87": [{"entity1": "The Church of Jesus Christ of Latter-day Saints", "entity2": "Baptism", "relation": "Author-Role", "description": "The Church of Jesus Christ of Latter-day Saints has a specific role in the context of Baptism, emphasizing its importance."}, {"entity1": "Heilongjiang", "entity2": "China", "relation": "Affiliation", "description": "Heilongjiang is a province in China, indicating an affiliation between the two entities."}, {"entity1": "Harbin", "entity2": "Heilongjiang", "relation": "Affiliation", "description": "Harbin is the capital of Heilongjiang, showing an affiliation between the city and the province."}, {"entity1": "Songhua River", "entity2": "Harbin", "relation": "Geographical Relationship", "description": "The Songhua River plays a significant role in the water supply of Harbin, indicating a geographical relationship."}, {"entity1": "Tier 2", "entity2": "Progress Monitoring", "relation": "Methodology", "description": "In Tier 2, progress monitoring is used to determine the success of interventions, showing a methodological relationship."}, {"entity1": "Socio-economic status", "entity2": "Student Learning Outcomes", "relation": "Impact", "description": "Socio-economic status can impact student learning outcomes, indicating a relationship between the two entities."}, {"entity1": "Handkerchief tree", "entity2": "Soil type", "relation": "Dataset-Origin", "description": "The handkerchief tree's growth can be influenced by soil type, showing a relationship between the tree and the type of soil it is planted in."}, {"entity1": "OTC plots", "entity2": "Plant genera", "relation": "Research Area", "description": "The OTC plots are used to study various plant genera, indicating a research area relationship."}, {"entity1": "Temperature", "entity2": "Handkerchief tree", "relation": "Environmental Factor", "description": "Temperature is an environmental factor that can affect the growth of the handkerchief tree."}, {"entity1": "Rainfall", "entity2": "Humidity rate", "relation": "Environmental Relationship", "description": "Rainfall can influence the humidity rate, showing an environmental relationship between the two entities."}, {"entity1": "Sunshine rate", "entity2": "Handkerchief tree", "relation": "Growth Factor", "description": "The sunshine rate can affect the growth of the handkerchief tree, indicating a relationship between the two entities."}], "b2ea4353-e377-4cf8-bf88-59689b9a0ee7": [{"entity1": "Figure 9", "entity2": "Single-Doc Support (r1) data", "relation": "Illustration", "description": "Figure 9 is used to illustrate the prompt for synthesizing Single-Doc Support (r1) data."}, {"entity1": "Simulated Instruction", "entity2": "Single-Doc Support (r1) data", "relation": "Generation", "description": "The Simulated Instruction is used to generate the Single-Doc Support (r1) data."}, {"entity1": "r1", "entity2": "Single-Doc Support (r1) data", "relation": "Association", "description": "r1 is associated with the Single-Doc Support (r1) data, indicating a specific type or instance of the data."}], "0d2603f2-eed6-4e32-8f8a-6363fd74b03b": [{"entity1": "Figure 11", "entity2": "Single-Doc Answer (r3)", "relation": "Publication Venue", "description": "Figure 11 is used to synthesize Single-Doc Answer (r3) data."}, {"entity1": "Simulated Instruction", "entity2": "q*", "relation": "Author-Role", "description": "The Simulated Instruction is used to generate q*."}, {"entity1": "a*", "entity2": "Single-Doc Answer (r3)", "relation": "Result", "description": "a* is the result of the Single-Doc Answer (r3) process."}, {"entity1": "q*", "entity2": "Simulated Instruction", "relation": "Tool/Resource", "description": "q* is generated using the Simulated Instruction as a tool or resource."}], "d248f1b3-bb7a-4edc-bf16-870ac7163dd0": [{"entity1": "Q. Leng", "entity2": "K. Uhlenhuth", "relation": "Co-author", "description": "Q. Leng and K. Uhlenhuth co-authored the paper 'Best practices for llm evaluation of rag applications'"}, {"entity1": "X. Wang", "entity2": "Z. Wang", "relation": "Co-author", "description": "X. Wang and Z. Wang co-authored the paper 'Searching for best practices in retrieval-augmented generation'"}, {"entity1": "M. Lee", "entity2": "S. An", "relation": "Co-author", "description": "M. Lee and S. An co-authored the paper 'Planrag: A plan-then-retrieval augmented generation for generative large language models as decision makers'"}, {"entity1": "D. Arora", "entity2": "A. Kini", "relation": "Co-author", "description": "D. Arora and A. Kini co-authored the paper 'Gar-meets-rag paradigm for zero-shot information retrieval'"}, {"entity1": "P. Lewis", "entity2": "E. Perez", "relation": "Co-author", "description": "P. Lewis and E. Perez co-authored the paper 'Retrieval-augmented generation for knowledge-intensive nlp tasks'"}, {"entity1": "S. Borgeaud", "entity2": "A. Mensch", "relation": "Co-author", "description": "S. Borgeaud and A. Mensch co-authored the paper 'Improving language models by retrieving from trillions of tokens'"}, {"entity1": "G. Izacard", "entity2": "P. Lewis", "relation": "Co-author", "description": "G. Izacard and P. Lewis co-authored the paper 'Few-shot learning with retrieval augmented language models'"}, {"entity1": "H. Trivedi", "entity2": "N. Balasubramanian", "relation": "Co-author", "description": "H. Trivedi and N. Balasubramanian co-authored the paper 'Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions'"}, {"entity1": "X. Ma", "entity2": "Y. Gong", "relation": "Co-author", "description": "X. Ma and Y. Gong co-authored the paper 'Query rewriting for retrieval-augmented large language models'"}, {"entity1": "N. Anderson", "entity2": "C. Wilson", "relation": "Co-author", "description": "N. Anderson and C. Wilson co-authored the paper 'Lingua: Addressing scenarios for live interpretation and automatic dubbing'"}, {"entity1": "Q. Leng", "entity2": "Databricks", "relation": "Affiliation", "description": "Q. Leng is affiliated with Databricks"}, {"entity1": "X. Wang", "entity2": "arXiv:2407.01219", "relation": "Publication Venue", "description": "X. Wang published the paper 'Searching for best practices in retrieval-augmented generation' on arXiv:2407.01219"}, {"entity1": "M. Lee", "entity2": "arXiv:2406.12430", "relation": "Publication Venue", "description": "M. Lee published the paper 'Planrag: A plan-then-retrieval augmented generation for generative large language models as decision makers' on arXiv:2406.12430"}, {"entity1": "D. Arora", "entity2": "arXiv:2310.20158", "relation": "Publication Venue", "description": "D. Arora published the paper 'Gar-meets-rag paradigm for zero-shot information retrieval' on arXiv:2310.20158"}, {"entity1": "P. Lewis", "entity2": "Advances in Neural Information Processing Systems", "relation": "Publication Venue", "description": "P. Lewis published the paper 'Retrieval-augmented generation for knowledge-intensive nlp tasks' in Advances in Neural Information Processing Systems"}, {"entity1": "S. Borgeaud", "entity2": "International conference on machine learning", "relation": "Publication Venue", "description": "S. Borgeaud published the paper 'Improving language models by retrieving from trillions of tokens' in International conference on machine learning"}, {"entity1": "G. Izacard", "entity2": "arXiv:2208.03299", "relation": "Publication Venue", "description": "G. Izacard published the paper 'Few-shot learning with retrieval augmented language models' on arXiv:2208.03299"}, {"entity1": "H. Trivedi", "entity2": "arXiv:2212.10509", "relation": "Publication Venue", "description": "H. Trivedi published the paper 'Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions' on arXiv:2212.10509"}, {"entity1": "X. Ma", "entity2": "arXiv:2305.14283", "relation": "Publication Venue", "description": "X. Ma published the paper 'Query rewriting for retrieval-augmented large language models' on arXiv:2305.14283"}, {"entity1": "N. Anderson", "entity2": "Proceedings of the 15th Biennial Conference of the Association for Machine Translation in the Americas", "relation": "Publication Venue", "description": "N. Anderson published the paper 'Lingua: Addressing scenarios for live interpretation and automatic dubbing' in Proceedings of the 15th Biennial Conference of the Association for Machine Translation in the Americas"}], "ae42aee1-77c9-4a75-9c68-8f8e7609d8a3": [{"entity1": "Paliouras", "entity2": "BioASQ-QA", "relation": "Author-Role", "description": "Paliouras is the author of the BioASQ-QA corpus."}, {"entity1": "Simon Lermen", "entity2": "LoRA Fine-tuning", "relation": "Author-Expertise", "description": "Simon Lermen is an expert in LoRA Fine-tuning."}, {"entity1": "Patrick Lewis", "entity2": "Retrieval-augmented generation", "relation": "Author-Expertise", "description": "Patrick Lewis is an expert in retrieval-augmented generation."}, {"entity1": "Nelson F Liu", "entity2": "Lost in the middle: How language models use long contexts", "relation": "Author-Role", "description": "Nelson F Liu is the author of the paper 'Lost in the middle: How language models use long contexts'."}, {"entity1": "Yang Liu", "entity2": "G-eval", "relation": "Author-Role", "description": "Yang Liu is the author of the G-eval paper."}, {"entity1": "Noor Nashid", "entity2": "Retrieval-based prompt selection for code-related few-shot learning", "relation": "Author-Role", "description": "Noor Nashid is the author of the paper 'Retrieval-based prompt selection for code-related few-shot learning'."}, {"entity1": "OpenAI", "entity2": "GPT-4 Technical Report", "relation": "Publication Venue", "description": "The GPT-4 Technical Report is published by OpenAI."}, {"entity1": "Alec Radford", "entity2": "Robust speech recognition via large-scale weak supervision", "relation": "Author-Role", "description": "Alec Radford is the author of the paper 'Robust speech recognition via large-scale weak supervision'."}, {"entity1": "Shamane Siriwardhana", "entity2": "Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering", "relation": "Author-Role", "description": "Shamane Siriwardhana is the author of the paper 'Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering'."}, {"entity1": "Yutao Zhu", "entity2": "Large language models for information retrieval: A survey", "relation": "Author-Role", "description": "Yutao Zhu is the author of the paper 'Large language models for information retrieval: A survey'."}, {"entity1": "BioASQ-QA", "entity2": "Scientific Data", "relation": "Publication Venue", "description": "The BioASQ-QA corpus is published in Scientific Data."}, {"entity1": "LoRA Fine-tuning", "entity2": "arXiv", "relation": "Publication Venue", "description": "The LoRA Fine-tuning paper is published on arXiv."}, {"entity1": "Retrieval-augmented generation", "entity2": "Advances in Neural Information Processing Systems", "relation": "Publication Venue", "description": "The retrieval-augmented generation paper is published in Advances in Neural Information Processing Systems."}, {"entity1": "Lost in the middle: How language models use long contexts", "entity2": "arXiv", "relation": "Publication Venue", "description": "The paper 'Lost in the middle: How language models use long contexts' is published on arXiv."}, {"entity1": "G-eval", "entity2": "arXiv", "relation": "Publication Venue", "description": "The G-eval paper is published on arXiv."}, {"entity1": "Retrieval-based prompt selection for code-related few-shot learning", "entity2": "International Conference on Software Engineering", "relation": "Publication Venue", "description": "The paper 'Retrieval-based prompt selection for code-related few-shot learning' is published in the International Conference on Software Engineering."}, {"entity1": "GPT-4 Technical Report", "entity2": "https://doi.org/10.48550/ARXIV.2303.08774", "relation": "Publication Date", "description": "The GPT-4 Technical Report is published on https://doi.org/10.48550/ARXIV.2303.08774."}, {"entity1": "Robust speech recognition via large-scale weak supervision", "entity2": "International Conference on Machine Learning", "relation": "Publication Venue", "description": "The paper 'Robust speech recognition via large-scale weak supervision' is published in the International Conference on Machine Learning."}, {"entity1": "Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering", "entity2": "Transactions of the Association for Computational Linguistics", "relation": "Publication Venue", "description": "The paper 'Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering' is published in Transactions of the Association for Computational Linguistics."}, {"entity1": "Large language models for information retrieval: A survey", "entity2": "arXiv", "relation": "Publication Venue", "description": "The paper 'Large language models for information retrieval: A survey' is published on arXiv."}], "a7c12e2b-b644-49cd-bd72-48bd0204dd1b": [{"entity1": "F. Xu", "entity2": "W. Shi", "relation": "Co-author", "description": "F. Xu and W. Shi are co-authors of the paper 'Recomp: Improving retrieval-augmented lms with compression and selective augmentation'."}, {"entity1": "E. Choi", "entity2": "F. Xu", "relation": "Co-author", "description": "E. Choi and F. Xu are co-authors of the paper 'Recomp: Improving retrieval-augmented lms with compression and selective augmentation'."}, {"entity1": "P. Xu", "entity2": "W. Ping", "relation": "Co-author", "description": "P. Xu and W. Ping are co-authors of the paper 'Retrieval meets long context large language models'."}, {"entity1": "X. Wu", "entity2": "L. McAfee", "relation": "Co-author", "description": "X. Wu and L. McAfee are co-authors of the paper 'Retrieval meets long context large language models'."}, {"entity1": "R. Xu", "entity2": "Z. Qi", "relation": "Co-author", "description": "R. Xu and Z. Qi are co-authors of the paper 'Knowledge conflicts for llms: A survey'."}, {"entity1": "C. Wang", "entity2": "H. Wang", "relation": "Co-author", "description": "C. Wang and H. Wang are co-authors of the paper 'Knowledge conflicts for llms: A survey'."}, {"entity1": "Y. Zhang", "entity2": "M. Khalifa", "relation": "Co-author", "description": "Y. Zhang and M. Khalifa are co-authors of the paper 'Merging generated and retrieved knowledge for open-domain qa'."}, {"entity1": "L. Logeswaran", "entity2": "M. Lee", "relation": "Co-author", "description": "L. Logeswaran and M. Lee are co-authors of the paper 'Merging generated and retrieved knowledge for open-domain qa'."}, {"entity1": "Z. Zhao", "entity2": "E. Monti", "relation": "Co-author", "description": "Z. Zhao and E. Monti are co-authors of the paper 'Enhancing contextual understanding in large language models through contrastive decoding'."}, {"entity1": "J. Lehmann", "entity2": "H. Assem", "relation": "Co-author", "description": "J. Lehmann and H. Assem are co-authors of the paper 'Enhancing contextual understanding in large language models through contrastive decoding'."}, {"entity1": "T. Zhu", "entity2": "Q. Liu", "relation": "Co-author", "description": "T. Zhu and Q. Liu are co-authors of the paper 'Unraveling cross-modality knowledge conflict in large vision-language models'."}, {"entity1": "F. Wang", "entity2": "Z. Tu", "relation": "Co-author", "description": "F. Wang and Z. Tu are co-authors of the paper 'Unraveling cross-modality knowledge conflict in large vision-language models'."}, {"entity1": "W. Zou", "entity2": "R. Geng", "relation": "Co-author", "description": "W. Zou and R. Geng are co-authors of the paper 'Poisonedrag: Knowledge poisoning attacks to retrieval-'."}, {"entity1": "B. Wang", "entity2": "J. Jia", "relation": "Co-author", "description": "B. Wang and J. Jia are co-authors of the paper 'Poisonedrag: Knowledge poisoning attacks to retrieval-'."}, {"entity1": "arXiv:2310.04408", "entity2": "Recomp: Improving retrieval-augmented lms with compression and selective augmentation", "relation": "Publication Venue", "description": "The paper 'Recomp: Improving retrieval-augmented lms with compression and selective augmentation' is published on arXiv:2310.04408."}, {"entity1": "The Twelfth International Conference on Learning Representations", "entity2": "Retrieval meets long context large language models", "relation": "Publication Venue", "description": "The paper 'Retrieval meets long context large language models' is published in The Twelfth International Conference on Learning Representations."}, {"entity1": "arXiv:2403.08319", "entity2": "Knowledge conflicts for llms: A survey", "relation": "Publication Venue", "description": "The paper 'Knowledge conflicts for llms: A survey' is published on arXiv:2403.08319."}, {"entity1": "The 2023 Conference on Empirical Methods in Natural Language Processing", "entity2": "Merging generated and retrieved knowledge for open-domain qa", "relation": "Publication Venue", "description": "The paper 'Merging generated and retrieved knowledge for open-domain qa' is published in The 2023 Conference on Empirical Methods in Natural Language Processing."}, {"entity1": "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "entity2": "Enhancing contextual understanding in large language models through contrastive decoding", "relation": "Publication Venue", "description": "The paper 'Enhancing contextual understanding in large language models through contrastive decoding' is published in Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies."}, {"entity1": "arXiv:2410.03659", "entity2": "Unraveling cross-modality knowledge conflict in large vision-language models", "relation": "Publication Venue", "description": "The paper 'Unraveling cross-modality knowledge conflict in large vision-language models' is published on arXiv:2410.03659."}, {"entity1": "arXiv:2406.04744", "entity2": "Crag\u2013comprehensive rag benchmark", "relation": "Publication Venue", "description": "The paper 'Crag\u2013comprehensive rag benchmark' is published on arXiv:2406.04744."}, {"entity1": "The Eleventh International Conference on Learning Representations", "entity2": "Generate rather than retrieve: Large language models are strong context generators", "relation": "Publication Venue", "description": "The paper 'Generate rather than retrieve: Large language models are strong context generators' is published in The Eleventh International Conference on Learning Representations."}, {"entity1": "arXiv:2311.09210", "entity2": "Chain-of-note: Enhancing robustness in retrieval-augmented language models", "relation": "Publication Venue", "description": "The paper 'Chain-of-note: Enhancing robustness in retrieval-augmented language models' is published on arXiv:2311.09210."}, {"entity1": "arXiv:2407.02485", "entity2": "Rankrag: Unifying context ranking with retrieval-augmented generation in llms", "relation": "Publication Venue", "description": "The paper 'Rankrag: Unifying context ranking with retrieval-augmented generation in llms' is published on arXiv:2407.02485."}, {"entity1": "arXiv:2401.15884", "entity2": "Corrective retrieval augmented generation", "relation": "Publication Venue", "description": "The paper 'Corrective retrieval augmented generation' is published on arXiv:2401.15884."}], "6414772e-a05c-41a3-982b-f05dd5f8121a": [{"entity1": "RQ-RAG", "entity2": "GPT-4", "relation": "Comparison", "description": "RQ-RAG is compared to GPT-4 in terms of performance across different RAG scenarios."}, {"entity1": "ChatQA-1.5", "entity2": "ChatQA-2.0", "relation": "Comparison", "description": "ChatQA-1.5 and ChatQA-2.0 are compared in terms of performance across different RAG scenarios."}, {"entity1": "RAG-Instruct", "entity2": "RQ-RAG", "relation": "Comparison", "description": "RAG-Instruct is compared to RQ-RAG in terms of performance across different RAG scenarios."}, {"entity1": "Single-hop QA", "entity2": "Multi-hop QA", "relation": "Comparison", "description": "Single-hop QA and Multi-hop QA are compared in terms of performance across different RAG scenarios."}, {"entity1": "TriviaQA", "entity2": "HotPotQA", "relation": "Comparison", "description": "TriviaQA and HotPotQA are compared in terms of performance across different RAG scenarios."}, {"entity1": "RQ-RAG Data", "entity2": "Self-RAG Data", "relation": "Comparison", "description": "RQ-RAG Data and Self-RAG Data are compared in terms of dataset characteristics."}, {"entity1": "LongAlpaca", "entity2": "SQuAD2.0", "relation": "Comparison", "description": "LongAlpaca and SQuAD2.0 are compared in terms of dataset characteristics."}, {"entity1": "NarrativeQA", "entity2": "RAG-12000", "relation": "Comparison", "description": "NarrativeQA and RAG-12000 are compared in terms of dataset characteristics."}, {"entity1": "Achiam et al.", "entity2": "GPT-4", "relation": "Author-Role", "description": "Achiam et al. are authors of GPT-4."}, {"entity1": "Chan et al.", "entity2": "RQ-RAG", "relation": "Author-Role", "description": "Chan et al. are authors of RQ-RAG."}, {"entity1": "Asai et al.", "entity2": "Self-RAG", "relation": "Author-Role", "description": "Asai et al. are authors of Self-RAG."}, {"entity1": "Liu et al.", "entity2": "ChatQA-1.5", "relation": "Author-Role", "description": "Liu et al. are authors of ChatQA-1.5."}, {"entity1": "Liu et al.", "entity2": "ChatQA-2.0", "relation": "Author-Role", "description": "Liu et al. are authors of ChatQA-2.0."}, {"entity1": "Yang et al.", "entity2": "HotPotQA", "relation": "Author-Role", "description": "Yang et al. are authors of HotPotQA."}, {"entity1": "RQ-RAG", "entity2": "Single-Doc Answer", "relation": "Experiment-Outcome", "description": "RQ-RAG is evaluated on Single-Doc Answer scenario."}, {"entity1": "RQ-RAG", "entity2": "Multi-Doc Answer", "relation": "Experiment-Outcome", "description": "RQ-RAG is evaluated on Multi-Doc Answer scenario."}, {"entity1": "ChatQA-1.5", "entity2": "Single-Doc Support", "relation": "Experiment-Outcome", "description": "ChatQA-1.5 is evaluated on Single-Doc Support scenario."}, {"entity1": "ChatQA-2.0", "entity2": "Multi-Doc Support", "relation": "Experiment-Outcome", "description": "ChatQA-2.0 is evaluated on Multi-Doc Support scenario."}, {"entity1": "RAG-Instruct", "entity2": "Useless Doc", "relation": "Experiment-Outcome", "description": "RAG-Instruct is evaluated on Useless Doc scenario."}], "7f8afd0d-9af4-4a54-9627-8c02f439ae1c": [{"entity1": "Llama-3.1-70B-Instruct", "entity2": "RAG Benchmarks", "relation": "Zero-shot performance", "description": "Llama-3.1-70B-Instruct achieved a zero-shot performance on RAG Benchmarks."}, {"entity1": "Llama-3.1-70B +RAG-Instruct", "entity2": "RAG Benchmarks", "relation": "Zero-shot performance", "description": "Llama-3.1-70B +RAG-Instruct achieved a zero-shot performance on RAG Benchmarks."}, {"entity1": "Llama-3-8B", "entity2": "RAG Benchmarks", "relation": "Zero-shot performance", "description": "Llama-3-8B achieved a zero-shot performance on RAG Benchmarks."}, {"entity1": "Llama-3.1-8B +RAG-Instruct", "entity2": "RAG Benchmarks", "relation": "Zero-shot performance", "description": "Llama-3.1-8B +RAG-Instruct achieved a zero-shot performance on RAG Benchmarks."}, {"entity1": "Table 4", "entity2": "RAG Benchmarks", "relation": "Publication Venue", "description": "Table 4 presents the zero-shot performance of different instruction datasets on RAG Benchmarks."}, {"entity1": "instruction datasets", "entity2": "RAG Benchmarks", "relation": "Zero-shot performance", "description": "Instruction datasets were evaluated on RAG Benchmarks for zero-shot performance."}, {"entity1": "Llama-3.1-8B-Instruct", "entity2": "RAG Benchmarks", "relation": "Zero-shot performance", "description": "Llama-3.1-8B-Instruct achieved a zero-shot performance on RAG Benchmarks."}, {"entity1": "Llama-3-8B +RAG-Instruct", "entity2": "RAG Benchmarks", "relation": "Zero-shot performance", "description": "Llama-3-8B +RAG-Instruct achieved a zero-shot performance on RAG Benchmarks."}, {"entity1": "Self-RAG", "entity2": "RAG Benchmarks", "relation": "Zero-shot performance", "description": "Self-RAG achieved a zero-shot performance on RAG Benchmarks."}, {"entity1": "instruction-tuned LMs", "entity2": "RAG Benchmarks", "relation": "Zero-shot performance", "description": "Instruction-tuned LMs were evaluated on RAG Benchmarks for zero-shot performance."}, {"entity1": "RAG instruction dataset", "entity2": "RAG Benchmarks", "relation": "Zero-shot performance", "description": "RAG instruction dataset was evaluated on RAG Benchmarks for zero-shot performance."}, {"entity1": "Llama-3.1-70B", "entity2": "RAG Benchmarks", "relation": "Zero-shot performance", "description": "Llama-3.1-70B achieved a zero-shot performance on RAG Benchmarks."}, {"entity1": "Zero-shot performance", "entity2": "RAG Benchmarks", "relation": "Evaluation metric", "description": "Zero-shot performance is an evaluation metric used on RAG Benchmarks."}, {"entity1": "RAG Benchmarks", "entity2": "Yang et al.", "relation": "Research Area", "description": "RAG Benchmarks are related to the research area of Yang et al."}, {"entity1": "Llama-2-7B +RAG-Instruct", "entity2": "RAG Benchmarks", "relation": "Zero-shot performance", "description": "Llama-2-7B +RAG-Instruct achieved a zero-shot performance on RAG Benchmarks."}, {"entity1": "Qwen2.5-7B +RAG-Instruct", "entity2": "RAG Benchmarks", "relation": "Zero-shot performance", "description": "Qwen2.5-7B +RAG-Instruct achieved a zero-shot performance on RAG Benchmarks."}, {"entity1": "Qwen2.5-7B-Instruct", "entity2": "RAG Benchmarks", "relation": "Zero-shot performance", "description": "Qwen2.5-7B-Instruct achieved a zero-shot performance on RAG Benchmarks."}, {"entity1": "ChatQA-1.5", "entity2": "RAG Benchmarks", "relation": "Zero-shot performance", "description": "ChatQA-1.5 achieved a zero-shot performance on RAG Benchmarks."}, {"entity1": "Llama-2-7B", "entity2": "RAG Benchmarks", "relation": "Zero-shot performance", "description": "Llama-2-7B achieved a zero-shot performance on RAG Benchmarks."}, {"entity1": "RQ-RAG", "entity2": "RAG Benchmarks", "relation": "Zero-shot performance", "description": "RQ-RAG achieved a zero-shot performance on RAG Benchmarks."}, {"entity1": "GPT-4o", "entity2": "RAG Benchmarks", "relation": "Zero-shot performance", "description": "GPT-4o achieved a zero-shot performance on RAG Benchmarks."}, {"entity1": "Qwen2.5-7B", "entity2": "RAG Benchmarks", "relation": "Zero-shot performance", "description": "Qwen2.5-7B achieved a zero-shot performance on RAG Benchmarks."}, {"entity1": "RAG-specific baselines", "entity2": "RAG Benchmarks", "relation": "Zero-shot performance", "description": "RAG-specific baselines were evaluated on RAG Benchmarks for zero-shot performance."}, {"entity1": "Training settings", "entity2": "RAG Benchmarks", "relation": "Experiment-Design", "description": "Training settings were used to design experiments on RAG Benchmarks."}, {"entity1": "Yang et al.", "entity2": "RAG Benchmarks", "relation": "Research Area", "description": "Yang et al. are related to the research area of RAG Benchmarks."}, {"entity1": "GPT-4o-mini", "entity2": "RAG Benchmarks", "relation": "Zero-shot performance", "description": "GPT-4o-mini achieved a zero-shot performance on RAG Benchmarks."}, {"entity1": "Llama-3-8B-Instruct", "entity2": "RAG Benchmarks", "relation": "Zero-shot performance", "description": "Llama-3-8B-Instruct achieved a zero-shot performance on RAG Benchmarks."}, {"entity1": "Llama-3.1-8B", "entity2": "RAG Benchmarks", "relation": "Zero-shot performance", "description": "Llama-3.1-8B achieved a zero-shot performance on RAG Benchmarks."}, {"entity1": "ChatQA-2.0", "entity2": "RAG Benchmarks", "relation": "Zero-shot performance", "description": "ChatQA-2.0 achieved a zero-shot performance on RAG Benchmarks."}], "5d077295-a41f-4735-ba10-e7de767615f9": [{"entity1": "Kelvin Guu", "entity2": "Kenton Lee", "relation": "Co-author", "description": "Kelvin Guu and Kenton Lee co-authored the paper 'Retrieval augmented language model pre-training' in 2020."}, {"entity1": "Kelvin Guu", "entity2": "Retrieval augmented language model pre-training", "relation": "Author-Role", "description": "Kelvin Guu is an author of the paper 'Retrieval augmented language model pre-training'."}, {"entity1": "Tsun hin Cheung", "entity2": "Kin Man Lam", "relation": "Co-author", "description": "Tsun hin Cheung and Kin Man Lam co-authored the paper 'Factllama: Optimizing instruction-following language models with external knowledge for automated fact-checking' in 2023."}, {"entity1": "Factllama", "entity2": "2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference", "relation": "Publication Venue", "description": "The paper 'Factllama: Optimizing instruction-following language models with external knowledge for automated fact-checking' was published at the 2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference."}, {"entity1": "Xanh Ho", "entity2": "Anh-Khoa Duong Nguyen", "relation": "Co-author", "description": "Xanh Ho and Anh-Khoa Duong Nguyen co-authored the paper 'Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps' in 2020."}, {"entity1": "Gautier Izacard", "entity2": "Mathilde Caron", "relation": "Co-author", "description": "Gautier Izacard and Mathilde Caron co-authored the paper 'Unsupervised dense information retrieval with contrastive learning'."}, {"entity1": "Gautier Izacard", "entity2": "Atlas", "relation": "Author-Role", "description": "Gautier Izacard is an author of the paper 'Atlas: Few-shot learning with retrieval augmented language models'."}, {"entity1": "Soyeong Jeong", "entity2": "Jinheon Baek", "relation": "Co-author", "description": "Soyeong Jeong and Jinheon Baek co-authored the paper 'Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity' in 2024."}, {"entity1": "Zhengbao Jiang", "entity2": "Frank F Xu", "relation": "Co-author", "description": "Zhengbao Jiang and Frank F Xu co-authored the paper 'Active retrieval augmented generation' in 2023."}, {"entity1": "Jiajie Jin", "entity2": "Yutao Zhu", "relation": "Co-author", "description": "Jiajie Jin and Yutao Zhu co-authored the paper 'Flashrag: A modular toolkit for efficient retrieval-augmented generation research' in 2024."}, {"entity1": "Qiao Jin", "entity2": "Bhuwan Dhingra", "relation": "Co-author", "description": "Qiao Jin and Bhuwan Dhingra co-authored the paper 'Pubmedqa: A dataset for biomedical research question answering' in 2019."}, {"entity1": "Mandar Joshi", "entity2": "Eunsol Choi", "relation": "Co-author", "description": "Mandar Joshi and Eunsol Choi co-authored the paper 'Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension' in 2017."}, {"entity1": "Kelvin Guu", "entity2": "International conference on machine learning", "relation": "Publication Venue", "description": "The paper 'Retrieval augmented language model pre-training' by Kelvin Guu was published at the International conference on machine learning in 2020."}, {"entity1": "Tsun hin Cheung", "entity2": "APSIPA ASC", "relation": "Publication Venue", "description": "The paper 'Factllama: Optimizing instruction-following language models with external knowledge for automated fact-checking' by Tsun hin Cheung was published at the APSIPA ASC in 2023."}, {"entity1": "Xanh Ho", "entity2": "Proceedings of the 28th International Conference on Computational Linguistics", "relation": "Publication Venue", "description": "The paper 'Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps' by Xanh Ho was published at the Proceedings of the 28th International Conference on Computational Linguistics in 2020."}, {"entity1": "Gautier Izacard", "entity2": "Transactions on Machine Learning Research", "relation": "Publication Venue", "description": "The paper 'Unsupervised dense information retrieval with contrastive learning' by Gautier Izacard was published at the Transactions on Machine Learning Research."}, {"entity1": "Soyeong Jeong", "entity2": "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics", "relation": "Publication Venue", "description": "The paper 'Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity' by Soyeong Jeong was published at the Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics in 2024."}, {"entity1": "Zhengbao Jiang", "entity2": "arXiv", "relation": "Publication Venue", "description": "The paper 'Active retrieval augmented generation' by Zhengbao Jiang was published at the arXiv in 2023."}, {"entity1": "Jiajie Jin", "entity2": "arXiv", "relation": "Publication Venue", "description": "The paper 'Flashrag: A modular toolkit for efficient retrieval-augmented generation research' by Jiajie Jin was published at the arXiv in 2024."}, {"entity1": "Qiao Jin", "entity2": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing", "relation": "Publication Venue", "description": "The paper 'Pubmedqa: A dataset for biomedical research question answering' by Qiao Jin was published at the Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing in 2019."}, {"entity1": "Mandar Joshi", "entity2": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing", "relation": "Publication Venue", "description": "The paper 'Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension' by Mandar Joshi was published at the Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing in 2017."}], "f899a0f4-457c-4cd2-9b49-b27dd9b0d074": [{"entity1": "Nikola Jovanovi\u0107", "entity2": "Robin Staab", "relation": "Co-author", "description": "Nikola Jovanovi\u0107 and Robin Staab co-authored the paper 'Ward: Provable RAG Dataset Inference via LLM Watermarks'."}, {"entity1": "Nikola Jovanovi\u0107", "entity2": "Maximilian Baader", "relation": "Co-author", "description": "Nikola Jovanovi\u0107 and Maximilian Baader co-authored the paper 'Ward: Provable RAG Dataset Inference via LLM Watermarks'."}, {"entity1": "Nikola Jovanovi\u0107", "entity2": "Martin Vechev", "relation": "Co-author", "description": "Nikola Jovanovi\u0107 and Martin Vechev co-authored the paper 'Ward: Provable RAG Dataset Inference via LLM Watermarks'."}, {"entity1": "Muhammad Kamran", "entity2": "Muddassar Farooq", "relation": "Co-author", "description": "Muhammad Kamran and Muddassar Farooq co-authored the paper 'A comprehensive survey of watermarking relational databases research'."}, {"entity1": "John Kirchenbauer", "entity2": "Jonas Geiping", "relation": "Co-author", "description": "John Kirchenbauer and Jonas Geiping co-authored the paper 'A watermark for large language models'."}, {"entity1": "John Kirchenbauer", "entity2": "Yuxin Wen", "relation": "Co-author", "description": "John Kirchenbauer and Yuxin Wen co-authored the paper 'A watermark for large language models'."}, {"entity1": "John Kirchenbauer", "entity2": "Jonathan Katz", "relation": "Co-author", "description": "John Kirchenbauer and Jonathan Katz co-authored the paper 'A watermark for large language models'."}, {"entity1": "John Kirchenbauer", "entity2": "Ian Miers", "relation": "Co-author", "description": "John Kirchenbauer and Ian Miers co-authored the paper 'A watermark for large language models'."}, {"entity1": "John Kirchenbauer", "entity2": "Tom Goldstein", "relation": "Co-author", "description": "John Kirchenbauer and Tom Goldstein co-authored the paper 'A watermark for large language models'."}, {"entity1": "John Kirchenbauer", "entity2": "Manli Shu", "relation": "Co-author", "description": "John Kirchenbauer and Manli Shu co-authored the paper 'On the reliability of watermarks for large language models'."}, {"entity1": "John Kirchenbauer", "entity2": "Khalid Saifullah", "relation": "Co-author", "description": "John Kirchenbauer and Khalid Saifullah co-authored the paper 'On the reliability of watermarks for large language models'."}, {"entity1": "John Kirchenbauer", "entity2": "Kezhi Kong", "relation": "Co-author", "description": "John Kirchenbauer and Kezhi Kong co-authored the paper 'On the reliability of watermarks for large language models'."}, {"entity1": "John Kirchenbauer", "entity2": "Kasun Fernando", "relation": "Co-author", "description": "John Kirchenbauer and Kasun Fernando co-authored the paper 'On the reliability of watermarks for large language models'."}, {"entity1": "John Kirchenbauer", "entity2": "Aniruddha Saha", "relation": "Co-author", "description": "John Kirchenbauer and Aniruddha Saha co-authored the paper 'On the reliability of watermarks for large language models'."}, {"entity1": "John Kirchenbauer", "entity2": "Micah Goldblum", "relation": "Co-author", "description": "John Kirchenbauer and Micah Goldblum co-authored the paper 'On the reliability of watermarks for large language models'."}, {"entity1": "John Kirchenbauer", "entity2": "Tom Goldstein", "relation": "Co-author", "description": "John Kirchenbauer and Tom Goldstein co-authored the paper 'On the reliability of watermarks for large language models'."}, {"entity1": "Kalpesh Krishna", "entity2": "Yixiao Song", "relation": "Co-author", "description": "Kalpesh Krishna and Yixiao Song co-authored the paper 'Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense'."}, {"entity1": "Kalpesh Krishna", "entity2": "Marzena Karpinska", "relation": "Co-author", "description": "Kalpesh Krishna and Marzena Karpinska co-authored the paper 'Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense'."}, {"entity1": "Kalpesh Krishna", "entity2": "John Wieting", "relation": "Co-author", "description": "Kalpesh Krishna and John Wieting co-authored the paper 'Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense'."}, {"entity1": "Kalpesh Krishna", "entity2": "Mohit Iyyer", "relation": "Co-author", "description": "Kalpesh Krishna and Mohit Iyyer co-authored the paper 'Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense'."}, {"entity1": "Tom Kwiatkowski", "entity2": "Jennimaria Palomaki", "relation": "Co-author", "description": "Tom Kwiatkowski and Jennimaria Palomaki co-authored the paper 'Natural questions: a benchmark for question answering research'."}, {"entity1": "Tom Kwiatkowski", "entity2": "Olivia Redfield", "relation": "Co-author", "description": "Tom Kwiatkowski and Olivia Redfield co-authored the paper 'Natural questions: a benchmark for question answering research'."}, {"entity1": "Tom Kwiatkowski", "entity2": "Michael Collins", "relation": "Co-author", "description": "Tom Kwiatkowski and Michael Collins co-authored the paper 'Natural questions: a benchmark for question answering research'."}, {"entity1": "Tom Kwiatkowski", "entity2": "Ankur Parikh", "relation": "Co-author", "description": "Tom Kwiatkowski and Ankur Parikh co-authored the paper 'Natural questions: a benchmark for question answering research'."}, {"entity1": "Tom Kwiatkowski", "entity2": "Chris Alberti", "relation": "Co-author", "description": "Tom Kwiatkowski and Chris Alberti co-authored the paper 'Natural questions: a benchmark for question answering research'."}, {"entity1": "Tom Kwiatkowski", "entity2": "Danielle Epstein", "relation": "Co-author", "description": "Tom Kwiatkowski and Danielle Epstein co-authored the paper 'Natural questions: a benchmark for question answering research'."}, {"entity1": "Tom Kwiatkowski", "entity2": "Illia Polosukhin", "relation": "Co-author", "description": "Tom Kwiatkowski and Illia Polosukhin co-authored the paper 'Natural questions: a benchmark for question answering research'."}, {"entity1": "Tom Kwiatkowski", "entity2": "Jacob Devlin", "relation": "Co-author", "description": "Tom Kwiatkowski and Jacob Devlin co-authored the paper 'Natural questions: a benchmark for question answering research'."}, {"entity1": "Tom Kwiatkowski", "entity2": "Kenton Lee", "relation": "Co-author", "description": "Tom Kwiatkowski and Kenton Lee co-authored the paper 'Natural questions: a benchmark for question answering research'."}, {"entity1": "Douglas B Lenat", "entity2": "CYC", "relation": "Author-Expertise", "description": "Douglas B Lenat is an expert in the field of CYC, a large-scale investment in knowledge infrastructure."}, {"entity1": "Mingchen Li", "entity2": "Halil Kilicoglu", "relation": "Co-author", "description": "Mingchen Li and Halil Kilicoglu co-authored the paper 'Biomedrag: A retrieval augmented large language model for biomedicine'."}, {"entity1": "Mingchen Li", "entity2": "Hua Xu", "relation": "Co-author", "description": "Mingchen Li and Hua Xu co-authored the paper 'Biomedrag: A retrieval augmented large language model for biomedicine'."}, {"entity1": "Mingchen Li", "entity2": "Rui Zhang", "relation": "Co-author", "description": "Mingchen Li and Rui Zhang co-authored the paper 'Biomedrag: A retrieval augmented large language model for biomedicine'."}, {"entity1": "Nikola Jovanovi\u0107", "entity2": "arXiv", "relation": "Publication Venue", "description": "Nikola Jovanovi\u0107 published the paper 'Ward: Provable RAG Dataset Inference via LLM Watermarks' on arXiv."}, {"entity1": "Muhammad Kamran", "entity2": "arXiv", "relation": "Publication Venue", "description": "Muhammad Kamran published the paper 'A comprehensive survey of watermarking relational databases research' on arXiv."}, {"entity1": "John Kirchenbauer", "entity2": "International Conference on Machine Learning", "relation": "Publication Venue", "description": "John Kirchenbauer published the paper 'A watermark for large language models' at the International Conference on Machine Learning."}, {"entity1": "John Kirchenbauer", "entity2": "PMLR", "relation": "Publication Venue", "description": "John Kirchenbauer published the paper 'A watermark for large language models' in PMLR."}, {"entity1": "John Kirchenbauer", "entity2": "arXiv", "relation": "Publication Venue", "description": "John Kirchenbauer published the paper 'On the reliability of watermarks for large language models' on arXiv."}, {"entity1": "Kalpesh Krishna", "entity2": "Advances in Neural Information Processing Systems", "relation": "Publication Venue", "description": "Kalpesh Krishna published the paper 'Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense' in Advances in Neural Information Processing Systems."}, {"entity1": "Tom Kwiatkowski", "entity2": "Transactions of the Association for Computational Linguistics", "relation": "Publication Venue", "description": "Tom Kwiatkowski published the paper 'Natural questions: a benchmark for question answering research' in Transactions of the Association for Computational Linguistics."}, {"entity1": "Douglas B Lenat", "entity2": "Communications of the ACM", "relation": "Publication Venue", "description": "Douglas B Lenat published the paper 'CYC: A large-scale investment in knowledge infrastructure' in Communications of the ACM."}, {"entity1": "Mingchen Li", "entity2": "arXiv", "relation": "Publication Venue", "description": "Mingchen Li published the paper 'Biomedrag: A retrieval augmented large language model for biomedicine' on arXiv."}], "5a3e847e-c4c4-4ebc-9464-6e9075a6064d": [{"entity1": "Shamane Siriwardhana", "entity2": "Rivindu Weerasekera", "relation": "Co-author", "description": "Shamane Siriwardhana and Rivindu Weerasekera are co-authors of the paper 'Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering'."}, {"entity1": "Umut Topkara", "entity2": "Mercan Topkara", "relation": "Co-author", "description": "Umut Topkara and Mercan Topkara are co-authors of the paper 'The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions'."}, {"entity1": "Hugo Touvron", "entity2": "Louis Martin", "relation": "Co-author", "description": "Hugo Touvron and Louis Martin are co-authors of the paper 'Llama 2: Open foundation and fine-tuned chat models'."}, {"entity1": "Harsh Trivedi", "entity2": "Niranjan Balasubramanian", "relation": "Co-author", "description": "Harsh Trivedi and Niranjan Balasubramanian are co-authors of the paper 'Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions'."}, {"entity1": "Meng-Hsiun Tsai", "entity2": "Fang-Yu Hsu", "relation": "Co-author", "description": "Meng-Hsiun Tsai and Fang-Yu Hsu are co-authors of the paper 'Fragile database watermarking for malicious tamper detection using support vector regression'."}, {"entity1": "Ellen Voorhees", "entity2": "Tasmeer Alam", "relation": "Co-author", "description": "Ellen Voorhees and Tasmeer Alam are co-authors of the paper 'TREC-COVID: constructing a pandemic information retrieval test collection'."}, {"entity1": "Calvin Wang", "entity2": "Joshua Ong", "relation": "Co-author", "description": "Calvin Wang and Joshua Ong are co-authors of the paper 'Potential for GPT technology to optimize future clinical decision-making using retrieval-augmented generation'."}, {"entity1": "Hongru Wang", "entity2": "Wenyu Huang", "relation": "Co-author", "description": "Hongru Wang and Wenyu Huang are co-authors of the paper 'Unims-rag: A unified multi-source retrieval-augmented generation for personalized dialogue systems'."}, {"entity1": "Yihan Wu", "entity2": "Zhengmian Hu", "relation": "Co-author", "description": "Yihan Wu and Zhengmian Hu are co-authors of the paper 'Dipmark'."}, {"entity1": "Shamane Siriwardhana", "entity2": "Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering", "relation": "Author-Role", "description": "Shamane Siriwardhana is an author of the paper 'Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering'."}, {"entity1": "Umut Topkara", "entity2": "The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions", "relation": "Author-Role", "description": "Umut Topkara is an author of the paper 'The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions'."}, {"entity1": "Hugo Touvron", "entity2": "Llama 2: Open foundation and fine-tuned chat models", "relation": "Author-Role", "description": "Hugo Touvron is an author of the paper 'Llama 2: Open foundation and fine-tuned chat models'."}, {"entity1": "Harsh Trivedi", "entity2": "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions", "relation": "Author-Role", "description": "Harsh Trivedi is an author of the paper 'Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions'."}, {"entity1": "Meng-Hsiun Tsai", "entity2": "Fragile database watermarking for malicious tamper detection using support vector regression", "relation": "Author-Role", "description": "Meng-Hsiun Tsai is an author of the paper 'Fragile database watermarking for malicious tamper detection using support vector regression'."}, {"entity1": "Ellen Voorhees", "entity2": "TREC-COVID: constructing a pandemic information retrieval test collection", "relation": "Author-Role", "description": "Ellen Voorhees is an author of the paper 'TREC-COVID: constructing a pandemic information retrieval test collection'."}, {"entity1": "Calvin Wang", "entity2": "Potential for GPT technology to optimize future clinical decision-making using retrieval-augmented generation", "relation": "Author-Role", "description": "Calvin Wang is an author of the paper 'Potential for GPT technology to optimize future clinical decision-making using retrieval-augmented generation'."}, {"entity1": "Hongru Wang", "entity2": "Unims-rag: A unified multi-source retrieval-augmented generation for personalized dialogue systems", "relation": "Author-Role", "description": "Hongru Wang is an author of the paper 'Unims-rag: A unified multi-source retrieval-augmented generation for personalized dialogue systems'."}, {"entity1": "Yihan Wu", "entity2": "Dipmark", "relation": "Author-Role", "description": "Yihan Wu is an author of the paper 'Dipmark'."}, {"entity1": "Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering", "entity2": "Transactions of the Association for Computational Linguistics", "relation": "Publication Venue", "description": "The paper 'Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering' was published in Transactions of the Association for Computational Linguistics."}, {"entity1": "The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions", "entity2": "Proceedings of the 8th workshop on Multimedia and security", "relation": "Publication Venue", "description": "The paper 'The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions' was published in Proceedings of the 8th workshop on Multimedia and security."}, {"entity1": "Llama 2: Open foundation and fine-tuned chat models", "entity2": "arXiv", "relation": "Publication Venue", "description": "The paper 'Llama 2: Open foundation and fine-tuned chat models' was published in arXiv."}, {"entity1": "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions", "entity2": "arXiv", "relation": "Publication Venue", "description": "The paper 'Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions' was published in arXiv."}, {"entity1": "Fragile database watermarking for malicious tamper detection using support vector regression", "entity2": "Third International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP 2007)", "relation": "Publication Venue", "description": "The paper 'Fragile database watermarking for malicious tamper detection using support vector regression' was published in Third International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP 2007)."}, {"entity1": "TREC-COVID: constructing a pandemic information retrieval test collection", "entity2": "ACM SIGIR Forum", "relation": "Publication Venue", "description": "The paper 'TREC-COVID: constructing a pandemic information retrieval test collection' was published in ACM SIGIR Forum."}, {"entity1": "Potential for GPT technology to optimize future clinical decision-making using retrieval-augmented generation", "entity2": "Annals of Biomedical Engineering", "relation": "Publication Venue", "description": "The paper 'Potential for GPT technology to optimize future clinical decision-making using retrieval-augmented generation' was published in Annals of Biomedical Engineering."}, {"entity1": "Unims-rag: A unified multi-source retrieval-augmented generation for personalized dialogue systems", "entity2": "arXiv", "relation": "Publication Venue", "description": "The paper 'Unims-rag: A unified multi-source retrieval-augmented generation for personalized dialogue systems' was published in arXiv."}, {"entity1": "Dipmark", "entity2": "arXiv", "relation": "Publication Venue", "description": "The paper 'Dipmark' was published in arXiv."}, {"entity1": "Shamane Siriwardhana", "entity2": "Association for Computational Linguistics", "relation": "Affiliation", "description": "Shamane Siriwardhana is affiliated with Association for Computational Linguistics."}, {"entity1": "Umut Topkara", "entity2": "ACM", "relation": "Affiliation", "description": "Umut Topkara is affiliated with ACM."}, {"entity1": "Hugo Touvron", "entity2": "arXiv", "relation": "Affiliation", "description": "Hugo Touvron is affiliated with arXiv."}, {"entity1": "Harsh Trivedi", "entity2": "arXiv", "relation": "Affiliation", "description": "Harsh Trivedi is affiliated with arXiv."}, {"entity1": "Meng-Hsiun Tsai", "entity2": "IEEE", "relation": "Affiliation", "description": "Meng-Hsiun Tsai is affiliated with IEEE."}, {"entity1": "Ellen Voorhees", "entity2": "ACM", "relation": "Affiliation", "description": "Ellen Voorhees is affiliated with ACM."}, {"entity1": "Calvin Wang", "entity2": "Annals of Biomedical Engineering", "relation": "Affiliation", "description": "Calvin Wang is affiliated with Annals of Biomedical Engineering."}, {"entity1": "Hongru Wang", "entity2": "arXiv", "relation": "Affiliation", "description": "Hongru Wang is affiliated with arXiv."}, {"entity1": "Yihan Wu", "entity2": "arXiv", "relation": "Affiliation", "description": "Yihan Wu is affiliated with arXiv."}], "0e804773-ecce-456a-b407-1d11045a9150": [{"entity1": "Yihan Wu", "entity2": "Dipmark", "relation": "Author-Role", "description": "Yihan Wu is an author of the paper introducing Dipmark, a stealthy, efficient, and resilient watermark for large language models."}, {"entity1": "arXiv:2310.07710", "entity2": "Dipmark", "relation": "Publication Venue", "description": "The paper introducing Dipmark is published on arXiv with the identifier arXiv:2310.07710."}, {"entity1": "YAGO", "entity2": "YAGO Knowledge", "relation": "Affiliation", "description": "YAGO is affiliated with YAGO Knowledge, as indicated by the provided URL https://yago-knowledge.org/."}, {"entity1": "Shi-Qi Yan", "entity2": "arXiv:2401.15884", "relation": "Author-Role", "description": "Shi-Qi Yan is an author of the paper published on arXiv with the identifier arXiv:2401.15884, titled Corrective retrieval augmented generation."}, {"entity1": "Xi Yang", "entity2": "arXiv:2305.08883", "relation": "Author-Role", "description": "Xi Yang is an author of the paper published on arXiv with the identifier arXiv:2305.08883, titled Watermarking text generated by black-box language models."}, {"entity1": "Zhilin Yang", "entity2": "HotpotQA", "relation": "Author-Role", "description": "Zhilin Yang is an author of the paper introducing HotpotQA, a dataset for diverse, explainable multi-hop question answering."}, {"entity1": "Hanlin Zhang", "entity2": "arXiv:2311.04378", "relation": "Author-Role", "description": "Hanlin Zhang is an author of the paper published on arXiv with the identifier arXiv:2311.04378, titled Watermarks in the sand: Impossibility of strong watermarking for generative models."}, {"entity1": "Ruisi Zhang", "entity2": "REMARK-LLM", "relation": "Author-Role", "description": "Ruisi Zhang is an author of the paper introducing REMARK-LLM, a robust and efficient watermarking framework for generative large language models."}, {"entity1": "Yue Zhang", "entity2": "arXiv:2309.01219", "relation": "Author-Role", "description": "Yue Zhang is an author of the paper published on arXiv with the identifier arXiv:2309.01219, titled Siren\u2019s song in the AI ocean: a survey on hallucination in large language models."}, {"entity1": "Zhi-hao Zhang", "entity2": "IEEE", "relation": "Publication Venue", "description": "The paper by Zhi-hao Zhang is published in the proceedings of the 2004 International Conference on Machine Learning and Cybernetics, which is affiliated with IEEE."}, {"entity1": "Huaqin Zhao", "entity2": "arXiv:2401.11641", "relation": "Author-Role", "description": "Huaqin Zhao is an author of the paper published on arXiv with the identifier arXiv:2401.11641, titled Revolutionizing finance with llms: An overview of applications and insights."}, {"entity1": "Yue Zhang", "entity2": "Yafu Li", "relation": "Co-author", "description": "Yue Zhang and Yafu Li are co-authors of the paper published on arXiv with the identifier arXiv:2309.01219."}, {"entity1": "Ruisi Zhang", "entity2": "USENIX Security Symposium", "relation": "Publication Venue", "description": "The paper by Ruisi Zhang is published in the proceedings of the 33rd USENIX Security Symposium."}, {"entity1": "YAGO", "entity2": "Statistical Hypothesis Test", "relation": "Dataset-Origin", "description": "YAGO and Statistical Hypothesis Test are mentioned together in the context of Wikipedia, indicating a potential relationship between the dataset and the statistical concept."}, {"entity1": "arXiv:2401.13256", "entity2": "arXiv", "relation": "Publication Venue", "description": "The paper with the identifier arXiv:2401.13256 is published on arXiv."}, {"entity1": "Wikipedia", "entity2": "Statistical Hypothesis Test", "relation": "Information Source", "description": "Wikipedia is cited as a source of information for the concept of Statistical Hypothesis Test."}], "0826b4e6-4c9b-47d9-9e2b-dabc61f1d35d": [{"entity1": "J. Chen", "entity2": "H. Lin", "relation": "Co-author", "description": "J. Chen and H. Lin are co-authors of the paper 'Benchmarking large language models in retrieval-augmented generation'"}, {"entity1": "Retrieval-Augmented Generation", "entity2": "Large Language Models", "relation": "Research Area", "description": "Retrieval-Augmented Generation is a research area related to Large Language Models"}, {"entity1": "AAAI Conference on Artificial Intelligence", "entity2": "J. Chen", "relation": "Publication Venue", "description": "J. Chen published a paper at the AAAI Conference on Artificial Intelligence"}, {"entity1": "Langchain-AI", "entity2": "Auto Evaluator", "relation": "Tool/Resource", "description": "Langchain-AI developed the Auto Evaluator toolkit"}, {"entity1": "Gaussian Processes", "entity2": "Machine Learning", "relation": "Methodology", "description": "Gaussian Processes is a methodology used in Machine Learning"}, {"entity1": "Edge Computing", "entity2": "Large Language Models", "relation": "Application", "description": "Edge Computing is an application area for Large Language Models"}, {"entity1": "H. Yu", "entity2": "A. Gan", "relation": "Co-author", "description": "H. Yu and A. Gan are co-authors of the paper 'Evaluation of retrieval-augmented generation: A survey'"}, {"entity1": "Grand View Research", "entity2": "Retrieval-Augmented Generation", "relation": "Research Funding-Grant", "description": "Grand View Research provided funding for research on Retrieval-Augmented Generation"}, {"entity1": "ACM SIGIR Conference on Research and Development in Information Retrieval", "entity2": "S. Hofst\u00e4tter", "relation": "Publication Venue", "description": "S. Hofst\u00e4tter published a paper at the ACM SIGIR Conference on Research and Development in Information Retrieval"}, {"entity1": "C. K. Williams", "entity2": "C. E. Rasmussen", "relation": "Co-author", "description": "C. K. Williams and C. E. Rasmussen are co-authors of the book 'Gaussian processes for machine learning'"}, {"entity1": "D. Duvenaud", "entity2": "Gaussian Processes", "relation": "Author-Expertise", "description": "D. Duvenaud is an expert in Gaussian Processes"}, {"entity1": "arXiv", "entity2": "H. Yu", "relation": "Publication Venue", "description": "H. Yu published a paper on arXiv"}, {"entity1": "ACM SIGCOMM 2024 Conference", "entity2": "H. Lim", "relation": "Publication Venue", "description": "H. Lim published a paper at the ACM SIGCOMM 2024 Conference"}, {"entity1": "J. Chen", "entity2": "X. Han", "relation": "Co-author", "description": "J. Chen and X. Han are co-authors of the paper 'Benchmarking large language models in retrieval-augmented generation'"}, {"entity1": "S. Hofst\u00e4tter", "entity2": "J. Chen", "relation": "Co-author", "description": "S. Hofst\u00e4tter and J. Chen are co-authors of the paper 'Fid-light: Efficient and effective retrieval-augmented text generation'"}, {"entity1": "H. Yu", "entity2": "A. Gan", "relation": "Collaboration-Type", "description": "H. Yu and A. Gan collaborated on the paper 'Evaluation of retrieval-augmented generation: A survey'"}, {"entity1": "Gaussian Processes", "entity2": "Machine Learning", "relation": "Theoretical Framework", "description": "Gaussian Processes is a theoretical framework used in Machine Learning"}, {"entity1": "Edge Computing", "entity2": "Retrieval-Augmented Generation", "relation": "Application", "description": "Edge Computing is an application area for Retrieval-Augmented Generation"}, {"entity1": "Langchain-AI", "entity2": "Auto Evaluator", "relation": "Innovation", "description": "Langchain-AI developed the innovative Auto Evaluator toolkit"}, {"entity1": "Grand View Research", "entity2": "Retrieval-Augmented Generation", "relation": "Market Analysis", "description": "Grand View Research provided market analysis for Retrieval-Augmented Generation"}, {"entity1": "ACM SIGIR Conference on Research and Development in Information Retrieval", "entity2": "S. Hofst\u00e4tter", "relation": "Publication Venue", "description": "S. Hofst\u00e4tter published a paper at the ACM SIGIR Conference on Research and Development in Information Retrieval"}, {"entity1": "C. K. Williams", "entity2": "C. E. Rasmussen", "relation": "Co-author", "description": "C. K. Williams and C. E. Rasmussen are co-authors of the book 'Gaussian processes for machine learning'"}, {"entity1": "D. Duvenaud", "entity2": "Gaussian Processes", "relation": "Author-Expertise", "description": "D. Duvenaud is an expert in Gaussian Processes"}, {"entity1": "arXiv", "entity2": "H. Yu", "relation": "Publication Venue", "description": "H. Yu published a paper on arXiv"}, {"entity1": "ACM SIGCOMM 2024 Conference", "entity2": "H. Lim", "relation": "Publication Venue", "description": "H. Lim published a paper at the ACM SIGCOMM 2024 Conference"}], "9e3dd641-4850-49f7-8e09-d06175885a02": [{"entity1": "Yingjiu Li", "entity2": "Huiping Guo", "relation": "Co-author", "description": "Yingjiu Li and Huiping Guo co-authored the paper 'Tamper detection and localization for categorical data using fragile watermarks' in 2004."}, {"entity1": "Yuying Li", "entity2": "Gaoyang Liu", "relation": "Co-author", "description": "Yuying Li and Gaoyang Liu co-authored the paper 'Generating Is Believing: Membership Inference Attacks against Retrieval-Augmented Generation' in 2024."}, {"entity1": "Zhe Lin", "entity2": "Yitao Cai", "relation": "Co-author", "description": "Zhe Lin and Yitao Cai co-authored the paper 'Towards document-level paraphrase generation with sentence rewriting and reordering' in 2021."}, {"entity1": "Peizhuo Lv", "entity2": "Pan Li", "relation": "Co-author", "description": "Peizhuo Lv and Pan Li co-authored the paper 'Ssl-wm: A black-box watermarking approach for encoders pre-trained by self-supervised learning' in 2024."}, {"entity1": "Hasan Mesut Meral", "entity2": "B\u00fclent Sankur", "relation": "Co-author", "description": "Hasan Mesut Meral and B\u00fclent Sankur co-authored the paper 'Natural language watermarking via morphosyntactic alterations' in 2009."}, {"entity1": "Travis Munyer", "entity2": "Abdullah Tanvir", "relation": "Co-author", "description": "Travis Munyer and Abdullah Tanvir co-authored the paper 'DeepTextMark: A Deep Learning-Driven Text Watermarking Approach for Identifying Large Language Model Generated Text' in 2023."}, {"entity1": "Saksham Rastogi", "entity2": "Danish Pruthi", "relation": "Co-author", "description": "Saksham Rastogi and Danish Pruthi co-authored the paper 'Revisiting the Robustness of Watermarking to Paraphrasing Attacks' in 2024."}, {"entity1": "Ryoma Sato", "entity2": "Yuki Takezawa", "relation": "Co-author", "description": "Ryoma Sato and Yuki Takezawa co-authored the paper 'Embarrassingly simple text watermarks' in 2023."}, {"entity1": "Mohamed Shehab", "entity2": "Elisa Bertino", "relation": "Co-author", "description": "Mohamed Shehab and Elisa Bertino co-authored the paper 'Watermarking relational databases using optimization-based techniques' in 2007."}, {"entity1": "Radu Sion", "entity2": "Mikhail Atallah", "relation": "Co-author", "description": "Radu Sion and Mikhail Atallah co-authored the paper 'Rights protection for relational data' in 2003."}, {"entity1": "Yingjiu Li", "entity2": "Proceedings of the 4th ACM workshop on Digital rights management", "relation": "Publication Venue", "description": "Yingjiu Li published the paper 'Tamper detection and localization for categorical data using fragile watermarks' in the Proceedings of the 4th ACM workshop on Digital rights management in 2004."}, {"entity1": "Yuying Li", "entity2": "arXiv preprint arXiv:2406.19234", "relation": "Publication Venue", "description": "Yuying Li published the paper 'Generating Is Believing: Membership Inference Attacks against Retrieval-Augmented Generation' in the arXiv preprint arXiv:2406.19234 in 2024."}, {"entity1": "Zhe Lin", "entity2": "arXiv preprint arXiv:2109.07095", "relation": "Publication Venue", "description": "Zhe Lin published the paper 'Towards document-level paraphrase generation with sentence rewriting and reordering' in the arXiv preprint arXiv:2109.07095 in 2021."}, {"entity1": "Peizhuo Lv", "entity2": "Proceedings of the 2024 Annual Network and Distributed System Security Symposium, NDSS\u201924", "relation": "Publication Venue", "description": "Peizhuo Lv published the paper 'Ssl-wm: A black-box watermarking approach for encoders pre-trained by self-supervised learning' in the Proceedings of the 2024 Annual Network and Distributed System Security Symposium, NDSS\u201924 in 2024."}, {"entity1": "Hasan Mesut Meral", "entity2": "Computer Speech & Language", "relation": "Publication Venue", "description": "Hasan Mesut Meral published the paper 'Natural language watermarking via morphosyntactic alterations' in the Computer Speech & Language in 2009."}, {"entity1": "Travis Munyer", "entity2": "arXiv preprint arXiv:2305.05773", "relation": "Publication Venue", "description": "Travis Munyer published the paper 'DeepTextMark: A Deep Learning-Driven Text Watermarking Approach for Identifying Large Language Model Generated Text' in the arXiv preprint arXiv:2305.05773 in 2023."}, {"entity1": "Saksham Rastogi", "entity2": "arXiv preprint arXiv:2411.05277", "relation": "Publication Venue", "description": "Saksham Rastogi published the paper 'Revisiting the Robustness of Watermarking to Paraphrasing Attacks' in the arXiv preprint arXiv:2411.05277 in 2024."}, {"entity1": "Ryoma Sato", "entity2": "arXiv preprint arXiv:2310.08920", "relation": "Publication Venue", "description": "Ryoma Sato published the paper 'Embarrassingly simple text watermarks' in the arXiv preprint arXiv:2310.08920 in 2023."}, {"entity1": "Mohamed Shehab", "entity2": "IEEE transactions on Knowledge and Data Engineering", "relation": "Publication Venue", "description": "Mohamed Shehab published the paper 'Watermarking relational databases using optimization-based techniques' in the IEEE transactions on Knowledge and Data Engineering in 2007."}, {"entity1": "Radu Sion", "entity2": "Proceedings of the 2003 ACM SIGMOD international conference on Management of data", "relation": "Publication Venue", "description": "Radu Sion published the paper 'Rights protection for relational data' in the Proceedings of the 2003 ACM SIGMOD international conference on Management of data in 2003."}, {"entity1": "Meta", "entity2": "Llama", "relation": "Affiliation", "description": "Meta is affiliated with Llama."}, {"entity1": "Meta", "entity2": "Llama RAG", "relation": "Affiliation", "description": "Meta is affiliated with Llama RAG."}, {"entity1": "Microsoft", "entity2": "Azure", "relation": "Affiliation", "description": "Microsoft is affiliated with Azure."}, {"entity1": "OpenAI", "entity2": "GPT", "relation": "Affiliation", "description": "OpenAI is affiliated with GPT."}], "dbf627bd-e916-445e-8257-d2d494af4d66": [{"entity1": "bge-reranker-large", "entity2": "text-embedding-ada-002", "relation": "Embedding Model Comparison", "description": "bge-reranker-large is compared with text-embedding-ada-002 in terms of MRR@10, MAP@10, Hits@10, and Hits@4 metrics."}, {"entity1": "GPT-4", "entity2": "ChatGPT", "relation": "LLM Comparison", "description": "GPT-4 and ChatGPT are compared in terms of accuracy."}, {"entity1": "Llama-2-70b-chat-hf", "entity2": "Mixtral-8x7B-Instruct", "relation": "LLM Comparison", "description": "Llama-2-70b-chat-hf and Mixtral-8x7B-Instruct are compared in terms of accuracy."}, {"entity1": "GPT-4", "entity2": "Google-PaLM", "relation": "LLM Comparison", "description": "GPT-4 and Google-PaLM are compared in terms of accuracy."}, {"entity1": "bge-reranker-large", "entity2": "llm-embedder", "relation": "Embedding Model Comparison", "description": "bge-reranker-large is compared with llm-embedder in terms of MRR@10, MAP@10, Hits@10, and Hits@4 metrics."}, {"entity1": "bge-large-en-v1.5", "entity2": "jina-embeddings-v2-base-en", "relation": "Embedding Model Comparison", "description": "bge-large-en-v1.5 is compared with jina-embeddings-v2-base-en in terms of MRR@10, MAP@10, Hits@10, and Hits@4 metrics."}, {"entity1": "intfloat/e5-base-v2", "entity2": "voyage-02", "relation": "Embedding Model Comparison", "description": "intfloat/e5-base-v2 is compared with voyage-02 in terms of MRR@10, MAP@10, Hits@10, and Hits@4 metrics."}, {"entity1": "hkunlp/instructor-large", "entity2": "Table 5", "relation": "Embedding Model Evaluation", "description": "hkunlp/instructor-large is evaluated in Table 5 for its retrieval performance."}, {"entity1": "GPT-4", "entity2": "Table 6", "relation": "LLM Evaluation", "description": "GPT-4 is evaluated in Table 6 for its generation accuracy."}, {"entity1": "Mixtral-8x7B-Instruct", "entity2": "Figure 3", "relation": "LLM Evaluation", "description": "Mixtral-8x7B-Instruct is evaluated in Figure 3 for its generation accuracy on different query types."}, {"entity1": "RAG", "entity2": "LLM", "relation": "Methodology", "description": "RAG is used to mitigate LLM hallucination issue by augmenting LLM with retrieval knowledge."}, {"entity1": "GPT-4", "entity2": "Mixtral-8x7B-Instruct", "relation": "Comparison", "description": "GPT-4 and Mixtral-8x7B-Instruct are compared in terms of their performance on different query types."}], "27ff2bbe-5ebf-42f8-b9e0-9bbfe8cf20b7": [{"entity1": "Jon Saad-Falcon", "entity2": "ARES", "relation": "Author-Role", "description": "Author of ARES framework"}, {"entity1": "Ivan Stelmakh", "entity2": "ASQA", "relation": "Author-Role", "description": "Author of ASQA framework"}, {"entity1": "Hugo Touvron", "entity2": "Llama", "relation": "Author-Role", "description": "Author of Llama model"}, {"entity1": "Xiaohua Wang", "entity2": "Retrieval-Augmented Generation", "relation": "Research Area", "description": "Researcher in Retrieval-Augmented Generation"}, {"entity1": "Dingjun Wu", "entity2": "Association for Computational Linguistics", "relation": "Affiliation", "description": "Affiliated with ACL"}, {"entity1": "Hao Yu", "entity2": "Retrieval-Augmented Generation", "relation": "Research Area", "description": "Researcher in Retrieval-Augmented Generation"}, {"entity1": "Yue Yu", "entity2": "RankRAG", "relation": "Author-Role", "description": "Author of RankRAG model"}, {"entity1": "Tianjun Zhang", "entity2": "Raft", "relation": "Author-Role", "description": "Author of Raft model"}, {"entity1": "Tianyi Zhang", "entity2": "BERTScore", "relation": "Author-Role", "description": "Author of BERTScore model"}, {"entity1": "Matei Zaharia", "entity2": "ARES", "relation": "Author-Role", "description": "Author of ARES framework"}, {"entity1": "Matei Zaharia", "entity2": "Raft", "relation": "Author-Role", "description": "Author of Raft model"}, {"entity1": "arXiv", "entity2": "ARES", "relation": "Publication Venue", "description": "ARES published on arXiv"}, {"entity1": "arXiv", "entity2": "ASQA", "relation": "Publication Venue", "description": "ASQA published on arXiv"}, {"entity1": "arXiv", "entity2": "Llama", "relation": "Publication Venue", "description": "Llama published on arXiv"}, {"entity1": "Association for Computational Linguistics", "entity2": "ASQA", "relation": "Publication Venue", "description": "ASQA published at ACL"}, {"entity1": "Association for Computational Linguistics", "entity2": "Dingjun Wu", "relation": "Affiliation", "description": "Dingjun Wu affiliated with ACL"}, {"entity1": "Abu Dhabi", "entity2": "Ivan Stelmakh", "relation": "Location", "description": "Ivan Stelmakh presented at Abu Dhabi"}, {"entity1": "Toronto", "entity2": "Dingjun Wu", "relation": "Location", "description": "Dingjun Wu presented at Toronto"}, {"entity1": "United Arab Emirates", "entity2": "Ivan Stelmakh", "relation": "Location", "description": "Ivan Stelmakh presented in United Arab Emirates"}, {"entity1": "Canada", "entity2": "Dingjun Wu", "relation": "Location", "description": "Dingjun Wu presented in Canada"}]}